connectivity versus entropy abu mostafa california institute technology ca abstract does connectivity neural network number synapses per neuron complexity problems entropy theory would suggest relation boolean functions implemented using circuit low connectivity eg using two input gates network problem examples using local learning rule prove entropy problem becomes lower bound connectivity network introduction most feature neural networks their learn function training samples ie their program themselves clearly given neural network cannot learn function networks learn functions independent learning network enough circuit complexity function fact network expected learn function rather than being designed function paper result lower bound connectivity network number synapses per neuron lower bound learning theory provides designed circuits low connectivity eg using two input gates boolean function follows learning mechanism lower bound mechanism american institute physics designed find low connectivity circuits perhaps hence lower bound connectivity cannot general learning mechanism local training sample into network each neuron has access those bits itself neurons connected assumption learning mechanisms used neural network models but may more plausible biological point view lower bound connectivity network given terms entropy environment provides training samples entropy measure environment amount information environment many different ways define entropy many technical next section shall results but here involved environment our model patterns represented bits visual different patterns generated given environment entropy essentially assumed about patterns environment likely generate them learning process number sample patterns generated random environment input network bit per neuron network uses information set its internal parameters itself particular environment because network architecture each neuron its bit best bits neurons connected hence learning rules local neuron does have global pattern being learned after learning process has taken each neuron perform function has interaction functions neurons what overall function network main result paper connectivity network less than entropy environment network cannot learn about environment proof show connectivity small final function each neuron independent environment hence overall network has information about environment learn about result neural network graph neurons edges synapses neurons define set neurons connected neuron neuron itself environment subset each sample environment during learning bits into neurons respectively consider neuron become thus neuron first each our result asymptotic function ao ao result consider environments ii probability distribution uniform likely occur neuron each generated environment each define function function relative frequency each binary vectors through vectors other words seen neuron clearly va va corresponding two environments have two functions neuron cannot difference between ea between range dt dt corresponds complete while corresponds maximum axe now position state main result let selected environments uniform probability distribution now random variable interested expected value case corresponds neuron information about environment while case corresponds neuron maximum information theorem these depending connectivity ao entropy theorem ao co proof given but corresponding part theorem most environments first bits through possible values times each goes through possible values once therefore patterns seen neuron fixed binary vectors length essentially uniform probability distribution ie same most environments neuron up same function environment hand what about case corresponding part theorem now patterns environment first bits assume most values out possible values binary vector length assume values assumed particular environment hand ie does environment therefore although neuron does have global information has about environment work supported research under prove main theorem basic properties about environments probability distribution uniform have generating elements uniform probability without follows while so functions defined bit vectors random variable fixed independent follows respect each bit same expected value into cells hence ea now prove theorem theorem co ao proof follows denote respectively last step follows fact na independent therefore prove theorem large assume ao let denote consider prn zero bits environment hence first term probability second term probability given so hence prn prn prn en en therefore jl jl jl prn prn follows terms zero term both zero but term zero en prn estimate expression get ed ao assumption lower bound goes goes upper bound hence upper bound expected value ed assume ao consider en estimate use fact va en en en need estimate en otherwise en diagonal terms such diagonal terms hence total contribution sum off diagonal terms such off diagonal terms hence total contribution nn sum diagonal off diagonal terms get en en last step follows much smaller than therefore estimate expression get assumption upper bound goes goes lower bound hence lower bound expected value references abu mostafa neural networks conference neural networks computing ed pp theory abu mostafa complexity information trans information theory vol pp abu mostafa complexity neural analog neural systems
learning general network electrical engineering california institute technology ca abstract paper method general network feedback connections network model considered consists interconnected groups neurons each group could interconnected could have feedback connections weights but between groups algorithm applied under certain constraint each group weight matrix network unique equilibrium state every input introduction has been shown last few large networks interconnected neuron like elements quite variety computational pattern recognition tasks well known neural network models model way layered feedforward network set given examples neural network models having feedback connections other hand have been example hopfield network shown quite computational tasks important though have method learning examples feedback network general way design thus using design method each different computational feedback expected computational given network because feedback networks state stable state thus processing performed several steps general allows more processing than single step feedforward case note fact feedforward network case feedback network therefore work consider problem general learning algorithm feedback networks learning algorithm feedback networks has attention following see fig example feedback network state network time goes equilibrium other types behavior such could occur interested having steady fixed output every input applied network therefore have following two important requirements network beginning initial condition state should equilibrium other have have unique american institute physics equilibrium state fact equilibrium state final output objective learning algorithm parameters weights network small steps so unique equilibrium state way result finally output close possible required each given input more than equilibrium state given input following problems iterations might updating weights so equilibrium states direction while other iterations different input examples different equilibrium state important point network after learning fixed input more than possible output other work recently training feedback network learning algorithms developed but problem unique equilibrium considered problem paper appropriate network learning algorithm proposed outputs fig network feedback network consider group neurons could connected see fig example weight matrix hopfield network inputs weighted before into network let weight matrix let input output vectors respectively our model following set proposed hopfield bounded function positive constant given input would like network after short give steady fixed output what initial network state beginning initial condition state towards unique equilibrium condition matrix theorem network symmetric other behavior unique equilibrium given input proof let ut ut two solutions let jt ii respect time ut dt dt using expression becomes dt ut ut fut using obtain ut ut fut ii dt again fut fut th row using mean value theorem get fut ii using expression jt get dt theorem positive both results obtain jt fact non negative follows jt goes zero therefore two solutions corresponding two initial conditions approach each other show asymptotic solution fact equilibrium simply ut ut constant above jt hence proof example function following used form sum weights should less than note function does have effect overall results have work our updating scheme constraint given theorem many cases large network necessary constraint might therefore general network next section general network following network example refer fig neurons into several groups within each group connections therefore group could interconnected ie could have feedback connections groups connected each other but way inputs network connected inputs groups each input have several connections several groups outputs network taken outputs part outputs certain group group constraint given theorem applied each group weight matrix let vector pairs function implemented would like minimize sum error given output vector group input vector learning process performed input examples network each time weights minimize outputs fig example general network each group represents network now consider single group let group weight matrix group matrix weights between outputs group inputs group yt output vector group let elements let number neurons group assume time constant sufficiently small so allow network equilibrium state given solution equation yt set groups whose outputs connected inputs group would like each update weight matrices so equilibrium direction error need therefore change error small change weight matrices let oe denote oe oe matrices whose th element respectively let column vector whose th element obtain following relations diagonal matrix whose diagonal element refer vector group vectors groups whose connected outputs group get refer va matrix wt group never so problem updating process prove let vector element obtain both get condition follows cannot zero vector thus matrix ai cannot each updating weights group group simply final outputs group groups connected group obtain their corresponding vectors using update weights same complete updating groups updating weights performed using following algorithm each group ea ea noise matrix whose elements axe independent zero mean parameters purpose noise allow local minima them note control parameter taken ea hence added noise more approach zero error solution sense because large error ie solution more noise weight matrices order local minima other hand error small global minimum solution hence do want much noise order out note once zero error solution added noise well become zero hence weight matrices become zero certain constraint constant its elements so back surface implementation example pattern recognition example considered fig shows set two dimensional training patterns three classes required design neural network three output neurons each neurons should sample corresponding class presented off otherwise ie would like design take network three neuron feedback network implemented obtained error same feedforward single layer network three neurons obtained error results feedforward network should two layer neuron first layer three second layer error finally two neurons first layer three second layer feedback case error fig pattern recognition example way method feedback networks has been proposed condition weight matrix obtained having fixed point so having more than possible output fixed input general structure networks presented network consists number feedback groups connected each other feedforward rule used update weights method applied pattern recognition example single layer feedback network obtained results other hand feedforward method achieved case more than layer hence larger number neurons than feedback case would like abu mostafa useful work supported research under otherwise dimensional vector whose component given otherwise rule put form column wt finally obtain required expression obtained respect get dimensional vector whose component given otherwise similar case results following required expression now finally consider oe let matrix whose element elements obtained equation fixed point group follows hence using rule into previous equation complete references new analysis university learning logic mit report center computational research science learning scheme threshold network june rumelhart internal representations error propagation rumelhart research group parallel distributed processing vol mit press hopfield neurons graded response have computational properties those two state neurons proc natl acad may learning rule perceptrons feedback environment proc first conf neural networks san diego june training time neural networks proc first conf neural networks san diego june back propagation neural networks vol
properties networks neuron like elements abstract complexity computational capacity multi layered feedforward neural networks neural networks purpose structured functions circuit complexity known results complexity theory applied instance neural network circuits particular classes functions implemented shallow circuits about learning complexity problems problem computational capacity class multi layered networks dynamics algebraic considered results presented storage programmed higher order structures between programming capacity shown made static fixed point structure random higher order phase shown introduction consider two computation neural networks consider problem complexity network required compute classes specified structured functions give basic known complexity neural network models but less circuit complexity computational thesis shallow circuits ie networks relatively few layers more hence structured random problems computed shallow constant depth circuits relatively few number polynomial elements demonstrate classes structured problems such low cost solutions problem complexity learning close problems observed limitations theoretical approach next turn classification much network given structure do ie computational capacity given construct university california san diego ca school electrical engineering university american institute physics sense problem considered above design structure perform given analysis higher order neural structures obtained polynomial threshold rules demonstrate these higher order networks class layered neural network present results storage these specifically case interactions demonstrate storage capacity order interaction order case random interactions type phase observed distribution fixed points function attraction depth complexity exist two classes constraints computations physical constraints these related hardware computation include time energy limitations relations space constraints these further into constraints instance exist problems ie functions such problem computable sense complexity constraints upper lower bounds amount such time number gates required compute given function instance exists exponential time algorithm problem provides computational upper bound view computational process nature may have been several times problems related physical perhaps degree constraints computations case complexity theory sense could our parallel computations both natural neural systems simple theory parallel processing level elements processors developed based ratio time between processors different classes problems different processor architecture approach does work parallel processing level circuits recent neural connectionist models based common structure interconnected networks linear polynomial threshold input output function units weights shall therefore complexity theory such circuits so similar theory based boolean gates rather technical easily found references consider circuit being graph boolean inputs boolean output graph correspond gates input units hidden units output unit circuit size circuit total number gates depth length input output layered forward circuit width average number computational units hidden layers elements first boolean threshold logic they sense boolean function implemented using either logic fact such function computed circuit depth two exponential size simple show fraction functions circuit exponential size approaches both cases ie random function general require exponential size circuit difficult construct functions prove exponential circuit necessary yet threshold logic more than boolean logic boolean gate compute function threshold gate compute order functions weights see lower bound upper bound classical see instance would hence appear plausible exist wide classes problems computed threshold logic circuits smaller than those required boolean logic important result threshold boolean logic point view has been demonstrated see proof result order compute function such circuit constant depth least boolean gates required shall demonstrate circuit depth two linear size sufficient computation such functions using threshold logic about between depth width circuit main complexity analysis show sense fact exists shallow ie constant depth circuits multiple reasons general fixed size number different functions computable circuit small depth number those computable circuit function computed given hidden units optimal would circuit depth two rn units single layer addition view computations feedforward inputs output unit shallow circuits compute circuit more difficult become time delays computations finally should given overall responses few given known time synaptic biological shallow least within data relative neurons their shallow circuit architecture taken analog factor entropy factor necessary high connectivity requirements neural systems previous analysis important class circuits threshold logic polynomial size shallow depth have seen general random function cannot computed such circuits many functions structured problems random natural what class functions computable such circuits while complete difficult several sub classes functions known computable shallow size circuits symmetric functions ie functions under input variables important class structured problems implemented shallow polynomial size circuits fact symmetric function computed threshold circuit depth two linear size hidden units output unit always sufficient demonstrate following consider binary inputs each values threshold gates units now possible inputs elements each row being each other ie row have same number zero given symmetric boolean function clearly assumes same value elements boolean row so function assumes value form axe most case function symmetric function now computed threshold gates single hidden layer neuron being activated number input number lower edge neuron being activated number input number upper edge input within number hidden neurons activated hidden layer hence single output unit compute given symmetric function see operations binary performed small depth circuits has shown fixed degree function such functions small shallow threshold circuits finally many interested value function small ie polynomial fraction total number possible inputs these functions implemented shallow circuits size circuit inputs so have been complexity threshold circuits now turn complexity ie problem weights required given function consider problem points two using so region points axe fixed problem polynomial time either goes problem becomes np complete result difficult see general learning problem np complete see different proof proof fact np complete case single threshold gate limitations complexity approach while variety structured boolean functions implemented relatively low cost networks linear threshold gates neurons different input output functions always even networks relatively simple boolean such linear threshold gate many relatively bounds axe computational cost complexity time single most important these threshold units their axe addition non biological connectionist models finally complexity results often asymptotic nature may range corresponding particular shall section few problem has do time learn often seen process both artificial models cf back propagation instance biological systems cf complex complexity theory order over wide variety single learning algorithm should polynomial time therefore what examples polynomial time polynomial size shallow threshold circuits back propagation type algorithms respect clear many tasks easily biological computer program has been found so learning algorithms ie whether exists complexity class problems functions program found learning examples programming such out valiant seen sense learning way function ie functions constructed polynomial time but cannot examples polynomial time suggests learning algorithms may have limitations addition most artificial applications seen so obtained through learning do best known though may many other reasons even such complexity class does exist learning algorithm may important because their work valiant polynomial time learning boolean distribution model additional limitations what learned examples without including additional learning may therefore turn out but algorithms need sub more global structure may find should evolution exponential time learning process polynomial time type learning capacity previous section our structure cost networks would compute specified boolean functions now consider what computational capacity threshold network given structure complexity out general networks capacity results shallow but perhaps circuits shall specified class higher order networks problems associative memory results here present involved consider systems threshold units each state corresponds system system neural states state space hence set our attention symmetric interaction systems between threshold elements let set clearly subset every state set definition homogeneous algebraic threshold network degree network threshold elements interactions specified set real coefficients evolution rule these systems readily seen natural case linear threshold networks added degrees interaction coefficients result programming over linear case has been several recently note each corresponding inputs our earlier computed hidden units layer single threshold unit thus higher order network network depth three first hidden layer has units second hidden layer has units output units feedback into input units note weights input first hidden layer first hidden layer second fixed computing various products weights second hidden layer output coefficients parameters these systems identified either long range interactions spin zero higher order neural networks state system sequence single spin field dynamics these symmetric higher order systems linear system higher order classical quadratic define homogeneous algebraic degree algebraic classical quadratic has been demonstrated functional non under evolution rule spin state these higher order networks seen following essentially zero dynamics because algebraic given equation under evolution rule system always stable state fixed point relation each neural states fixed points hence system dynamics computational capacity system system applications different depending whether interactions random programmed case random interactions itself natural spin glass while programmed interactions applications higher order neural network models consider two cases turn programmed interactions here whether given sets binary vectors stored fixed points selection interaction coefficients such sets vectors stored stable states interaction coefficients chosen vectors energy state space each vector region attraction around physical associative memory such evolution network terms computations error associative memory here maximum number states stored fixed points appropriate algebraic threshold network represents information storage capacity such higher order neural networks let represent degree algebraic threshold network let set vectors require fixed points algebraic threshold network refer these vectors memories define storage capacity algebraic threshold network degree number chosen memories stored high probability appropriate coefficients network theorem algorithm independent storage capacity homogeneous algebraic threshold network degree less than equal sum products rule classical rule case cf references extended networks higher order coefficients axe constructed sum products theorem storage capacity algorithm applied homogeneous algebraic threshold network degree less equal cf spectral rule spectral rule states space generated point cf approach extended higher now describe let denote matrix coefficients wt ie note zero diagonal nature interactions have been increase capacity let matrix memories form extended binary matrix let diagonal matrix positive diagonal terms spectral algorithm coefficients theorem storage capacity spectral algorithm best random interactions consider homogeneous algebraic threshold networks whose weights random natural higher order spin interactions show asymptotic estimate number fixed points structure asymptotic results usual case linear threshold networks interactions have been set ii each random distributed zero mean definition given state stable each case usual case fixed points parameter essentially measure well attraction fixed point following phase expected number fixed points have attraction above certain depth let expected number stable states theorem corresponding each fixed interaction order exists positive constant such ca axe parameters depending interaction order possible design shallow polynomial size threshold circuits compute wide class structured problems thesis shallow circuits compute more than circuits out case higher order networks results appear point same direction neural networks fixed degree number states essentially order total number fixed points appear exponential number least random interaction case though them have constant attraction references abu mostafa number synapses per neuron analog vlsi neural systems ed ii theory neural networks thesis california technology june venkatesh number stable points spin neural networks higher vol pp venkatesh fixed points algebraic threshold networks higher order correlation model associative memory neural networks computing new conf proc vol theory state properties spin glass spin glass vol pp computations theory complex systems computation program certain cortex parallel distributed processing vol rumelhart pp mit press spin glass vol pp optimal lower bounds small depth circuits proc th pp hopfield neural networks physical computational proc natl acad vol pp complexity connectionist learning various functions computer information science technical report vol valiant boolean proc th cells functional dendritic trans soc vol pp venkatesh capacity hopfield associative memory ieee trans theory vol pp number stable points spin glass memory data report vol pp ed analog vlsi neural systems complexity appear computational lower bounds number threshold functions ieee trans comp vol pp threshold logic its applications polynomial time algorithms regular threshold vol pp functions associative memories neural networks computing new conf proc vol threshold circuits polynomial computation algebraic methods theory lower bounds boolean circuit complexity proc th valiant theory vol pp valiant learning trans soc vol pp venkatesh linear point rules applications pattern associative memory thesis california institute technology venkatesh linear associative neural networks appear ieee trans theory venkatesh communication bounds threshold gate trans comp vol pp time proc th ieee pp
centric models orientation map primary visual cortex computer science dow abstract visual cortex horizontal organization preferred orientations orientation cells follows two rules have similar orientation many different orientations observed local region several orientation models these constraints found spacing topological index their singularities using rate orientation change measure models compared published experimental results introduction has been known exist orientation sensitive neurons visual cortex these cells specific patterns regions visual field ie cells field best patterns such cells levels but edges specific individual cell particular orientation called preferred orientation its response edge preferred orientation orientation sensitive cells have regular primary cortex electrode into depth cortex column cells have same preferred orientation least upper layers electrode across cortical surface preferred orientations change regular so recorded orientations appear rotate distance horizontal structure referred orientation map orientation map defined surface every point has associated preferred orientation such cells cortex simulations paper orientation map considered surface paper upper layers striate cortex two dimensional orientation map has architecture function visual cortex organization orientation sensitive cells degree organization connections striate cortex plausible orientation generated models connections cells layer models specific connections appropriate patterns input paper models connections produce orientation map but orientation cells rather centers distributed across cortical surface uniform spacing american institute physics orientation map represents primary visual cortex has been known map hence visual field global but relations between points visual field cortical surface well known description level given cortical location such each region visual field population cells necessary orientations inputs both description orientation map allow distances between orientation zones particular orientation suggest much cortical being analysis given feature given location visual field models orientation map hubel parallel stripe model model orientation map parallel stripe model first published hubel wiesel model has been several times their many model consists parallel each orientation ocular dominance stripes model ie through orientations rate change electrode stripes should changing called orientation drift rate respect array orientation parallel stripe model does several long penetrations through striate cortex first out model penetrations have low orientation drift rates over distances because electrode ocular dominance stripes therefore parallel orientation stripes would expected within single orientation column over distance its orientation drift rate equal zero such results have never been observed second reversals direction orientation drift seen yet parallel stripe model stripes ocular system do themselves reversals third should negative correlation between orientation drift rate drift rate orientation changing electrode should single ocular dominance stripe low drift rate changing electrode should single orientation stripe low orientation drift rate clearly recent hubel see their both orientation often have high drift rates same electrode track ie they show positive correlation show orientation ocular dominance column centric models topological index model proposed braitenberg braitenberg has orientations around centers like centers distances about model reversals horizontal penetrations approach suggests other fact class centtic models centers form discontinuities otherwise field orientations different topological types possible their topological index topological index parameter computed around recording rotation field elements figure value index indicates amount rotation indicates direction rotation index orientations rotate through index rotation positive index indicates orientations rotate same sense taken around negative index indicates rotation topological singularities stable under so field elements each index remains thus may have orientations out like may center four types discontinuities considered here these most stable ie their change figure topological singularities positive index indicates orientations rotate same direction taken around negative index indicates rotation orientations rotate through around centers around centers cytochrome oxidase puffs topological singularities change orientation structure changing orientation map minimize discontinuities their models order smoothness constraint upper layers striate cortex zones little orientation have been these zones their high cytochrome oxidase have been referred cytochrome oxidase puffs refer them puffs centers centtic models cytochrome oxidase puffs discontinuities orientation map but see below braitenberg has centers model should correspond puffs dow bauer proposed model centers puffs proposed similar model centers puffs last two models discontinuities interpuff zones but they assume rectangular cytochrome oxidase puffs set centric models two parameters models considered here whether positive singularities every puff puffs whether singularities ls four centric models figure centers puffs centers interpuff zones both centers puffs centers puffs centers interpuff zones both centers puffs model corresponds braitenberg model rectangular array rather than observed organization cytochrome oxidase regions fact rectangular braitenberg model figure model proposed dow bauer model proposed should models same model bit have same relationship figure four centric models represent cytochrome oxidase puffs interpuff zones singularities those points simulated horizontal electrode recordings made four models compare their orientation drift rates those published recordings computer simulations figure interpuff distances chosen correspond puff centers along their long along short density arrays chosen sampling frequency observed hubel horizontal electrode recording experiments about cells per therefore cell density simulation arrays about six times shown figure models produce simulated electrode data published recording results eg they reversals changing orientations orientation drift rate number reversals different models models figure shown rectangular arrays important characteristics models such interpuff zones dependent real cytochrome oxidase puffs irregular figure small set puffs region figure each centtic models irregular array model electrode track shown figure several problems models developed regular implemented irregular real system models have different properties singularities interpuff zones have been models now have interpuff discontinuities before they comparisons orientation drift rate two sets centtic models computer simulations set rectangular array figure set irregular puff array figure point generate many tracks simulation arrays information compared published records orientation drift rate slope between models real electrode tracks data rather perhaps process orientation map approach fit line use slope line reversals tracks require fit approach used hubel wiesel because data always clear what hubel report track their figure has two reversals yet between st their track what point change slope considered rather than noise approach used here use local slope measure problem reversals fast slope computer single electrode track several long single slope average taken each point track these samples local over small large should small noise orientation measures large out reversals slope figure centric model realistic puff array simulated electrode track resulting data shown model shown here but other models array measures using several sizes applied six published horizontal electrode tracks upper layers striate cortex figure figure fit line between every points high slopes larger give lower slopes those tracks reversals window consistent measures six tracks therefore window chosen comparisons between published data centric models measure average slope degrees per six published samples track data compared hubel measure penetrations their slope measures centtic models slope measure applied several tracks random locations simulation arrays slope computed each simulated electrode track average slopes models shown table models centers have higher slopes than those centers models centers every puff have higher slopes than puff models thus orientation drift rate having rates model both rectangular irregular arrays most realistic slope values table average slopes centric models rectangular irregular array array numbers slope measure window applied published records average slope constraints orientation map our definition orientation map each cell have orientation whose independent its but much general results electrode penetrations two constraints data first these smoothness orientation changes regular electrode through upper layers cells have similar orientation discontinuities do occur but other constraint fact orientation always changing distance although rate change may sequences constant orientation they do occur they never distance reasons parallel stripe model two constraints orientation map may put follows smoothness constraint points have similar orientation constraint orientations should represented within small region cortical surface second constraint bit than data experimental results show orientations change distance orientations present within region but constraint important respect visual processing these constraints first minimize slope orientation drift rate while second rate thus organization orientation map physical systems elements constraints properties such systems many optimal solutions better than result many plausible orientation map these two constraints generate plausible simulated electrode tracks points out need comparisons between models experimental results centric models two constraints what possible mechanisms constraints generate orientation map smoothness local could individual cells cortex cells similar larger scale rather than while first constraint may individual cells second constraint distributed over region cells such cells its through required orientations topological singularities earlier definition include orientations within region these centers across surface cortex constraint may fact amount orientation drift rate function density distribution ie more centers per unit give higher drift rates has been same topological model but different low drift rates model may increased density centers model same relationship models possible obtain realistic orientation drift rates density centers ls these increase number interpuff singularities given possible combinations centers may more than set centers spacing cytochrome oxidase regions results realistic orientation drift rates cortical architecture types thus have preferred orientations generated mechanism but have been nature dendritic more recently computer simulations have shown orientation sensitive units may obtained developed using simple hebbian rules synaptic weights given appropriate network parameters orientation neural networks centtic models quite different approach cell programmed center distance specific orientation so individual cell does orientation develop without both these mechanisms may effect produce final orientation map map may form cortex but cells organization centers could have two map first additional centers could up cells more specific second spacing centers distinct uniform scale map orientation map could have orientation zones regions changing orientations spacing puffs hence architecture cortex appropriate variety feature sensitive cells each location has cortical given distances connectivity cell given orientation estimate many other zones same orientation cell may given orientation model has many orientation zones per unit orientation specific cells visual cortex have been distribution cell features cortex hubel wiesel orientation organization ocular dominance their model centers might identified cells later centric models have identified centers cytochrome oxidase regions again orientation map ocular dominance array puffs themselves related array while have related form function machine vision have general purpose variety algorithms related processing visual information more recently many computer artificial vision systems have their attention towards connectionist systems neural different features different values those features may system architecture has proposed feature natural layered networks networks our work more specific cortical but study cortical feature provide important design artificial systems references hubel wiesel lond hubel wiesel lond hubel wiesel proc soc lond proc proc natl acad proc natl acad braitenberg braitenberg dow bauer lond dow bauer neurosci science wt vision research hubel wiesel comp neurol hubel wiesel comp neurol hubel nature hubel neurosci bauer dow brain hubel wiesel comp neurol models visual cortex hubel wiesel comp neurol hubel wiesel comp neurol brain neurosci comp neurol proc soc lond braitenberg models visual cortex models visual cortex vision research science models visual cortex computer vision nature brain proc first conf neural networks june
simulations suggest information processing roles currents hippocampal neurons mit technology center biological information processing institute technology abstract computer model hippocampal pyramidal cell hpc described data variety order develop consistent description cell type model non linear somatic currents hpc structure neuron cable approximation model simulations wide range somatic electrical behavior ii hpcs demonstrate possible roles various currents information processing computational properties neurons several neuronal computation including connectivity synapses dendritic linear parameters cell membrane well non linear time membrane referred currents channels classical description neuronal function contribution membrane channels generating action potential firing threshold relationship between steady state stimulus firing frequency clear role these channels may much more complex resulting variety computational information processing biological neural net american institute physics hippocampal neurons over wide variety non linear channels have been described many cells particular several neurons such neuron hippocampal pyramidal cell hpc hpc channels their wide range temporal voltage dependent dependent characteristics results complex behavior responses these cortical cells example hpc channels activated thus action potential shape other channels have modulating response hpcs over these channels various technical constraints including small size extended structure hpcs used experiments electrical behavior hpcs computer simulations method data variety order develop consistent description cell type model referred here putative mechanisms voltage dependent dependent channel have been used generate simulations somatic electrical behavior hpcs suggest mechanisms information processing single cell level model has been used suggest experimental designed test simulation results model simulations wide range somatic electrical behavior hpcs demonstrate possible functional roles various currents model non linear somatic currents including three putative na currents ina ina rep ina six currents have been ic after two ca currents ic structure hpc approximation assumed linear while conditions dendritic tree single cable hpc so called conditions cable close tree addition although hpc have non linear membrane assumed first approximation contribution currents membrane may somatic response somatic stimulus model structure assumes axon soma current under these conditions into soma circuit part paper address following neural nets using elements have simple responses connected each other what function channels observed real neurons results hpc model study suggest purpose these paper shall possible roles non linear channels neuronal information processing given nature many currents have presented model important view results based interaction many model elements neural information first step biological computations computational properties neurons requires information neuronal output classical description assumes information spike frequency single output variable proportional firing frequency other information degrees including relative phase inputs frequency modulation during single firing due mechanisms spike shape note these variables patterns repetitive firing relative phase different inputs single cell important low firing rates but becomes less so firing frequency approaches time constant membrane other rate process synaptic eg synaptic channel frequency modulation during may important interaction given output other inputs neuron firing due mechanisms cell input may single spikes may considered cases repetitive firing responses important example cells transmission function finally modulation spike shape may have several later modulation hpc currents order modulation hpc currents considered potential information processing mechanisms necessary several currents described here such have been identified example cholinergic list active non remains seen whether yet mechanisms modulating other hpc currents example three na currents proposed present model possible such mechanisms later hpc currents information processing role given channel hpc electrical response its temporal characteristics function voltage other variables complicated fact channels allowing both linear non linear operations eg particular current over first approximation act changing time constant membrane other currents sub time act changing membrane voltage complicated ways example role na currents action potential different hpc currents may information processing neuron have each current hpc response simple inputs our research inputs have been basic short somatic current steps single spikes long somatic current steps spike current steps dendritic cable response these inputs functional roles hpc current spike shape spike threshold ina rep ic putative functional roles hpc somatic currents role eg ca activation current currents into three non modulation spike shape modulation firing threshold both single repetitive spikes modulation steady state membrane time constant modulation repetitive firing specifically relationship between input frequency initial later steady state spike table roles hpc currents simulations note while four characteristics last two so table possible roles modulation again has been assumed neural information steady state frequency modulation eg number spikes per second over time output information neuron example proportional spike frequency its neuron specific fact action potential propagation such long timing spikes parameter may stimulus constant current figure classical relation between total neuronal input current stimulus spike firing frequency line biological relationships line ina current changes allows way neurons information processing various classical input output relation neuron more biological input output relations relationships have several features either including threshold shape note particular output increased stimulus currents hpc simulations show figure figure blocking putative ina rep has effect cell up response stimulus would otherwise stable spike both currents currents role here first spike both lower threshold high between spikes does get low enough reset ina second spikes due these na currents result do currents much because time levels currents less less ca smaller spikes ca dependent activation currents net result between spikes again does reset ina although current being here theoretical sec sec figure simulation repetitive firing response constant current into soma cell stimulus about na shown cell short firing blocking ina allows mechanism neurons response left seen over response range possible roles modulation spike threshold somatic firing threshold input spike effect change cell simple example blocking ina hpc model threshold about could cell input patterns would otherwise generate action potentials two firing threshold cell static dynamic thus rate soma membrane approaches threshold important along magnitude threshold general threshold level several reasons including currents eg ina activation currents eg levels time sec stimulus time sec na rep time sec stimulus rep figure blocking putative na currents hpc repetitive firing response lower stimulus than corresponds response shown figure thus possible example dendritic synaptic input input input same current soma dendritic input have onset due cable properties onset could allow delay onset spike spikes similar current applied more would have onset sub threshold activation phase would delay spike possible roles modulation somatic spike shape important shape individual spike generated soma first assume spike shape particular spike width soma spike generating membrane once soma effect spike soma may may spike shape dependent both degree spike propagation linear properties pre synaptic membrane axon transmission both linear non linear length more shape somatic action potential pre synaptic axon could spike non linear once threshold response would action potential whose shape would independent threshold soma response other ie membrane linear propagation somatic point down axon would linear somatic signal axon cable properties likely brain between these spike axon non axon length what role could somatic action potential shape modulating pre synaptic signal least three first has been demonstrated fact function pre synaptic membrane voltage waveform thus modulation somatic spike width may much down line mechanism changing spike seen neuron modulation somatic spike width could modulation given neurons second pyramidal cell often back soma somatic synapses result feedback case modulation somatic spike could feedback complicated ways short finally somatic spike shape may role transmission spikes branch points example consider branch point two here spike may able branch sufficiently transmission spike down branch spike down branch spike may both modulation somatic spike shape could used cells output times allowing transmission hpc other times transmission set neurons hpcs much has been obtained roles various hpc currents modulating somatic spike shape example dependent current ic simulations demonstrate effect ic shape individual action potentials shown figure line sec line sec current figure role ic during spike simulation left ic current simulation right blocking ic results spike assumption somatic vs non somatic currents research somatic response hpc has been under assumption data hpc currents activity channels soma considered channel their final functional soma so called somatic channels may therefore channels dendritic pre synaptic membrane example spike channels pre synaptic membrane modulation these channels eg neuron may factor act other hand dendritic field given neuron possible have certain channels thus allowing response modulating these further potential roles membrane channels computation other possible roles currents modulating hpc response many other potential ways hpc currents may hpc response example relationship between ca ca dependent currents ic may possible information processing mechanisms ca important second several example but least three negative feedback mechanisms ca voltage dependent ca currents thus ca ca ca currents possible information processing mechanism could modulation important role repetitive firing simulations suggest blocking current ic step further repetitive firing though after many more spikes blocking both these currents may allow other mechanisms control repetitive firing perhaps could put neuron into quite different region neurons vs single cells graded modulation hpc currents paper have considered contribution various channels ie population given channel type either activated channels description may two ways first possible blocking mechanism given channel may have graded effect example possible cholinergic input homogeneous over soma membrane given time these activated either case possible cholinergic bound thus channels second result channel consider both single cell down spike figure figure due more response population response size population depending architecture cortical region example activation cholinergic hippocampal region may effect hpcs individual hpcs region may either off probability behavior population graded response graded response turn cholinergic activity point information processing properties isolated neurons may behavior population while likely single pyramidal cell have zero functional effect neuron system spectrum behavior its functional units may range single channels specific dendritic tree single cell cortical up through main references somatic electrical behavior hippocampal pyramidal neurons thesis institute technology voltage analysis hippocampal neurons brain research current cells press computation neurons synapses paper center biological information processing mit pyramidal cell nature current hippocampal neurons theoretical approaches complex systems new approach interaction current ca dependent current control initial repetitive firing hippocampal neurons mechanisms action potential fast hippocampal pyramidal cells
neural networks template matching real time classification action potentials real neurons engineering applied science california institute technology ca abstract much experimental study real neural networks classification neural signals ie action potentials recorded experimental most classification single well isolated neurons recorded time those interested sampling many single neurons simultaneously waveform classification becomes paper describe three approaches problem each designed isolated neural events but classify overlapping events real time first present two waveform classification using neural network template matching approach these two compared simple template matching implementation analysis real neural signals simple template matching better solution problem than either neural network approach introduction many have been system using single sample electrical activity single neurons brain have become more complex dynamics these networks has become serial sampling may provide information necessary functional organization addition likely necessary develop new techniques sample multiple neurons simultaneously over last several have developed two different methods data our initial design involved many within brain more recently have been more approach recent technology multi based fig using these able readily activity patterns larger number neurons research multi single neuron recording techniques has become clear used neural signals many brain locations technical associated sampling data these signals sampling itself report specifically consider need neural action potentials known spikes each many parallel recording channels problem processing multi single neuron data more single neuron recordings institute physics analog signals through whose output indicates computer same time analog data accuracy well signal single neuron overlap fig approach large numbers channels recorded same time necessary classification paper describe three approaches have developed do layer lower layer fig being developed our multi single unit recording cortex complete surface view recording several neuronal action potentials recorded such electrode cortex while our design objective neural waveforms multiple channels overall objective research sample many single neurons possible therefore natural our develop neural waveform classification scheme enough allow us more than neuron per recording do now have particular signal neural but several possible neurons see fig while general signals different neurons have different waveforms classification neurons recorded same channel firing simultaneously simultaneously produce combination waveforms fig need last previous classify neural signals see see our objective design circuit would different waveforms even though neuronal quite similar shape fig same waveform even though such often result changes amplitude recorded signal brain relative electrode considerably recording noise neural recordings fig overlapping waveforms likely events point view provide real time performance allowing problems hardware due need classify neural signals many channels simultaneously simply based algorithm each channel work but rather multiple small independent hardware need constructed signal recorded electrode fig electrode recording two neuronal cell actual multi neuron recording note two waveforms overlapping data different noise levels classification algorithms methods problem multiple neural signals single voltage records two steps first waveforms present particular signal identified templates generated second these waveforms data records first step have modified component analysis described templates distinct waveforms found initial sample analog data further second step us here specifically paper compare three new approaches waveform classification overlapping spikes other design criteria above these approaches modified template matching scheme two applied neural network implementations first consider neural network approaches point what follows real neurons whose signals want classify referred neurons while computing elements applied neural networks called neural network approach overall problem neural waveforms best seen problem presence noise much recent work neural type network algorithms has demonstrated these networks work quite well problems particular recent paper hopfield describe network suggest map problem template matching into similar energy functional network they has form connectivity between hopon hopon voltage output hopon ii input current hopon each hopon has input output equation set see hence minimum network constructed described below correspond proposed solution particular waveform classification problem template matching using hopfield type neural net have taken following approach template matching using neural network simplicity classification problem two waveforms have constructed neural network made up two groups each other waveform classification follows first used presence voltage signal channel above set threshold threshold presence possible neural signal data around stored samples note limitations single real neuron cannot more than once time so waveform particular type occur data sample action potentials order so window include full signal single waveforms next step later data values into hopfield network designed minimize mean error between actual data linear combination different delays templates each hopon set waveform represents particular temporal delay waveform network terms energy function let xt input waveform amplitude time amplitude template denote template time present input waveform appropriate energy function first term designed minimize mean error best second term each assumes values sets diagonal elements third term processing same neuronal signal described above occur once per sample expression connection matrix jl input current seen inputs correlations between actual data various delays templates constant term modified hopfield network more fig above full hopfield type network well isolated spikes noise levels but overlapping spikes has local minima problem more more than two waveforms network further need our network hardware full hopfield network difficult current technology see below these reasons developed modified neural network approach necessary hardware complexity has improved performance let us information ii use them these have pre processing before being into hopfield network after these rule out large number possible template combinations size problem thus use much smaller hence more neural network find optimal solution simple define modified ii two template case xt st st st st case spikes cross correlations between different delays cross correlations between input xt weighted combination st st now xt ie overlap first template time delay second template time delay ai presence noise zero but equal noise ai simple algorithm may errors solution problem overlapping spikes described below but now let us consider problem non overlapping spikes case compare input cross correlation correlations st so non overlapping cases xt xt st noise minimum ai represents correct classification presence noise these zero but equal noise input xt give errors our solution noise related problem few minima three have chosen our case each minimum either known corresponding linear combination templates overlapping cases simple template non overlapping cases three neuron hopfield type network programmed so each neuron corresponds each cases input xt network remains after first step correlation comparisons note simple template matching described below used field type network simple template matching these neural network approaches simple template matching scheme now describe below approach out most require least complex three approaches first step again data based possible neural signal difference between recorded waveform possible combinations two identified templates consists distances between input possible cases generated combinations two templates st st best fit possible combinations templates actual voltage signal compare performance each three approaches common set test data using following first used component method generate two templates analog neural activity recorded two actual spike waveform templates use ratio second set analog recordings made action potential events spectral characteristics recording noise these two real neural recordings objective being construct realistic records while what correct solution template matching problem each spike shown fig data sets corresponding different noise signal constructed out simulations templates themselves records waveform changes due brain often seen real recordings addition two waveform test sets constructed three waveform sets generating third template average first two templates further comparisons three approaches described above considered non overlapping overlapping spikes performance three different approaches two classification first case correct classification order timing two waveforms second scheme classification correct order two waveforms correct but timing time most applications sufficient compare performance results three approaches waveform classification implemented simulations performance comparison two templates non overlapping waveforms shown fig low noise signal below each three approaches performance close accuracy each ratio increased neural network implementations less less well respect simple template matching algorithm full field type network considerably than modified network range most often found real data simple template matching performed considerably better than either neural network approaches simple template matching estimate fit waveform template could used events should eg signals due noise noise level amplitude noise level amplitude degrees overlap line criteria line less criteria simple template matching hopfield network modified field network fig comparisons three approaches two non overlapping overlapping waveforms neural network approaches different degrees waveform overlap two templates overlapping waveforms fig compare waveforms fig local minima problem full neural network demonstrated improved performance modified network again overall performance noise clearly best simple template matching noise level low modified approach better two neural networks due correlation number between input data template noise level high errors correlation numbers may right combination smaller network case its performance little than larger hopfield network fig degrees overlap produce most neural network approaches average levels found real neural data seen neural networks most problem delays between two waveforms small enough resulting waveform like larger waveform three templates non fig shown comparisons between full hopfield network approach simple template matching approach waveforms performance these two approaches much more than two waveform case fig although simple template matching optimal method overlapping waveform condition neural network approach fig particular implementation neural network approach does scale well ao co noise level amplitude noise level amplitude noise level amplitude hopfield network simple template matching line criteria line less criteria noise fig comparisons performance three waveforms waveforms two waveforms overlapping three waveforms overlapping hardware comparisons described earlier important design work neural signals analog records real time many simultaneously active sampling because run algorithms computer real time channels simultaneously necessary design hardware each channel do have design vlsi implementations our well large neural networks need hardware implementations let us consider example two template case comparisons let neurons per template neuron each delay template rn iterations stable state equation step size samples template number connections full hopfield network total synaptic so two templates rn thus full hopfield type network requires system large put single vlsi chip work real time want analog system need have many easily synapses yet technology nets size modified hopfield type network other hand less do obtain minimum values have do about additions find possible require comparisons find three minima associated input cross correlations same full neural network ie modified approach network used small fast additions construct synapses synaptic neural networks simple template matching simple example perform about additions comparisons find minimum additions considerably less time hardware than fact because method addition operations our design work suggests single chip able do two template classification little chip might able more than channel essentially real time template matching using full hopfield type neural network found noise changes signal waveform two neural waveform classification problem three waveform case network does perform well further network requires many connections therefore results hardware implementation overall performance modified neural network approach better than full hopfield network approach computation has been hardware requirements considerably less value specific network specified problem even modified neural network less well than simple template matching algorithm has hardware implementation using simple template matching algorithm our simulations suggest possible two three waveform single vlsi chip using technology real time error characteristics further such chip able classify overlapping neural signals references ieee trans soc neurosci proc ieee brain neurosci methods hopfield proc natl acad hopfield hopfield ieee trans circuits would like contribution these able work supported
neural network implementation approaches connection machine va abstract connection machine cm allows neural network simulations use simple data control structures two approaches described allow parallel computation models functions parallel models weights parallel propagation models activation error each approach allows models interconnect structure dynamic hopfield model implemented each approach six sizes over same number cm processors provide performance comparison introduction simulations neural network models perform various computations linear functions defined program weighted sums real numbers stored array values model dependent parameters like time frequency activation synaptic weight error error back propagation models computational models interconnect structure particular model relationships between arrays defined program connection machine cm these relationships hardware processors interconnected dimensional communication network constructed define higher dimensional between processors communication network parallel transfers defined over these these may dynamic cm parallel operations array temporal references memory single temporal distributed processors two approaches neural network simulations cm described both approaches use data provided lisp virtual machine data control structures associated each approach performance data hopfield model implemented each approach presented data structures functional neural network model implemented lisp stored uniform parallel variable pvar data structure cm data structure may columns pvars columns given cm virtual processors each cm physical processor may virtual processors first approach described cm processors used represent edge set models graph structure second approach described each processor represent unit link incoming link models structure activation error through models interconnect structure simulated values american institute physics over many such result execution single cm cm required part distributed over enough processors so up certain processors cm large numbers each approach requires serial transfers model parameters states over communication channel between host cm certain times simulation approach edge list approach edge list network graph cm edge per cm processor interconnect weights each edge stored memory processors array host machine stores current activation units approach may considered represent abstract synapses cm interconnect structure model described sets id numbers rid sid rid id units activation sid id units activation each id unique network input units never set output units never set various set relations eg symmetric defined over id used high level representation networks interconnect structure these relations into pvar columns interconnect complexity simulated model virtual processor memory cm used space functions used compute weighted sums activation fig shows interconnect structure its edge list representation cm ai sact aj fig edge list representation interconnect structure representation use few six pvars model hebbian adaptation rid sid interconnect weight ract ai sact aj learn rate error back propagation requires addition error interconnect weight term unit pvars described above interconnect weight pvar stores weight interconnect activation pvar sact stores current activation aj unit specified rid unit specified sid activation pvar ract stores current weighted activation error pvar stores error unit specified sid variety eg point boolean field exist lisp define type size pvars memory speed up execution using small number pvars amount memory used each cm processor so maximum virtualization hardware processors neural model specified models require multiple input activation pvars specified edges may have different number input activation pvars than uniform data structure approach pvar has used input activation pvars use particular edge edge list approach allows structure simulated model change because edges may added up virtual processor time without operation control structure edges may processor because rid sid operation performed before particular update operation processors edges units selected update second simulation approach composite approach uses more complicated data structure units incoming represented update approach use parallel segmented scans form weighted sum input activation parallel segmented scans allow like computation weighted sums many units once pvar columns have unique values unit incoming link link representations data structures input units hidden units output units sets three pvar column types fig shows representation same model fig implemented composite approach ii ii fig composite representation interconnect structure fig cm processors units incoming represented respectively cm cube address used parallel transfer activation shown below structure these model interconnect multiple sets these may stored pvars segmented scans represented operation above structure basic composite approach pvar set model hebbian adaptation forward forward forward transfer address interconnect weight act ai act aj threshold learn rate current unit id unit id level column type back error requires addition backward backward backward transfer address error previous interconnect weight term forward backward boolean pvars control segmented scanning operations over unit pvar each type scanning pvar each type copy scanning forward transfer pvar stores cube addresses forward cube address parallel transfer activation backward transfer pvar stores cube addresses backward cube address parallel transfer error interconnect weight activation error pvars have same functions edge list approach current unit id stores current units id number unit id stores id number unit edge list networks graph contents these pvars have link pvar columns level pvar stores level unit network type pvar stores unique pvar column type these last three pvars used processor number processors involved operation again edges units added processor memories units out new structure processors level column type current unit id unit id values consistent model number cm virtual processors required represent given model cm each approach given units nn non zero interconnects eg symmetric model edge list approach simply nn edges nn cm virtual processors composite approach requires two virtual processors each interconnect virtual processor each unit nn cm virtual processors total difference between number processors required two approaches table shows processor cm virtualization requirements each approach over range model sizes model sizes cm processors required run grid size number units edge list cm level nn run grid size number units composite cm level control structures control neural network simulations lisp stored host computer eg connected cm high speed communication line neural network simulations lisp use small subset total set processor selection reset processor selection parallel set global sum parallel multiplication parallel parallel parallel global memory references parallel segmented scans copy cm processors them list active processors their contents may parallel list active processors may made used time subset processors may time contents processor selection reset current selected set processors selected parallel allows pvars selected processor set values step global tree sum across cm processors grid cube address particular pvars parallel additions pvars selected cm processors step parallel exponential function contents specified pvar over selected processors parallel segmented scans two functions copy cm processors scanning across grid cube addresses scanning may forward backward ie cube address order respectively show edge list approach required hebbian learning model construct fig activation update operation usual compute each weighted sum particular unit has been four parallel operations selection reset processors particular unit activation rid parallel multiplication weight sact tree sum sum activation particular unit connected computed activation array host processors particular unit activation sid array value host those processors rid activation sum weight sact set ract activation sid set sact activation fig shows fig activation update kernel edge approach hebbian weight update kernel aj set weight learn rate ract sact fig hebbian weight edge list approach edge list activation update kernel essentially serial because steps involved applied unit time weight parallel error back propagation computing errors units each layer model required activation update error back propagation require transfers arrays host every step other common computations used neural networks computed parallel using edge list approach fig shows kernel parallel computation number units sum weight ract sact sum input sact fig kernel computation energy equation although input pvar input defined edges non zero those edges associated input units fig shows pvar structure parallel computation hopfield weight segmented scanning produce weights step number patterns stored ract vs vs sact vs vs weight fig pvar structure parallel computation hopfield weight fig shows lisp kernel used pvar structure fig set weight scan ract sact pvar include self fig parallel computation hopfield weight edge list activation update updating method used composite approach fig shows lisp kernel activation update using composite approach fig shows lisp kernel hebbian learning operation composite approach level set act scan act copy pvar include self set act act weight type act act level set act scan act pvar include self type act fig activation update kernel composite approach set act scan act copy pvar include self type set act act set weight weight learn rate act act fig hebbian weight update kernel composite approach number interconnects may updated more used less computation becomes because less processors analysis performance results presented next section analysis space time requirements cm implementation approaches simplicity use model hebbian adaptation magnitude requirements activation weight updating compared both cm implementation approaches basic serial matrix approach given model space requirements conventional serial machine nn locations growth space weight matrix system interconnect structure edge list approach uses six pvars each processor uses processors locations composite approach uses pvars processors units processors interconnects given model composite approach uses locations cm implementations take up same space serial implementation but space serial implementation memory space cm implementations interconnected processors memory time analysis approaches time order compute activation update hebbian weight update serial machine weighted sums computed activation update require nn additions operations time order magnitude time order magnitude weight matrix update weight matrix elements edge list approach weighted sums parallel weights model weight sact tree sum sum products unit see fig operations time order magnitude same order magnitude obtained serial machine further performance activation update function number interconnects processed composite approach weighted sums steps see fig five selection operations segmented copy scan before parallel multiplication parallel multiplication parallel transfer products segmented scan sums step composite activation update time order magnitude performance independent number interconnects processed next section shows quite weights model updated three parallel steps using edge list approach see fig weights model updated parallel steps using composite approach see fig either case weight update operation has time order magnitude time complexity results obtained composite approach computation energy equation hopfield given pvar structures see used same operations performed time order magnitude above operations time cost generating addresses pvars used parallel transfers values scanning what above analysis shows time complexity space complexity cm programming use many processors possible every step performance comparison simulations hopfield spin glass model run six different model sizes over same number physical cm processors provide performance comparison between implementation approaches hopfield network chosen performance comparison because its simple well known convergence dynamics because uses small set pvars allows wide range network sizes degrees virtualization run run six edge list approach six composite approach table shows model sizes run each each run virtualization level necessary number processors required each simulation two patterns stored five test patterns two two test patterns have their centers two have row column random pattern each hand produce cross number columns patterns increase size networks increases performance cm rather than performance network model used simple model simple pattern set chosen minimize model dynamics performance performance presented execution speed versus model size size number interconnects model execution speed interconnects updated per second nn number units model time used update units model units updated three times each pattern convergence output activation stable over two value average samples fig shows activation update time both approaches fig shows interconnect update speed plots both approaches edge list approach composite approach performance shown lisp model size each correspond model sizes levels cm virtualization shown table activation update time vs model size model size fig activation update times interconnect update speed comparison edge list approach vs composite approach oe model size fig edge list interconnect update fig shows order magnitude performance difference between approaches off performance each approach function number virtual processors supported each physical processor performance around virtualization edge list approach virtualization composite approach interconnect structure neural network models defined over set processors provided cm architecture provides performance programming small subset provided lisp further performance scale up long more than virtualization required while complexity analysis composite activation update suggests its performance should independent number interconnects processed performance results show performance dependent number interconnects processed because level virtualization required after physical processors dependent number interconnects processed virtualization performance complexity analysis edge list activation update shows its performance should same serial implementations results suggest composite approach over edge list approach but used virtualization level higher than mechanism composite activation update suggest networks simulated compare performance single layer networks because parallel transfers provide type activation updated networks while activation transfers updated single layer networks mit ai use similar approach implementation their approach weights connected units simultaneously activation forward error backward performance better than presented control lisp use cm addition being allows control over important through lisp cm number new features performance neural network simulations lisp larger processor memory point processors point processors increase execution while larger processor memories provide larger number virtual processors performance turn around points allowing higher performance through higher cm references introduction data level technical report hopfield neural networks physical systems computational proc acad vol pp learning connection machine mit technical report
speech recognition experiments perceptrons research abstract artificial neural networks recognition simple speech such isolated paper two more difficult set set polysyllabic words set difficult because difficult because timing variation polysyllabic word recognition time pre alignment based dynamic programming set recognition improved attention recognition better than both implemented single layer perceptron introduction artificial neural networks perform well simple pattern recognition tasks trained layered network conventional trained same tokens they most part whether artificial neural networks more difficult speech recognition problems polysyllabic recognition difficult because multi words large timing variation difficult vocabulary set consists words vocabulary short low energy show simple layer perceptron both problems well input representation used sufficient examples given two spectral representations fft fast lpc linear spectrum time described pre speech templates based energy finally attention artificial neural network beginning word recognition accuracy set increased layered neural network relative earlier trained simple process layered networks have been american institute physics applied speech recognition recognition speech variation layered network uses feedback model constraints useful learning speech hidden neurons within layered network used form solutions specific problems number hidden units required related problem though single hidden layer form more than two layers form second layer may useful more stable learning representation presence noise though neural nets have been shown perform well conventional techniques neural nets may do better classes have perceptrons simple perceptron input layer output layer neurons connected each other hidden neurons often called layer system single layer weights input output figure shows layer perceptron sense speech patterns two dimensional grid input consists point spectrum each time each inputs connected each output neurons though sampling connections shown output neuron corresponding each pattern class neurons have linear weighted inputs activation units figure single layer perceptron time frequency array sample data each output neuron corresponds pattern class full connected input array few connections shown input word fit grid region algorithm algorithm variation proposed threshold approximation method first threshold energy zero rate level other than zero used being input representations two different input representations used study first representation both time frequency speech hamming number sample points point fft spectrum computed produce template spectral samples each time frames template time window length three frequency window length comparison lpc spectrum computed using order model sample hamming analysis performed using method resulting spectrum over three time frames sample spectra neural nets shown figure notice relative smoothness lpc spectrum models spectral fft lpc figure fft lpc time frequency plots neural nets time left frequency right dynamic time alignment conventional speech recognition systems often time based dynamic programming used time two utterances obtain optimal alignment between their spectral frames variation dynamic programming energy rather than spectra energy template chosen each pattern class incoming patterns figure shows five utterances neural nets both before after time alignment notice improved alignment energy figure energy plots five different utterances neural nets same utterances after dynamic time alignment polysyllabic words three five chosen five tokens each recorded single variable number tokens used simple perceptron study effect training set size performance two performance measures used classification accuracy rms error measure training tokens obtain additional experimental data points figure output responses perceptron trained per class left four tokens per class right figure shows two plots output perceptron trained four tokens respectively per class plots show network response function output left test word index right note more training tokens produce more map map should have along diagonal table shows results these experiments three different representations fft lpc time lpc table classification accuracy function number training tokens input representation learned classify patterns cases fft single training pattern table polysyllabic word recognition number training tokens fft lpc time lpc different performance measure rms error degree trained network output responses measure over non trained tokens output network rms error jl figure shows plots rms error function input representation training patterns note fft representation error lpc about less time lpc better than non lpc many made ie much larger than words lpc preferred time alignment could useful similar words increased number training tokens results improved performance cases number training tokens figure rms error versus number training tokens various input representations set vocabulary set vocabulary consists words tokens each classes recorded single sizes training test sets used training other total recognition figure shows lpc templates classes notice due sound common tokens feature left tokens sound figure lpc time frequency plots tokens set words figure time frequency plots weight values connected each output neuron through trained perceptron figure shows similar plots weights learned network trained tokens each class these like spectra weight associated each spectral sample note patterns have structure recognition accuracy classes notice weights along small negative response sound common classes network has learned vocabulary notice beginning most templates shows network has much its note particular beginning these could onset note complicated pattern sound classes produce relatively weight weight space therefore little sense network should time sound perhaps additional training might activity network would find little consistent difference second network attention first frames each input produce average spectrum these average spectra used simple scheme recognition accuracy function performance first word most action figure matrix set first each word words time frames into first word lpc spectra using th order model network trained new templates performance increased classification errors out recognition matrix shown figure learning times experiments about through training set weights average spectral values rather than random values learning time artificial neural networks high performance pattern recognition applications matching conventional have shown difficult speech problems such time alignment artificial neural networks perform high accuracy layer perceptrons learn these difficult tasks their simplicity but because references neural network ieee conference systems pp connectionist ieee conference neural networks san diego ca june serial order parallel distributed processing approach report institute science ca may connectionist models learn theory representation multi layered neural networks comparisons between neural net conventional ieee conference neural networks san diego ca june linear speech new perceptrons mit press rumelhart internal representations error propagation parallel distributed processing vol rumelhart mit press pp algorithm isolated utterances vol programming word recognition ieee trans speech signal processing vol parallel network technical report university electrical engineering computer science analysis neural network ieee conference neural networks san ca june
analysis learning behavior neuronal models school electrical engineering computer science university presented ieee conference neural information processing published ieee conference address further school american institute physics analysis learning behavior neuronal models school electrical engineering computer science abstract paper convergence behavior number neuronal plasticity models recent research suggests neuronal behavior adaptive particular memory stored within neuron associated synaptic weights learning number adaptive neuronal models have been proposed three specific models paper specifically model sutton barto model recent trace model paper conditions convergence position convergence rate convergence these models they applied classical simulation results presented analysis introduction number static models describe behavior neuron have been use more recently research suggests static view may rather parameters within neuron learning internal parameters neurons may themselves repetitive input become learning thus neurons describe behavior neuronal plasticity number models have been proposed may have been more recently sutton barto new model most recent mrt model paper primary objective paper convergence behavior these models during adaptation general neuronal model used paper shown figure number neuronal inputs input corresponding synaptic weights weighted inputs yt taken zero neuronal inputs assumed take values zero synaptic weights take values purpose paper though weights may well bounded relative magnitude weights neuronal inputs well defined point put bound magnitude weights neuronal output result simplicity operation linear ca output figure convergence analysis assume two neuronal inputs classical environment simplicity analysis techniques extended number inputs classical two inputs stimulus xt stimulus xt sutton barto model more recently sutton barto have proposed adaptive model based both signal trace output trace given below wit wit both positive condition convergence order analysis ie other words becomes wit wit yt above assumption analysis convergence conditions because yt respectively previous section relation so convergence ratio test possible matrix due neuronal output equation include neuronal output yt parameter vector wt yt wt xt xt yt ws ws show convergence need set magnitude less than its hence condition convergence see adaptation constant chosen less than sum inputs same techniques extended number inputs following same above position convergence having convergence sutton barto model plasticity want find out next what location system remains have seen earlier convergence weights change so does neuronal output denote position ws ws other words ws parameter vector always into weighted ie ws so easily found shown within region convergence magnitude third less than convergence contribution third hence ws what position would given initial conditions rate convergence have seen chosen sutton barto model have expression position next want find out fast convergence rate convergence measure fast initial parameter approaches optimal position asymptotic rate convergence spectral radius case convergence analysis sutton barto model neuronal plasticity mrt model neuronal plasticity most recent trace mrt model neuronal plasticity developed considered cross between sutton barto model model adaptation synaptic weights follows wit wit yt comparison sutton barto model show term right hand factor wt used speed up convergence shown later output trace been most recent output hence most recent trace model input trace most recent input condition convergence now condition convergence mrt model due presence factor second term ratio test cannot applied here convergence behavior further let us matrix xt matrix operation above equation quadratic complete convergence analysis equation difficult order convergence behavior note term convergence second quadratic term hence convergence analysis first term readily see above primary convergence factor dependent convergence obtained synaptic inputs being active bounded shown condition convergence bounded readily see adaptation constant chosen convergence simulations theoretical analysis these three adaptive neuronal models based classical these models have been simulated using single several test have been designed compare actual simulation results conditions convergence value adaptation constant set value between sutton barto model simulation given fig shows convergence obtained expected theoretical analysis mrt model simulation results given fig shows convergence obtained expected theoretical analysis theoretical location convergence sutton barto model shown figure readily seen simulation results theoretical iterations figure plots outputs versus iterations model different values constant figure plots neurons outputs versus iterations model different values rate convergence synaptic weights optimal values scale error found earlier slope line rate convergence sutton barto model given figure while mrt model given figure clear figure form line slope readily mrt model given figure line but much larger slope convergence paper have convergence behavior three adaptive neuronal models analysis see model does constant active inputs output convergence model model behavior would realistic outputs figure output static values barto values constant iterations figure neurons output static values model different values adaptation constant analysis sutton barto model shows model adaptation constant chosen bounds found model due structure model both location convergence rate convergence found have new model neuronal plasticity called most recent trace mrt model certain exist between mrt model sutton barto model between mrt model model analysis shows update synaptic weights quadratic resulting polynomial rate convergence simulation results show much convergence rate obtained mrt model references sutton barto vol applied methods press analysis neuronal plasticity school electrical engineering computer science university american institute physics conference neural networks computing
capacity kanerva associative memory exponential university ca abstract capacity associative memory defined maximum words stored address within given sphere attraction shown sphere packing address length increases capacity associative memory exponential growth rate binary entropy function bits radius sphere attraction exponential growth capacity achieved kanerva associative memory its parameters set these optimal values provided exponential growth capacity kanerva associative memory sub linear growth capacity hopfield associative memory associative memory its capacity our model associative memory following let address datum vector ls vector rn ls let address datum pairs stored associative memory associative memory presented input address close stored address should produce output word close corresponding contents specific let us associative memory errors within hamming distance equal sphere around each called sphere attraction called radius attraction capacity associative memory maximum number words while fraction errors capacity defined because address datum pairs have been stored clearly associative memory correct fraction errors sequence stored address datum pairs consider example sequence several different words address memory retrieve contents words other associative number words retrieve them their contents useful definition capacity between these two paper interested such most sequences addresses most sequences data memory correct fraction errors define work supported science under american institute physics most sequences sense set sequences total probability than sequences sequences difficult compute capacity given associative memory inputs length outputs length though compute asymptotic rate increases rn increase given associative memories approach taken towards capacity hopfield associative memory take same approach towards capacity kanerva associative memory towards associative memories general next section provide upper bound rate growth capacity associative memory our general model shown sphere packing capacity exponential rate growth binary entropy function bits radius attraction later section turn out exponential growth capacity achieved kanerva associative memory its parameters set exponential growth capacity kanerva associative memory sub linear growth capacity hopfield associative memory upper bound capacity our definition capacity associative memory such most sequences addresses most sequences data memory correct fraction errors clearly upper bound capacity exists sequence addresses such most sequences data memory correct fraction errors now expression upper bound let radius attraction let sphere attraction set most hamming distance assumption memory fraction errors every address word size easily shown independent equal out total bit addresses least addresses retrieve least addresses retrieve least addresses retrieve so follows total number distinct most now shown binary entropy function bits function whose magnitude grows more than constant times thus total number distinct most set most sequences bit words large number distinct words rn figure neural net representation kanerva associative memory signals input output each signal its weight each incoming signals sufficiently large see follows general function bounded ie exists constant such thus exists constant such should bound has fixed indicates maximum exponential rate growth sequence addresses address space upper bound most such sequences optimal sense large kanerva associative memory take fact kanerva associative memory kanerva associative memory two layer neural network shown figure first layer preprocessor second layer usual hopfield array preprocessor essentially each bit input address into large bit internal representation whose size does capacity kanerva associative memory known capacity hopfield array grows vector random independent figure matrix representation kanerva associative memory signals right input left output shown functional matrix multiplication such assumption bit internal representation function bit input address most bits information independent bits information primary contribution therefore preprocessor map each bit input address into large bit internal representation operation preprocessor easily described consider matrix representation shown figure matrix ls assumption required analysis function ith ith row within hamming distance otherwise ith input parameters two parameters kanerva associative memory set number representation small comparison number hence considered internal representation second memory usual way internal representation jl threshold function whose ith input than ith input less than ith column memory location whose address ith row every within hamming distance ith row location hence known access radius number memory approach taken paper linear rate grows exponential rate grows out capacity grows fixed exponential rate depending these exponential rates sufficient but simple polynomial bounds errors due capacity kanerva associative memory let input address length let output word length assumed most polynomial ie let access radius let number memory locations let radius attraction let number stored words vectors matrix assumed vectors random variables finally given vector let define np co np theorem sufficiently large proof see exponential growth rate number stored words less than every sufficiently large address length preprocessor matrix such associative memory correct fraction errors most sequences address datum pairs thus lower bound exponential growth rate capacity kanerva associative memory access radius number memory locations figure shows function radius attraction fixed access radius increases fact address datum pairs stored fraction errors increases co lower point but off less shall see provide optimal performance given shown figure behavior function behavior simple remains while simply down difference conditions under kanerva associative memory random component although number memory locations does increase capacity does increase random figure defined upper component capacity so many have number memory locations less than perhaps most important figure sphere packing upper bound achieved particular pp upper bound achieved particular equal thus optimal values parameters respectively these functions shown figure these optimal values sphere packing bound seen exponential rate capacity equal exponential growth rate number memory locations aj kanerva have capacity proportional number memory locations ie constant thus our results consistent those kanerva provided polynomial constant usual their result capacity simply proportional number memory locations figure defined upper bound capacity without bound our does because have related number memory locations ro input fact our provides relationships between following variables capacity number memory locations input output radius attraction access radius therefore able ro results ro case provide optimal values well described general model associative memory selected useful definition its capacity upper bound growth capacity such associative memory shown sphere packing ro exponential rate binary entropy function radius attraction operation kanerva associative memory lower bound exponential growth its capacity lower bound upper bound optimal values memory parameters provided these optimal values previous results capacity kanerva associative memory proportional number memory locations cannot our problem those results ro case references venkatesh capacity hopfield associative memory ieee information theory capacity kanerva associative memory ieee information theory kanerva self theory memory rep center study information ca kanerva parallel structures computer memory neural ed new american institute physics comparison between distributed memory hopfield type neural network models rep research institute computer science ca
