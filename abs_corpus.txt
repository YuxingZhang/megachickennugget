does connectivity neural network number synapses per neuron relate complexity problems handle measured entropy switching theory would suggest relation boolean functions implemented using circuit low connectivity eg using two input gates network learns problem examples using local learning rule prove entropy problem becomes lower bound connectivity network
describe family learning algorithms operate recurrent connected neuromorphic network like boltzmann machine presence noise these networks learn modifying synaptic connection strengths basis correlations seen locally each synapse describe version supervised learning network analog activation functions demonstrate unsupervised competitive learning approach weight saturation decay play important role describe preliminary experiments reinforcement noise used search procedure identify above described phenomena elements unify learning techniques physical microscopic level these chosen ease implementation vlsi have designed cmos test chip rules speed up learning about over equivalent simulation speedup due parallel analog computation summing multiplying weights activations use physical processes generating random noise components test chip noise amplifier neuron amplifier transistor adaptive synapse each separately testable these components integrated into neuron synapse network finally point out techniques reducing area electronic correlational synapse both technology design show algorithm study implemented naturally electronic systems
paper generalizes backpropagation method general network containing feedback connections network model considered consists interconnected groups neurons each group could fully interconnected could have feedback connections possibly asymmetric weights but loops between groups allowed stochastic descent algorithm applied under certain inequality constraint each intra group weight matrix ensures network possess unique equilibrium state every input
artificial neural network developed recognize bipolar patterns function formal neuron generalized replacing multiplication convolution weights transfer functions nonlinear transform following adaptation hebbian learning rule delta learning rule generalized accordingly resulting learning weights delays neural network first developed spatial patterns thus generalized spario temporal patterns tested using set bipolar input patterns derived speech signals showing robust classification model phonemes
complexity computational capacity multi layered feedforward neural networks examined neural networks special purpose structured functions examined perspective circuit complexity known results complexity theory applied special instance neural network circuits particular classes functions implemented circuits conclusions drawn about learning complexity open problems dual problem determining computational capacity class multi layered networks dynamics algebraic considered formal results presented storage capacities higher order structures tradeoff between ease programming capacity shown precise determination made static fixed point structure random higher order constructs phase transitions laws shown
visual cortex monkey horizontal organization preferred orientations orientation selective cells follows two rules neighbors tend have similar orientation preferences many different orientations observed local region several orientation models satisfy these constraints found differ spacing topological index their singularities using rate orientation change measure models compared published experimental results
investigate behavior different learning algorithms networks neuron like units test cases use simple pattern association problems such xor problem symmetry detection problems algorithms considered either versions boltzmann machine learning rule based backpropagation errors propose analyze generalized delta rule linear threshold units find performance given learning algorithm depends strongly type units used particular observe networks units quite generally exhibit significantly better learning behavior than corresponding versions demonstrate weight structure symmetries problem lead increase learning speed
being able record electrical activities number neurons simultaneously likely important study functional organization networks real neurons using extracellular record several neurons approach studying response properties sets adjacent therefore likely related neurons do necessary correctly classify signals generated these different neurons paper considers problem classifying signals such extracellular recording based upon their shapes specifically considers classification signals case spikes overlap temporally
much experimental study real neural networks relies proper classification sampled neural signals ie action potentials recorded experimental animals most neurophysiology laboratories classification task simplified limiting investigations single well isolated neurons recorded time those interested sampling activities many single neurons simultaneously waveform classification becomes serious paper describe three approaches problem each designed recognize isolated neural events but separately classify temporally overlapping events real time first present two formulations waveform classification using neural network template matching approach these two formulations compared simple template matching implementation analysis real neural signals reveals simple template matching better solution problem than either neural network approach
based anatomical physiological data have developed computer simulation olfactory cortex capable reproducing spatial temporal patterns actual cortical activity under variety conditions using simple hebb type learning rule conjunction cortical dynamics emerge anatomical physiological organization model simulations capable establishing cortical representations different input patterns basis these representations lies interaction distributed highly interconnections between modeled neurons have shown different representations stored minimal interference following learning these representations input degradation allowing reconstruction representation following partial presentation original training stimulus further have demonstrated degree overlap cortical representations different stimuli modulated instance similar input patterns induced generate distinct cortical representations discrimination while inputs induced generate overlapping representations both features presumably important classifying olfactory stimuli
simd connection machine cm allows construction neural network simulations use simple data control structures two approaches described allow parallel computation models nonlinear functions parallel modification models weights parallel propagation models activation error each approach allows models structure physically dynamic hopfield model implemented each approach six sizes over same number cm processors provide performance comparison
artificial neural networks anns capable accurate recognition simple speech vocabularies such isolated digits paper two more difficult vocabularies set set words set difficult because contains weak discriminants difficult because timing variation word recognition time pre alignment technique based dynamic programming set recognition improved focusing attention recognition better than both vocabularies implemented single layer perceptron
potential presynaptic information processing within arbor single axon discussed paper current knowledge about activity dependence firing threshold conditions required conduction failure similarity nodes along single axon reviewed electronic circuit model site low conduction axon presented response single frequency stimulation electronic circuit acts filter
paper wish analyze convergence behavior number neuronal plasticity models recent neurophysiological research suggests neuronal behavior adaptive particular memory stored within neuron associated synaptic weights varied adjusted achieve learning number adaptive neuronal models have been proposed literature three specific models analyzed paper specifically hebb model sutton barto model recent trace model paper examine conditions convergence position convergence rate convergence these models they applied classical conditioning simulation results presented verify analysis
new neural network classifier propose transforms classification problem into coding theory problem decoding noisy input vector feature space transformed into internal representation code space error correction decoded space classify input feature vector its class two classes codes give high performance matrix code maximal length sequence code show number classes stored neuron system linear significantly more than using hopfield type memory classifier
various simulations cortical subnetworks have something like phase transitions respect key parameters demonstrate such transitions indeed exist analogous array models related array models classical phase transitions describe steady state behavior may exist but distinct qualitative changes transient behavior key system parameters pass through critical values
transient phenomena associated forward biased silicon structures show remarkable similarities biological neurons devices play role similar two terminal switching elements hodgkin huxley equivalent circuit diagrams devices provide simpler more realistic neuron than transistors op they have such low power current requirements they could used massive neural networks observed properties simple circuits containing devices include action potentials refractory periods threshold behavior excitation inhibition summation over synaptic inputs synaptic weights temporal integration memory network connectivity modification based experience activity firing thresholds coupling sensors graded signal outputs dependence firing rate input current transfer functions simple artificial neurons inputs outputs have been measured correlated input coupling
propose learning rules recurrent neural networks high order interactions between neurons designed networks exhibit desired associative memory function perfect storage retrieval pieces information andor sequences information complexity
report study relationship between eeg amplitude values unit spike output cortex awake motivated relationship takes form sigmoid curve describes normalized pulse output normalized wave input curve fitted using nonlinear regression described its slope maximum value measurements made both excitatory inhibitory neurons cortex these neurons known form negative feedback loop both classes cells described same parameters sigmoid curve asymmetric region maximal slope toward excitatory side data compatible model burst generation other existing neural nets being discussed implications signal processing reviewed particular relationship sigmoid slope efficiency neural computation examined
advances brain theory need two complementary approaches analytical investigations measurements well synthetic modelling supported computer simulations generate hypothesis structures neural tissue paper research second line described starting inspired model andor associative motivated basic control tasks pre conditions conditions studied cooperation such units hierarchical assumed general layout brain
interaction set sufficient many cases explain seemingly complex behavioral responses exhibited varied classes biological systems combinations stimuli shown straightforward generalization phenomenon allows efficient implementation effective algorithms appear respond changing environmental conditions examples processing techniques presented paper applications simulated behavior synthesis path planning pattern analysis clustering engineering design optimization
intracellular recordings spinal cord cerebral cortex neurons have provided new evidence correlational strength connections relation between shapes postsynaptic potentials associated increased firing probability these cells excitatory postsynaptic potentials produce peaks resemble large part derivative epsp additional synaptic noise peak but peak area ie number above chance triggered per epsp remains proportional epsp amplitude typical epsp about per epsp consequences these data information processing connections discussed effects sequential links calculated effects underlying connections net effect parallel pathways sum individual contributions
hopfield neural network model associative memory generalization replaces two state neurons neurons taking richer set values two classes neuron input output relations developed convergence stable states st class continuous relations second class allowed quantization rules neurons information capacity networks second class found order bits network neurons generalization sum outer products learning rule developed investigated well american institute physics
computer program has been designed implemented allow analyze oscillatory behavior simulated neural networks cyclic connectivity computer program implemented instruments system results numerous experiments discussed program cycles allows user construct operate neural networks containing cyclic connection paths aid powerful interface numerous cycles have been studied including cycles more activation points non cycles cycles variable path lengths interacting cycles final class interacting cycles important due its ability implement time dependent goal processing neural networks
describe method constructing higher order neural networks respond under geometric transformations input space requiring each unit satisfy set constraints interconnection weights particular structure imposed network network built using such architecture maintains its invariant performance independent values weights assume learning rules used form nonlinearities network invariance exhibited network usually trivial sort eg responding average input case translation invariance whereas higher order networks perform useful functions still exhibit invariance derive weight constraints translation rotation scale several combinations these transformations report results simulation studies
information retrieval neural network viewed procedure network computes most probable map estimate unknown information viewpoint allows class probability distributions neural network acquire explicitly specified learning algorithms neural network search most probable member designed statistical tests decide true environmental probability distribution developed example applications theory highly nonlinear back propagation learning algorithm networks hop field anderson discussed
process sensory data sensory brain areas preserve information about both similarities differences among learned cues without latter would lost whereas without former degraded versions cue would thought distinct cues would recognized have constructed model cortex incorporating large number biophysical anatomical physiological parameters such two step excitatory firing thresholds necessary conditions long term potentiation ltp synapses three distinct types inhibitory currents short long currents long ahp sparse connectivity between bulb layer ii cortex excitatory fibers nonlinear dendritic summation etc have tested model its ability learn difference preserving encodings incoming sensory cues biological characteristics model enable produce multiple encodings each input cue such way different cell firing activity model preserve both similarity difference particular probabilistic al release properties pitiform synapses give rise probabilistic postsynaptic voltage levels combination activity local patches inhibitory interneurons layer select rs single layer ii cells time locked firing rhythm enables distinct spatial patterns read out against relatively background firing rate trials using physiological rules induction ltp yield stable layer ii cell spatial firing patterns learned cues multiple simulated olfactory input patterns ie those share many chemical features give rise strongly overlapping bulb firing patterns activating many shared lateral olfactory lot axons layer pitiform cortex turn yields highly overlapping layer cell excitatory potentials enabling spatial layer ii cell encoding preserve overlap similarity among similar inputs same time those synapses enhanced learning process cause stronger cell firing yielding strong cell specific ahp currents local inhibitory interneurons effectively select alternate cells fire once strongly firing cells have ahp these alternate cells activate their recurrent activating distinct populations synapses layer ib potentiation these synapses combination those still active lot axons selectively enhance response cells tend differences among even similar cues empirical tests computer simulation have shown after training initial spatial layer ii cell firing responses similar cues enhance similarity cues such overlap response equal greater than overlap research supported part under grants national science foundation under grant american institute physics input cell firing bulb eg two cues overlap give rise response patterns overlap more later cell firing patterns after ahp increasingly enhance differences among even similar patterns so cues input overlap give rise output responses overlap less than difference enhancing response measured respect its input overlaps reduced near zero response overlaps enables structure distinguish between even similar cues other hand similarity enhancing response properly viewed partitioning mechanism mapping quite distinct input cues onto nearly identical response patterns category therefore use statistical metric information value measure value produced pitiform simulation network
efficient realization using current silicon technology large connection networks more than billion connections requires these networks exhibit high degree communication real neural networks exhibit significant locality yet most network models have little paper connectivity requirements simple associative network analyzed using communication theory several techniques based communication theory presented improve robustness network face sparse local structures discussed potential problems information distributed widely
many connectionist learning models implemented using gradient descent least squares error function output teacher signal present model generalizes particular back propagation using power metrics small rs city block error metric approximated large rs maximum metric approached while standard backpropagation model results implementation back propagation described several experiments done show different values may desirable various purposes different values may appropriate reduction effects outliers noise modeling input space more compact clusters modeling statistics particular domain more naturally way may more meaningful eg speech vision
describe new learning procedure networks contain groups nonlinear units closed loop aim learning discover codes allow activity vectors visible group represented activity vectors hidden group way test whether code accurate representation try reconstruct visible vector hidden vector difference between original reconstructed visible vectors called reconstruction error learning procedure aims minimize error learning procedure has two passes first pass original visible vector passed around loop second pass average original vector reconstructed vector passed around loop learning procedure changes each weight amount proportional product presynaptic activity difference post synaptic activity two passes procedure much simpler implement than methods like back propagation simulations simple networks show usually converges rapidly good set codes analysis shows certain restricted cases performs gradient descent squared reconstruction error
paper outlines schema movement control based two stages signal processing higher stage neural network model treats cerebellum array adjustable motor pattern generators network uses sensory input trigger elemental pattern generators evaluate their performance actual outputs produced intrinsic circuitry includes recurrent loops thus capable self sustained activity these outputs motor commands local feedback systems called motor latter control forces lengths individual muscles overall control thus achieved two stages adaptive cerebellar network generates array feedforward motor commands set local feedback systems translates these commands into actual movements
describe two optical neural computing first closed optical feedback loop used implement auto associative image recall second perceptron like learning algorithm implemented
previous work nets continuous valued inputs led generative procedures construct convex decision regions two layer percepttons hidden layer arbitrary decision regions three layer percepttons two hidden layers here demonstrate two layer perceptton classifiers trained back propagation form both convex disjoint decision regions such classifiers robust train rapidly provide good performance simple decision regions complex decision regions required convergence time long performance often better than nearest neighbor classifiers three neural net classifiers presented provide more rapid training under such situations two use fixed weights first two layers similar classifiers estimate probability density functions using histograms third feature map classifier uses both unsupervised supervised training provides good performance little supervised training situations such speech recognition much unlabeled training data available architecture classifier used implement neural net nearest neighbor classifier
inverse matrix calculation considered optimization have demonstrated problem rapidly solved highly interconnected simple neuron like analog processors network matrix inversion based concept neural network designed implemented electronic hardware modifications network readily applicable solving linear simultaneous equation efficiently features circuit potential speed due parallel processing robustness against variations device parameters
use highly developed system localize track acquire their environment neural organization system illustrates well importance four fundamental ingredients representation architecture search knowledge intelligent system addition design architecture illustrates goal directed system effectively utilizes feedback its environment anatomical analysis neural networks involved target tracking indicated neurons within region brainstem connections between sensory inputs motor outputs spinal cord analysis suggested these neurons integrate selective temporal patterns sensory input through rapidly adapting type peripheral filter responding continuously increasing stimulus concentration connectivity response patterns cells nature peripheral response suggest unique function cells may enable continuously track stimulus source once its has been computed
information capacity sparse distributed memory sdm hopfield type neural networks investigated under approximations used here shown total information stored these systems proportional number connections network constant same sdm hopfield type models independent particular model order model approximations numerically same analysis used show sdm store sequences spatiotemporal patterns addition time delayed connections allows retrieval context dependent temporal patterns minor modification sdm used store correlated patterns
recently many modifications model have been proposed both learning occur given network never saturates function effectively due information learning updates continue these networks need introduce performance measures addition information capacity evaluate different networks mathematically define quantities such plasticity network efficacy information vector probability network saturation these quantities analytically compare different networks
great interest abilities neural networks mimic qualitative neural symbols less work has been performed using neural networks process floating point sometimes stated neural networks inherently inaccurate therefore best suited reasoning nevertheless potential speed massively parallel operations make neural net number interesting topic explore paper discuss our work demonstrate certain applications neural networks achieve significantly higher numerical accuracy than more conventional techniques particular prediction future values chaotic time series performed high accuracy analyze neural net able do process show large class functions may accurately approximated backpropagation neural net two network uses functional approximation perform either interpolation signal processing applications extrapolation symbol processing applications neural nets therefore use quite familiar methods perform their tasks geometrical viewpoint here seems useful approach analyzing neural network operation relates neural networks well studied topics functional approximation
new distributed neural information processing model proposed explain response characteristics vestibulo ocular system reflect more accurately anatomical neurophysiological data afferent fibers nuclei model head motion hair cells hair cell signals processed multiple synapses primary afferent neurons exhibit continuum varying dynamics model application concept neural networks description findings nerve allows us formulate mathematically behavior assembly neurons whose physiological characteristics vary according their anatomical properties
general formulae mapping optimization problems into systems ordinary differential equations associated artificial neural networks presented comparison made optimization using gradient search methods performance measure settling time initial state target state simple analytical example illustrates situation dynamical systems representing artificial neural network methods would faster than those representing settling time investigated more complicated optimization problem using computer simulations problem simplified version problem medical imaging determining cerebral activity measurements scalp simulations showed gradient based systems typically times faster than systems based current neural network optimization methods
information theoretic optimization principle proposed development each processing stage perceptual network principle maximum information preservation states signal transformation realized each stage maximizes information output signal values stage convey about input signals values stage subject certain constraints presence processing noise quantity being maximized shannon information rate provide motivation principle simple model cases derive its consequences discuss algorithmic implementation show principle may lead biologically relevant neural architectural features such topographic maps map distortions orientation selectivity extraction spatial temporal signal correlations possible connection between information theoretic principle principle minimum entropy production suggested
synchronous discrete model average memory capacity associative memories compared hopfield memories means calculation percentage good recall random dimension different numbers stored vectors memory capacity found much smaller than upper bound two dimensions average has about capacity corresponding hopfield memory same number neurons orthonormal coding increases effective storage capacity memory capacity limitations due spurious stable states arise much same way hopfield memories occurrence spurious stable states avoided replacing another nonlinear process here called dominant label selection simplest winner take net gives fault sensitive memory fault tolerance improved use orthogonal transformation optical application latter fourier transform implemented simply
has been interest neural like processing systems example two parallel distributed processing discussed parallel distributed systems connectionist models neural nets value passing systems multiple context systems symbolic manipulation paradigms artificial intelligence seems partly responsible attention promise massively parallel systems implemented hardware paper relates simple neural like systems based multiple context other well known namely production systems length sequence prediction finite state machines machines presents earlier sequence prediction results new light
paper discuss why special purpose chips needed useful implementations connectionist neural networks such applications pattern recognition classification three chip designs described hybrid programmable connection matrix analog connection matrix adjustable connection strengths digital best match chip common feature designs distribution arithmetic processing power data storage minimize data movement ams distributed computation chip jj conventional node complexity transistors figure graph addressable node complexity size conventional computer chips memories contain simple nodes each few transistors but processing power cpu chips essentially complex node neural network chips distributed computation region chips contain many simple fixed processors local data storage after american institute physics
have studied attraction fixed point oscillatory attractors electronic analog neural network basin measurement circuitry network feedback loop scanned initial conditions examines resulting attractor fixed points memories show associative memory network leads irregular basin shapes network includes analog time delay circuitry have shown delay symmetric networks introduce oscillatory attractors conditions leading oscillation related presence reducing connections stabilize delay network
class neural networks whose performance analyzed signal space environment alternating projection neural networks perform projecting between two more constraint sets criteria desired unique convergence easily established network configured either homogeneous layered form number patterns stored network order number input hidden neurons output neurons take two states trained layered easily configured converge iteration more generally convergence exponential rate convergence improved use sigmoid type nonlinearities network relaxation andor increasing number neurons hidden layer manner network responds data specifically trained ie generalizes directly evaluated analytically
present paper survey utilize results qualitative theory large scale interconnected dynamical systems order develop qualitative theory hop field model neural networks our approach view such networks interconnection many single neurons our results terms qualitative properties individual neurons terms properties structure neural networks aspects neural networks address include asymptotic stability exponential stability instability equilibrium estimates trajectory bounds estimates domain attraction asymptotically stable equilibrium stability neural networks under structural perturbations
binary synaptic matrix chip has been developed electronic neural networks matrix chip contains programmable array long channel binary connection elements implemented cmos process neurons kept synaptic chip serves building block multi chip synaptic network large size alternative programmable long channel connection elements series switches cmos test chips obtain weak synaptic connections although require additional processing steps they promise substantial savings area performance synaptic chip neuron system associative memory test application discussed
bit serial vlsi neural network described initial architecture synapse array through silicon layout board design issues surrounding bit serial computation arithmetic discussed parallel development hybrid neural network learning recall capabilities reported bit serial network along projected specification neuron bit serial board operating mhz technique extended synapses network update time ms using technique time calculations through synapse array
novel network type introduced uses unit length vectors local variables example its applications associative memory nets defined their performance analyzed real systems corresponding such models eg neurobiological networks limit cycle oscillators optical have their feedback path
have developed neural network consists interconnected grossberg center off surround used optimize function related log likelihood function decoding convolutional codes more general signal deconvolution problems connections network confined neighboring representative types networks themselves vlsi implementation analytical experimental results convergence stability network have been found structure network used distributed representation data items while allowing fault tolerance replacement units
general method deriving backpropagation algorithms networks recurrent higher order networks introduced propagation activation these networks determined differential equations error signal backpropagated integrating associated differential equation method introduced applying recurrent generalization feedforward backpropagation network method extended case higher order networks constrained dynamical system training content addressable memory essential feature adaptive algorithms adaptive equation has simple outer product form preliminary experiments suggest learning occur rapidly networks recurrent connections continuous formalism makes new approach more suitable implementation vlsi
many optimization models neural networks need constraints restrict space outputs subspace satisfies external criteria using energy methods yield forces act upon state neural network penalty method quadratic energy constraints added existing optimization energy has become popular recently but guaranteed satisfy constraint conditions other forces neural model multiple constraints paper present basic differential multiplier method satisfies constraints exactly create forces gradually apply constraints over time using neurons estimate lagrange multipliers basic differential multiplier method differential version method multipliers numerical analysis prove differential equations locally converge constrained minimum examples applications differential method multipliers include enforcing analog decoding problem enforcing valid traveling salesman problem
error propagation nets have been shown able learn variety tasks static input pattern mapped onto static output pattern paper presents generalisation these nets deal time varying patterns three possible architectures explored example nets applied problem speech coding time sequence speech data coded net decoded another use dynamic nets gives better signal noise ratio than achieved using static nets
coarse coded symbol memories have several neural network symbol processing models order determine these models would scale first have understanding coarse coded representations define general structure coarse coded symbol memories derive mathematical relationships among their essential parameters size set size computed capacity schemes agrees well actual measurements coarse coded working memory touretzky distributed connectionist production system
study distributed memory systems has produced number models work well limited domains until recently application such systems realworld problems has been difficult because storage limitations their inherent architectural serial simulation computational complexity recent development memories unrestricted storage capacity feedforward architectures has way application such systems complex pattern recognition problems such problems sometimes features describe environment thus significant portion pattern environment often non separable review current work high density memory systems their network implementations discuss general learning algorithm such high density memories review its application separable point sets finally introduce extension method learning probability distributions non separable point sets
have developed methodology manually training autonomous control systems based artificial neural systems applications rule set governing experts decisions difficult formulate used extract rules information expert receives actions he takes properly constructed networks imitate rules behavior permits them function autonomously they trained spanning set possible situations training provided manually either under direct supervision system indirectly using background mode network training data expert performs his day day tasks demonstrate these methods have trained network drive vehicle through simulated traffic
ability obtain three dimensional structure visual motion important survival human non human primates using parallel processing model current work explores biological visual system might solve problem might go about understanding solution
self organization multi layered networks realized time sequential organization successive neural layers lateral inhibition operating surround firing cells each layer provides unsupervised capture excitation patterns presented previous layer presenting patterns increasing complexity co network higher levels hierarchy capture concepts implicit pattern set
general method tensor product representation described distributed representation method allows fully distributed representation symbolic structures roles structures well those roles arbitrarily non local fully partially localized special cases reduce existing cases connectionist representations structured data tensor product representation generalizes these few existing examples fully distributed representations structures representation saturates larger structures represented permits recursive construction complex representations simpler ones independence capacities generate maintain multiple parallel extends naturally continuous structures continuous representational patterns permits values serve variables enables analysis interference symbolic structures stored associative memories leads characterization optimal distributed representations roles algorithm learning them
aim paper explore spatial organization neural networks under markovian assumptions what concerns behaviour individual cells interconnection mechanism properties neural nets relevant image modeling pattern analysis spatial computations stochastic two dimensional image fields involved first approach develop random neural network model based upon simple probabilistic assumptions whose organization studied means discrete event simulation investigate possibility approximating random networks behaviour using analytical approach theory general product form networks neural network described open network nodes moving node node represent connections between nodes expressed terms suitably selected routing probabilities obtain solution model under different affecting time stimulation each node visited results concerning distribution excitation network function network topology external stimulation arrival pattern compared measures obtained simulation validate approach followed
recognizing patterns temporal context important such tasks speech recognition motion detection signature verification propose architecture time serves its own representation temporal context encoded state nodes contrast approach portions architecture represent time example these ideas demonstrate architecture inputs temporal feature detectors otherwise standard back propagation model experiments involving motion detection word discrimination serve illustrate novel features system finally discuss possible extensions architecture
propose new scheme construct neural networks classify patterns new scheme has several novel features focus attention important attributes patterns ranking order extract most important ones first less important ones later training use information measure instead error function multi perceptron like architecture formed decision made according tree structure learned attributes new scheme expected self organize perform well large scale problems american institute physics
efficient method self organizing associative databases proposed together applications robot systems proposed databases cn associate ny input output first half discussion algorithm self organization proposed aspect hardware produces new style neural network latter half part handwritten letter recognition autonomous mobile robot system demonstrated
networks simple analog processors having neuron like properties have been employed compute good solutions variety optimization problems paper presents neural net solution resource allocation problem arises providing local access wide area communication network problem described terms energy function mapped onto analog computational network simulation results characterizing performance neural computation presented
increasing number patients using after sound detected through electrical stimulation remaining peripheral auditory nervous system although great progress has been achieved area useful speech recognition has been attained either single multiple channel coding evidence suggests necessary would effectively natural speech perception system simulate temporal other phenomena found natural receptors currently implemented end presented here computational model using artificial neural networks ann incorporate natural phenomena artificial cochlear ann model presents series advantages implementation such systems first hardware requirements constraints power size processing speeds taken into account together development software before actual neural structures defined second ann model abstraction natural neurons carries necessary ingredients close mapping implementing necessary functions third processing like sorting majority functions could implemented more efficiently requiring local decisions fourth ann model allows function modifications through parametric modification software permits variety fine tuning experiments patients those permit user freedom system modification real time allowing finer more subjective fit differences condition operation individuals remaining peripheral auditory system
family neuromorphic networks specifically designed communications optical signal processing applications presented information encoded utilizing sparse optical orthogonal code sequences basis binary signals generalized synaptic connectivity matrix binary values addition high capacity associative memory resulting neural networks used implement general functions such code filtering code mapping code joining code code projecting
paper presents artificial neural network concept oscillator networks individual form point processes constitute form information transmitted between joining neurons type communication assumed most other models typically continuous discrete value passing networks limiting messages received each processing unit time markers signal firing other units presents significant implementation advantages our model neurons fire absence perturbation interaction present scheduled advanced delayed firing neighboring neurons networks such neurons become global oscillators exhibit multiple attractors arbitrary initial states energy minimization learning procedures make network converge oscillatory modes satisfy multi dimensional constraints such networks directly represent routing scheduling problems consist ordering sequences events
paper describes approach dimensional object recognition complex log mapping combined distributed associative memory create system recognizes objects regardless changes rotation scale information database used classify object reconstruct version object estimate magnitude changes scale rotation system response moderate amounts noise occlusion several experiments using real gray scale images presented show feasibility our approach
paper presents model adaptive automata constructed simpler adaptive information processing elements first half paper describes model second half discusses its significant adaptive properties using computer simulation examples among these properties network model elements adapt appropriately single reinforcement channel provides same positive negative reinforcement signal adaptive elements network same time holds multiple input multiple output multiple layered sequential networks holds network elements hidden their outputs directly seen external environment
potential adaptive networks learn categorization rules model human performance studied comparing natural artificial systems respond new inputs ie they generalize like humans networks learn deterministic categorization task variety alternative individual solutions analysis constraints imposed using networks minimal number hidden units shows minimal configuration sufficient explain predict human performance few solutions found shared both humans minimal adaptive networks further analysis human network generalizations indicates initial conditions may provide important constraints generalization new technique call learning described finding appropriate initial conditions
propose optimality principle training unsupervised feedforward neural network based upon maximal ability reconstruct input data network outputs describe algorithm used train either linear nonlinear networks certain types nonlinearity examples applications problems image coding feature detection analysis stereograms presented
reinforcement based connectionist architecture learns associative maps continuous multidimensional environments discovered locations positive negative recorded do subnetworks respectively outputs subnetworks relevant current goal combined compared current location produce error vector vector backpropagated through motor perceptual mapping network produce action vector leads system towards do locations away locations demonstrated simulated robot posed target task
class fast supervised learning algorithms presented they use local representations multiple scales resolution approximate functions piece wise continuous inspired cmac model algorithms learn orders magnitude more rapidly than typical implementations back propagation while often achieving comparable generalization furthermore unlike most traditional function approximation methods algorithms well suited use real time adaptive signal processing unlike simpler adaptive systems such linear predictive coding adaptive linear kalman filter new algorithms capable efficiently capturing structure complicated non linear systems illustration algorithm applied prediction chaotic timeseries
optimization techniques applied problem learning feedforward neural networks addition having superior convergence properties optimization techniques such method significantly more efficient than backpropagation algorithm these results based experiments performed small boolean learning problems noisy real valued learning problem hand written character recognition
classifier systems machine learning systems incorporating genetic algorithm learning mechanism although they respond inputs neural networks respond their structure representation learning mechanisms differ those employed neural network researchers same sorts domains result might conclude these two types machine learning intrinsically different two papers taken together prove instead classifier systems neural networks equivalent paper half equivalence demonstrated through description transformation procedure map classifier systems into neural networks isomorphic behavior several commonly used paradigms employed neural network researchers required order make transformation work these noted their appropriateness discussed paper discussion practical these results comments their
work introduces new method called self organizing neural network algorithm demonstrates its use system identification task algorithm constructs network chooses neuron functions adjusts weights compared back propagation algorithm identification chaotic time series results shows constructs simpler more accurate model requiring less training data epochs algorithm applied generalized classifier
introduce learning algorithm multilayer neural networks composed binary linear threshold elements whereas existing algorithms reduce learning process minimizing cost function over weights our method treats internal representations fundamental entities determined once correct set internal representations arrived weights found local biologically plausible perceptton learning rule tested our learning algorithm four problems symmetry parity combined symmetry parity
address question network expected generalize random training examples chosen arbitrary probability distribution assuming future test examples drawn same distribution among our results following bounds appropriate sample rs network size assume show random examples feedforward network linear threshold functions nodes weights so least fraction examples correctly classified has confidence certainty network correctly classify fraction future test examples drawn same distribution conversely fully connected feedforward nets hidden layer learning algorithm using fewer than random training examples distributions examples consistent appropriate weight choice fail least fixed fraction time find weight choice correctly classify more than fraction future test examples
new training paradigm called paradigm introduced tasks network learn choose preferred pattern set alternatives based examples human expert preferences paradigm input network consists two alternatives trained experts better paradigm applied learning backgammon board game expert selects move set legal moves comparison training much higher levels performance achieved networks much smaller coding schemes arc much simpler easier understand furthermore possible set up network so always produces consistent rank
paper proposes means using knowledge network determine functionality relevance individual units both purpose understanding networks behavior improving its performance basic idea iteratively train network certain performance criterion compute measure relevance identifies input hidden units most critical performance automatically least relevant units technique used simplify networks eliminating units convey redundant information improve learning performance first learning hidden units ones away thereby generalization understand behavior networks terms minimal rules
concept stochastic boltzmann machine bm attractive decision making pattern classification purposes probability attaining network states function network energy hence probability attaining particular energy minima may associated probabilities making certain decisions because ill stochastic nature complexity bm fairly high therefore such networks likely used practice paper suggest way drawback bm into deterministic network call boltzmann network functionally equivalent bm but has feed forward structure low complexity annealing required conditions under such conversion feasible given learning algorithm based conjugate gradient method provided somewhat akin backpropagation algorithm
nonlinearity required before matched filtering minimum error receivers additive noise present highly non gaussian experiments performed determine whether correct clipping nonlinearity could provided single input multi layer perceptron trained back propagation found multi layer perceptron input output node nodes first hidden layer nodes second hidden layer could trained provide clipping nonlinearity fewer than noiseless corrupted waveform samples network trained relatively high signal noise ratio used front end linear matched filter detector greatly reduced probability error clipping nonlinearity formed network similar used current receivers designed noise provided similar substantial improvements performance
large fraction recent work artificial neural nets uses multilayer perceptrons trained back propagation algorithm described rumelhart et al algorithm converges slowly large complex problems such speech recognition thousands iterations may needed convergence even small data sets paper show training multilayer perceptrons identification problem nonlinear dynamic system solved using extended kalman algorithm although computationally complex kalman algorithm usually converges few iterations describe algorithm compare back propagation using twodimensional examples
paper provides systematic analysis recurrent backpropagation algorithm introducing number new results main limitation algorithm assumes convergence network stable fixed point order error signals show experiment eigenvalue analysis condition violated chaotic behavior avoided next examine advantages over standard backpropagation algorithm build stable fixed points corresponding input patterns makes appropriate tool content addressable memories many function learning inverse problems
issues scaling generalization have emerged key issues current studies supervised learning examples neural networks questions such many training patterns training cycles needed problem given size difficulty represent choose useful training exemplars considerable theoretical practical importance several intuitive rules have been obtained empirical studies but few rigorous results paper summarize study generalization simplest possible case perceptron networks learning linearly separable functions task chosen majority function ie return majority input units number useful properties find many aspects multilayer networks learning large difficult tasks simple domain concrete numerical results even analytic understanding achieved
improved learning paradigm offers significant reduction computation time during supervised phase described based extending role neuron plays artificial neural systems prior work has regarded neuron strictly passive non linear processing element synapse other hand primary source information processing knowledge retention work role neuron extended its adaptively learning phase temperature sigmoid function such parameter during learning both synaptic interconnection weights neuronal temperatures tm optimized so capture knowledge contained within training set method allows each neuron possess update its own characteristic local temperature algorithm has been applied logic type problems such xor parity problem resulting significant decrease required number training cycles
rumelhart has proposed method choosing minimal simple representations during learning back propagation networks approach used dynamically select number hidden units co construct representation appropriate problem thus improve generalization ability back propagation networks method rumelhart suggests involves adding penalty terms usual error function paper introduce minimal networks idea compare two possible biases weight search space these biases compared both simple counting problems speech recognition problem general constrained search does seem minimize number hidden units required expected increase local minima
paper addresses problem determining weights set linear filters model cells so maximize ensemble averaged information cells output values jointly convey about their input values given statistical properties ensemble input vectors quantity maximized shannon information rate equivalently average mutual information between input output several models role processing noise analyzed biological motivation considering them described simple models nearby input signal values space time correlated cells resulting optimization process include center surround cells cells sensitive temporal variations input signal
number learning models have recently been proposed involve calculations temporal differences derivatives continuous time models these models like most adaptive network models formulated terms frequency activation useful abstraction neuronal firing rates more precisely evaluate implications neuronal model may develop model discrete pulse coded information point out many functions properties neuronal processing learning may depend subtle ways pulse coded nature information coding transmission properties neuron systems compared formulations terms activation computing temporal derivatives differences proposed sutton both more stable easier more realistic pulse coded system these models terms pulse coding our motivation has been enable us draw further connections between real time behavioral models learning biological circuit models underlying learning memory
paper show neural networks speech recognition constructed modular fashion exploiting hidden structure previously trained phonetic networks performance resulting larger phonetic nets found good performance nets themselves approach avoids learning times would necessary larger networks allows incremental learning large time delay neural networks constructed incrementally applying these modular training techniques achieved recognition performance
preliminary results speaker speech recognition reported method combines neural networks speech recognition used build recognition systems transient sounds property extractors variable resolution time frequency domains used speech model human auditory system preferred fft front end module
propose new neural network model its learning algorithm proposed neural network consists four layers input hidden output final output layers hidden output layers multiple using proposed pattern information cooperative learning algorithm possible learn analog data accurately obtain smooth outputs using neural network have developed speech production system consisting phonemic symbol production speech parameter production have producing natural speech waves high accuracy
connectionist model uses temporal information speech signal recognition classifies rates directions formant center transitions uses adaptive method associate transition events each system uses explicit spatial temporal representations through delay lines uses implicit parametric temporal representations formant transition classification through node activation onset decay transition delays sub networks analogous visual motion detector cells recognizes six consonant vowel tested unseen data recognizes its training
space environment laboratory has university construct small expert system forecasting called performed well human have constructed three layer back propagation connectionist network learns well does success suggests connectionist network perform task knowledge engineering automatically study internal representations constructed network may give insights reasoning processes human brain
discuss paper architectures probabilistic rule bases parallel manner using theoretical basis recently introduced information theoretic models begin describing our non neural learning algorithm theory quantitative rule modelling followed discussion exact nature two particular models finally work through example our approach going database rules inference network compare networks performance theoretical limits specific problems
application neural networks spread spectrum signals multiple access environment considered study motivated large part fact system conventional matched filter receiver suffers severe performance degradation relative interfering signals become large near far problem furthermore optimum receiver near far problem complex practical use receivers based multi layer percepttons considered simple robust alternative optimum solution optimum receiver used benchmark performance neural net receiver particular proven identifying decision regions neural networks back propagation algorithm modified version used train neural net importance sampling technique introduced reduce number simulations necessary evaluate performance neural nets examples considered proposed neural net receiver significantly outperforms conventional receiver
study evaluates performance multilayer perceptron frequency sensitive competitive learning network identifying five commercial aircraft radar measurements performance neural network classifiers compared nearest neighbor maximum likelihood classifiers our results indicate problem neural network classifiers relatively insensitive changes network topology noise level training data while problem traditional algorithms outperform these simple neural classifiers neural networks show potential improved performance
new class neural network aimed early visual processing described call neural analog diffusion enhancement layer network consists two levels coupled through feedback connections lower level two dimensional diffusion map accepts visual features input activity over larger scales function time upper layer fed activity diffusion layer local maxima extreme form contrast enhancement using network local these local maxima fed back diffusion layer using surround anatomy maxima available output network network dynamics serves cluster features multiple scales function time used variety early visual processing tasks such extraction high curvature points along edge line end detection gap filling generation points perceptual grouping multiple scales correspondence path long range apparent motion building shape representations invariant location orientation scale small deformation visual field
propose parallel network simple processors find color boundaries spatial changes illumination spread uniform colors within marked regions
alvinn autonomous vehicle neural network layer back propagation network designed task road following fly alvinn takes images camera laser range input produces output direction vehicle should travel order follow road training has been conducted using simulated road images successful tests autonomous navigation test vehicle indicate network effectively follow real under certain field conditions developed perform task differs dramatically network trained under various conditions suggesting possibility novel adaptive autonomous navigation system capable its processing conditions hand
currently most complex determination control tasks ultimately governed ground based systems conventional board systems face severe computational introduced serial operating inherently parallel problems new computer architectures based anatomy human brain seem promise high speed fault tolerant solutions limitations serial processing paper discusses applications artificial neural networks problem star pattern recognition determination
neural network applied problem recognizing kanji characters using ac propagation network learning algorithm feed forward network trained recognize similar handwritten kanji characters addition two new methods utilized make training effective recognition accuracy higher than conventional methods analysis connection weights showed trained networks hierarchical structure kanji characters strategy trained networks makes high recognition accuracy possible our results suggest neural networks effective kanji character recognition
pool handwritten signatures used train neural network task deciding whether given signature network feedforward net binary image input hidden layer single unit output layer weights adjusted according backpropagation algorithm signatures into software program through use electronic camera binary signatures normalized centered performance examined function training set network structure best scores order true signature rejection false signature
murphy vision based kinematic controller path planner based connectionist architecture implemented video camera series robot arm layout sensory motor maps cerebral cortex internal representations consist four coarse coded populations simple units representing both static dynamic aspects sensory motor environment previously reported work murphy first learned direct kinematic model his camera arm system during period extended practice used mental model guide his hand visual targets murphy has been extended two ways first he now learns inverse differential kinematics his arm addition ordinary direct kinematics allows his hand directly towards visual target without need search secondly he now deals much more difficult problem reaching presence obstacles
computing inverse dynamics robot arm active area research control literature learn inverse dynamics training neural network measured response physical ann input network temporal window measured positions output vector train network data measured first two cmu direct drive arm ii moves through randomly generated sample pick place trajectories test generalization new trajectory compare its output measured physical arm network shown generalize root mean square deviation interpreted weights network terms velocity acceleration filters used conventional control theory
barn owl has representations space its midbrain used head so visual auditory stimuli centered visual field view present models computer simulations these structures address various problems construction map space auditory sensory problem driving motor system these maps compare results biological data
have previously developed simple mathematical model formation ocular dominance columns visual cortex model provides common framework variety activity dependent biological studied analytic computational results together now reveal following inputs specific each eye locally correlated their firing within arbor radius monocular cells robustly form organized intra cortical interactions into column broader correlations within each eye anti correlations between eyes create more purely monocular cortex positive correlation over arbor radius yields almost perfectly monocular cortex most features model understood analytically through decomposition into eigenfunctions linear stability analysis allows prediction widths other features biological parameters
modeling studies memory based neural networks both selective enhancement depression synaptic strengths required efficient storage information sejnowski kohonen bienenstock et al sejnowski tesauro have tested assumption hippocampus cortical structure brain involved long term memory brief high frequency activation excitatory synapses hippocampus produces increase synaptic strength known long term potentiation ltp last many ltp known hebbian requires simultaneous release neurotransmitter presynaptic terminals coupled postsynaptic et al miller et al mechanism persistent reduction synaptic strength could balance ltp has yet been demonstrated studied associative interactions between separate inputs onto same dendritic trees hippocampal pyramidal cells field ca found low frequency input itself does change synaptic strength either increase associative ltp decrease strength associative longterm depression ltd depending upon whether positively correlated second high frequency input ltp synaptic strength hebbian ltd elicited presynaptic firing postsynaptic sufficient block postsynaptic activity thus associative ltp associative ltd capable information contained covariance between separate converging hippocampal inputs present address neuroscience ny usa present address computational neurobiology laboratory institute box san ca usa covariance synaptic strengths hippocampus
olfactory bulb mammals discrimination mathematical model based anatomy described simulations produce hz modulated activity coherent across bulb observed field potentials decision states odor information here thought stable cycles rather than point stable states typical simpler neuro computing models analysis simulations show group coupled non linear oscillators responsible oscillatory activities determined odor input bulb appropriate inputs higher centers enhance sensitivity particular model provides framework understand transform between odor input output olfactory cortex
present new hypothesis cerebellum plays key role actively controlling acquisition sensory information nervous system paper explore idea examining function simple cerebellar related behavior vestibulo ocular reflex vor eye movements generated minimize image retina during rapid head movements considering system point view statistical estimation theory our results suggest transfer function vor often regarded static slowly feature system should actually continuously rapidly changed during head movements further suggest these changes under direct control cerebellar cortex propose experiments test hypothesis
weakly electric fish explores its environment generating electric fields detecting small perturbations fields resulting nearby objects accordingly fish detects objects basis sequence electric images whose temporal spatial properties depend timing electric its body position relative objects its environment interested investigating these utilize timing body position during exploration aid object discrimination have developed element simulation self generated electric fields so reconstruct electrosensory consequences body position electric timing fish paper describes finite element simulation system presents preliminary electric field measurements being used tune simulation
recently proposed model explain sensory maps could enhance resolution through arrangement broadly tuned receptors have extended model general case polynomial weighting schemes proved response function polynomial same order further demonstrated polynomials eigenfunctions system finally suggested biologically plausible mechanism sensory representation external stimuli resolution far inter receptor separation
new learning algorithm storage static periodic attractors biologically inspired recurrent analog neural networks introduced network nodes periodic attractors may stored programming network vector independent patterns stored stability patterns basin geometry rates convergence may controlled orthonormal patterns operation reduces kind periodic outer product allows additive incremental learning standing wave cycles may stored mimic kind spatial patterns appear neural activity olfactory bulb cortex during predict pattern recognition behavior conditioning experiments these attractors arise during through hopf bifurcation act decision point their selection input pattern
primate visual system learns recognize true direction pattern motion using local detectors capable detecting component motion orientation moving edge multilayer feedforward network model similar model presented input patterns each consisting randomly oriented contours moving particular direction input layer units component direction speed tuning curves similar those recorded neurons primate visual area project area mt network trained many such patterns until most weights proportion units second layer solve aperture problem eg show same direction tuning curve peak gratings pattern direction selective neurons first appear area mt
analyze mathematical model retinal selective cells based recent electrophysiological data show its computation motion direction robust against noise speed
have developed oriented general purpose simulation system facilitate modeling neural networks simulator implemented under windows designed support simulations many levels detail specifically intended use both applied network modeling simulation detailed realistic models examples current models developed under system include mammalian olfactory bulb cortex invertebrate central pattern generators well more abstract connectionist simulations
consider layer node input neural network whose nodes compute linear threshold functions their inputs show np complete decide whether exist weights thresholds three nodes network so produce output consistent given set training examples extend result other simple networks result suggests those looking perfect training algorithms cannot escape inherent computational difficulties considering simple regular networks suggests importance given training problem finding appropriate network input encoding problem left open problem extend our result nodes non linear functions such sigmoids
hidden markov models widely used automatic speech recognition they inherently incorporate sequential character speech signal statistically trained priori choice model topology limits their flexibility another drawback these models their weak discriminating power multilayer perceptrons now promising tools connectionist approach classification problems have already been successfully tested speech recognition problems sequential nature speech signal remains difficult handle kind machine paper discriminant hidden markov model defined shown particular multilayer perceptron contextual extra feedback input units considered general form such markov models
boltzmann machine has been introduced means perform global optimization multimodal objective functions using principles simulated annealing paper consider its utility spurious free content addressable memory provide bounds its performance context show exploit machines ability escape local minima order use constant temperature associative pattern retrieval noisy environments association rule creates sphere influence around each stored pattern used along machines dynamics match machines noisy input pre stored patterns spurious fixed points whose attraction recognized rule due machines finite probability escape state results apply boltzmann machine asynchronous net binary threshold elements model they provide network worst case best case bounds networks performance allow polynomial time tradeoff studies design parameters
describe recent results automatic development recursive distributed representations variable sized data structures work certain types ai style data structures now represented fixed width analog vectors simple inferences performed using type pattern associations neural networks another arises these representations become self similar limit once chaos many interesting new questions about representational basis intelligence emerge discussed
parsing learning massively parallel self tuning context free capable parsing sentences unbounded length mainly due its parse tree representation scheme system capable improving its parsing performance through presentation training examples
attempt explaining human inferencing spreading activation particularly structured connectionist paradigm has resulted building systems nodes perform inferencing examining patterns activation spread paper demonstrate simple network inferencing performed passing activation over weights learned distributed hm thus account provided explains relationship between structured distributed connectionist approaches
time delay response neurons network induce sustained oscillation chaos present stability criterion based local stability analysis prevent sustained oscillation symmetric delay networks show example chaotic dynamics non symmetric delay network
research artificial neural networks has generally homogeneous architectures nervous systems natural animals exhibit heterogeneity both their elements patterns interconnection heterogeneity crucial flexible generation behavior essential survival complex dynamic environment may provide powerful insights into design artificial neural networks paper describe heterogeneous neural network controlling walking simulated insect controller inspired neurobiological literature insect locomotion exhibits variety stable different speeds simply varying activity single cell adapt perturbations natural consequence its design
new optimization strategy mean field annealing presented its application map restoration noisy range images derived experimentally verified
research involves method finding global maxima constraint networks process but unlike most others annealing schedule temperature instead determined locally units each update thus processing done unit level two major practical benefits processing way processing continue bad areas network while good areas remain stable processing bad areas long constraints remain poorly satisfied ie does stop after number cycles result method avoids requiring externally determined annealing schedule but finds global maxima more quickly consistently than externally scheduled systems comparison boltzmann machine et al made finally implementation method computationally trivial
introduce optimization approach solving problems computer vision involve multiple levels abstraction our objective functions include compositional hierarchies cast vision problems graph matching problems formulate graph matching terms constrained optimization use analog neural networks perform optimization method applicable perceptual grouping model matching preliminary experimental results shown
distributed connectionist production system neural network complex dynamical properties visualizing energy its component modules leads better intuitive understanding model suggests ways its dynamics controlled order improve performance difficult cases
describe adaptive network tin learns transition function sequential system observations its behavior integrates two tin tin tin constructs state representations examples system behavior its dynamics main topics paper tin transition functions noisy state representations environmental data during training while operation produces sequences transitions response variations input dynamics both nets based adaptive resonance theory grossberg give results experiment tin learned behavior system recognizes strings even number ls
dept uc present simplified model human cochlea realized electrical elements simulation model shows retains four signal processing features whose importance argue basis engineering logic evolutionary evidence furthermore cochlea does model achieves massively parallel signal processing economic way means shared elements extracting what believe five essential features cochlea design useful front end filter process acoustic images obtain better understanding auditory system
describe pulse stream firing integrated circuits implement asynchronous analog neural networks synaptic weights stored dynamically weighting uses time division neural pulses neuron receiving neuron mos transistors their state act variable control time division thus achieved small synapse circuit cell vlsi chip set design uses xm cmos technology
paper describes cmos artificial neuron circuit directly derived voltage gated channel model neural membrane has low power dissipation small layout geometry principal behind work include high performance more accurate neuron need higher density practical neural network implementations
reconstructing surface sparse sensory data well known problem computer vision paper describes experimental analog vlsi chip smooth surface interpolation sparse depth data eight node network designed cmos successfully tested network minimizes second order energy surface circuit directly implements coupled model surface reconstruction addition chip provide gaussian like smoothing images
extremely compact analog fully parallel implementation class recurrent neural networks applicable wide variety based integration proposed while contrast enhancement data compression adaptation mean input intensity capabilities network well suited processing sensory information feature extraction content addressable memory system network global function thus achieve stable storage itself addition model readily function front end processor analog adaptive resonance circuit
digital two dimensional self feature maps presented method based subspace classification using technique weight vector approximation orthogonal projections produce network discussed over million effective binary weights applied ms using conventional details number image recognition tasks including character recognition object described
design fully analog version self organizing feature map neural network has been several parts design fabrication feature map algorithm modified accommodate circuit solutions various computations required performance effects measured simulating design part speech recognition system circuits included implement both activation computations weight learning external access analog weight values provided facilitate weight initialization testing static storage fully analog implementation requires order magnitude less area than comparable hybrid version developed earlier
have fabricated test chip cmos perform supervised learning manner similar boltzmann machine patterns presented per second chip learns solve xor problem few milliseconds have demonstrated capability do unsupervised competitive learning functions chip components examined performance assessed
discuss synthetic receptors sensing these based magnetic field sensors effect structures fabricated using standard cmos these receptors biased small detect presence objects sensor they detect magnitude direction magnetic field
automatic speech recognition artificial perception problem input raw continuous patterns symbols desired output may words meaning text symbolic most successful approach automatic speech recognition based stochastic models stochastic model theoretical system whose internal state output undergo series transformations governed probabilistic laws application speech recognition unknown patterns sound treated they outputs stochastic system information about classes patterns encoded structure these laws probabilities their operation most popular type sm known hidden markov model several reasons why sm approach has been so successful describe shape spectrum has principled way describing temporal order together variability both compatible hierarchical nature speech structure powerful algorithms decoding respect model recognition adapting model fit significant amounts example data learning theoretical mathematical foundations enable extensions smoothly eg many typical system speech signal first described sequence acoustic vectors spectrum cross sections equivalent rate say per second pattern assumed consist sequence segments corresponding discrete states model each segment acoustic vectors drawn distribution characteristic state but otherwise independent another states before after systems controlled relationship between states phones speech science but most properties notions speech assume important ignored most sm approaches pattern recognition theory level parameters models usually adjusted using baum welch re estimation method so maximise likelihood data given model right do form model actually appropriate data but parameter optimisation method needs concerned speech recognition discrimination between classes phonemes words hmm recognition algorithm designed find best explanation input terms model tracks scores plausible current states generator away explanations lead current state better explanation dynamic programming may away explanations lead current state much worse than best current state score pruning producing search method important keep many hypotheses hand particularly current input ambiguous connectionist neural network approaches start strong pre types process used they claim reference new theories cognitive processing actual mechanisms used usually simpler than those sm methods but mathematical theory what learnt computed instance more difficult particularly structures have been proposed dealing temporal structure connectionist approaches speech network whose inputs accept speech data would have internal state contains necessary information about past input output would accurate early could training networks their own dynamics particularly difficult especially unable specify what internal state should working methods training fixed points recurrent non networks has attempted train various types network full state feedback arrangement limits his recurrent connections self loops hidden output units but even so theory such recursive non linear filters other extreme systems treat whole time frequency amplitude array resulting initial acoustic analysis input network require label output example performance et al report multi speaker small vocabulary isolated word recognition tasks approach those best hmm techniques available same data invariance temporal position trained into network presenting patterns random positions fixed time window et al use powerful compromise arrangement thought either smaller networks across time spread network single small network internal delay lines time delay neural network recurrent links except trivial ones output so training using backpropagation great problem may think finite response non linear filter reported results consonant discrimination encouraging better than those hmm system same data system insensitive position virtue its construction kohonen has constructed demonstrated large vocabulary isolated word unrestricted vocabulary continuous speech transcription systems inspired neural network ideas but implemented algorithms more suitable current digital signal processor cpu chips kohonens map technique thought unsupervised adaptive constrained put its reference points non linear low dimensional sub space his learning vector technique used initial labeling combines advantages classic nearest neighbor method discriminant training among other types network have been applied speech interesting class based correlations weight vectors dot product but distances reference points radial basis function theory developed multi dimensional interpolation shown suitable many feed forward networks used advantage difficult find useful positions reference points define first non linear transformation followed linear output transformation weights found methods fast straightforward reference points adapted using methods based backpropagation related methods include potential functions kernel methods modified network much gained form careful comparison theory stochastic model neural network approaches speech recognition nn perform speech decoding way like sm algorithm have state states generarive model state include information about distribution possible generator states given pattern so far state transition function update distribution depending current speech input clear whether such internal representation behavior learned scratch otherwise unstructured recurrent network stochastic model based algorithms seem have edge present dealing temporal sequences discrimination based training inspired nn techniques may make significant difference performance would seem area nns have most offer finding non linear transformations data take us space perhaps related formant parameters comparisons more relevant phonetic decisions than purely auditory ones eg resulting transformation could viewed set feature detectors perhaps nn should posterior probabilities states sm directly art applying stochastic model neural network approach choose class models networks realistic enough likely able capture between speech sounds words instance yet have structure makes algorithms building detail models based examples interpreting particular unknown patterns future systems need exploit regularities described allow construction high performance systems large vocabularies their adaptation characteristics each new user speech recognition stochastic model based methods work best present but current systems generally far inferior humans even situations usefulness higher level processing minimal predict next generation systems based combination connectionist sm theory techniques speech knowledge used rather soft way decide structure should long before distinction have been making references theory stochastic processes pp
midbrain barn owl contains map like representation sound source direction used precisely head toward targets interest computed interaural difference sound level present models computer simulations two stages level difference processing qualitatively agree known anatomy physiology make several striking predictions
central pattern generator ganglion well defined biological neural network neuron network modulated many inputs these inputs network produce multiple output patterns three simple mechanisms determining cells active modulating synaptic efficacy changing intrinsic response properties individual neurons importance intrinsic response properties neurons network function modulation discussed
interneurons leech ganglia receive multiple sensory inputs make synaptic many motor neurons these hidden units coordinate several different behaviors used physiological anatomical constraints construct model local reflex dynamical networks trained experimentally derived input output patterns using recurrent back propagation units model modified include electrical synapses multiple synaptic time constants properties hidden units emerged simulations matched those leech model data support distributed rather than representations local reflex these results explain aspects local circuitry
traditional studying neural coding characterize known stimuli average neural responses face nearly opposite task decoding short segments spike train extract information about unknown time varying stimulus here present strategies characterizing neural code point view algorithms real time stimulus reconstruction based single sample spike train these methods applied design analysis experiments identified movement sensitive neuron fly visual system far know first instance direct reading neural code has been accomplished
most complex behaviors appear governed internal motivational states modify animals responses its environment therefore considerable interest understand neural basis these motivational states drawing upon work neural basis feeding have developed heterogeneous artificial neural network controlling feeding behavior simulated insect demonstrate feeding artificial insect shares many characteristics motivated behavior natural animals
brain represents surface topographic map somatosensory cortex map has been shown experimentally use dependent fashion throughout life present neural network simulation competitive dynamics underlying cortical plasticity detailed analysis receptive field properties model neurons during simulations cortical digit nerve section
well known neural responses particular brain regions spatially organized but general principles have been developed relate structure brain map nature associated computation parallel computers maps sort quite similar brain maps arise computation distributed across multiple processors paper discuss relationship between maps computations these computers suggest similar considerations might apply maps brain
generic model oscillating cortex assumes minimal coupling justified known anatomy shown function memory using previously developed theory network has explicit excitatory neurons local inhibitory feedback forms set nonlinear oscillators coupled long range connections using local hebb like learning rule primary higher order synapses ends long range connections system learns store kinds oscillation amplitude patterns observed olfactory visual cortex rule derived more general projection algorithm recurrent analog networks analytically guarantees content addressable memory storage continuous periodic sequences capacity fourier components node network spurious attractors
firing patterns populations cells cat visual cortex exhibit oscillatory responses range hz furthermore groups neurons many apart highly synchronized long cells have similar orientation tuning investigate two basic network architectures incorporate either nearest neighbor global feedback interactions conclude non local feedback plays fundamental role initial synchronization dynamic stability oscillations
has been known many years specific regions working cerebral cortex display periodic variations correlated cellular activity while olfactory system has been focus much work similar behavior has recently been observed primary visual cortex have developed models both olfactory visual cortex observed oscillatory properties these networks using these models have examined dependence oscillatory behavior single cell properties network architectures discuss idea oscillatory events recorded cerebral cortex may intrinsic architecture cerebral cortex whole these rhythmic patterns may important neuronal activity during sensory processing
outline computational model development specific eye brain circuits model comprises self organizing map forming network uses local hebb rules constrained molecular markers various simulations development maps fish described
level individual neurons release increases cells excitatory inhibitory inputs present model effects network neural like elements argue changes individual elements do affect their ability detect signal ignore noise same changes cell network such elements do improve signal detection performance network whole show result used computer simulation behavior account effect cns signal detection performance human subjects
study networks spiking neurons spikes poisson process state cell determined instantaneous firing rate limit high firing rates our model reduces studied hopfield find inclusion spiking results several new features such noise induced between off states cells probability currents usual description network dynamics terms energy surfaces taking account spikes allows us network parameters such synaptic weights against experiments real synapses realistic forms post synaptic response network dynamics suggests novel dynamical learning mechanism
paper presents results simulation spatial relationship between inferior nucleus lateral rat cerebellum principal objective modeling effort resolve apparent between proposed organization projections cerebellar cortex suggested anatomical experiments more organization apparent physiological mapping results suggest several unique features circuit may contribute appearance organization using anatomical techniques but detailed patterns projections seen physiological techniques more accurate representation afferent organization region cortex
mammalian visual cortex orientation selective simple cells detect straight lines may adapted detect curved lines instead test biologically plausible hebbian single neuron model learns oriented receptive fields upon exposure unstructured noise input maintains orientation selectivity upon exposure edges bars orientations positions model learn arc shaped receptive fields upon exposure environment thus new experiments try induce curved receptive field may provide insight into plasticity simple cells model suggests cells single spatial frequency may induce more striking spatial frequency orientation dependent effects than observed
spiking neurons integrate threshold fire used study transmission frequency modulated signals through layered networks firing correlations between cells input layer found modulate transmission signals under certain dynamical conditions level activity providing each cell source synaptic input average membrane produced synaptic input sufficiently below threshold firing correlations between cells input layer could greatly signal present subsequent layers sufficiently close threshold firing synchrony between cells initial layers could longer effect propagation signals latter case fire neurons could effectively modeled simpler analog elements governed linear input output relation
inputoutput properties compartment model neuron systematically explored taken work model neuron contain several active conductances including potassium conductance dendritic compartment driven accumulation intradendritic calcium dynamics conductances potentials governed set coupled first order differential equations integrated numerically set internal parameters model conductance rate constants time constants thresholds etc study parameter sensitivity set trials run input driving neuron kept fixed while each internal parameter varied others left fixed study inputoutput relation input square wave varied frequency magnitude while internal parameters system left fixed resulting output firing rate rate inputoutput relation model neuron studied turns out much more sensitive modulation certain dendritic potassium current parameters than plasticity synapse efficacy per amount current due synapse activation would turn suggest has been recently observed experimentally potassium current may more important focus neural plasticity than synaptic efficacy
analytic solutions information theoretic evolution equation connection strength three layer feedforward neural net visual information processing presented results receptive fields feature cells correspond eigenvector maximum eigenvalue integral equation first kind derived evolution equation connection strength symmetry breaking mechanism parity violation has been identified responsible changes morphology receptive field conditions formation different explicitly identified
eight neural net conventional pattern classifiers gaussian nearest neighbor standard back propagation adaptive back propagation feature map learning vector quantizer binary decision tree implemented serial computer compared using two speech recognition two artificial tasks error rates statistically equivalent almost tasks but classifiers orders magnitude memory requirements training time classification time ease nearest neighbor classifiers trained rapidly but required most memory tree classifiers provided rapid classification but complex adapt back propagation classifiers typically required long training times had intermediate memory requirements these results suggest classifier selection should often depend more practical considerations concerning memory computation resources restrictions training classification times than error rate work department air force air force office scientific research practical characteristics neural network
well known automatic learning algorithm applied fixed corpus data size corpus places upper bound number degrees freedom model contain generalize well because amount hardware neural network typically increases dimensionality its inputs challenging build high performance network classifying large input patterns paper several techniques addressing problem discussed context isolated word recognition task
developing phoneme based speaker dependent continuous speech recognition system embedding multilayer perceptton mlp ie feedforward artificial neural network into hidden markov model hmm approach shown mlps approximating maximum posteriori map probabilities could thus embedded emission probability estimator hmms using contextual information window input frames have been able improve frame phoneme performance over corresponding performance simple maximum likelihood ml even map probabilities estimated without benefit context recognition words continuous speech so simply improved use mlp several modifications original scheme necessary getting acceptable performance shown here word recognition performance simple discrete density hmm system appears somewhat better mlp methods used estimate emission probabilities
two approaches explored integrate neural net classifiers hidden markov model hmm speech both attempt improve speech pattern discrimination while retaining temporal processing advantages approach used neural nets provide second stage discrimination following recognizer small vocabulary task radial basis function rbf back propagation neural nets reduced error rate substantially rbf classifier larger vocabulary task neural net classifiers did reduce error rate they outperformed gaussian gaussian mixture neighbor knn classifiers another approach neural nets low level acoustic phonetic feature extractors classifying phonemes based single msec frames discriminant rbf neural net classifiers outperformed gaussian mixture classifiers performance little classifying phones accumulating scores across frames phonetic segments using single node recognizer work department air force air force office scientific research hmm speech recognition neural net discrimination sequence second stage classifier node averages viterbi segmentation figure second stage discrimination system recognition based scores each node second stage classifier adjust weights each node provide improved discrimination
present number time delay neural network tdnn based architectures multi speaker phoneme recognition use speech two four compare performance various architectures against baseline recognition rate single tdnn six speaker series modular designs leads highly modular multi network architecture capable performing six speaker recognition task speaker dependent rate addition its high recognition rate so called meta pi architecture learns without direct supervision recognize speech particular male speaker using internal models other male speakers
neural network approaches pattern recognition use discrimination based training method show once have modified output layer multilayer perceptron provide mathematically correct probability distributions replaced usual squared error criterion probability based score result equivalent maximum mutual information training has been used successfully improve performance hidden markov models speech recognition network specially constructed perform recognition computations given kind stochastic model based classifier obtain discrimination based training parameters models examples include hmm based word call
attempt combine neural networks knowledge speech science build speaker independent speech recognition system knowledge utilized designing preprocessing input coding output coding output supervision architectural constraints handle temporal aspect speech combine delays activations hidden output units input level back propagation sequences learning algorithm networks local self loops strategy demonstrated several experiments particular discrimination task application speech theory hypothesis dramatically improved generalization
effects parameter modifications imposed hardware constraints self organizing feature map algorithm examined performance measured error rate speech recognition system included algorithm part front end processing system parameters varied included weight connection strength quantization adaptation quantization distance measures circuit approximations include device characteristics process variability experiments using isolated word database speakers demonstrated degradation performance weight quantization below bits competitive nature algorithm constraints linearity makes excellent candidate fully analog circuit implementation prototype circuits have been fabricated characterized following constraints established through simulation efforts
acoustic speech recognition degrades presence noise compensatory information available visual speech signals around speakers mouth previous attempts using these visual speech signals improve automatic speech recognition systems have combined acoustic visual speech information symbolic level using heuristic rules paper demonstrate alternative approach visual acoustic speech information training feedforward neural networks map visual signal onto corresponding short term spectral amplitude envelope acoustic signal information directly combined degraded acoustic significant improvements demonstrated vowel recognition noise degraded acoustic signals these results compared performance humans well other pattern matching estimation algorithms
distinctive ecg patterns created heart normally present devices monitor ecg ecg signal make diagnosis based parameters author discusses use neural network classify ecg signals directly without parameterization input such network translation invariant distinctive features ecg may appear chosen ecg segment input insensitive patient patient variability rhythm pattern
paper describes neural network algorithm performs temporal pattern matching real time trained line single pass requires single template training each representative class continuously changes background noise deals transient signals having low noise ratios works presence non gaussian noise makes use context dependencies outputs bayesian probability estimates algorithm has been adapted problem passive signal detection classification runs connection machine correctly classifies within ms onset signals embedded noise subject considerable uncertainty
our effort develop modular neural system invariant learning recognition objects introduce here new module architecture called aspect network constructed around adaptive dendritic synapses builds upon our existing system processes shapes classifies them into view categories ie aspects invariant illumination position orientation scale projective deformations views aspect network learns transitions between these aspects graph like structure initially network object recognition emerges accumulating evidence over multiple views activate competing object hypotheses
describe model recognize two dimensional shapes image independent their orientation position scale model called traffic efficiently represents structural relation between object each its component features encoding fixed viewpoint invariant transformation features reference frame objects weights connectionist network using hierarchy such transformations increasing complexity features each successive layer network recognize multiple objects parallel implementation traffic described along experimental results demonstrating networks ability recognize viewpoint invariant manner
contour maps provide general method recognizing two dimensional shapes but images give rise such maps people good recognizing objects shapes them maps encoded easily long feature vectors suitable recognition associative memory these properties contour maps suggest role them early visual perception direction sensitive neurons visual cortex mammals supports view
have constructed two axis camera system roughly analogous single human eye artificial eye combines signals generated two rate motion information extracted visual analysis stabilize its camera process similar vestibulo ocular response vor vor eye learns system model incrementally modified adapt changes its structure performance environment eye example robust sensory system performs computations significant use mobile robots
achieve high rate image data compression while high quality reconstructed image good image model efficient way represent specific data each image introduced based physiological knowledge characteristics inhibitory interactions between them human visual system mathematically coherent parallel architecture image data compression utilizes markov random field image model interactions between vast number filter banks proposed
paper explores use model neural network motor learning taylor presented neural network designs do nearest neighbor lookup early paper their nearest neighbor network augmented local model network fits local model set nearest neighbors network design equivalent local regression network architecture represent smooth nonlinear functions yet has simple training rules single global optimum network has been used motor learning simulated arm simulated running machine
cmac storage scheme has been used basis software implementation associative memory system ams itself major part learning control loop major disadvantage concept degree local generalization area interpolation fixed paper deals algorithm self organizing variable generalization ams based ideas kohonen
performance sensitivity cmac network studied scenario faults introduced into adjustable weights after training has been accomplished found fault sensitivity reduced increased generalization loss weight faults considered but sensitivity increased weight faults
given set input output training samples describe procedure determining time sequence weights dynamic neural network model arbitrary input output process formulate input output mapping problem optimal control problem defining performance index minimized function time varying weights solve resulting nonlinear two point boundary value problem yields training rule performance index chosen rule turns out continuous time generalization outer product rule earlier suggested hopfield designing associative memories learning curves new technique presented
nonlinear neural framework called generalized hopfield network proposed able solve parallel distributed manner systems nonlinear equations method applied general nonlinear optimization problem demonstrate implementing three most important optimization algorithms namely augmented generalized reduced gradient successive quadratic programming methods study results dynamic view optimization problem offers straightforward model optimization computations thus significantly extending practical limits problems formulated optimization problem gain
present novel modular recurrent connectionist network architecture learns robustly perform incremental parsing complex sentences sequential input word time our networks learn do semantic role assignment noun phrase clause recognition sentences passive center embedded clauses networks make syntactic semantic predictions every point time previous predictions expectations violated arrival new our networks induce their own grammar rules dynamically transforming input sequence words into these networks generalize display tolerance input has been corrupted ways common spoken language
phonological structure human languages yet highly constrained through combination connectionist modeling linguistic analysis attempting develop computational basis nature phonology present connectionist architecture performs multiple simultaneous mutation operations sequences phonemes introduce novel additional primitive clustering clustering provides interesting alternative both iterative relaxation accounts processes such vowel harmony our resulting model efficient because processes entirely parallel using feed forward circuitry
order single layer recursive network easily learns deterministic finite state machine recognize regular grammars enhanced version neural net state machine connected through common error term external analog memory combination interpreted neural net automata neural net finite state machine given primitives pop able read top through gradient descent learning rule derived common error function hybrid network learns effectively use actions memory learn simple grammars
present application back propagation networks handwritten digit recognition minimal preprocessing data required but architecture network highly constrained specifically designed task input network consists normalized images isolated digits method has error rate about reject rate digits provided us service
propose new way construct large scale neural network handwritten kanji characters recognition neural network consists parts collection small scale networks trained individually small number kanji characters network integrates output small scale networks process facilitate integration these recognition rate total system comparable those small scale networks our results indicate proposed method effective constructing large scale network without loss recognition performance
order detect presence location domains amino acid sequences built system based neural network hidden layer trained back propagation program designed efficiently identify proteins exhibiting such domains characterized few localized regions low overall national research foundation new protein sequence database scanned evaluate programs performance obtained low rates false coupled moderate rate false positives
present two connectionist architectures chunking symbolic rewrite rules uses backpropagation learning other competitive learning although they developed chunking same sorts rules two differ their representational abilities learning behaviors
consider robot around performing actions sensing resulting environmental states robots task construct internal model its environment model allow predict consequences its actions what sequences actions take reach particular goal states schapire schapire have studied problem have designed symbolic algorithm explore infer structure finite state environments heart algorithm representation environment called update graph have developed connectionist implementation update graph using highly specialized network architecture back propagation learning exploration strategy choosing random tions network outperform schapire algorithm simple problems network has additional strength accommodate stochastic environments perhaps virtue suggests generalizations update graph representation do arise traditional symbolic perspective
present general systematic method neural network design based genetic algorithm technique works conjunction network learning rules addressing aspects networks architecture connectivity learning rule parameters networks optimized various criteria such learning speed generalization robustness connectivity approach model independent describe prototype system employs backpropagation learning rule experiments several small problems have been conducted each case has produced networks perform significantly better than randomly generated networks its initial population computational feasibility our approach discussed
sparse distributed memory sdm associative memory model based mathematical properties high dimensional binary address spaces genetic algorithms search technique high dimensional spaces inspired evolutionary processes dna genetic memory hybrid above two systems memory uses genetic algorithm dynamically its physical storage locations reflect correlations between stored addresses data example presented raw weather data genetic memory discovers specific features weather data well memory utilize information effectively architecture designed maximize ability system scale up handle real world problems
have developed static dynamic information layered neural network learning systems emphasis placed creating new make use spatial size information color applied these tools study back propagation learning simple boolean have obtained new insights into dynamics learning process
goal work has been identify neuronal elements cortical column most likely support learning nonlinear associative maps show particular style network learning algorithm based locally tuned receptive fields maps naturally onto cortical hardware gives coherence variety features cortical anatomy physiology whose relations learning remain poorly understood
paper present upper bounds learning rates hybrid models employ combination both self organized supervised learning using radial basis functions build receptive field representations hidden units learning performance such networks nearest neighbor heuristic improved upon multiplying individual receptive field widths suitable overlap factor present results indicating optimal values such overlap factors present new algorithm determining receptive field centers method more hidden units regions input space function output better learning number patterns hidden units small
neurons sum up their inputs non linear way simulations suggest distributed fine non linearity exploited during learning small sigmoids synapse dendritic tree up right areas their respective input spaces report show abstract highly nested tree quadratic transfer function associated each self using single global reinforcement scalar perform binary classification tasks procedure works well solving difficult phoneme classification task well back propagation does faster furthermore does calculate error gradient but uses statistical scheme build moving models reinforcement signal
methodology faster supervised learning dynamical nonlinear neural networks presented exploits concept operators enable computation changes networks response due perturbations system parameters using solution single set appropriately constructed linear equations lower bound speedup per learning iteration over conventional methods calculating neuromorphic energy gradient number neurons network
new form deterministic boltzmann machine learning procedure presented efficiently train network modules discriminate between input vectors according criterion new technique directly utilizes free energy these mean field modules represent probability criterion met free energy being readily manipulated learning procedure although conventional deterministic boltzmann learning fails extract higher order feature shift network bottleneck combining new mean field modules mutual information objective function rapidly produces modules perfectly extract important higher order feature without direct external supervision
new learning algorithm learning choice internal recently introduced whereas many algorithms reduce learning process minimizing cost function over weights our method treats internal representations fundamental entities determined algorithm applies search procedure space internal representations cooperative adaptation weights eg using perceptton learning rule
central problem connectionist modelling control network architectural resources during learning present approach weights reflect coarse prediction history coded distribution values parameterized mean standard deviation these weight distributions weight updates function both mean standard deviation each connection network vary function error signal stochastic delta rule consequently weights maintain information their central tendency their uncertainty prediction such information useful establishing policy concerning size complexity network growth new nodes example during problem solving present network undergo producing two nodes node measured its coefficient variation shown number benchmark problems networks find minimal architectures reduce computational complexity overall increase efficiency representation learning interaction member cognitive science laboratory princeton university princeton nj
work introduces new method called self organizing neural network algorithm compares its performance back propagation signal separation application problem separate two signals data signal male speech signal added transmitted through khz channel signals sampled khz using supervised learning attempt made reconstruct them algorithm constructs its own network topology during training shown much smaller than bp network faster trained free trial network design characterize bp
simple method training dynamical behavior neural network derived applicable training problem discrete time networks arbitrary feedback algorithm resembles back propagation error function minimized using gradient based method but optimization carried out hidden part state space either instead addition weight space computational results presented simple dynamical training problems requires response signal time steps past
selective sampling form directed search greatly increase ability connectionist network generalize accurately based information previous samples network may trained data selectively sampled regions domain unknown realizable cases distribution known cost drawing points target distribution negligible compared cost labeling them proper classification approach justified its applicability problem training network power system security analysis benefits selective sampling studied analytically results confirmed experimentally
popular class unsupervised algorithms competitive algorithms traditional view competition winner adapts given case propose view competitive adaptation attempting fit simple probability generators such gaussians set data points maximum likelihood fit model type suggests form competition adapt proportion relative probability input each investigate application soft competitive model placement radial basis function centers function interpolation show soft model give better performance little additional computational cost
method analog vectors continuous feedback model proposed analog vectors mean vectors whose components real valued vectors stored set equilibria network network model consists layer visible neurons layer hidden neurons propose learning algorithm results adjusting positions equilibria well their stability simulation results confirm effectiveness method
have used information theoretic ideas derive class practical nearly optimal schemes adapting size neural network removing weights network several improvements expected better generalization fewer training examples required improved speed learning andor classification basic idea use second derivative information make tradeoff between network complexity training set error experiments confirm usefulness methods real world application
have calculated both analytically simulations rate convergence long times backpropagation learning networks without hidden units our basic finding units using standard transfer function convergence error large most logarithmic corrections networks hidden units other transfer functions may lead slower polynomial rate convergence our analytic calculations presented tesauro he here focus more detail our empirical measurements convergence rate numerical simulations our analytic results
ht development image segmentation system real time image processing applications apply classical decision analysis viewing segmentation pixel classification task use supervised training derive classifier our system set examples particular pixel classification problem study test suitability connectionist method against two statistical methods gaussian maximum likelihood classifier first second third degree polynomial classifiers solution real world image segmentation problem taken research classifiers derived using three methods performance classifiers training data set well separate entire test images measured
multi layer percepttons trained classification trees two different techniques have recently become popular given enough data time both methods capable performing arbitrary non linear classification first consider important differences between multi layer percepttons classification trees conclude enough theoretical basis superiority technique over other reason performed number empirical tests three real world problems power system load forecasting power system security prediction speaker independent vowel identification cases even piecewise linear trees multi layer perceptron performed well better than trained classification trees performance comparisons
have done empirical study relation number parameters weights feedforward net generalization performance two experiments reported use simulated data sets well controlled parameters such signal noise ratio continuous valued data second train network vector mel real speech samples each case use back propagation train feedforward net discriminate multiple class pattern classification problem report results these studies show application cross validation techniques prevent overfitting
learning dynamics back propagation algorithm investigated complexity constraints added standard least mean square lms cost function shown loss generalization performance due avoided using such complexity constraints furthermore energy hidden representations weight distributions observed compared during learning attempt made explaining results terms linear non linear effects relation gradient descent learning algorithm
properties cluster multiple back propagation bp networks examined compared performance single bp network underlying idea effect within cluster improves performance fault tolerance five networks initially trained perform same input output mapping following training cluster created computing average outputs generated individual networks output cluster used desired output during training feeding back individual networks comparison single bp network cluster multiple generalization significant fault tolerance appear cluster advantage follows simple you single cluster time but you cannot them time
recent years many researchers have investigated use markov random fields mrfs computer vision they applied example reconstruct surfaces sparse noisy depth data coming output visual process integrate early vision processes label physical discontinuities paper show applying mean field theory those mrfs models class neural networks obtained those networks speed up solution mrfs models method restricted computer vision
rigorous analysis finite precision computational neural network pattern classifier via probabilistic approach presented even though exist negative results capability perceptron show following positive results given pattern vectors each represented cn bits uniformly distributed high probability perceptron perform possible binary patterns moreover resulting neural network requires small proportion nn memory would required complete storage patterns further perceptron algorithm takes arithmetic operations high probability whereas other methods such linear programming takes worst case indicate mathematical connections vlsi circuit testing theory random matrices
within context protocol learning perceptron algorithm shown learn arbitrary half space time probability distribution examples taken uniform over unit sphere here accuracy parameter surprisingly fast standard approaches involve solution linear programming problem involving constraints dimensions modification distribution independent protocol learning proposed distribution function learned may chosen these may communicate argued definition more reasonable applicable real world learning than under definition perceptron algorithm shown distribution independent learning algorithm show uniform distributions classes infinite dimension including convex sets class nested differences convex sets learnable
decision making tasks involve delayed consequences common yet difficult address supervised learning methods accurate model underlying dynamical system these tasks formulated sequential decision problems solved dynamic programming paper discusses reinforcement learning terms sequential decision framework shows learning algorithm similar implemented adaptive critic element used pole barto sutton anderson further developed sutton fits into framework adaptive neural networks play significant roles modules approximating functions required solving sequential decision problems
experimental evidence has shown analog neural networks extremely fault tolerant particular their performance does appear significantly precision limited analog neurons limited precision essentially compute weighted multilinear threshold functions into regions hyperplanes behaviour neural networks investigated canonical set threshold values although they exist binary neural networks weights made integers bits number processors without increasing hardware running time weights made while increasing running time constant multiple hardware small polynomial binary neurons used running time allowed increase larger constant multiple hardware allowed increase slightly larger polynomial symmetric function computed constant depth size function computed constant depth size alternating neural networks neural networks closely related model analog neural networks limited precision
comparison algorithms minimize error functions train trajectories recurrent networks reveals complexity off these algorithms related time independent suggested causal scalable algorithms possible activation dynamics adaptive neurons fast compared behavior learned standard continuous time recurrent backpropagation used example
paper suggests statistical framework parameter estimation problem associated unsupervised learning neural network leading exploratory projection pursuit network performs feature extraction dimensionality reduction
introduce cost function learning feed forward neural networks explicit function internal representation addition weights learning problem formulated two simple perceptrons search internal representations back propagation recovered limit frequency successful solutions better algorithm than back propagation weights hidden units updated same ie once every learning step
vestibulo ocular reflex vor primary mechanism controls compensatory eye movements stabilize retinal images during rapid head motion primary pathways system feed forward inputs outputs oculomotor system visual feedback used directly vor computation system exploit motor learning perform correctly has proposed model adapting vor gain using image information retina have designed tested analog largescale integrated vlsi circuitry implements simplified version adaptive vor model
long term goal our laboratory development analog resistive network based vlsi implementations early intermediate vision algorithms demonstrate experimental circuit smoothing segmenting noisy sparse depth data using resistive edge detection circuit computing zero crossings using two resistive grids different demonstrate robustness our algorithms fabricated analog cmos vlsi chips these circuits onto small mobile operating real time laboratory environment
distributed neuron synapses have been integrated active area mm mm using double single poly well cmos technology distributed neuron synapses blocks call switch matrices between each these provide interconnections small area units network various configurations possible configurations network network two networks etc numbers separated indicate number units per layer including input layer weights stored analog form mos synaptic weights resolution their full scale value limitation arises due charge injection access switch charge other parameters like gain shape nonlinearity programmable
cmos synapse chips containing cross array programmable synapses have been fabricated blocks fully parallel implementation neural networks synapses based hybrid digital analog design utilizes chip bit data store weights two multiplying compute weighted outputs synapses exhibit bit resolution excellent monotonicity consistency their transfer characteristics neuron hardware incorporating four synapse chips has been fabricated investigate performance feedback networks optimization problem solving study assignment net hopfield city traveling salesman problem net have been implemented hardware networks ability obtain optimum near optimum solutions real time has been demonstrated
paper explores whether analog circuitry adequately perform constrained optimization constrained optimization circuits designed using differential multiplier method these circuits fulfill time varying constraints correctly example circuits include quadratic programming circuit constrained
paper present novel implementation widely used back propagation neural net learning algorithm connection machine cm general purpose massively parallel computer hypercube topology implementation runs about million interconnections per second processor cm main communication operation used nearest neighbor communication techniques developed here easily extended implement other algorithms layered neural nets cm other massively parallel computers have higher degree connections among their processors
mapping back propagation mean field theory learning algorithms onto generic simd computer described architecture adequate these applications close optimum attained expressions find learning rates given array
short account given various investigations neural network properties classic work early work statistical mechanics magnetic materials fault tolerance via parallel distributed processing memory learning pattern recognition described
describe computational model development specific eye brain circuits model comprises self organizing map forming network uses local hebb rules constrained determined molecular markers various simulations development eye brain maps fish described particular successful simulations experiments meyer
feature selective cells primary visual cortex several species organized hierarchical topographic maps stimulus features like position visual space orientation ocular dominance order understand describe their spatial structure their development investigate self organizing neural network model based feature map algorithm model explains map formation dimension reducing mapping high dimensional feature space onto two dimensional lattice such similarity between features feature combinations translated into spatial proximity corresponding feature selective cells model able reproduce several aspects spatial structure cortical maps visual cortex
development projections retinas cortex mathematically analyzed according previously proposed thermodynamic formulation self organization neural networks three types included visual afferent pathways assumed two models model considered separately model center pathways considered addition model shows ocular dominance spatial patterns ocular dominance histograms reveals binocular model displays spatially modulated irregular patterns shows single peak behavior histograms compare simulated results observed results ocular dominance spatial patterns histograms models agree closely those seen monkeys cats
simple classical spin models well known xy models long range interactions occur pattern given operator generate many structural properties characteristic ocular dominance columns orientation patches seen cat primate visual cortex
exploring significance biological complexity neuronal computation here demonstrate hebbian synapses modeled hippocampal pyramidal cells may give rise two novel forms self organization response structured synaptic input first basis relationships between synaptic cell may become tuned small subset its input space second same mechanisms may produce clusters synapses across space dendrites latter type self organization may functionally significant presence nonlinear dendritic
combining experiments computational modeling have shown cholinergic modulation may enhance associative memory function pitiform olfactory cortex have shown analogue selectively suppresses synaptic transmission between cells within pitiform cortex while leaving input connections tested computational model cortex selective suppression applied during learning enhances associative memory performance
have devised scheme reduce complexity dynamical systems belonging class includes most biophysically realistic neural models reduction based transformations variables perturbation expansions preserves high level fidelity original system techniques illustrated hodgkin huxley system augmented hodgkin huxley system
main point paper stochastic neural networks have mathematical structure corresponds quite closely quantum field theory neural network derived spin remains show efficacy such description
self organization recurrent feature discovery networks studied perspective dynamical systems bifurcation theory reveals parameter regimes multiple equilibria limit cycles equilibrium networks perform principal component analysis
present new way derive optimizing dynamics formulation mechanics used obtain both standard novel neural net dynamics optimization problems demonstrate derive standard descent dynamics well variants introduce computational attention mechanism
hopfield network hopfield provides simple model associative memory neuronal structure model based highly artificial assumptions especially use formal two state neurons hopfield graded response neurons hopfield what happens replace formal neurons real biological neurons address question two steps first show simple model neuron capture relevant features neuron spiking ie wide range spiking frequencies realistic distribution interspike intervals second construct associative memory linking these neurons together analytical solution large fully connected network shows hopfield solution valid neurons short refractory period refractory period longer than critical duration solutions qualitatively different associative character solutions preserved
simple architecture algorithm analytically guaranteed associative memory storage analog patterns continuous sequences chaotic attractors same network described matrix inversion determines network weights given prototype patterns stored units capacity node network weights costs unit per static attractor two per fourier component each sequence four per chaotic attractor spurious attractors function special coordinate system approach transient states stored trajectories unsupervised supervised incremental learning algorithms pattern classification such competitive learning bootstrap easily implemented architecture into recurrent network higher order weights used model cortex stores oscillatory chaotic attractors hebb rule hierarchical sensory motor control networks may constructed interconnected cortical patches these network modules network performance being investigated application problem real time handwritten digit recognition
show analytically stability two dimensional lateral inhibition neural networks depends local connection topology various network topologies calculate critical time delay onset oscillation continuous time networks present analytic phase diagrams characterizing dynamics discrete time networks
de fully recurrent networks thought dynamic systems dynamics shaped perform content addressable memories recognize sequences generate trajectories unfortunately several problems arise first convergence state space guaranteed second learned fixed points trajectories necessarily stable finally might exist spurious fixed points andor spurious trajectories do correspond patterns paper introduce new energy function presents solutions these problems present efficient gradient descent algorithm directly acts stability fixed points trajectories size shape corresponding basin attraction results illustrated simulation small content addressable memory
development learning algorithms generally based upon minimization energy function fundamental requirement compute gradient energy function respect various parameters neural architecture eg synaptic weights neural principle requires solving system nonlinear equations each parameter model computationally expensive new methodology neural learning time dependent nonlinear mappings presented exploits concept operators enable fast global computation networks response perturbations systems parameters importance time boundary conditions functions discussed algorithm presented sensitivity equations solved ie forward time along nonlinear dynamics neural networks methodology makes real time applications hardware implementation temporal learning feasible
coherent oscillatory activity large networks biological artificial neural units may useful mechanism coding information single perceptual object regularities within data set consider dynamics large array simple coupled oscillators under variety connection schemes particular interest rapid robust phase locking results sparse scheme each oscillator strongly coupled randomly selected subset its neighbors
paper studies dynamical aspects neural systems delayed negative feedback modelled nonlinear delay differential equations these systems undergo hopf bifurcation stable fixed point stable limit cycle oscillation certain parameters varied shown their frequency oscillation robust parameter variations noisy fluctuations property makes these systems good candidates onset oscillation both additive parametric noise sense state variable more time near fixed point than would absence noise case noise affects delayed variable ie system has memory finally shown distribution delays rather than fixed delay fixed point solution
show simple spin system biased its critical point encode spatial characteristics external signals such objects visual field temporal correlation functions individual spins qualitative arguments suggest firing neurons should described planar spin unit length such xy models exhibit critical dynamics over broad range show extract these spins spike trains measure interaction using simulations small clusters cells static correlations among spike trains obtained large arrays cells agreement predictions these dynamic correlations display predict ed encoding spatial suggest novel representation object temporal correlations may relevant recent experiments oscillatory neural firing visual cortex
multi layered neural networks have recently been proposed nonlinear prediction system modeling although proven successful modeling time invariant nonlinear systems inability neural networks characterize temporal variability has so far been obstacle applying them complicated nonstationary signals such speech paper present network architecture called hidden control neural network modeling signals generated nonlinear dynamical systems restricted time variability approach taken here allow mapping implemented multi layered neural network change time function additional control input signal network trained using algorithm based back propagation segmentation algorithms estimating unknown control together networks parameters approach applied several tasks including modeling time varying nonlinear systems speaker independent recognition connected digits yielding word accuracy
work describe new method adjusts time delays widths time windows artificial neural networks automatically input units weighted gaussian input window over time allows learning rules delays widths derived same way used weights our results phoneme classification task compare well results obtained tdnn et al manually optimized same task
present new neural network model processing temporal patterns model gamma neural model general convolution delay model arbitrary weight kernels wt show gamma model formulated partially additive model temporal hebbian learning rule derived establish links related existing models temporal processing
goal has been construct supervised artificial neural network learns incrementally unknown mapping result network consisting combination art backpropagation proposed called network art network used build focus supervised backpropagation network network has advantage being able dynamically expand itself response input patterns containing new information simulation results show network outperforms classical maximum likelihood method estimation discrete dynamic nonlinear transfer function
study representation static patterns temporal associations neural networks broad distribution signal delays certain class such systems simple intuitive understanding spatio temporal computation becomes possible help novel lyapunov functional allows quantitative study asymptotic network behavior through statistical mechanical analysis present analytic calculations both retrieval quality storage capacity compare them simulation results
work extends computational learning theory situations concepts vary over time eg system identification time varying plant have extended formal definitions concepts learning provide framework algorithm track concept evolves over time given framework focusing memory based algorithms have derived pac style sample complexity results determine example tracking feasible have used similar framework focused incremental tracking algorithms have derived bounds mistake error rates specific concept classes
present large vocabulary continuous speech recognition system based linked predictive neural networks system uses neural networks predictors speech frames yielding distortion measures used stage algorithm perform continuous speech recognition system already speech speech translation system currently achieves word accuracy tasks respectively outperforming several simple hmms tested found accuracy speed slightly improved use hidden control inputs conclude strengths predictive approach
neural network architecture designed locating word boundaries identifying words phoneme sequences architecture tested three sets studies first highly redundant corpus restricted vocabulary generated network trained limited number phonemic variations words corpus tests network performance transfer set yielded low error rate second study network trained identify words expert speech transfer test error rate correct simultaneous identification words word boundaries third study used output phoneme classifier input word word boundary identification network error rate transfer test set task overall these studies provide first step identifying words connected neural network
previous work has shown ability multilayer perceptrons mlps estimate emission probabilities hidden markov models hmms advantages speech recognition system incorporating both mlps hmms best discrimination ability incorporate multiple sources evidence features temporal context without restrictive assumptions distributions statistical independence paper presents results speaker dependent portion english language resource management database results support previously reported utility mlp probability estimation continuous speech recognition additional approach use mlps nonlinear predictors autoregressive hmms while shown more compatible hmm formalism still suffers several limitations approach generalized take account time correlation between successive observations without restrictive assumptions about driving noise
through use neural network classifiers careful feature selection have achieved high accuracy speaker independent spoken letter recognition isolated letters broad category segmentation performed location segment boundaries allows us measure features specific locations signal such vowel onset important information letter classification performed feed forward neural network recognition accuracy test set speakers neural network classifiers used pitch tracking broad category segmentation letter strings our research has been extended recognition names between letters searching database names achieved first choice name retrieval work has continuous letter classifier does frame frame phonetic classification spoken letters
neural prediction model speech recognition model based pattern prediction multilayer percepttons its effectiveness confirmed speaker independent digit recognition experiments paper presents improvement model its application large vocabulary speech recognition based units improvement involves
research re particular form neural network described has terminals acoustic patterns class labels speaker parameters method training network tune speaker parameters particular speaker based trick supervised network unsupervised mode describe experiments using approach isolated word recognition based whole word hidden markov models results indicate improvement over speaker independent performance unlabelled data performance close achieved labelled data
novel unsupervised neural network dimensionality reduction seeks directions presented its connection exploratory projection pursuit methods discussed leads new statistical insight synaptic modification equations governing learning bienenstock cooper neurons importance dimensionality reduction principle based solely distinguishing features demonstrated using motivated phoneme recognition experiment compared feature extraction using back propagation network
paper describe several extensions our earlier work utilizing segment based approach formulate our segmental framework report our study use multi layer perceptrons detection classification examine outputs network compare network performance other classifiers our investigation performed within set experiments attempts recognize american english independent speaker evaluated database our system achieves accuracy
spoken language natural efficient flexible means communication among humans computers play increasing role our important address issue providing human machine interface through spoken language paper describe our recent efforts moving beyond speech recognition into spoken language understanding specifically report development navigation exploration system called application have used basis performing research spoken language understanding
paper summary project aims results project focus use neuro computing techniques tackle various problems remain speech recognition first results use feedforward nets phonetic units classification isolated word recognition speaker adaptation
have been studying performance delayed matching sample task gain insight into processes mechanisms animal uses during recognizes targets natural signals return paper describes novel neural network architecture called network have developed account performance network combines information multiple classify targets about accuracy contrast standard backpropagation network performed about accuracy
signal processing capabilities biological neurons investigated temporally coded signals neurons increase transmission capacity signal suggested bi threshold neurons high threshold low threshold switching firing modes extract signal embedded firing encoded signal network neurons delayed line circuitry signal processing temporally coded input signal transformed spatially mapping firing intervals output network thus decoding specific firing interspike intervals network provides band pass filtering capability variability timing original signal decoded
using unsupervised learning procedure network trained ensemble images same two dimensional object different positions orientations sizes each half network object tries produce output set parameters have high mutual information parameters output other half network given ensemble training patterns parameters two network agree position orientation size whole object them after training network reject instances other shapes using fact predictions made its two two competing networks trained unlabelled mixture images two objects they cluster training cases basis objects shapes independently position orientation size
model based neural vision system presented here determines position identity three dimensional objects two stereo images scene described terms shape primitives line segments derived edges scenes their relational structure recurrent neural matching network solves correspondence problem assigning corresponding line segments right left stereo images relational scene description generated matched second neural network against models model base quality solutions convergence speed both improved using mean field approximations
second order architecture presented here translation rotation scale invariant processing images mapped input units new architecture has complexity weights opposed weights usually required third order rotation invariant architecture reduction complexity due use discrete frequency information simulations show favorable comparisons other neural network architectures
previous work mi cf me showed feedforward network area like input layer units hebb rule develop area mt like second layer units solve aperture problem pattern motion present study extends earlier work more complex motions et al showed neurons large receptive fields macaque visual area mst sensitive different senses rotation receptive field location movement singularity network mt like second layer trained tested combinations patterns third layer units learn detect specific senses rotation position independent fashion despite having position dependent direction selectivity within their receptive fields
paper presents neural network nn approach problem correspondence problem finding correct matches between pixels lines stereo pair possible matches posed non iterative many mapping two layer feed forward nn architecture developed learn code nonlinear complex mapping using back propagation learning rule training set important aspect technique none typical constraints such uniqueness continuity explicitly imposed applicable constraints learned internally coded enabling more flexible more accurate than existing methods approach successfully tested several stereograms shown net generalize its learned mapping cases outside its training set advantages over marr poggio algorithm discussed shown performance superior
neural network model motion segmentation visual cortex described model preprocessing motion signals motion oriented contrast filter filter long range cooperative motion mechanisms motion cooperative competitive loop cc loop control phenomena such induced motion motion capture motion total model system motion boundary contour system computed parallel static before both systems generate boundary representation three dimensional visual form perception present investigations clarify static modified use motion segmentation problems notably analyzing ambiguous local movements aperture problem complex moving shape suppressed actively into coherent global motion signal
exact structure motion ill posed computation therefore sensitive noise work describe qualitative shape representation based sign gaussian curvature computed directly motion without computation exact depth map directions surface show humans curvature sense three points undergoing motion two three four views success rate significantly above chance simple rbf net has been trained perform same task
formulate problem optimizing sampling natural images using array linear filters optimization information capacity constrained noise levels individual channels penalty construction long range interconnections array low signal noise ratios optimal filter characteristics correspond bound states equation signal spectrum plays role potential resulting optimal filters similar those observed mammalian visual cortex retinal ganglion cells lower observed scale invariance natural images plays essential role construction bialek ruderman
dark adapted visual system count reliability limited thermal noise rod photoreceptors processing circuitry between rod cells brain essentially noiseless fact may close optimal here design optimal signal processor estimates time varying light intensity retina based rod signals show first stage optimal signal processing involves passing rod cell output through linear filter characteristics determined entirely rod signal noise spectra filter general fact first stage visual signal processing task low output first stage filter intracellular voltage response bipolar cell first anatomical stage retinal signal processing recent data photoreceptors extract relevant spectra make parameter free quantitative predictions bipolar cell response agreement experiment essentially perfect far know first predictive theory neural dynamics
retina response off ganglion cells central reduced movement receptive field surround through computer simulation model takes into account their anatomical physiological properties show interactions between four neuron types two bipolar two may responsible generation lateral conductance change sensitive inhibition model shows four neuron circuit account previously observed movement sensitive ganglion cell sensitivity allows visualization prediction spatio temporal pattern activity change sensitive retinal cells
light adaptation la allows cone vision remain functional between time day even though time their intensity response characteristic limited log units light mechanism underlying la localized outer segment isolated cone found adding characteristic cone shifted along intensity domain neural network involving feedback synapse horizontal cells involved ambient light level periphery equivalent electrical circuit three different channels feedback used model static behavior cone simulation showed interactions between feedback synapse light sensitive conductance outer segment shift curves along intensity domain provided mechanism during maximally light response
have designed tested dimensional pixel analog cmos vlsi chip intensity edges real time device exploits chip photoreceptors natural filtering properties resistive networks implement scheme similar motivated difference gaussians operator proposed marr hildreth our chip computes zero crossings associated difference two exponential weighting functions derivative across zero crossing above threshold edge reported simulations indicate technique extend well two dimensions
inspired visual motion detection model retina computational architecture used early barn owl have designed chip employs correlation model report dimensional field motion scene real time using subthreshold analog vlsi techniques have fabricated successfully tested transistor chip using standard mosis process
present generic neural network architecture capable controlling non linear network composed dynamic parallel linear maps gated non linear switches using recurrent form back propagation algorithm control achieved optimizing control gains task adapted switch parameters mean quadratic cost function computed across plant trajectory minimized along performance constraint approach demonstrated control task consisting commercial aircraft difficult wind conditions show network yields excellent performance while remaining within acceptable response constraints
describe real time robot navigation system based three vlsi neural network modules these resistive grid path planning nearest classifier localization using range data flight red sensor sensory motor associative network dynamic obstacle avoidance
alvinn autonomous vehicle neural network project addresses problem training artificial neural networks real time perform difficult perception tasks back propagation network uses inputs video camera imaging laser drive cmu modified van paper describes training techniques allow alvinn learn under minutes autonomously control human response new situations using these techniques alvinn has been trained drive variety circumstances including single obstacle off road environments speeds up per
propose new parallel hierarchical neural network model enable motor learning simultaneous control both trajectory force integrating control method our previous neural network control model using feedback error learning scheme furthermore two hierarchical control laws apply model derived using moore matrix related minimum muscle change trajectory other related minimum motor command change trajectory human arm redundant dynamics level joint generated muscles therefore acquisition inverse model ill posed problem combination these control laws feedback error learning resolve ill posed problem finally efficiency parallel hierarchical neural network model shown learning experiments using artificial muscle arm computer simulations
have used neural network compute corrections images written eliminate proximity effects caused iterative methods effective but require prohibitively computation time have instead trained neural network perform equivalent corrections resulting significant speed up have examined hardware implementations using both analog digital electronic networks both had small error compared iterative results additionally verified neural network correctly generalized solution problem include patterns contained its training set have experimentally verified approach cambridge instruments exposure system
present new connectionist planning method interaction unknown environment world model constructed using gradient descent deriving optimal actions respect future reinforcement planning applied two steps experience network proposes plan subsequently optimized gradient descent chain world models so optimal reinforcement may obtained actually run appropriateness method demonstrated application pole balancing task
barto sutton watkins introduced grid task example temporal difference planning asynchronous dynamical programming paper considers effects changing coding input stimulus demonstrates self supervised learning particular form hidden unit representation improves performance
summary results dyna class architectures intelligent systems based approximating dynamic programming methods dyna architectures integrate trial error reinforcement learning execution time planning into single process operating world learned forward model world describe show results two dyna architectures dyna dyna using navigation task results shown simple dyna system simultaneously learns trial error learns world model plans optimal using world model show dyna architectures based learning easy adapt use changing environments
present algorithm based reinforcement state learning techniques solve control scheduling problems particular have devised simple learning scheme called learning weights associative search element either positively such system move towards desired shortest possible trajectory improve learning rate variable reinforcement scheme employed negative reinforcement values varied depending whether failure occurs normal mode operation furthermore realize simulated annealing scheme accelerated learning system same failed state successively negative reinforcement value increased examples studied these learning schemes have demonstrated high learning rates therefore may prove useful learning
paper examines class neuron based learning systems dynamic control rely adaptive range coding sensor inputs sensors assumed provide binary coded range vectors describe system state these vectors input neuron like processing elements output decisions generated these neurons turn affect system state subsequently producing new inputs reinforcement signals environment received various intervals evaluated neural weights well range boundaries determining output decisions goal maximizing future reinforcement environment preliminary experiments show promise adapting neural receptive fields learning dynamical control observed performance method exceeds earlier approaches adaptive range coding
feedforward layered network implements mapping required control unknown stochastic nonlinear dynamical system training based novel approach combines stochastic approximation ideas backpropagation method applied control admission into system operating time varying environment
work addresses three problems reinforcement learning adaptive neuro control non markovian interfaces between learner environment line learning based system realization adaptive algorithm described based system realization two interacting fully recurrent running networks may learn parallel problems parallel learning adaptive randomness described interacting systems combined vector valued adaptive previous have been scalar
response wind american turns away runs circuit underlying initial turn escape response consists three populations individually nerve cells appears employ distributed representations its operation have reconstructed several neuronal behavioral properties system using simplified neural network models backpropagation learning algorithm constrained known structural characteristics circuitry order test refine model have compared models responses various lesions insects responses similar lesions
neural network simulations flight system have been developed understand insect uses complex simulation networks account spatial distribution cells well operating range stochastic cellular firing history each neuron addition motor neuron firing patterns flight command sequences utilized simulation training against both cellular flight motor neuron firing patterns trained networks accurately cellular firing patterns these turn controlled motor neuron firing patterns drive during flight such networks provide both neurobiological analysis tools first generation controls use
three dimensional structures protein have been predicted using neural networks feed forward neural network trained class functionally but structurally proteins using backpropagation learning network generated structure information form binary distance constraints protein binary distance between two distance between them less than certain threshold distance otherwise distance constraints predicted trained neural network utilized generate protein using steepest descent minimization approach
computer sciences university wi describe application hybrid machine learning algorithm task recognizing important genetic sequences symbolic portion system utilizes inference rules provide roughly correct method recognizing class dna sequences known splice junctions map domain theory into neural network provide training examples using samples neural networks learning algorithm adjusts domain theory so properly classifies these dna sequences our procedure general method incorporating knowledge into artificial neural networks present experiment molecular demonstrates value doing so
diagnosis faults complex real time control systems complicated task has solution traditional methods have neural networks successfully employed faults controlled systems paper discusses means use develop appropriate databases training testing order select optimum network architectures provide reasonable estimates classification accuracy these networks new samples recent work applying neural nets adaptive control active system presented
study has demonstrated artificial neural networks anns used characterize seismic sources using high frequency seismic data have taken novel approach using research tool obtaining seismic source information specifically depth focus fire characteristics economic rather than feature classifier between explosion populations overall have found anns have potential applications seismic event characterization identification beyond feature classifier future studies these techniques should applied actual data seismic events recorded new seismic arrays results study indicates ann should evaluated part seismic event identification system
artificial neural network ann trained recognize pattern particular future backpropagation errors algorithm used encode relationship between desired output fundamental variables plus technical variables into year past data ann market positions future would have made less than ann trained able predict
university neural network algorithms have proven useful recognition individual segmented characters their recognition accuracy has been limited accuracy underlying segmentation algorithm conventional rule based segmentation algorithms difficulty characters broken noisy problem these situations often cannot properly segment character until recognized yet cannot properly recognize character until segmented present here neural network algorithm simultaneously segments recognizes integrated system algorithm has several novel features uses supervised learning algorithm backpropagation but able take position independent information targets self organize activities units competitive fashion infer information demonstrate ability overlapping hand printed
dimensionality set face images male female subjects reduced via network extracted features do correspond features used previous face recognition systems such ratios distances between facial elements rather they whole face features call given layer back propagation networks trained classify input features identity state gender automatically extracted provide sufficient basis gender identity several among training set network human compared found networks tend more distant than humans do
identification animals has biological importance humans good making determination visually but machines have matched ability neural network trained discriminate human faces performed well humans set exemplars images sampled compressed using xx fully connected back propagation network activities hidden units input back propagation trained produce values male female faces networks average error rate compared favorably humans who averaged errors those humans
paper proposes fuzzy neural expert system following two functions generalization information derived training data knowledge form fuzzy neural network extraction fuzzy rules linguistic relative importance each part trained neural network paper gives method extract automatically fuzzy rules trained neural network prove effectiveness validity proposed fuzzy neural expert system fuzzy neural expert system medical diagnosis has been developed
analog neural networks feedback used implement take networks turn networks used class nonlinear error correcting codes such networks construct capable decoding more powerful codes consider several families interconnected networks analyze their performance terms coding theory metrics consider feasibility embedding such networks vlsi
harmonic grammar legendre et al connectionist theory linguistic well formedness based assumption well formedness sentence measured harmony negative energy corresponding connectionist state assuming lower level connectionist network obeys few general connectionist principles but otherwise construct higher level network equivalent harmony function captures most relevant global aspects lower level network paper extend tensor product representation smolensky fully representations recursively structured objects like sentences lower level network show theoretically example power new technique parallel distributed structure processing
network trained back propagation map expressions form noun noun semantic representation networks performance analyzed over several simulations training sets both english german translation attempted presenting expression network trained language generate semantic representation semantic representation presented network trained other language generate appropriate
previous paper touretzky showed adding clustering operation connectionist phonology model produced parallel processing account certain phenomena paper show addition second primitive greatly increases power model present examples non language appear require rule ordering least depth four adding circuitry structure models perception input string able handle these examples two steps conclude phonology derivation largely replaced
higher order recurrent neural network architecture learns recognize generate languages after being trained exemplars studying these networks perspective dynamical systems yields two interesting first learning process illustrates new form mechanical inference induction phase transition small weight adjustment causes bifurcation limit behavior network phase transition corresponds onset networks capacity generalizing arbitrary length strings second study automata resulting acquisition previously published languages indicates while architecture guaranteed find minimal finite automata consistent given exemplars np hard problem architecture does appear capable generating languages exploiting fractal chaotic dynamics end paper hypothesis relating linguistic generafive capacity behavioral regimes non linear dynamical systems
competitive learning unsupervised algorithm classifies input patterns into mutually exclusive clusters neural net framework each cluster represented processing unit others pool input pattern present simple extension algorithm allows construct discrete distributed representations discrete representations useful because they relatively easy analyze their information content readily measured distributed representations useful because they explicitly encode similarity basic idea apply competitive learning iteratively input pattern after each stage input pattern component captured representation stage component simply weight vector unit competitive pool procedure forces competitive pools different stages encode different aspects input algorithm essentially same traditional data compression technique known vector quantization although neural net perspective suggests potentially powerful extensions approach
lack alternative models search decision processes have provided dominant paradigm human memory access using two more cues despite evidence against search access process present alternative process search based calculating intersection sets targets activated two more cues two methods computing intersection presented using information about possible targets other cue target strengths memory matrix analysis using orthogonal vectors represent cues targets demonstrates both processes simulations using sparse distributed representations demonstrate performance latter process tasks involving cues
connectionist model human category learning fits broad spectrum human learning data its architecture based psychological theory related networks using radial basis functions perspective cognitive psychology combination exemplar based representation learning perspective seen incorporating constraints into back propagation networks appropriate modelling human learning
network based splines described automatically adapts number units unit parameters architecture network each application
multi layer percepttons often slow learn nonlinear functions complex local structure due global nature their function approximations shown standard multi layer percepttons actually special case more general network formulation incorporates splines into node computations allows novel spline network architectures developed combine generalization capabilities scaling properties global multi layer feedforward networks computational efficiency learning speed local computational paradigms simulation results presented well known spiral problem show effectiveness spline net approach
local variable selection has proven powerful technique approximating functions high dimensional spaces used several statistical methods including cart others see references these algorithms paper present tree structured network generalization these techniques network provides framework understanding behavior such algorithms modifying them particular applications
examine ability radial basis functions rbfs generalize compare performance several types rbfs use inverse dynamics idealized two joint arm test case find without proper choice norm inputs rbfs have poor generalization properties simple global scaling input variables greatly improves performance suggest efficient methods approximate distance metric
have created radial basis function network new computational unit whenever pattern presented network network learns new units adjusting parameters existing units network performs poorly presented pattern new unit response presented pattern network performs well presented pattern network parameters updated using standard lms gradient descent predicting glass chaotic time series our network learns much faster than do those using back propagation uses comparable number synapses
develop sequential adaptation algorithm radial basis function rbf neural networks gaussian nodes based method projections method makes use each observation efficiently network mapping function so obtained consistent information optimal least la norm sense rbf network projections adaptation algorithm used predicting chaotic time series compare its performance adaptation scheme based method stochastic approximation show projections algorithm converges underlying model much faster
introduce oriented non radial basis function networks generalization radial basis function networks euclidean distance metric exponent gaussian replaced more general polynomial permits definition more general regions orientations case hyper surface estimation scheme requires smaller number hidden units curse dimensionality associated kernel type case image hidden units correspond features image parameters associated each unit correspond rotation scaling translation properties particular feature context scheme means image represented small number features transformation image rotation scaling translation correspond identical transformations individual features scheme used considerable advantage purposes image recognition analysis
consider feed forward neural networks non linear hidden layer linear output units transfer function hidden layer either bell shaped sigmoid bell shaped case show polynomials hand theory equation other relevant understanding properties corresponding networks particular these techniques yield simple proofs universal properties ie fact reasonable function approximated degree precision linear combination functions addition framework problem learning equivalent problem time course diffusion process results obtained bell shaped case applied case sigmoid transfer functions hidden layer yielding similar results conjecture related problem generalization briefly examined
paper show discrete affine wavelet transforms provide tool analysis synthesis standard feedforward neural networks shown wavelet frames li constructed based upon sigmoids spatio spectral localization property wavelets exploited defining topology determining weights feedforward network training network constructed using synthesis procedure described here involves minimization convex cost functional therefore avoids inherent standard backpropagation algorithms extension these methods li discussed
learning input output mapping set examples regarded front point view form learning closely related theory have previously shown poggio equivalence between regularization class three layer networks call regularization networks note extend theory introducing ways dealing two aspects learning learning presence unreliable examples outliers learning positive negative examples
describe multi network modular connectionist architecture captures fact many tasks have structure level intermediate assumed local global function approximation schemes main architecture combines associative competitive learning order learn task task decomposition discovered forcing networks comprising architecture learn training patterns result competition different networks learn different training patterns thus learn partition input space performance architecture what vision task multi task presented
compare performance modular architecture composed competing expert networks suggested jacobs jordan hinton performance single back propagation network complex but low dimensional vowel recognition task simulations reveal system capable interesting complex task type decomposition strongly influenced nature input gating network expert use each case modular architecture exhibits consistently better generalization many variations task
introduce framework training architectures composed several modules framework uses statistical formulation learning systems provides unique formalism describing many classical connectionist algorithms well complex systems several algorithms interact allows design hybrid systems combine advantages connectionist algorithms well other learning algorithms
describe recurrent connectionist network called uses set written given style new style extension traditional algorithmic composition technique transition tables specify probability next note function previous context central use grounded representation pitch
genetic algorithms used select create features select reference exemplar patterns machine vision speech pattern classification tasks complex speech recognition task genetic algorithms required more computation time than traditional approaches feature selection but reduced number input features required factor five features difficult artificial machine vision task genetic algorithms able create new features polynomial functions original features reduced classification error rates almost neural net nearest neighbor knn classifiers unable provide such low error rates using original features genetic algorithms used reduce number reference exemplar patterns knn classifier training pattern vowel recognition problem classes genetic algorithms reduced number stored exemplars without significantly increasing classification error rate applications genetic algorithms easy apply found good solutions many fewer trials than would required exhaustive search run times long but these results suggest genetic algorithms practical pattern classification problems faster serial parallel computers developed
dept electrical engineering university ca learning increase rate evolution population biological effect our simulations show population artificial neural networks solving pattern recognition problem learning much learning leads slow evolution genes whereas intermediate amount optimal moreover given total number training occurs different individuals within each generation receive different numbers rather than equal numbers because genetic algorithms gas help avoid local minima energy functions our hybrid learning ga systems applied successfully complex highdimensional pattern recognition problems
given training data should choose particular network classifier family networks different complexities paper discuss application stochastic complexity theory classifier design problems provide insights into problem particular introduce notion admissible models whereby complexity models under consideration affected among other factors class entropy amount training data our prior belief particular discuss implications these results respect neural architectures demonstrate approach real data medical diagnosis task
introduce method efficient design boltzmann machine hopfield net computes arbitrary given boolean function method based efficient simulation acyclic circuits threshold gates boltzmann machines consequence show various concrete boolean functions relevant classification problems computed scalable boltzmann machines guaranteed converge their global maximum configuration high probability after many steps
present compare learning rate schedules stochastic gradient descent general algorithm includes lms line backpropagation means clustering special cases introduce search type schedules outperform classical constant running average lt schedules both speed convergence quality solution
paper prove vectors lvq learning algorithm converge do showing learning algorithm performs stochastic approximation convergence obtained identifying appropriate conditions learning rate underlying statistics classification problem present modification learning algorithm argue results convergence lvq error bayesian optimal error appropriate parameters become large
paper explores effect initial weight selection feed forward networks leaming simple functions back propagation technique first demonstrate through use monte carlo techniques magnitude initial condition vector weight space significant parameter convergence time variability order further understand result additional deterministic experiments performed results these experiments demonstrate extreme sensitivity back propagation initial weight configuration
inspired information theoretic idea minimum description length add term back propagation cost function network complexity give details procedure called weight elimination describe its dynamics clarify meaning parameters involved bayesian perspective complexity term interpreted assumption about prior distribution weights use procedure predict time series notoriously noisy series exchange rates
simple linear case mathematical analysis training generalization validation performance networks trained gradient descent least mean square cost function provided function learning parameters statistics training data base analysis predicts generalization error dynamics dependent priori initial weights particular generalization error might sometimes within computable range during extended training cases analysis provides bounds optimal number training cycles minimal validation error speech labeling task predicted effects qualitatively tested observed computer simulations networks trained linear non linear back propagation algorithm
study evolution generalization ability simple linear perceptron inputs learns imitate teacher perceptron system trained binary example inputs generalization ability measured testing agreement teacher possible binary input patterns dynamics may solved analytically exhibits phase transition imperfect perfect generalization except point generalization ability approaches its asymptotic value exponentially critical down near transition relaxation time right critical point approach perfect generalization follows power law presence noise generalization ability degraded amount above
while network problem layer threshold nets np hard learning examples alone backpropagation baum has now proved learner employ queries hidden unit credit assignment problem pac load nets up four hidden units polynomial time empirical tests show method learn far more complicated functions such randomly generated networks hidden units algorithm easily approximates function using single layer hidden units requires minutes cpu time learn bit parity accuracy
series careful experiments measure tile average capability neural networks trained variety simple functions these experiments designed test whether average generalization performance worst case bounds obtained formal learning theory using vapnik chervonenkis et al indeed find cases average generalization significantly better than vc bound approach perfect performance exponential number examples rather than result bound other cases do find behavior vc bound these cases numerical closely related contained bound
solla bell laboratories corner rd nj usa learning time simple neural network model obtained through analytic computation eigenvalue spectrum hessian matrix describes second order properties cost function space coupling coefficients form eigenvalue distribution suggests new techniques learning process provides theoretical justification choice centered versus biased state variables
school computer science university pa present unified framework number different ways generalize properly during learning sources random information network effectively augmenting training data random information complexity function computed therefore increased generalization degraded analyze replicated networks number identical networks independently trained same data their results averaged conclude almost always results decrease expected complexity network therefore increases expected generalization simulations effect presented broken symmetry considered consider unit backpropagation network trained exclusive without hidden units problem point learning would stop resulting mean squared weights zero output always error but saddle point placing discrimination boundary error properly point correctly two errors giving mse shown figure networks initialized small random weights noise during training break symmetries sort but breaking symmetry something has been lost consider knn classifier constructed knn program training data who has knn program construct classifier they receive training data thus considering classification abstract entity know its complexity cannot exceed training data plus complexity program fixed but necessarily case backpropagation network because
patterns drawn dimensional feature space according probability distribution obeys weak smoothness criterion show probability random input pattern nearest neighbor classifier using random reference patterns asymptotically satisfies sufficiently large values here denotes probability error infinite sample limit most twice error bayes classifier although value coefficient depends upon underlying probability distributions exponent largely distribution free thus obtain concise relation between classifiers ability generalize finite reference sample dimensionality feature space well analytic validation well known curse dimensionality
consider different types single hidden layer feedforward nets without direct input output connections using either threshold sigmoidal activation functions main results show direct connections threshold nets double recognition but interpolation power while using sigmoids rather than thresholds allows least both various results given vc dimension other measures recognition capabilities
develop new feedforward neural network representation functions into based level sets function show upper bound tile nodes needed represent within uniform error st constant ve show number bits needed represent weights network order achieve approximation given bound entropy functional class under consideration
introduce geometric approach investigating power threshold circuits viewing variable boolean functions vectors tools linear algebra linear programming derive new results boolean functions using threshold gates using approach obtain upper bounds number spurious memories networks number functions implementable depth threshold circuit lower bound number orthogonal input required threshold function necessary condition arbitrary set input implement threshold lower bound error introduced approximating boolean functions using sparse polynomials limit effectiveness known lower bound technique based computing correlations among depth threshold circuits implementing boolean constructive proof every boolean function input variables threshold function many input functions none significantly correlated these lead key concerning threshold circuit complexity particularly those based tile so called spectral analysis approach moreover our geometric approach yields simple proofs based elementary linear algebra many these earlier results
paper after into classification problem considered various research discussions concerning reasons performances three chosen algorithms cart classification regression tree more recent versions popular induction tree technique known multi layer perceptron mlp proposed compare performances these algorithms under two criteria classification generalisation found general mlp has better classification generalisation compared other two algorithms
richard laboratory mit ma seven different pattern classifiers implemented serial computer compared using artificial speech recognition tasks two neural network radial basis function high order polynomial network five conventional classifiers gaussian mixture linear tree nearest neighbor tree nearest neighbor evaluated classifiers chosen representative different approaches pattern classification complement extend those evaluated previous study lee previous study both demonstrate classification error rates equivalent across different classifiers they powerful enough form minimum error decision regions they properly tuned sufficient training data available practical characteristics such training time classification time memory requirements differ orders magnitude these results suggest selection classifier particular task should guided so much small differences error rate but practical considerations concerning memory usage computational resources ease implementation restrictions training classification times
performance seven minimization algorithms compared five neural network problems these include variable step size algorithm conjugate gradient several methods explicit analytic numerical approximations hessian
problem color clustering defined shown problem assigning large number hundreds thousands vectors small number clusters finding those clusters such way they best represent full color image using distinct colors computational problem paper problem solved using classical techniques means clustering vector quantization out same application competitive learning kohonen self organizing feature maps quality result much pseudo color result resembles true color image rms quantization error run time kohonen map provides best solution
feedback connections required so teacher signal output neurons modify weights during supervised learning relaxation methods needed learning static patterns full time feedback connections feedback network learning techniques have achieved wide because still greater computational efficiency back propagation show simulation relaxation networks kind implementing vlsi capable learning large problems like back propagation networks incorporates deterministic mean field theory learning well stochastic boltzmann learning multiple chip electronic system implementing these networks make high speed parallel learning them feasible future
high speed implementation cmac neural network designed using cmos logic technology used implement two general purpose cmac associative memory each board implements up independent cmac networks total million adjustable weights each cmac network configured have integer inputs integer outputs response times typical cmac networks well below making networks sufficiently fast most robot control problems many pattern recognition signal processing problems
adaptive solutions architecture chip general purpose chip has processors each local memory running capable implementing most current neural network algorithms chip learning paper discusses implementation back propagation algorithm array these chips shows performance figures accurate hardware simulator eight chip configuration board update billion connections per second learning mode process billion connections per second feed forward mode
describe cmos neural net chip network architecture contains binary programmable connections building block neurons several building blocks connected form long neurons up binary connections form neurons analog connections multi layer networks implemented chip have integrated chip into board system together digital signal processor fast memory system currently use image processing applications chip extracts features such edges corners binary gray level images
neural network pattern recognition feature extraction analog ccd parallel processing architecture developed laboratory particularly well suited computational requirements shared weight networks such implementation using ccd architecture simulated modification training procedure improves network performance under limited arithmetic precision would imposed ccd architecture presented
massively parallel digital stochastic architecture hi described performs competitive kohonen types learning vlsi design shown neuron fits within small inexpensive mosis frame yet used build larger networks several hundred neurons neuron operates speed mhz allows network process training examples per second use level sensitive scan logic provides chip fault reliable neural systems built
during sleep brain mind undergo tightly linked precisely specified set changes state level neurons process has been modeled variations volterra equations cyclic fluctuations brainstem cell populations neural network models based upon rapidly developing knowledge specific population their differential responses have yet been developed furthermore most preliminary attempts have been made model across states our own attempts link rapid eye movement rem sleep neurophysiology cognition using neural network approaches summarized paper
based general non stationary point process model computed estimates synaptic coupling strength efficacy function time after stimulus onset between inhibitory its target postsynaptic cell cochlear nucleus data consist spike trains pairs neurons responding brief tone recorded vivo our results suggest synaptic efficacy non stationary further synaptic efficacy shown approximately linearly related average presynaptic spike rate second order analysis suggests latter result due non linear interactions synaptic efficacy less strongly correlated postsynaptic rate correlation consistent across neural pairs
recently miller have hebbian correlational rules synaptic development visual system miller has studied such rules case two populations fibres particularly two eyes analysis has so far assumed each two populations has exactly same correlational structure constraint considering effects small correlations within between eyes permits study stability solutions predict circumstances qualitative changes seen including production rather than driven units
develop model independent method characterizing reliability neural responses brief stimuli approach allows us measure discriminability similar stimuli based real time response single neuron neurophysiological data obtained neuron visual system furthermore recordings made photoreceptor cells quantify signal noise ratios peripheral visual system photoreceptors form input visual system reliability their signals ultimately determines reliability visual discrimination task case movement detection limit computed compared neurons reliability under favorable conditions performance neuron closely approaches theoretical limit means under these conditions nervous system adds little noise process computing movement correlations signals photoreceptor array
single neocortical neurons powerful multi layered networks recent compartmental modeling study has shown voltage dependent membrane nonlinearities present complex dendritic tree provide virtual layer local nonlinear processing elements between synaptic inputs final output cell body analogous hidden layer multi layer network paper abstract model neuron introduced called incorporates aspects dendritic cluster sensitivity phenomenon seen these detailed biophysical modeling studies shown using hebb type learning rule used extract higher order statistics set training patterns spatial ordering synaptic connections onto dendritic tree potential neurobiological relevance these higher order statistics nonlinear pattern discrimination studied within full compartmental model neocortical pyramidal cell using training set high dimensional sparse random patterns
single nerve cells static properties have traditionally been viewed building blocks networks show phenomena contrast approach study here overall network activity control single cell parameters such input well time space constants parameters crucial excitability integration using detailed computer simulations neocortical pyramidal cells show spontaneous background firing network provides means setting these parameters mechanism control through large conductance change membrane induced both non nmda nmda excitatory inhibitory synapses activated spontaneous background activity
dendritic trees cortical pyramidal neurons seem suited perform local processing inputs explore implications complexity computational power neurons simulated realistic biophysical model hippocampal pyramidal cell spot high density patch inhibitory ca dependent channels patch ca channels present dendritic branch point spot induced relationship between strength synaptic input probability neuronal fn ing effect could interpreted analog stochastic xor
ion channels dynamical systems nervous system their distribution within membrane communication information between neurons but information integrated within cell here argument presented anti hebbian rule changing distribution voltage dependent ion channels order voltage dendrites simulations show rule account self dynamical receptive field properties such resonance direction selectivity creates conditions conduction within cell signals cell has been exposed various possible cellular implementations such learning rule proposed including activity dependent channel proteins plane membrane
consider noisy bistable single neuron model driven periodic external modulation modulation introduces correlated switching between states driven noise information flow through system modulation output switching events leads strong peaks power spectrum signal noise ratio snr obtained power spectrum measure information content neuron response increasing noise intensity snr passes through maximum effect has been called stochastic resonance treat problem within framework recently developed approximate theory valid limits weak noise intensity weak periodic forcing low forcing frequency comparison results theory those linear system fft presented
during visual development projections retinal ganglion cells lateral geniculate nucleus lgn cat refined produce ocular dominance precise topographic mapping normal development depends upon activity suggesting key role activity dependent synaptic plasticity recent experiments retina show during early development waves activity pass across et al provide first simulations demonstrate such retinal waves conjunction hebbian synaptic competition early arrival contralateral axons account observed patterns projections normal experimentally treated animals
test whether known neurons spinal cord sufficient account connectionist neural network simulation done using identical cells connected according experimentally established patterns demonstrated network oscillates stable manner same phase relationships among neurons observed model used explore coupling between identical oscillators neurons have dual role rhythm generators between oscillators produce phase relations observed among segmental oscillators during
animal locomotion patterns controlled recurrent neural networks called central pattern generators although oscillate autonomously its rhythm phase well coordinated state physical system using sensory inputs paper propose learning algorithm neural physical oscillators specific phase relationships sensory input connections modified correlation between cellular activities input signals simulations show learning rule used setting sensory feedback connections well coupling connections between central sensory mechanisms locomotion control patterns animal locomotion such walking generated recurrent neural networks located segmental ganglia spinal these networks produce basic rhythms locomotion without sensory inputs called central pattern generators cpgs physical systems locomotion such combined physical environments have their own oscillatory characteristics therefore order realize efficient locomotion frequency phase oscillation well coordinated state physical system example patterns drive leg muscle coordinated configuration leg its contact ground state other doya yoshizawa oscillation pattern cpg largely affected inputs has been shown et al et al oscillation cpg cyclic stimuli sensory neurons over wide range frequency both negative positive feedback pathways found those systems function sensory inputs requires computational studies neural physical dynamical systems algorithms learning rhythmic patterns recurrent neural networks have been derived doya yoshizawa williams paper propose learning algorithm neural oscillator rhythmic input signals specific phase relationship well known coupling between nonlinear oscillators their frequencies relative phase between oscillators determined parameters coupling difference their intrinsic frequencies example either phase anti phase oscillation results symmetric coupling between neural oscillators similar intrinsic frequencies kawato efficient locomotion involves subtle phase relationships between physical variables motor commands accordingly our goal derive learning algorithm tune sensory input connections relative phase between physical neural oscillators kept specific value required task learning synchronization deal following continuous time model cpg network kl xt represent states outputs cpg neurons yt represents sensory inputs assume connection weights already established so network oscillates without sensory inputs goal learning find input connection weights make network state xt xt input signal yt yt specific relative phase objective function phase locking standard way derive learning algorithm find out objective function minimized approximate waveforms xit yt waves linear relationship xt specifies phase locked oscillation xt yt example have matrix even sinusoidal objective function zt kl adaptive synchronization neural physical oscillators determines specific relative phase between xt yt thus call pit phase lock matrix learning procedure using above objective function derive learning procedure oscillation xt yt first appropriate phase lock matrix identified while relative phase between xt yt changes gradually time feedback mechanism applied so network state xt kept close target waveform yt suppose actually have appropriate phase relationship between xt yt phase lock matrix obtained gradient descent et respect pit follows pit rl rl xit coupling between xt yt weak enough their relative phase changes time unless their intrinsic frequencies exactly equal systems completely noiseless modulating learning coefficient performance index total system example speed locomotion possible obtain matrix satisfies requirement task once phase lock matrix derived control xt close yt using gradient et respect network state simplest feedback algorithm add term cpg dynamics follows ri xit xit jl kl feedback gain set small enough so feedback term does intrinsic oscillation cpg case small additional decay term have xit jl kl equivalent equation input weights doya yoshizawa delayed synchronization tested above learning scheme delayed synchronization task find coupling weights between neural oscillators so they specific time delay used following coupled cpg model zx jl kl yt denote indices two cpgs goal learning waveforms yt time delay used performance index learning coefficient equation modulated deviation zt its running average using following equations zt figure learning delayed synchronization neural oscillators dotted solid curves represent yt yt respectively coupling cat cat adaptive synchronization neural physical oscillators first two cpgs trained independently oscillate sinusoidal waveforms period using continuous time back propagation learning doya yoshizawa each cpg composed two neurons time constants output functions tanh instead following two step procedure described previous section network dynamics learning equations simulated concurrently parameters ra figure shows oscillation two cpgs without coupling figures through show phase locked waveforms after learning time units different desired delay times zero locomotion next applied learning rule simplest locomotion system involves critical phase lock between state physical system motor command zero locomotion system shown figure physical system composed wheel weight moves back track fixed wheel ground changing its balance displacement weight order move wheel given direction weight moved specific phase rotation angle wheel motion equations shown first cpg network trained oscillate sinusoidal waveform period doya yoshizawa network consisted output two hidden units time constants ri output functions tanh next output cpg used drive weight force position velocity weight rotation angle angular velocity wheel used sensory feedback inputs after scaling order eliminate effect biases xt yt used following learning equations rotation speed wheel employed performance index zt after smoothing following equation zt learning coefficient modulated equations time constants rt rs ra each training run started random configuration wheel after ten seconds doya yoshizawa figure learning zero locomotion adaptive synchronization neural physical oscillators figure example motion wheel without sensory feedback rhythms cpg physical system each other wheel left right figure shows example wheel motion after runs training parameters first oscillation cpg down sensory inputs accelerated rotation wheel right direction compared patterns sensory input connections made after learning different sizes table shows connection weights output unit positive connection forces weight right hand side wheel stabilize rotation negative connection smaller radius rhythm cpg wheel fast weight up positive input larger radius makes weight both ends track down rhythm cpg table sensory input weights output unit pt radius cm cm cm cm cm discussion architectures cpgs lower determined genetic information nevertheless way animal utilizes sensory inputs adaptive characteristics physical environments changing dimensions its body parts back propagation through forward models physical systems applied learning sensory feedback jordan jacobs learning nonlinear dynamics locomotion systems difficult task moreover multi layer back propagation appropriate biological model learning learning rule similar covariance learning rule sejnowski biological model long term potentiation synapses acknowledgements authors thank those who gave comments our nips conference work partly supported grants science doya yoshizawa references feedback motor control doya yoshizawa adaptive neural oscillator using continuous time back propagation learning neural networks neural network underlying locomotion synaptic cellular mechanisms neuron jordan jacobs learning control unstable system forward modeling touretzky ed advances neural information processing systems san mateo ca morgan kaufmann kawato two coupled neural oscillators model journal theoretical biology learning state space trajectories recurrent neural networks neural computation sejnowski covariance storage hippocampus et al eds
dynamic behavior network model consisting excitatory coupled binary neurons global inhibition studied analytically numerically prove random input signals output network consists synchronized apparently random noisy activity our results suggest synchronous generated simple neuronal architecture incoming signals synchronization process oscillations themselves do play constructive role therefore considered
investigate model excitatory neurons have dynamical thresholds display both potentiation property leads oscillatory behavior responsible ability model perform segmentation ie decompose mixed input into oscillations activities cell assemblies memories affected potentiation responsible these oscillations after input turned off ie system serves model short term has limited capacity reminiscent number
present multi state time delay neural network ms tdnn extension tdnn robust word recognition unlike most other hybrid methods ms tdnn alignment search procedure into connectionist architecture allows word level supervision resulting system has ability sequential order units while optimizing recognizer performance paper present extensive new evaluations approach over speaker dependent speaker independent connected alphabet
focused gamma network proposed possible implementations gamma neural model focused gamma network compared focused backpropagation network tdnn time series prediction problem system identification problem
recently much interest has been generated regarding speech recognition systems based hidden markov models hmms neural network nn such systems attempt combine best features both models temporal structure hmms discriminative power neural networks work define time warping neuron extends operation formal neuron back propagation network warping input pattern match optimally its weights show single layer network neurons equivalent gaussian recognition system propose discriminative power system using back propagation discriminative training andor generalizing structure recognizer multi layered net performance proposed network evaluated highly isolated word multi speaker recognition task results indicate does recognition performance improve but separation between classes enhanced allowing us set up rejection criterion improve confidence system
issues relating estimation hidden markov model hmm local probabilities discussed particular note radial basis functions rbf networks tied mixture density modelling additionally highlight differences between these methods arising different training criteria employed present method connectionist training modified resolve these differences discuss preliminary experiments finally discuss problems discriminative training
subject paper integration multi layered artificial neural networks ann probability density functions such gaussian mixtures found continuous density hidden markov models hmm first part paper present hybrid parameters system simultaneously optimized respect single criterion second part paper study relationship between density inputs network density outputs networks few experiments presented explore perform density estimation anns
present janus speech speech translation system utilizes diverse processing strategies including connectionist learning traditional ai knowledge representation approaches dynamic programming stochastic techniques janus translates continuously spoken english german into german english japanese janus currently achieves translation fidelity english speech german speech present janus system along comparative evaluations its processing components special emphasis connectionist modules university now research technology center et al
propose paradigm modeling speech production based neural networks focus characteristics system using real physiological data articulator movements emg muscle activity neural network learns forward dynamics relating motor commands muscles ensuing articulator behavior after learning simulated perturbations used properties acquired model such natural frequency finally cascade neural network used generate continuous motor commands sequence discrete targets
recognition system reported recognizes names over brief between letters system uses separate neural networks locate segment boundaries classify letters letter scores used search database names find best scoring name speaker independent classification rate spoken letters system correct name between letters time database names
paper presents system generating connectionist parsing networks example based formal grammar systems toward spoken language tasks networks exhibit three strengths important application speech processing they learn parse generalize well compared grammars they several types noise they learn use multi modal input presented architecture performance analyses along several dimensions demonstrate features performance compared traditional grammar based parsing systems
paper considers problem calculus connectionist networks based energy minimization given logic knowledge base bound symmetric network constructed like machine hopfield network searches proof given query resolution based proof length longer than exists global minima energy function associated network represent such proofs network generated size bound linear knowledge size restrictions type logic formulas represented network inherently fault tolerant cope
present parallel distributed semantic network architecture addresses problems sequencing ambiguity resolution natural language understanding network stores their using multiple networks structured form semantic net mechanism called propagation filters employed control communication between networks properly sequence components phrase resolve ambiguities simulation results indicate networks propagation filters successfully represent high level knowledge trained relatively quickly provide parallel inferencing knowledge level
have developed four language automatic language identification system high quality speech system uses neural network based segmentation algorithm segment speech into seven broad phonetic categories phonetic features computed these categories input second network performs language classification system trained tested separate sets speakers american english japanese currently performs accuracy test set
present modular network architecture learning algorithm based incremental dynamic programming allows single learning agent learn solve multiple markovian decision tasks mdts significant transfer learning across tasks consider class mdts called composite tasks formed temporally number simpler elemental mdts architecture trained set composite elemental mdts temporal structure composite task assumed unknown architecture learns produce temporal decomposition shown under certain conditions solution composite constructed computationally inexpensive modifications solutions its constituent elemental mdts
paper examines whether temporal difference methods training connectionist networks such td algorithm cn successfully applied complex real world problems number important practical issues identified discussed general theoretical perspective these practical issues examined context case study td applied learning game backgammon outcome self play apparently first application algorithm complex nontrivial task found zero knowledge built network able learn scratch play entire game fairly strong intermediate level performance clearly better than conventional commercial programs fact comparable networks trained massive human expert data set hidden units these network have apparently discovered useful features goal computer games research furthermore set hand features added input representation resulting networks reach near expert level performance have achieved good results against world class human play
system employing connectionist networks music processing presented after being trained using error backpropagation system capable producing four part style given part our system solves musical real world problem performance level appropriate musical practice power based new coding scheme capturing relevant information integration backpropagation symbolic algorithms hierarchical system combining advantages both
network model temporal sequencing state dependent features described model motivated data characterizing different states computer studies demonstrate unique states sequencing exist within same network under different cholinergic influences relationships between state dependent modulation memory sequencing learning discussed
do you want neural net algorithm learn sequences do limit conventional gradient descent approximations instead use sequence learning algorithm do implement following method history compression matter what final goals train network predict its next input previous ones inputs convey new information ignore inputs but let inputs plus information about time step they become inputs higher level network same kind working slower self adjusting time scale go building hierarchy such networks principle reduces descriptions event sequences without loss information thus supervised reinforcement learning tasks alternatively you may use two recurrent networks multi level predictor hierarchy into single recurrent net experiments show systems based these principles require less computation per time step many fewer training sequences than conventional training algorithms recurrent nets finally you modify above method such defined fashion but continuous fashion
exist large classes time series such those nonlinear moving average components well modeled feedforward networks linear models but modeled recurrent networks show recurrent neural networks type nonlinear autoregressive moving average model practical ability shown results competition sound power light recurrent networks gave best performance electric load forecasting
second order recurrent networks recognize simple finite state languages over induced positive negative examples using complete gradient recurrent network sufficient training examples constrain definition language induced solutions obtained correctly recognize strings arbitrary length method extracting finite state automaton corresponding optimized network demonstrated
simple second order recurrent networks shown readily learn small known regular grammars trained positive negative strings examples show similar methods appropriate learning unknown examples their strings training algorithm incremental real time recurrent learning method computes complete gradient updates weights end each string after during training dynamic clustering algorithm extracts production rules neural network has learned methods illustrated extracting rules unknown deterministic regular grammars many cases extracted grammar outperforms neural net extracted classifying unseen strings
present framework programming hidden unit representations simple recurrent networks based use hint units additional targets output layer present two ways network trained within framework input patterns act operators information encoded context units patterns activation over context units act functions input sequences simulations demonstrate network learn represent three different functions simultaneously canonical discriminant analysis used investigate operators functions represented space hidden unit activations
two well known learning algorithms recurrent neural networks back propagation rumelhart el al forward propagation williams main drawback back propagation its off line backward path time error line requirement many practical applications although forward propagation algorithm used line manner drawback heavy computation load required update high dimensional sensitivity matrix operations each time step therefore develop fast forward algorithm challenging task paper proposed forward learning algorithm order faster operations each time step than sensitivity matrix algorithm basic idea instead integrating high dimensional sensitivity dynamic equation solve forward time its function avoid redundant computations update weights whenever error numerical example classifying state trajectories using recurrent network presented faster speed proposed algorithm than williams algorithm
winner take wta networks inhibitory interconnections used determine most highly activated pool units important part many neural network models unfortunately convergence normal wta networks extremely sensitive magnitudes their weights hand tuned generally provide right amount inhibition across relatively small range initial conditions paper presents winner take networks use regulatory unit provide competitive inhibition units network regulatory unit dynamically adjusts its level activation during competition provide right amount inhibition between drive single winner dynamic adaptation allows networks perform winner take function nearly network size initial condition using connections addition regulatory unit biased find level inhibition necessary upon most units therefore serve winners take network
because eye muscles never do deal external write equation relates firing rate eye position velocity situation cns head velocity linear manner using high background rate linearity circuits generate eye movements has allowed signal processing involved including neural network integrates these ideas often summarized block diagrams unfortunately they little value describing behavior single neurons finding supported neural network models
have investigated properties neurons inferior temporal cortex monkeys performing pattern matching task simple backpropagation networks trained discriminate various stimulus conditions basis measured neuronal signal trained networks predict neuronal response waveforms spatial patterns stimuli results indicate neurons convey temporally encoded information about both current patterns well about their behavioral context decoding neuronal signals visual pattern recognition
have previously described unsupervised learning procedure discovers spatially coherent properties world maximizing information parameters extracted different parts sensory input convey about common underlying cause given random dot stereograms curved surfaces procedure learns extract surface depth because property coherent across space learns interpolate depth location nearby locations becker hinton paper propose two new models handle surfaces discontinuities first model attempts detect cases discontinuities reject them second model develops mixture expert learns detect locations discontinuities specialized asymmetric do cross discontinuities
have constructed recurrent network images moving object retina simulated eye structure network motivated organization primate visual target tracking system basic components complete target tracking system simulated including visual processing sensory motor interface motor control our model simpler structure function performance than primate system but many complexities inherent complete system present recurrent eye tracking network using distributed representation image motion visual processing retinotopic eye maps velocity motor interface estimate retinal velocity motor figure overall structure visual tracking model target eye
networks reconstructing sparse noisy function often use edge field segment function into homogeneous regions approach assumes these regions do overlap have disjoint parts often false example images contain regions split object cant properly reconstructed using type network have developed network overcomes these limitations using support maps represent segmentation signal our approach support each region signal explicitly represented results initial implementation demonstrate method reconstruct images motion sequences contain complicated occlusion
network vision systems make inferences information across levels representational abstraction low level through intermediate scene segments high level relevant object descriptions paper shows such networks realized markov random fields mrfs show first construct mrf functionally equivalent transform parameter network thus establishing principled probabilistic basis visual networks second show these mrf parameter networks more capable flexible than traditional methods particular they have well defined probabilistic interpretation intrinsically incorporate feedback offer richer representations decision capabilities
shown both changes viewing position illumination conditions prior recognition using combinations images taken different viewing positions different illumination conditions shown agreement psychophysical findings computation requires least sign bit image input contours alone sufficient
neurons encoding simple visual features area such orientation direction motion color organized retinotopic maps recent physiological experiments have shown responses many neurons other cortical areas modulated direction gaze have developed neural network model visual cortex explore hypothesis visual features encoded coordinates early stages visual processing new experiments suggested testing hypothesis using electrical psychophysical observations
visual attention ability dynamically restrict processing subset visual field researchers have long argued such mechanism necessary efficiently perform many intermediate level visual tasks paper describes novel neural network model visual attention current system models search target objects scenes containing multiple natural task people studied extensively requires attention networks behavior closely matches known psychophysical data visual search visual attention matches much physiological data attention provides novel view functionality number visual areas paper concentrates biological plausibility model its relationship primary visual cortex superior colliculus posterior parietal areas
exhibit systematic way derive neural nets vision problems involves formulating vision problem bayesian inference decision comprehensive model visual domain given probabilistic grammar
combined neural network rule based approach suggested general framework pattern recognition approach enables unsupervised supervised learning respectively while providing probability estimates output classes probability maps utilized higher level analysis such feedback smoothing over output label maps identification unknown patterns pattern discovery suggested approach presented demonstrated texture analysis task correct classification rate achieved both unstructured structured natural texture advantages probabilistic approach pattern analysis demonstrated
visual object recognition involves identification images objects seen arbitrary suggest approach object recognition view represented collection points given their location image object modeled set views together correspondence between views show novel view object expressed linear combination stored views consequently build linear operator distinguishes between views specific object views other objects operator implemented using neural network architectures relatively simple structures
proposed feature extraction method related recent statistical theory friedman based biologically motivated model neuronal plasticity bienenstock et al method has been recently applied feature extraction context recognizing objects single views here describe experiments designed analyze nature extracted features their relevance theory psychophysics object recognition
method structural risk minimization refers tuning capacity classifier available amount training data capacity influenced several factors including properties input space nature structure classifier learning algorithm actions based these three factors combined here control capacity linear classifiers improve generalization problem handwritten digit recognition risk minimization capacity empirical risk minimization common way training given classifier adjust parameters classification function fx minimize error etrain ie frequency errors set training examples etrain estimates expected risk based empirical data provided available examples method thus called empirical risk minimization but classification function fx minimizes empirical risk does necessarily minimize generalization error ie expected value risk over full distribution possible inputs their corresponding outputs such generalization error cannot general computed but estimated separate test set other ways vapnik solla estimating include leave moving control method review see capacity guaranteed risk family classification functions fx characterized its capacity vapnik chervonenkis dimension vc dimension such capacity defined maximum number training examples learnt without error possible binary vc dimension cases simply given number free parameters classifier but most practical cases quite difficult determine analytically vc theory provides bounds let fx set classification functions capacity probability number training examples simultaneously classification functions fx generalization error lower than guaranteed risk defined etrain ep etrain rl proportional small etrain close fixed number training examples training error decreases capacity increases while both guaranteed risk generalization error go through minimum before minimum problem capacity small amount training data beyond minimum problem key issue therefore match capacity classifier amount training data order get best generalization performance method structural risk minimization provides way achieving goal structural risk minimization let us choose family classifiers define structure consisting nested subsets elements family defining such structure ensure capacity subset classifiers less than subset method amounts finding subset pt classifier minimizes empirical risk within such subset yields best overall generalization performance two problems arise implementing select ii find good structure problem arises because have direct access our experiments use minimum either select pt show these two minima close good structure reflects priori knowledge few provided theory solve problem ii find best compromise between two competing terms et reducing causes decrease but increase good structure should such decreasing vc dimension happens expense smallest possible increase training error now examine several ways such structure built structural risk minimization character recognition principal component analysis optimal brain damage weight decay consider three apparently different methods improving generalization performance principal component analysis preprocessing transformation input space optimal brain damage architectural modification through weight pruning regularization method weight decay modification learning algorithm case linear classifier these three approaches shown here control capacity learning system through same underlying mechanism reduction effective dimension weight space based curvature properties mean squared error mse cost function used training linear classifier mse training consider binary linear classifier fx function takes two values indicating class belongs vc dimension such classifier equal dimension input space number weights empirical risk given kl th example corresponding desired output problem minimizing etrain function approached different ways but often replaced problem minimizing mean square error mse cost function differs nonlinear function has been removed curvature properties mse cost function three structures investigate rely curvature properties mse cost function consider dependence mse parameters wi training leads optimal value parameter way reducing capacity set wi zero linear classifier reduces mse increase resulting setting wi lowest order proportional curvature mse wt decrease capacity should achieved smallest possible expense mse increase directions weight space corresponding small mse curvature good candidates elimination curvature mse specified hessian matrix second derivatives mse respect weights linear classifier hessian matrix given twice correlation matrix training inputs hessian matrix symmetric get cross terms assume simplicity first component vector constant set so corresponding weight introduces bias value vapnik solla facilitate decisions about simultaneous elimination several directions weight space elements hessian matrix after eigenvalues corresponding eigenvectors give principal directions takes mse rotated axis increase due setting simple form quadratic approximation becomes exact equality linear classifier corresponding small eigenvalues good directions elimination principal component analysis common way reducing capacity classifier reduce dimension input space thereby reduce number necessary free parameters weights principal component analysis pca feature extraction method based eigenvalue analysis input vectors dimension approximated linear combination vectors forming normal basis coefficients linear combination form vector dimension optimal basis least square sense given rn eigenvectors corresponding rn largest eigenvalues correlation matrix training inputs matrix structure obtained ranking classifiers according vc dimension classifier reduced optimal brain damage linear classifier pruning implemented two different but equivalent ways change input coordinates principal axis representation prune components corresponding small eigenvalues according pca train mse cost function ii change coordinates principal axis representation train mse first prune weights get weight vector dimension rn procedure understood preprocessing whereas procedure ii involves modification structure classifier network architecture two procedures become identical weight elimination ii based smallest eigenvalue criterion procedure ii reminiscent optimal brain damage obd weight pruning procedure applied after training obd best candidates pruning those weights minimize increase defined equation rn weights kept do necessarily correspond largest rn eigenvalues due extra factor equation either implementation vc dimension reduced weight decay capacity controlled through additional term cost function minimized simultaneously linear classifiers ranked according norm weight vector structure constructed structural risk minimization character recognition allowing within subset those classifiers satisfy positive bounds cr form increasing sequence ca cr sequence matched decreasing sequence positive lagrange multipliers such our training problem stated minimization mse within specific set implemented through minimization new cost function mse equivalent weight decay procedure mechanical analogy term like energy weights zero easier directions small curvature mse weights zero along principal directions hessian matrix associated small eigenvalues principal axis representation minimum cost function mse simple function minimum mse limit weight factor weights become negligible xi remain xi effect compared weight pruning pruning weights such reduces capacity otherwise analogy introduce weight decay capacity expression arises various theoretical valid broad spectra eigenvalues smoothing higher order units regularization combining several different structures achieves further performance improvements combination exponential smoothing preprocessing transformation input space regularization modification learning algorithm shown here improve character recognition generalization ability dramatically improved further
developed neural net architecture segmenting complex images ie localize two dimensional geometrical shapes scene without prior knowledge objects positions sizes scale variation built into network deal varying sizes algorithm has been applied video images find their identification numbers over characters located correctly data base images despite large variation lighting conditions often poor quality characters part network executed processor board containing analog neural net chip et al while rest implemented software model digital signal processor
present feed forward network architecture recognizing unconstrained handwritten multi digit string extension previous work recognizing isolated digits architecture single digit recognizer replicated over input output layer network coupled viterbi alignment module chooses best interpretation input training errors propagated through viterbi module novelty procedure segmentation done feature maps developed space displacement neural network rather than input pixel space
present neural network algorithm simultaneously performs segmentation recognition input patterns self detect input pattern locations pattern boundaries demonstrate neural network architecture character recognition using nist database report results resulting system simultaneously segments recognizes overlapping characters broken characters noisy images high accuracy
hand printed digits modeled splines governed about control points each known digit control points have preferred locations deformations digit generated moving control points away their locations images digits produced placing gaussian generators uniformly along spline real images recognized finding digit model most likely have generated data each digit model use elastic matching algorithm minimize energy function includes both deformation energy digit model log probability model would generate pixels image model lowest total energy uniform noise process included model image generation pixels noise digit model fitting poorly segmented image digit models learn modifying locations control points
method described generating plan like reflexive obstacle avoidance behaviour mobile robot experiments reported here use simulated vehicle primitive range sensor avoidance behaviour encoded set continuous functions perceptual input space these functions stored using trained variant adaptive critic algorithm vehicle explores its adapts its responses sensory stimuli so negative reinforcement arising collisions strategies local navigation therefore acquired explicitly goal driven fashion resulting trajectories form elegant paths through environment
whenever agent learns control unknown environment two principles have combined namely exploration long term optimization exploitation short term optimization many real valued connectionist approaches learning control realize exploration randomness action selection might costs assigned negative experiences basic idea presented paper make agent explore unknown regions more directed manner achieved so called map trained predict controllers accuracy used guiding exploration based bistable system enables smoothly switching attention between two behaviors exploration exploitation depending expected costs knowledge gain appropriateness method demonstrated simple robot navigation task
neural network solution proposed solving path planning problems faced mobile robots proposed network two dimensional neurons forming distributed representation robots workspace lateral interconnections between neurons cooperative so network exhibits oscillatory behaviour these oscillations used generate solutions dynamic programming equation context path planning simulation experiments imply these networks locate global optimal paths even presence substantial levels circuit noise dynamic programming path planning consider dof robot moving about dimensional world robots location denoted real vector collection locations forms set called workspace admissible point workspace location robot may set admissible points called free workspace free complement represents collection obstacles robot moves through workspace along path denoted parameterized curve pt admissible path lies robots free workspace assume initial robot position desired final position robot path planning problem find admissible path such optimality criterion satisfied path planning problem may stated more precisely optimal control viewpoint treat robot dynamic system characterized state vector control vector highest levels control hierarchy assumed robots dynamics modeled differential equation equation says state velocity equals applied control define what optimal performance functional introduced cp norm vector functional cp unity lies free workspace infinite otherwise weighting functional used insure control does take robot into obstacles equation ls optimality criterion minimizes robots control effort while controls do satisfy terminal constraints definitions optimal path planning problem states final time find control ut minimizes performance functional powerful method minimization problem use dynamic programming according dynamic programming optimal control obtained gradient optimal return function other words xj optimal return functional satisfies bellman equation dynamic optimization problem given above equation easily shown oj cp cp first order nonlinear partial differential equation terminal boundary condition once equation has been solved optimal path determined following gradient solutions equation generally obtained numerically solution approach numerically integrates full discretization equation time using terminal condition starting point proposed numerical solution attempting find characteristic trajectories nonlinear first order nonlinearities insure these characteristics exist locally ie open neighborhood about terminal condition resulting numerical solutions therefore valid local sense reflected fact errors introduced discretization process eventually result numerical solutions underlying principle optimality equation solving path planning problems local solutions based numerical integration equation acceptable due local nature resulting solutions global solutions required these may obtained solving associated variational problem assume optimal return function time known closed set variational solution equation states optimal return time point neighborhood boundary set given min tf oscillatory neural fields globally optimal path planning denotes norm vector equation easily generalized other vector applies regions cp ie robots free workspace obstacles tf tf other words optimal return obstacles oscillatory neural fields proposed neural network consists mn neurons called neural field neurons put correspondence ordered pairs ij ordered pair ij sometimes called neurons label associated jth neuron set neuron labels denoted neurons whose labels lie called neighbors jth neuron neuron characterized two states short term activity sta state scalar representing neurons activity response currently applied stimulus long term activity lta state scalar representing neurons average activity response recently applied stimuli each neuron produces output unit step function sta state ie fix fx neuron called active inactive its output unity zero respectively each neuron characterized set constants these constants either externally applied inputs internal parameters they disturbance rate constant position vector position vector vector mapping neuron onto robots workspace rate constant models sta states underlying dynamic time constant rate constant used encode whether neuron maps onto obstacle robots workspace external disturbance used networks search optimal path evolution sta lta states controlled state equations these equations assumed change synchronous fashion sta state equation summation over neurons contained within neighborhood neuron function gx zero function used prevent neurons activity level below zero network parameters controlling strength lateral interactions between neurons lta state equation equation means lta state every time jth neurons output changes specific choices interconnection weights result oscillatory behaviour specific network under consideration cooperative field ij kl ij without loss generality assumed external bounded between zero assumed rate constants either zero unity path planning application rate constants used encode whether given neuron represents obstacle point free workspace consequently neuron called obstacle neuron neuron called free space neuron under these assumptions has been shown once free space neuron turns active oscillating period provided has least free space neuron neighbor path planning neural fields oscillatory neural field introduced above used generate solutions bellman iteration eq respect norm assume neuron sta lta states zero time assume position vectors form regular grid points constant controlling grids size assume external but zero other words specific neuron label ij kl ij zero otherwise assume neighborhood structure consist jth neuron its eight nearest neighbors these assumptions has been shown lta state jth neuron time given gn pit pit length shortest path respect norm fact seen quite clearly examining lta states dynamics small closed neighborhood about kth neuron first note lta state equation simply lta state every time neurons sta state its output neuron oscillates after has been initially activated lta state represent time neuron first activated time turn simply length shortest path site initial particular consider neighborhood set kth neuron lets assume kth neuron has yet been activated neighbor has been activated lta state given value see kth neuron activated next cycle have max simply dual form bellman iteration shown equation other words over free space neurons conclude network solving bellman equation respect norm light discussion use cooperative neural fields path planning straightforward first apply disturbance neuron mapping onto desired terminal position pt allow field generate sta oscillations neuron mapping onto robots current position activated stop oscillatory behaviour resulting lta state distribution jth neuron equals negative minimum distance respect norm neuron initial disturbance optimal path generated sequence controls gradient lta state distribution oscillatory neural fields globally optimal path planning fig sta activity waves fig lta distribution several simulations cooperative neural path planner have been implemented most complex case studied these simulations assumed array neurons several obstacles irregular shape size randomly distributed over workspace initial disturbance introduced desired terminal location sta oscillations observed neuronal outputs shown figure figure clearly shows neuronal activity propagating away initial disturbance neuron upper right hand corner figure activity waves propagate around obstacles without activity waves reach neuron mapping onto robots current position sta oscillations turned off lta distribution resulting particular simulation run shown figure figure light regions denote areas large lta state dark regions denote areas small lta state generation optimal path computed robot moving towards its goal let robots current position neurons position vector robot generate control takes position associated jth neurons neighbors particular control chosen so robot moves neuron whose lta state largest neighborhood set other words next position vector chosen such its lta state max way because lta distribution optimality property local control strategy guaranteed generate optimal path respect norm connecting robot its desired terminal position should noted selection control done analog neural network case lta states neurons neighborhood set used inputs neural net competitive interactions network always select direction largest lta state neuronal dynamics analog nature important consider impact noise implementation analog systems generally exhibit noise levels effective dynamic ranges being most bits noise enter network several ways lta state equation have noise term lta noise so lta distribution may optimal distribution our experiments assumed lta noise additive white noise may enter selection robots controls selection noise case robots next position position vector such iid array stochastic processes simulation results reported below assume noise processes positive uniformly distributed iid processes
present two neural network controller leaming schemes based learning modular architecture recognition control multiple manipulated objects first scheme gating network trained acquire object specific representations recognition number objects sets objects second scheme estimation network trained acquire function specific rather than object specific representations directly estimate physical parameters both recognition networks trained identify manipulated objects using andor visual information after learning appropriate motor commands manipulation each object control networks
approach uses neural networks refine knowledge written form simple rules extend idea further presenting algorithm mathematical equations governing controller determine topology initial weights network further trained using backpropagation apply method task controlling temperature water producing statistically significant gains accuracy over both standard neural network approach non learning controller furthermore using knowledge weights network produces statistically less variation accuracy compared networks initialized small random numbers
method transforming performance evaluation signals both space time into signals supervised learning algorithms presented jordan jacobs examined simple observation concerning through models trained redundant inputs their networks explains original architecture suggests modification internal world model encodes action space exploration crucially input redundancy forward model added learning time example task balancing thereby reduced about times
large class motor control tasks requires each cycle controller its current state choose action achieve specified state dependent goal behaviour paper optimization learning rate number experimental control decisions before adequate performance obtained robustness importance necessary expense computation per control cycle memory requirement motivated observation robot requires two thousand learning steps achieve adequate performance robot gets while learning always whereas moderate computational expense increasingly powerful computer hardware assume existence inexpensive controllers within few years so even processes control cycles low tens milliseconds have machine instructions make their decisions paper outlines learning control scheme aims make effective use such computational power memory based learning memory based learning approach applicable both classification function learning experiences presented learning box explicitly memory set input output pairs prediction required output novel input memory obtain experiences inputs close these local used determine locally consistent output query three memory based techniques nearest kernel regression local weighted regression shown accompanying figure moore ut nearest yi kernel regression known minimizes zi local weighted yi general
backpropagation algorithm used both recognition generation time trajectories used recognizer has been shown performance network greatly improved adding structure architecture same true trajectory generation particular new architecture corresponding tdnn proposed results show dramatic improvement performance generation hand written characters combination tdnn tdnn compact encoding suggested
introduce demonstrate bootstrap method construction inverse function robot kinematic mapping using sample data unsupervised learning clustering techniques used pre image neighborhoods order learn partition configuration space into subsets over kinematic mapping supervised learning used separately each partitions approximate inverse function ill posed inverse kinematics function thereby regularized global inverse kinematics solution developed
accurate saccades require interaction between brainstem circuitry cerebellum model interaction described based principle feedback error learning model part brainstem superior colliculus acts simple feedback controller knowledge initial eye position provides error signal cerebellum correct eye muscle nonlinearities cerebellum modelled cmac adjust appropriately gain brainstem burst generators internal feedback loop so size burst direction errors system rapidly learns make accurate horizontal eye movements starting position adapts subsequent simulated eye muscle displacement saccadic target
based model presented controls simulated kinematic arm during goal directed reaches network generates quasi feedforward motor command learned using training signals generated movements each target network selects sets output subset pattern generators during movement feedback turns off pattern generators task facing individual pattern generators recognize arm reaches target turn off distributed representation motor command resembles population vectors seen io produced naturally these simulations
using double step target displacement paradigm mechanisms underlying arm trajectory modification investigated using short msec inter stimulus intervals resulting hand motions initially directed between first second target locations kinematic features modified motions accounted superposition scheme involves addition two independent point motion units moving hand toward internally specified location second moving between location final target location similarity between inferred internally specified locations previously reported measured end points first saccades double step eye movement studies may suggest similarities between perceived target locations eye hand motor control
work discusses various optimization techniques proposed models controlling arm movements particular minimum muscle change model investigated dynamic simulator monkeys arm including single double joint muscles utilized generate horizontal hand movements hand trajectories produced algorithm discussed
current make use simple classification algorithms determine patient conditions subsequently enable proper simplicity primarily due constraints power dissipation area available sub threshold implementation artificial neural networks offer potential classifiers higher performance than available paper explore several classifier architectures implementation issues
head common yet potentially serious disorder detected its early stages magnetic resonance imaging have developed multi layer perceptron networks trained conjugate gradient optimization single magnetic resonance images head accuracy training data accuracy test data
automated monitoring vigilance attention intensive tasks such air control operation highly desirable operator would monitor operator against have taken first step toward goal using feedforward neural networks trained backpropagation interpret event related potentials eeg associated periods high low vigilance accuracy our system erp data set averaged over minutes better than accuracy obtained using linear discriminant analysis practical vigilance monitoring require prediction over shorter time periods able average over little minutes still get correct prediction vigilance measure additionally achieved similarly good performance using segments eeg power spectrum short sec
bayesian framework give principled account prior knowledge such imperfect analytic domain theories optimally incorporated into networks locally tuned units choosing specific architecture applying specific training our method proved successful overcoming data deficiency problem large scale application devise neural control line achieves application significantly higher accuracy than optimally tuned standard algorithms such sigmoidal backpropagation outperforms state art solution
paper deals application neural networks remote sensing observations because complexity application large amount data problem cannot solved using single method solution propose build nn architectures several nn together such system suffer generic problem propose solutions they allow reach accurate performances multi valued function approximations probability estimations results compared six other methods have been used problem show methodology have developed general used large variety applications
present development decoder complex binary error correcting code via training examples decoded received words our decoder connectionist architecture describe two separate solutions level solution cascaded networks decoder enhanced decoder solution simplifies mapping problem solved decoding although both solutions meet our basic approach constraint simplicity compactness enhanced decoder our second basic constraint being generic solution
paper tree based neural network friedman modelling yield strength described inputs time series model temperature rate time output corresponding yield found based model reveals variables functional dependence nonlinear significant results compared those obtained using kalman filter based online tuning method other classification methods eg cart bayesian classification found based method consistently outperforms other methods
five experiments performed using several neural network architectures identify location wave time ordered graphical results medical test baseline results first experiment found correct identification target wave cases other experiments investigated effect different architectures preprocessing raw data results methods used seem most appropriate time oriented graphical data has clear starting point such rather than continuous tests such
paper briefly describes artificial neural network visual processing network capable image motion type stimulus most popular methods detection subset second order visual motion stimuli known drift balanced processing stages network described paper into model capable simultaneous motion extraction edge detection determination occlusion
routing scheme uses neural network has been developed aid establishing point point communication through interconnection networks neural network network type examined hopfield hopfield work problem establishing through random shared memory distributed computing system addressed performance neural network routing scheme compared two more traditional approaches exhaustive search routing greedy routing results suggest neural network may competitive certain
have created new networks signals have been mixed either time delays via filtering first show subset learning rules principle minimum output power apply principle extensions network have delays feedback path our networks perform well real speech music signals have been mixed using time delays filtering
ccd based processor call presented implements fully connected input output two layer network cascaded form multilayer networks used parallel additional input output nodes device computes clocked mhz network weights specified six bits accuracy stored chip programmable digital memories neural network pattern recognition system using ccd image feature extractor devices described additionally report ccd output circuit exploits inherent nonlinearities charge injection process realize adjustable threshold sigmoid chip area
ccd based signal processing ic computes fully parallel single vector matrix multiplication has been designed fabricated tm process device incorporates array charge coupled devices ccd hold analog matrix charge encoding matrix elements input vectors digital bit accuracy
biological retinas extract spatial temporal features attempt reduce complexity performing visual tasks have built tested silicon retina encodes several useful temporal features found vertebrate retinas cells our silicon retina selective direction highly sensitive positive contrast changes around ambient light level tuned particular velocity inhibitory connections null direction perform direction selectivity silicon retina die consists array photoreceptors
goal perception extract invariant properties underlying world computing contrast edges retina reduces light intensities spanning decades variation stroke solves dynamic range problem extracts relative us step closer goal have built silicon retina models major synaptic interactions outer layer vertebrate retina using current mode cmos circuits namely synapses between horizontal cells produce receptive field cone horizontal cell gap junctions determine its size chip has pixels mm die pm well technology fully functional
board described contains neural network chip digital signal processor analog neural network arithmetic unit chip performs mixed processing combination allows high speed end end execution numerous signal processing applications including preprocessing neural net calculations steps board evaluates neural networks times faster than alone board suitable implementing large million connections networks sparse weight matrices three applications have been implemented board network detection text blocks handwritten digit recognizer neural network recognition based segmentation
use constrained optimization select operating parameters two circuits simple transistor square root circuit analog vlsi artificial cochlea automated method uses computer controlled measurement test choose chip parameters minimize difference between actual circuits behavior specified goal behavior choosing proper circuit parameters important compensate deviations adjust circuit performance within certain range biologically motivated analog vlsi circuits become increasingly complex more parameters setting these parameters hand become more thus automated parameter setting method great value automated parameter setting integral part goal based engineering design methodology circuits constructed parameters enabling wide range behaviors tuned desired behaviors automatically
novel segmentation algorithm has been developed utilizing penalty instead more common quadratic regularizer functional piece wise constant constraint segmented data energy guaranteed problems local complex methods necessary find unique global minimum interpreting minimized energy generalized power nonlinear resistive network continuous time analog segmentation circuit constructed
present experimental data analog cmos chip implements adaptive neural network testing procedures results time frequency domain described these include weight convergence trajectories extraction signal noise separation statistically complex signals such speech
many auditory consider temporal adaptation auditory nerve key aspect speech coding auditory periphery experiments models auditory localization pitch perception suggest temporal adaptation important element practical auditory processing have designed fabricated successfully tested analog integrated circuit models many aspects auditory nerve response including temporal adaptation
demonstrate self organizing system based ring oscillators employ system two ways both thought feature extractors acts set images exposed repeatedly system strictly linear feature extractor other serves signal optic communications both systems implement unsupervised competitive learning embedded within mode interaction dynamics between modes set ring oscillators after training period modes become associated different image features frequencies within incoming data stream
learning posed problem function estimation two principles solution considered empirical risk minimization structural risk minimization these two principles applied two different statements function estimation problem global local systematic improvements prediction power illustrated application code recognition
bayesian model comparison framework reviewed bayesian occams razor explained framework applied feedforward networks making possible objective comparisons between solutions using alternative network architectures objective choice magnitude type weight decay terms quantified estimates error bars network parameters network output framework generates measure effective number parameters determined data relationship bayesian model comparison recent work prediction generalisation ability el ai moody discussed bayesian inference occams razor science central task develop compare models account data gathered typically two levels inference involved task data modelling first level inference assume models true fit model data typically model includes free parameters fitting model data involves inferring what values those parameters should probably take given data repeated each model second level inference task model comparison here current address cambridge cb wish compare models light data sort preference ranking alternatives example consider task interpolating noisy data set data set could interpolated using splines model polynomials feedforward neural networks first level inference find each individual model best fit process sometimes known learning second level inference want rank alternative models state our particular data set example splines probably best interpolation model modelled polynomial should probably best neural network data set has eight hidden units model comparison difficult task because possible simply choose model fits data best more complex models always fit data better so maximum likelihood model choice leads us implausible models poorly occams razor principle states complex models should preferred simpler ones bayesian methods automatically quantitatively occams razor without
paper investigate average case model concept learning give results place popular statistical physics vc dimension theories learning curve behavior common framework
complexity learning dimensional neural networks has been shown elsewhere linear size network network has huge number units cortex has even linear time might furthermore algorithm given achieve time based single serial processor biologically implausible work consider more natural parallel model processing demonstrate expected time complexity constant ie independent size network holds even inter node communication channels short local thus more biological vlsi constraints
report learning measurements system composed learning chip data generators training pattern presentation windows based software interface neuron learning chip has adaptive synapses perform boltzmann mean field learning using separate noise gain controls have used system do learning experiments parity problem system set time limits learning speed about patterns per second roughly independent system size
paper applies theory probably approximately correct pac learning multiple output feedforward threshold networks weights certain shown sample size reliable learning bounded above formula similar required single output networks best previously obtained bounds improved cases
batch gradient descent minimum quadratic form time constant better than minimum maximum eigenvalues hessian matrix respect recently shown adding momentum term improves although batch case here show momentum lower further regard gradient descent momentum dynamic system explore error surface showing saturation error accounts variety effects observed simulations popular heuristics
many machine learning applications has access training data but high level priori knowledge about desired behavior system example known advance output character recognizer should invariant respect small spatial distortions input images translations rotations scale changes have implemented scheme allows network learn derivative its outputs respect distortion operators our choosing reduces learning time amount training data but provides powerful language what generalizations wish network perform
define concept polynomial uniform convergence relative frequencies probabilities distribution dependent context let let probability distribution let family events family fi has property polynomial uniform convergence probability maximum difference over between relative frequency probability event exceed given positive most sample frequency evaluated has size polynomial given sample let vapnik chervonenkis dimension family fi fn expectation show has property polynomial uniform convergence iff exists such applications distribution dependent pac learning discussed
study particular type boltzmann machine bipartite graph structure called our interest using such machine model probability distribution binary input vectors analyze class probability distributions modeled such machines showing each class includes arbitrarily good distribution set vectors binary inputs present two learning algorithms these machines first learning algorithm standard gradient ascent heuristic computing maximum likelihood estimates parameters ie weights thresholds model here give closed form gradient significantly easier compute than corresponding gradient general boltzmann machine second learning algorithm greedy method creates hidden units computes their weights time method variant standard method projection pursuit density estimation give experimental results these learning methods synthetic data natural data domain handwritten digits
present distribution free model incremental learning concepts vary time concepts caused change while incremental learning algorithm attempts track changing concepts minimizing error between current target concept hypothesis single intersection two half show average mistake rate depends maximum rate modify concept these theoretical predictions verified simulations several learning algorithms including back propagation
general relationship developed between vc dimension statistical lower capacity shows vc dimension lower bounded order statistical lower capacity network trained random samples relationship explains quantitatively generalization takes place after relates concept generalization consistency capacity optimal classifier over class classifiers same structure capacity bayesian classifier furthermore provides general methodology evaluate lower bound vc dimension feedforward multilayer neural networks general methodology applied two types networks important hardware implementations two layer networks binary weights integer thresholds hidden units zero threshold output unit single neuron networks binary zero threshold specifically obtain here total number ow dl weights networks dl represent networks respectively
paper address important question machine learning what kind network architectures work better what kind problems projection pursuit learning network has similar structure hidden layer sigmoidal neural network general method based continuous version projection pursuit regression developed show projection pursuit regression works better angular smooth functions than laplacian smooth functions exists ridge function approximation scheme avoid curse dimensionality approximating functions
important issue neural computation dynamic range weights neural networks many experimental results learning indicate weights networks grow prohibitively large size inputs here address issue studying between depth size weights polynomial size networks linear threshold elements ltes show efficient way simulating network ltes large weights network ltes small weights particular prove every depth polynomial size network ltes large integer weights simulated depth polynomial size network ltes bounded integer weights prove these results use tools harmonic analysis boolean functions our technique quite general provides insights other problems example able improve best known results depth network linear threshold elements computes comparison sum product two bits numbers maximum sorting bit numbers
has been observed numerical simulations weight decay improve generalization feed forward neural network paper explains why proven weight decay has two effects linear network first suppresses irrelevant components weight vector choosing smallest vector solves learning problem second size chosen right weight decay effects static noise targets improves generalization quite lot shown extend these results networks hidden layers non linear units finally theory confirmed numerical simulations using data
describe neural network called rulenet learns explicit symbolic condition action rules formal string manipulation domain rulenet discovers functional categories over elements domain various points during learning extracts rules operate these categories rules back into rulenet training process called iterative projection incorporating rules way rulenet exhibits enhanced learning generalization performance over alternative neural net approaches integrating symbolic rule learning category learning rulenet has capabilities go beyond purely symbolic system show architecture applied problem case role assignment natural language processing yielding novel rule based solution
propose empirically evaluate method extraction rules trained neural networks our method operates context three step process learning uses rule based domain knowledge combination neural networks empirical tests using problems molecular biology show rules our method extracts trained neural networks closely reproduce accuracy network they superior rules derived learning system directly symbolic rules expert
paper present neural network architecture discovers recursive decomposition its input space based generalization modular architecture jacobs jordan hinton architecture uses competition among networks recursively split input space into nested regions learn separate associative mappings within each region learning algorithm shown perform gradient ascent log likelihood function captures architectures hierarchical structure
way neural networks so they generalize better add extra term io error function penalize complexity propose new penalty term distribution weight values modelled mixture multiple gaussians under model set weights simple weights clustered into subsets so weights each cluster have similar values allow parameters mixture model adapt he same time network learns simulations demonstrate complexity term more effective than previous complexity terms
alternative typical technique selecting training examples independently fixed distribution formulated analyzed current example presented repeatedly until error item reduced criterion value another item randomly selected convergence time dramatically increased decreased heuristic depending task sensitive value
stochastic gradient descent general algorithm includes lms line backpropagation adaptive means clustering special cases standard choices learning rate both adaptive fixed functions time often perform quite poorly contrast our recently proposed class search converge learning rate schedules moody display theoretically convergence rate superior ability escape poor local minima user responsible setting key parameter propose here new methodology creating first completely automatic adaptive learning rates achieve optimal rate convergence
although detection invariant structure given set input patterns many recognition tasks connectionist learning rules tend focus directions high variance principal components prediction paradigm often used here suggest more direct approach invariant learning based anti hebbian learning rule unsupervised two layer network implementing method competitive setting learns extract coherent depth information random dot stereograms
several parallel analogue algorithms based upon mean field theory approximations underlying statistical mechanics formulation requiring externally prescribed annealing schedule now exist finding approximate solutions difficult combinatorial optimisation problems they have been applied salesman problem well various issues computational vision cluster analysis show here given algorithm combined natural way notions areas constrained optimisation adaptive simulated annealing yield single efficient parallel relaxation technique externally prescribed annealing schedule longer required results numerical simulations city city problems presented show ensuing algorithms typically order magnitude faster than algorithms alone show superior solutions well
method proposed improving generalization capability feedforward network trained backpropagation algorithm use artificial training vectors obtained adding noise original training vectors discuss connection such backpropagation training noise kernel density kernel regression estimation compare simulated examples backpropagation backpropagation noise kernel regression mapping estimation pattern classification contexts
connections between spline approximation approximation rational functions feedforward neural networks studied potential improvement degree approximation going single two hidden layer networks examined results regarding degree approximation achievable knot positions chosen basis probability distribution examples rather than function values extended
feedforward networks composed units compute sigmoidal function weighted sum their inputs have been much investigated tested approximation estimation capabilities networks using functions more complex than sigmoids three classes functions tested polynomials rational functions flexible fourier series unlike sigmoids these classes fit non monotonic functions they compared three problems prediction boston housing count robot arm inverse dynamics complex units attained clearly superior performance robot arm problem highly non monotonic pure approximation problem noisy nonlinear boston housing problems differences among complex units revealed polynomials did poorly whereas flexible fourier series comparable sigmoids
paper concerned problem learning networks functions involved smooth examples such networks those whose neural transfer functions piecewise linear those whose error function defined terms norm up now networks whose neural transfer functions piecewise linear have received little consideration literature but possibility using function defined terms norm has received attention latter work problems occur gradient methods used error functions have been addressed paper draw upon recent results field present algorithm case our motivation work out fact have been able show backpropagation error function based upon norm overcomes difficulties occur using norm
present iterative algorithm nonlinear regression based construction sparse polynomials polynomials built sequentially lower higher order selection new terms accomplished using novel look ahead approach predicts whether variable contributes remaining error algorithm based tree growing heuristic lms trees have extended approximation arbitrary input features addition provide new theoretical justification heuristic approach algorithm shown discover known polynomial samples make accurate estimates pixel values image processing task
constructive algorithm proposed feed forward neural networks uses node splitting hidden layers build large networks smaller ones small network forms approximate model set training data split creates larger more powerful network approximate solution already found smaller network modelling system generated data leads oscillation those hidden nodes whose weight vectors cover regions input space more detail required model these nodes identified split two using principal component analysis allowing new nodes cover two main modes each oscillating vector nodes selected splitting using principal component analysis oscillating weight vectors examining hessian matrix second derivatives network error respect weights second derivative method applied input layer provides useful relative parameters classification task node splitting standard multi layer perceptton equivalent introducing hinge decision boundary allow more detail learned initial results promising but further evaluation indicates long range effects decision boundaries cause new nodes back old node position gained problem does occur networks localised receptive fields such radial basis functions mixtures technique appears work well node splitting algorithm feed forward neural networks
automatic determination proper neural network topology over sized networks important area study has previously been addressed using variety techniques paper present information measure based new approach problem hidden units removed based their information measure im measure decision tree induction techniques reflects degree hyperplane formed hidden unit between training data classes show results applying three classification tasks demonstrate removes substantial number hidden units without significantly affecting network performance
algorithm building functional models data uses genetic search discover combinations basis functions used build least squares regression model because produces population models evolve over time rather than single model allows analysis possible other regression based approaches
probabilistic neural network algorithm represents likelihood function given class sum identical isotropic gaussians practice often excellent pattern classifier outperforming other classifiers including backpropagation robust respect affine transformations feature space lead poor performance certain data have derived extension called weighted allowing anisotropic gaussians ie gaussians whose covariance multiple identity matrix covariance optimized using genetic algorithm interesting features its redundant logarithmic encoding large population size experimental results validate our claims
designed trained connectionist network generate new given few exemplars during learning our network constructed distributed internal representation well letters despite fact each training instance both letter necessary have separate but interconnected hidden units letter representations several alternative architectures successful
compare two strategies training connectionist well models statistical pattern recognition probabilistic strategy based notion bayesian discrimination ie optimal classification achieved classifier learns posteriori class distributions random feature vector differential strategy based notion identity largest class posteriori probability feature vector needed achieve bayesian discrimination each strategy directly linked family objective functions used supervised training procedure prove probabilistic strategy linked error measure objective functions such mean squared error cross entropy typically used train classifiers necessarily requires larger training sets more complex classifier architectures than those needed approximate bayesian discriminant function contrast prove differential strategy linked objective functions requires minimum classifier functional complexity training examples necessary approximate bayesian discriminant function specified precision measured probability error present our proofs context game chance unfair die repeatedly show game dice paradigm root statistical pattern recognition tasks demonstrate simple extension concept leads us general information theoretic model sample complexity statistical pattern recognition
three methods improving performance gaussian radial basis function rbf networks tested task rbf new example classified computing its euclidean distance set centers chosen unsupervised methods application supervised learning learn non euclidean distance metric found reduce error rate rbf networks while supervised learning each centers variance resulted inferior performance best improvement accuracy achieved networks called generalized radial basis function networks center locations determined supervised learning after training words rbf classifies letters correct while scores letters correct separate test set these other experiments conclude supervised learning center locations important radial basis function learning
optimizing performance self organizing feature maps like kohonen map involves choice output space topology present topographic product measures preservation neighborhood relations criterion optimize output space topology map regard global dimensionality well dimensions individual directions test topographic product method synthetic mapping examples but speech data latter application our method suggests output space dimensionality recent recognition results same data set
present here interesting experiment quick modeling humans performed independently small samples several languages two over last three years comparisons decision tree procedures neural net processing given these conjecture human reasoning better represented latter but substantially different both implications strong convergence hypothesis between neural networks machine learning discussed now expanded include human reasoning comparisons
two projection based feedforward network learning methods regression problems studied compared paper popular back propagation learning other projection pursuit learning unlike parametric method non estimates unknown nonlinear functions sequentially neuron neuron layer layer each iteration while jointly estimating interconnection weights terms learning efficiency both methods have comparable training speed based optimization algorithm while more parsimonious terms learning robustness toward noise outliers more sensitive outliers
existing metrics learning performance feed forward neural networks do provide basis comparison because choice training epoch limit determine results comparison propose new metrics have desirable property being independent training epoch limit efficiency measures yield correct networks proportion training effort optimal epoch limit provides efficiency learning performance modelled statistically asymptotic performance estimated implementation details may found
present novel classification regression method exploratory projection pursuit training projection pursuit regression supervised training yield new family penalty terms improved generalization properties demonstrated real world problems
paper describes technique learning both number states topology hidden markov models examples induction process starts most specific model consistent training data generalizes successively merging states both choice states merge stopping criterion guided bayesian posterior probability compare our algorithm baum welch method estimating fixed size models find induce minimal hmms data cases fixed estimation does converge requires redundant parameters converge
artificial neural networks interconnected collection certain nonlinear devices examples commonly used devices include linear threshold elements sigmoidal elements radial basis elements employ results harmonic analysis theory rational approximation obtain almost tight lower bounds size ie number elements neural networks class neural networks our techniques applied quite general includes feedforward network each element piecewise approximated low degree rational function example prove depth network sigmoidal units linear threshold elements computing parity function variables have size fixed addition prove lower bound almost tight showing parity function computed sigmoidal units linear threshold elements depth network these almost tight bounds first known complexity results size neural networks depth more than two our lower bound techniques yield unified approach complexity analysis various models neural networks feedforward structures moreover our results indicate context computing highly oscillating symmetric boolean func tions networks continuous output units such sigmoidal elements do offer significant reduction size compared networks linear threshold elements binary outputs
recurrent networks recurrent networks incorporate associative memory techniques sequential structure easily quickly trained using gradient descent techniques generate sequences discrete outputs trajectories through continuous space performance found superior ordinary recurrent networks these sequence generation tasks
boosting algorithm learning machine error rate less than arbitrarily low error rate algorithm discussed here depends having large supply independent training samples show problem generate ensemble learning machines whose performance optical character recognition problems dramatically improved over single network report effect boosting four databases handwritten consisting digits segmented codes state service following national institute testing nist digits upper case lower case use two performance measures raw error rate reject rate required achieve error rate patterns boosting improved performance cases factor three
memory based classification algorithms such radial basis functions nearest neighbors typically rely simple distances euclidean dot product particularly meaningful pattern vectors more complex better suited distance measures often expensive rather ad hoc elastic matching templates propose new distance measure made locally invariant set transformations input computed efficiently tested method large handwritten character databases provided post office nist using invariances respect translation rotation scaling line method consistently outperformed other systems tested same databases
artificial neural network ann commonly modeled threshold circuit network interconnected processing units called linear threshold gates depth network represents number unit delays time parallel computation size circuit number gates measures amount hardware known traditional logic circuits consisting unbounded fan gates would require least depth compute common arithmetic functions such product two bit numbers unless allow size fan increase exponentially show paper anns much more powerful than traditional logic circuits particular prove iterated addition computed depth ann multiplication division computed depth anns polynomial size bounded integer weights respectively moreover follows known lower bound results these anns optimal depth indicate these techniques applied construct polynomial size depth ann depth ann multiple product
although considerable interest has been shown language inference automata induction using recurrent neural networks success these models has mostly been limited regular languages have previously demonstrated neural network automaton model capable learning deterministic context free languages eg languages examples learning task computationally intensive paper ways priori knowledge about task data could used efficient learning observe such knowledge often experimental learning nontrivial languages eg
address problem learning unknown function together several pieces information hints know about function introduce method generalizes learning examples learning hints canonical representation hints defined illustrated new types hints hints represented learning process examples examples function treated equal rest hints during learning examples different hints selected processing according given schedule present two types schedules fixed schedules specify relative emphasis each hint adaptive schedules based well each hint has been learned so our learning method compatible descent technique may choose use
resource allocation network ran modified reinforcement learning paradigm existing hidden units rather than adding new units after units continue learn via back propagation resulting algorithm tested learning network learns solve pendulum problem solutions found faster average algorithm than without
multi layered neural network hidden layers viewed computing distributed representation input several encoder experiments have shown representation space small fully used but computing such representation requires completely nodes case hidden nodes noisy unreliable find error correcting schemes emerge simply using noisy units during training random errors during backpropagation result spreading representations apart average minimum distances increase probability predicted coding theoretic considerations furthermore effect noise machine against node failure thereby potentially extending useful machine
relationships between learning development evolution nature taken suggest model developmental process whereby manipulated genetic algorithm ga might expressed form neural networks go learn grammar generating polynomial time series prediction correspond ordered sequence define grammar expressed generate weights modified learning individuals prediction error used determine ga new gene operator appears critical formation new genetic alternatives preliminary but encouraging results presented
paper describes system probabilistic knowledge bases combines neural symbolic learning methods uses modified version backpropagation refine certainty factors style rule base uses information gain heuristic add new rules results two actual expert knowledge bases demonstrate combined approach performs better than previous methods
incremental higher order non recurrent network combines two properties found useful learning sequential tasks higherorder connections incremental
performance comparison two self organizing networks kohonen feature map recently proposed growing cell structures made purpose several performance criteria self organizing networks proposed motivated models tested three example problems increasing difficulty kohonen feature map demonstrates slightly superior results simplest problem other more difficult more realistic problems growing cell structures exhibit significantly better performance every criterion additional advantages new model parameters constant over time size well structure network determined automatically
given set training examples determining appropriate number free parameters challenging problem constructive learning algorithms attempt solve problem automatically adding hidden units therefore free parameters during learning explore alternative class algorithms called algorithms number units fixed but number free parameters gradually increases during learning architecture investigate composed rbf units lattice imposes flexible constraints parameters network approach include variable subset selection robust parameter selection multiresolution processing interpolation sparse training data
new boundary radial basis function rbf classifier rbf centers near class boundaries described classifier creates complex decision boundaries regions occur corresponding rbf outputs similar predicted square error measure used determine many centers add determine stop adding centers two experiments presented demonstrate advantages classifier uses artificial data two classes two input features each class contains four clusters but cluster near decision region boundary other uses large seismic database seven classes input features both experiments classifier provides lower error rate fewer centers than required more conventional rbf gaussian mixture mlp classifiers
large vc dimension classifiers learn difficult tasks but usually impractical because they generalize well they trained huge quantities data paper show even high order polynomial classifiers high dimensional spaces trained small amount training data yet generalize better than classifiers smaller vc dimension achieved maximum margin algorithm generalized technique applicable wide variety classifiers including percepttons polynomial classifiers pi unit networks radial basis functions effective number parameters adjusted automatically training algorithm match complexity problem shown equal number those training patterns closest patterns decision boundary supporting patterns bounds generalization error speed convergence algorithm given experimental results handwritten digit recognition demonstrate good generalization compared other algorithms
propose simple well principled way computing optimal step size gradient descent algorithms line efficient computationally applicable large backpropagation networks trained large data sets main technique estimating principal eigenvalues eigenvectors objective functions second derivative matrix hessian does require even calculate hessian several other applications technique proposed up learning eliminating parameters
investigate use information second order derivatives error function perform network pruning ie removing weights trained network order improve generalization simplify networks reduce hardware storage requirements increase speed further training cases enable rule extraction our method optimal brain obs significantly better than magnitude based methods optimal brain damage le solla often remove wrong weights obs permits pruning more weights than other methods same error training set thus yields better generalization test data crucial obs relation calculating inverse hessian matrix training data structural information net obs permits reduction weights over backpropagation weight decay three benchmark problems et al obs optimal brain damage magnitude based methods obs correct weights trained xor network every case finally whereas sejnowski used weights their network used obs prune network weights yielding better generalization
proposed model time warping invariant neural networks twinn handle time continuous signals although twinn simple modification well known recurrent neural network analysis has shown twinn completely removes time warping able handle difficult classification problem shown twinn has certain advantages over current available sequential processing schemes dynamic hidden markov time delayed neural neural network finite analyzed time continuity employed twinn out kind structure longer input history compared neural network finite automata may help understand well accepted fact learning grammatical reference had start short strings training set numerical example used trajectory classification problem problem making feature variable sampling rates having internal states continuous dynamics time data phase space trajectories shown difficult other schemes twinn problem has been learned iterations benchmark trained exact same problem tdnn completely failed expected
new incremental cascade network architecture has been presented paper discusses properties such cascade networks investigates their generalization abilities under particular constraint small data sets evaluation done cascade networks consisting local linear maps using time series prediction task benchmark our results indicate bring potential large networks problem extracting information small data sets without running risk overfitting cascaded network architectures more favorable than broad architectures contain same number nodes
bootstrap algorithm computational intensive procedure derive nonparametric confidence intervals statistical estimators situations analytic solution intractable applied neural networks estimate predictive distribution unseen inputs consistency different bootstrap procedures their convergence speed discussed small scale simulation experiment shows applicability bootstrap practical problems its potential use
previously ve have introduced idea neural network transfer learning target problem up using weights obtained network trained related source task here present new algorithm called discriminability based transfer uses information measure estimate utility hyperplanes defined source weights target network weight magnitudes accordingly several experiments demonstrate target networks initialized via learn significantly faster than networks initialized randomly
algorithm presented performs gradient descent weight space artificial neural network ann using finite difference approximate gradient method novel achieves computational complexity similar node perturbation but does require access activity hidden neurons possible due stochastic relation between perturbations weights neurons ann algorithm similar weight perturbation optimal terms hardware requirements used training vlsi anns
vector quantization useful data compression competitive learning minimizes reconstruction error appropriate algorithm vector quantization unlabelled data vector quantization labelled data classification has different objective minimize number different algorithm appropriate show variant kohonens lvq algorithm seen multiclass extension algorithm restricted class case proven converge bayes optimal classification boundary compare performance lvq algorithm modified version having decreasing window normalized step size ten class vowel classification problem
many techniques model selection field neural networks correspond well established statistical methods method stopped training other hand network trained until error further validation set examples training stopped true model selection doesnt require convergence training process paper show performance significantly enhanced extending model selection method stopped training include dynamic topology modifications dynamic weight pruning modified complexity penalty term methods weighting penalty term adjusted during training process
have designed architecture span gap between cognitive science address explore issues discrete symbol processing system arise continuum complex dynamics like oscillation synchronization employed its operation affect its learning show discrete time recurrent network architecture constructed connected oscillatory associative memory modules described continuous nonlinear ordinary differential equations modules learn connection weights between themselves cause system evolve under clocked machine cycle sequence transitions attractors within modules much digital computer evolves transitions its binary attractors architecture thus employs principle computing attractors used macroscopic systems reliable computation presence noise have specifically constructed system functions finite state automaton recognizes generates infinite set six symbol strings defined grammar symbol processing system but analog input oscillatory representations time steps machine cycles system implemented rhythmic variation bifurcation parameter holds input context modules their attractors while hidden output modules change state hidden output states while context modules load those states new context next cycle input superior noise has been demonstrated systems dynamic attractors over systems static attractors synchronization binding between coupled oscillatory attractors different modules has been shown important reliable transitions synchronization grammatical inference oscillating net
inverse kinematics problem redundant ill posed nonlinear two different issues result need form regularization existence multiple solution branches global ill existence excess degrees freedom local certain classes learning methods applied input output data generated forward function used globally problem partitioning domain forward mapping into finite set regions over inverse problem well posed local regularization accomplished appropriate parameterization redundancy consistently over each region result ill posed problem transformed into finite set well posed problems each solved separately construct approximate direct inverse functions
way speed up reinforcement learning enable learning happen simultaneously multiple resolutions space time paper shows create learning hierarchy high level learn set tasks their who turn learn satisfy them sub need initially understand their commands they simply learn maximise their reinforcement context current command illustrate system using simple task system learns get around satisfying commands multiple levels explores more efficiently than standard flat learning builds more comprehensive map
paper describes technique called input reconstruction reliability estimation determining response reliability restricted class multi layer percepttons mlps technique uses networks ability accurately encode input pattern its internal representation measure its reliability more accurately network able reconstruct input pattern its internal representation more reliable network considered provides good estimate reliability mlps trained autonomous driving results presented reliability estimates provided used select between networks trained different driving situations
artificial neural nets generalize better fewer examples order generalize successfully neural network learning methods typically require large training data sets introduce neural network learning method generalizes many fewer data points instead prior knowledge encoded previously learned neural networks example robot control learning tasks reported here previously learned networks model effects robot actions used guide subsequent learning robot control functions each observed training example target function eg robot control policy learner explains observed example terms its prior knowledge explanation infer additional information about shape slope target function shape knowledge used bias generalization learning target function results presented applying approach simulated robot task based reinforcement learning
recent research reinforcement learning has focused algorithms based principles dynamic programming dp most promising areas application these algorithms control dynamical systems impressive results have been achieved significant between practice theory particular convergence proofs problems continuous state action spaces systems involving non linear function approximators such multilayer percepttons paper presents research applying dp based reinforcement learning theory linear quadratic regulation important class control problems involving continuous state action spaces requiring simple type non linear function approximator describe algorithm based learning proven converge optimal controller large class problems describe slightly different algorithm locally convergent optimal function demonstrating possible using non linear function approximator dp based learning
primate brain solve two important problems grasping movements first problem concerns recognition objects specifically does brain integrate visual motor information object second problem concerns hand shape planning specifically does brain design hand configuration suited shape object manipulation task neural network model solves these problems has been developed operations network divided into learning phase optimization phase learning phase internal representations depend objects task acquired integrating visual somatosensory information optimization phase most suitable hand shape grasping object determined using relaxation computation network present address parallel distributed processing research dept kawato
task used example illustrate utility direct associative reinforcement learning methods learning control under real world conditions uncertainty noise task complexity due use less than presence uncertainty magnitude times despite extreme degree uncertainty our results indicate direct reinforcement learning used learn robust reactive control strategy results
trajectory extension learning new technique learning control robots assumes exists parameter desired trajectory smoothly varied region easy dynamics region desired behavior may have more difficult dynamics gradually varying parameter practice movements remain near desired path while neural network learns approximate inverse dynamics example average speed motion might varied inverse dynamics slow movements simpler dynamics fast movements provides example more general concept practice strategy sequence intermediate tasks used simplify learning complex task show example application idea real joint direct drive robot arm
within simple test application feed forward short term planning robot trajectories dynamic environment studied action network embedded system architecture contains separate world model continuously fed short term predicted spatio temporal obstacle trajectories receives robot state feedback action net allows external switching between alternative planning tasks generates goal directed motor actions subject robots kinematic dynamic constraints such collisions moving obstacles avoided using supervised learning examples optimal planner mapping over structure level adapted parsimonious higher order network training database generated dynamic programming algorithm extensive simulations reveal local planner mapping highly nonlinear but effectively represented chosen powerful net model excellent generalization occurs unseen obstacle configurations discuss limitations feed forward growing planning mml learning spario temporal planning dynamic programming teacher
three step method function approximation fuzzy system proposed first membership functions initial rule representation learned second rules compressed much possible using information theory finally computational network constructed compute function value system applied two control examples learning upper control system learning control system controlled model car
invariance objects identity transformed over time provides powerful cue perceptual learning present unsupervised learning procedure maximizes mutual information between representations adopted feed forward network consecutive time steps demonstrate network learn entirely unsupervised classify ensemble several patterns observing pattern trajectories even though transitions object another between trajectories same learning procedure should widely applicable variety perceptual learning tasks
neurons area mt primate visual cortex encode velocity moving objects present model mt cells aggregate responses form such velocity representation two different sets units local receptive fields receive inputs motion energy filters set units forms estimates local motion while second set computes utility these estimates outputs second set units gate outputs first set through gain control mechanism active process selecting subset local motion responses integrate into more global responses distinguishes our model previous models velocity estimation model yields accurate velocity estimates synthetic images containing multiple moving targets varying size luminance spatial frequency profile deals well number transparency phenomena
multiple single neuron responses recorded single electrode behaving monkeys sinusoidal gratings presented cells overlapping receptive fields stimulus varied along several visual dimensions degree dimensional calculated large population neurons found continuum several cells showed different temporal response dependencies variation different stimulus dimensions ie tuning modulated firing necessarily same mean firing rate describe multidimensional receptive field use simultaneously recorded responses compute multi neuron receptive field describing information processing capabilities group cells using dynamic correlation analysis propose several computational schemes multidimensional spatiotemporal tuning groups cells implications neuronal coding stimuli discussed
classical computational model stereo vision incorporates uniqueness inhibition constraint feature match thereby ability handle transparency model uniqueness constraint argue smoothness constraint provide excitation support required transparency computation modification fails neighborhoods sparse features propose bayesian approach stereo vision priors over transparent surfaces disparity its segmentation into multi layer depth representation simultaneously computed smoothness constraint support within each layer providing mutual excitation non neighboring transparent partially regions test results various random dot other stereograms presented
visual processing ability deal missing noisy information crucial occlusions unreliable feature detectors often lead situations little direct information about features available available information usually sufficient highly constrain outputs discuss bayesian techniques extracting class probabilities given partial data optimal solution involves integrating over missing dimensions weighted local probability densities show obtain closed form approximations bayesian solution using gaussian basis function networks framework extends naturally case noisy features simulations complex task hand gesture recognition validate theory both integration weighting input densities used performance decreases number missing noisy features performance substantially degraded either step omitted
simplified models lateral geniculate lgn striate cortex illustrate possibility feedback lgn may used robust low level pattern analysis information fed back lgn cortex using full fan out so pathway extensive cortico cortical communication while keeping number necessary connections small
human vision systems integrate information across long spatial ranges example moving stimulus appears viewed briefly ms yet sharp viewed longer exposure ms suggests visual systems combine information along trajectory matches motion stimulus our self organizing neural network model shows developmental exposure moving stimuli direct formation horizontal trajectory specific motion integration pathways representations moving stimuli these results account data potentially model other phenomena such visual
work apply texture classification network remote sensing image analysis goal extract characteristics area depicted input image thus achieving segmented map region have recently proposed combined neural network rule based framework texture recognition framework uses unsupervised supervised learning provides probability estimates output classes describe texture classification network extend demonstrate its application image analysis domain
have designed neural network detects direction optic flow presence eye movements performance network consistent human psychophysical its output neurons show great similarity component cells area mstd monkey visual cortex now show using assumptions about kind eye movements observer likely perform our model generate various other cell types found mstd well
ensemble dynamics stochastic learning algorithms studied using theoretical techniques statistical physics develop equations motion weight space probability densities stochastic learning algorithms discuss equilibria diffusion approximation provide expressions special cases lms algorithm equilibrium densities general thermal gibbs distributions objective function being minimized but rather depend upon effective potential includes diffusion effects finally present exact analytical expression time evolution density learning algorithm weight updates proportional sign gradient
paper discuss asymptotic properties most commonly used variant backpropagation algorithm network weights trained means local gradient descent examples drawn randomly fixed training set learning rate gradient updates held constant simple backpropagation using stochastic approximation results show training process approaches batch training provide results rate convergence further show small approximate simple back propagation sum batch training process gaussian diffusion unique solution linear stochastic differential equation using approximation indicate reasons why simple backpropagation less likely get local minima than batch training process demonstrate empirically number examples
presence outliers existing self organizing rules principal component analysis pca perform poorly using statistical physics techniques including gibbs distribution binary decision fields effective propose self organizing pca rules capable outliers while various pca related tasks such obtaining first principal component vector first principal component vectors directly finding subspace spanned first vector principal component vectors without solving each vector individually comparative experiments have shown proposed robust rules improve performances existing pca algorithms significantly outliers present
analyze query committee algorithm method filtering informative queries random stream inputs show two member committee algorithm achieves information gain positive lower bound prediction error decreases exponentially number queries show particular exponential decrease holds query learning thresholded smooth functions
analyse effects analog noise synaptic arithmetic during multilayer perceptron training cost function include noise penalty terms predictions made light these calculations suggest fault tolerance generalisation ability learning trajectory should improved such noise injection extensive simulation experiments two distinct classification problems claims results appear perfectly general training schemes weights adjusted incrementally have wide ranging implications applications particularly those involving inaccurate analog neural vlsi
present information theoretic derivation learning algorithm clusters unlabelled data linear discriminants contrast methods try preserve information about input patterns maximize information gained observing output robust binary implemented sigmoid nodes derive local weight adaptation rule via gradient ascent objective demonstrate its dynamics simple data sets relate our approach previous work suggest directions may extended
have attempted use information theoretic quantities neuronal connection structure spike trains two point mutual information its maximum value channel capacity between neurons found useful sensitive detection estimation synaptic strength respectively three point mutual information among three neurons could give their interconnection structure therefore our information theoretic analysis shown powerful technique neuronal connection structure concrete examples its application simulated spike presented
use statistical mechanics study generalization large committee machines architecture fields replica calculation yields generalization error limit large number hidden units continuous weights generalization error falls off asymptotically proportional number training examples per weight binary weights find discontinuous transition poor perfect generalization followed wide region broken replica symmetry found within region low temperatures fully connected architecture generalization error calculated within annealed approximation both binary continuous weights find transitions symmetric state specialized hidden units discontinuous generalization error
present algorithm creating neural network produces accurate probability estimates outputs network implements gibbs probability distribution model training database model created new transformation relating joint probabilities attributes database weights gibbs potentials distributed network model theory transformation presented together experimental results advantage approach network weights prescribed without iterative gradient descent used classifier network tied outperformed published results variety databases
chaos recurrent neural networks depend architecture synaptic coupling strength studied here randomly architecture normalizing variance synaptic weights produce bifurcation parameter dependent variance slope transfer function but independent connectivity allows sustained activity chaos reaching critical value even weak connectivity small size find numerical results theoretical ones previously established fully connected infinite sized networks moreover route towards chaos numerically quasi periodic type first bifurcation hopf bifurcation
recurrent networks threshold elements have been studied associative memories pattern recognition devices while most research has fully connected symmetric networks relax stable fixed points asymmetric networks show richer dynamical behavior used sequence generators flexible pattern recognition devices paper approach problem predicting complex global behavior class random asymmetric networks terms network parameters these networks show fixed point effectively behavior depending parameter values our approach used set parameters necessary obtain desired complexity dynamics approach provides qualitative insight into why system behaves does suggests possible applications
analyze detail performance hamming network classifying inputs distorted versions its stored memory patterns activation function memory neurons original hamming network replaced simple threshold function resulting threshold hamming network correctly classifies input pattern probability using om connections single iteration drastically reduces time space complexity hamming network classifiers
present framework enabling detailed description performance hopfield like attractor neural networks ann first two iterations using bayesian approach find performance improved history based term included neurons dynamics further enhancement networks performance achieved choosing neurons those become active given iteration basis magnitude their post synaptic potentials contribution biologically plausible dynamics especially marked conditions low firing activity sparse connectivity two important characteristics mammalian cortex such networks performance attained higher than performance two independent iterations represents upper bound performance history independent networks
method creating non linear encoder decoder multidimensional data compact representations presented commonly used technique extended allow non linear representations objective function activations individual hidden units shown result minimum dimensional encodings respect error reconstruction
neural networks binary weights important both theoretical practical points view paper investigate learnability single binary percepttons binary perceptron networks binary percepttons each input unit connected perceptron give polynomial time algorithm pac learns these networks under uniform distribution algorithm able identify both network connectivity weight values necessary represent target function these results suggest under reasonable distributions perceptron networks may easier learn than fully connected networks
two theorems lemma presented about use estimator cross validation method model selection theorem gives asymptotic form estimator combined model selection criterion asymptotic form used obtain fit model model selection criterion used negative average predictive choice based idea cross validation method lemma provides formula further exploration model selection criterion theorem gives asymptotic form model selection criterion regression case parameters optimization criterion has penalty term theorem asymptotic equivalence model selection criterion moody cross validation method distance measure between response regression function takes form squared difference
learning curves show neural network improved number training examples increases related network complexity present paper asymptotic properties their relation two learning curves concerning predictive loss generalization loss other training loss result gives natural definition complexity neural network moreover provides new criterion model selection
compare activation functions terms approximation power their feedforward nets consider case analog well boolean input
connection drawn between rational functions realization theory dynamical systems feedforward neural networks allows us single hidden layer scalar neural networks almost arbitrary analytic activation functions terms strictly proper rational functions hence solve uniqueness problem such networks
have trained networks ii units short range connections simulate simple cellular automata exhibit complex chaotic behaviour three levels learning possible decreasing order difficulty learning underlying automaton rule learning asymptotic dynamical behaviour learning training history levels learning achieved without weight sharing different automata provide new insight into their dynamics
feed forward networks fixed hidden units networks compared against category remaining feed forward networks variable hidden units networks two broad classes tasks finite domain considered approximation every function open subset functions representation every first task found both network categories require same minimal number synaptic weights second task general position shown networks threshold logic hidden units have approximately times fewer hidden units than network have
number hybrid multilayer markov model hmm speech recognition systems have been developed recent years morgan paper present new architecture training allows modeling context dependent phonetic classes hybrid gained different degrees context dependence order obtain robust estimate probabilities tests resource management database have shown substantial advantages context dependent mlps over earlier mlps have shown substantial advantages hybrid approach over pure approach
study demonstrates paradigm modeling speech production based neural networks using physiological data speech neural network learns forward dynamics relating motor commands muscles ensuing articulator behavior allows articulator trajectories generated motor commands constrained phoneme input strings global performance parameters these movement trajectories second neural network generates parameters used synthesize speech
paper discusses parameterization speech analog cochlear model tradeoff between time frequency resolution viewed fundamental difference between conventional analysis cochlear signal processing broadband rapid changing signals models response exhibits wavelet like analysis scale domain preserves good temporal resolution frequency each spectral component broadband signal accurately determined intervals instantaneous firing rates auditory fibers such properties cochlear model demonstrated natural speech synthetic complex signals
channel problem important problem high speed communications sequences symbols transmitted distorted neighboring symbols traditionally channel problem considered channel inversion operation problem approach direct correspondence between error probability residual error produced channel inversion operation paper optimal equalizer design formulated classification problem optimal classifier constructed bayes decision rule general nonlinear efficient hybrid equalizer approach has been proposed train equalizer error probability new equalizer has been shown better than linear equalizer experimental channel
would like incorporate speaker dependent such gender otherwise speaker independent speech recognition system paper discuss gender dependent neural network tuned each gender while sharing most speaker independent parameters use classification network help generate gender dependent phonetic probabilities statistical hmm recognition system gender classification net predicts gender high accuracy resource management test set integration into our hybrid hmm neural network recognizer provided improvement recognition score statistically significant resource management test set
matched filtering has been most powerful techniques employed transient detection here show dynamic neural network outperforms conventional approach artificial neural network ann trained supervised learning schemes need supply desired signal time although interested detecting transient paper show effects detection agreement different strategies construct desired signal extension bayes decision rule desired signal optimal static classification performs worse than desired signals constructed random noise prediction during background
connectionist speech recognition systems often between training testing criteria problem addressed multi state time delay neural network ms tdnn hierarchical phoneme word classifier uses modulate its connectivity pattern directly trained word level targets consistent use word accuracy criterion during both training testing leads high system performance even limited training data until now ms tdnn has been applied primarily small vocabulary recognition word spotting tasks paper apply architecture large vocabulary continuous speech recognition demonstrate our ms tdnn outperforms other systems have been tested cmu conference database
recently state art large vocabulary continuous speech recognition has employed hidden markov modeling hmm model speech sounds attempt improve over hmm developed hybrid system integrates hmm technology neural networks present concept segmental neural net snn phonetic modeling taking into account frames phonetic segment simultaneously snn overcomes well known conditional independence limitation several speaker independent experiments resource management corpus hybrid system showed consistent improvement performance over baseline system
multi state time delay neural network ms tdnn integrates nonlinear time alignment procedure phoneme spotting capabilities tdnn into connectionist speech recognition system word level classification error backpropagation present ms tdnn recognizing continuously letters task characterized small but highly vocabulary our ms tdnn achieves word accuracy speaker tasks outperforming previously reported results same databases propose training techniques aimed improving sentence level performance including free alignment across word boundaries word duration modeling error backpropagation sentence rather than word level architectures integrating specialized subset speakers achieved further improvements
paper reports performance two methods recognition based segmentation strings line hand printed characters input strings consist sequence coordinates methods designed work run mode constraint spacing between characters while both methods use neural network recognition engine graph algorithmic post processor their approaches segmentation quite different first method call input segmentation uses combination heuristics identify particular segmentation points second method call output segmentation relies empirically trained recognition engine both recognizing characters identifying relevant segmentation points
propose paper statistical model planar hidden markov model describing statistical properties images model generalizes single dimensional hmm used speech processing planar case model useful efficient segmentation algorithm similar viterbi algorithm hmm exist present conditions terms parameters sufficient guarantee planar segmentation problem solved polynomial time describe algorithm algorithm optimally image model therefore insensitive elastic distortions images using algorithm joint optimal segmentation recognition image performed thus overcoming traditional ocr systems segmentation performed independently before recognition leading recognition errors approach evaluated using set isolated hand written digits overall digit recognition accuracy achieved analysis results showed even simple case recognition isolated characters elimination elastic distortions enhances performance significantly expect advantage approach even more significant tasks such connected writing known high accuracy method recognition
developing electric power encountered service area large utility using application approaches input dimension reduction decomposition network training projection pursuit regression representations ability algorithms like quickly find reasonable weighting vectors enable us architecture selection problem reducing high dimensional gradient fitting single input single output introduce dimension reduction algorithms select features relevant subsets set many variables based minimizing index level set closely related projection index combine them implement neural network version projection pursuit performance achieved our approach trained data tested data comparable achieved our earlier study backpropagation trained networks
hidden markov models hmms applied several important problems molecular biology introduce new convergent learning algorithm hmms unlike classical baum welch algorithm smooth applied line batch mode without usual viterbi most likely path approximation left right hmms states trained represent several protein families including cases models derived capture important statistical properties families used efficiently number important tasks such multiple alignment motif detection classification division biology california institute technology department psychology university
planar widely used technique detecting estimating risk disease neural networks learned interpret determined individual expert ratings standard error backpropagation compared standard lms lms combined layer rbf units using leave out method generalization tested cases training time determined automatically cross validation performance best performance attained network three hidden units per view compares favorably human experts
have designed fabricated tested analog vlsi chip computes radial basis functions parallel have developed synapse circuit approximates quadratic function aggregate these circuits form radial basis functions these radial basis functions averaged together using
analog cmos vlsi neural processing chip has been designed fabricated device employs pulse stream neural state capable computing million connections per second addition basic results performance chip solving real world problems demonstrated
real time computation motion real images using single chip integrated sensors hard present two analog vlsi schemes use pulse domain neuromorphic circuits compute motion pulses variable width rather than graded potentials represent natural evaluating temporal relationships both algorithms measure speed timing moving edge image our first model inspired algorithm fly yields non monotonic response rs velocity curve present data chip implements model our second algorithm yields response rs velocity curve currently being translated into silicon
describe analog vlsi implementation multi dimensional gradient estimation descent technique minimizing scalar function implementation uses noise injection multiplicative correlation estimate derivatives anderson intended application technique setting circuit parameters chip automatically rather than manually gradient descent optimization may used adjust synapse weights backpropagation other chip learning implementation approach combines features continuous multi dimensional gradient descent potential annealing style optimization present data measured our analog vlsi implementation
field software neural networks has been rapidly last years but their importance still being they provide increasing levels design simulation analysis neural networks our object oriented framework show high degrees transparency flexibility complex experiments obtained basic design inspired natural way researchers explain their computational models experiments performed networks building blocks extended easily mechanisms have been integrated facilitate construction analysis complex architectures among these mechanisms automatic configuration building blocks experiment multiple run time
networks local inhibition shown have enhanced computational performance respect classical hopfield like networks particular critical capacity network increased well its capability store correlated patterns chaotic dynamic behaviour exponentially long transients devices indicates associative memory implementation based programmable logic device here presented neurons circuit implemented device solution possibility change parts project weights transfer function whole architecture simple software configuration into chip
demonstrate use digital signal processing board construct hybrid networks consisting computer model neurons connected biological neural network system operates real time synaptic connections realistic effective conductances therefore synapses made computer model neuron integrated correctly postsynaptic biological neuron method provides us ability add additional completely known elements biological network study their effect network activity moreover changing parameters model neuron possible assess role individual conductances activity neuron network present address de de la present address de place de dr
several research group ing integrated circuit models biological auditory processing these circuit have taken several including monitor display simple scanned out put display parallel analog suitable da acquisition systems paper describe alternative output method silicon auditory models suitable direct ce digital computers present address hi mi anatomical unit rd oxford ox oxford ac present address ca present address rd san ca silicon auditory processors computer
typical methods gradient descent neural network learning involve calculation derivatives based detailed knowledge network model requires extensive time consuming calculations each pattern presentation high precision makes difficult implement vlsi present here perturbation technique measures calculates gradient technique uses actual network measuring device errors modeling neuron activation synaptic weights do cause errors gradient descent method parallel nature easy implement vlsi describe theory such algorithm analysis its domain applicability simulations using outline hardware implementation
basic connectionist principles imply should take form systems parallel soft constraints defining optimization problem solutions well formed structures language such harmonic grammars have been successfully applied number problems theory natural languages shown formal languages specified harmonic grammars rather than conventional serial re write rule systems harmonic grammars legendre have been studying symbolic computation human cognition arise naturally higher level virtual machine realized appropriately designed lower level connectionist networks basic computational principles approach these analyzed lower level mental representations distributed patterns connectionist activity analyzed higher level these same representations constitute symbolic structures particular symbolic structure characterized set using collection structural roles ri each may fi constituent symbolic smolensky co corresponding lower level description activity vector these tensor product representations defined recursively themselves complex structures represented vectors turn recursively defined tensor product representations smolensky smolensky analyzed lower level mental processes massively parallel numerical activation spreading analyzed higher level these same processes constitute form symbol manipulation entire structures possibly involving recursive embedding manipulated parallel smolensky legendre et al smolensky lower level description activation spreading processes satisfies certain mathematical properties process analyzed higher level construction symbolic structure including given input structure maximizes harmony equivalently minimizes energy computed either lower level particular mathematical function numbers activation pattern higher level function symbolic constituents comprising structure simplest cases core function written lower connectionist level simply quadratic form networks activation vector its connection weight matrix higher level cc each harmony having two symbolic constituents same structure ci constituents particular structural roles may same grossberg hinton sejnowski hinton sejnowski hopfield hopfield legendre et al smolensky smolensky once connectionist well formedness identified linguistic well formedness following results legendre et al explicit form harmony function computed sum terms each measures well formedness arising within single structure pair constituents their particular structural roles grammar thus identified set soft rules each form linguistic structure simultaneously contains constituent structural role constituent structural role add harmony value quantity may positive negative set such soft rules constraints preferences defines harmonic grammar constituents soft rules include both those given input hidden constituents assigned input grammar problem computational harmonic grammars formal languages construct structure containing both input hidden constituents highest overall harmony hs harmonic formal conceptual ideas linking first proposed cognitive phonology harmonic phonology press application natural language see legendre et al legendre et al legendre et al legendre et al press grammar has more recently evolved into non numerical formalism called optimality theory has been successfully applied range problems phonology smolensky smolensky preparation comprehensive discussion overall research program see smolensky et al formal languages means expressive power grammar apply specification formal languages eg context free language cfl specified set soft rules form given so string iff maximum harmony tree terminals has say crucial limitation these soft rules each may refer pair constituents sense they second order simplifies describe pairs those both constituents same these actually correspond first order soft rules exist cfl tree well formed iff its local trees local tree node its children thus rules need refer pairs nodes fall single local tree ie parent child pairs andor pairs value entire tree sum numbers each such pair nodes given soft rules defining clear general context free grammar pairwise evaluation doesnt consider eg following go ill formed local tree here parent two children pairwise well formedness fail detect ill formedness first rule says left child second right child third left ill formedness detected examining three nodes simultaneously seeing single rule possible approach would extend rules higher than second order involving more than two constituents corresponds functions degree higher than such functions go beyond standard connectionist networks pairwise connectivity requiring networks defined over rather than ordinary graphs natural alternative requires change but instead special kind grammar cfl basic trick modification idea taken generalized phrase structure grammar et al theory adapts study natural languages useful introduce new normal form harmonic normal form smolensky hnf hnf rules three types ai ai further requirement branching rule given left hand side unique branching condition here use letters denote symbols have two sorts non terminals general symbols like symbols like aa ai see every cfl does indeed have hnf grammar first take normal form each necessarily binary branching rule replace symbol left hand side ai using different value each branching rule given left hand side ii add rule ai general category may have several legal branching expansions into specialized ai each has legal branching expansion makes possible determine well formedness entire tree simply examining each parentchild pair separately entire tree iff every parentchild pair unique branching condition enables us evaluate harmony tree simply adding up collection numbers specified soft rules each node each link tree now cfl specified harmonic grammar first find hnf grammar generate set soft rules defining harmonic grammar via correspondences gh ai start symbol ct ai ra node add ra node add ai node add root root add left child add left child ai add right child ai add soft rules ra ra root first order evaluate tree nodes remaining second order soft rules legal rules evaluating tree links assigns legal parse tree root other tree thus iff maximal completion tree has proof ve evaluate harmony tree conceptually breaking up its nodes links into pieces each contributes either legal trees complete positive negative contributions trees have leading total decomposition nodes links proceeds follows replace each undirected link tree pair directed links pointing up parent other down child link legal parentchild pair corresponding legal rule contribute break into two contributions each directed links similarly break up non terminal nodes into sub nodes non terminal node labelled harmonic grammars formal languages ai has two children legal trees break such node into three sub nodes corresponding each link child corresponding upward link parent ai according soft rule contribution node ai distributed three contributions each sub node similarly non terminal node labelled has child legal tree so break into two sub nodes link child upward link contribution soft rule similarly into two contributions each sub node need break up terminal nodes legal trees have link upward parent contribution ra already evaluate harmony tree examining each node now decomposed into set sub nodes determining contribution made node its directed links double count link contributions way half contribution each original undirected link each nodes consider first non terminal node labelled ai has legal parent have upward link parent contributes corresponding sub node has legal left child link contribute corresponding sub node similarly right child thus total contribution node has legal parent two legal children each missing legal child parent node contributes so contribution node general case legal children parents node result holds non branching non terminals labelled difference now child could legal left child happens legal start symbol root position sub node corresponding upward link parent legal parent usual but rather soft rule result still holds even case simply agree count root position itself legal parent start symbols finally holds terminal node labelled such node have child but might have missing legal parent thus total harmony tree hn hn given minus total missing legal children parents nodes tree thus each node has legal parent its required legal children otherwise because grammar harmonic normal parse tree legal iff every every node has legal parent its required smolensky legal children legal parentchild defined pairwise terms parent child blind other children might present absent thus have established desired result maximum parse string has iff now see understand soft rules gh generalize beyond context free languages soft rules say each node makes negative contribution equal its while each link makes positive contribution equal its node link number links nodes attached legal tree negative contributions nodes made time node present these positive contributions links link legal grammar so order apply strategy unrestricted simply set magnitude negative contributions nodes equal their determined grammar illustrate technique showing solves problem simple three rule grammar go introduced early section corresponding hnf grammar given above construction avoid adding start node above terminal nodes below suppose both valid start symbols terminal nodes corresponding gh assigns ill formed tree harmony according both missing legal parent missing two legal children introducing now necessary version helps but enough both have each leaf node missing legal parent respectively ai node corresponding legal child but correct parse string has technique generalized context free unrestricted type formal languages equivalent machines languages they generate eg ith production rule unrestricted grammar ri replaced two rules ri ri introducing new non terminal symbols fi corresponding soft rules harmonic grammar kth parent fi add kth child fi add rule fi node add soft rules ra defined context free case legendre many helpful discussions research presented here has been supported part nsf grant university research work harmonic grammars formal languages references grossberg absolute stability global pattern formation parallel memory storage competitive neural networks ieee transactions systems smolensky tensor product production system modular architecture representation connection science generalized phrase structure grammar university press cambridge ma brain state box neural model gradient descent algorithm mathematical psychology unified framework connectionist systems biological phonology oxford press phonology intelligent system editors between psychology university press hinton sejnowski analyzing cooperative computation proceedings conference cognitive science society ny hinton sejnowski learning boltzmann machines rumelhart research group editors parallel distributed processing cognition volume foundations pages mit ma
neural network models have been their inability make use compositional representations paper describe series psychological phenomena demonstrate role structured representations cognition these findings suggest people compare relational representations via process structural alignment process have captured model cognition symbolic
demonstrate paper certain forms rule based knowledge used neural network normalized basis functions give probabilistic interpretation network architecture describe several ways rule based knowledge preserved during training present method complexity reduction tries minimize number rules number after training refined rules extracted analyzed
describe model visual word recognition accounts several aspects temporal processing sequences briefly presented words model utilizes new representation written words based dynamic time warping multidimensional scaling visual input passes through cascaded perceptual comparison detection stages describe these dynamical processes account several aspects word recognition including priming
propose model development geometric reasoning children explicitly involves learning model uses neural network initialized understanding geometry similar second children through presentation series examples model shown develop understanding geometry similar children who trained using similar materials
representations semantic information about words necessary many applications neural networks natural language processing paper describes efficient corpus based method inducing distributed semantic representations large number words statistics means large scale linear regression representations successfully applied word sense using nearest neighbor method
processes our ability quickly recognize familiar objects within complex visual input scene paper implemented neural network model described attempts specify selective visual attention perceptual invariance transformations might work together order segment select recognize objects out complex input scenes containing multiple possibly overlapping objects organized feature maps serve input two main processing dealing location information what pathway computing shape attributes objects location based attention mechanism operates early stage visual processing selecting region visual field processing additionally location based attention plays important role invariant object recognition appropriate normalization processes within what pathway object recognition supported through segmentation visual field into distinct entities order represent different segmented entities same time model uses oscillatory binding mechanism connections between pathway what pathway lead flexible cooperation between different functional producing overall behavior consistent variety psychophysical data
electrosensory system weakly electric fish pathways first order sensory nucleus have been shown influence gain its output neurons underlying neural mechanisms gain control capability yet fully understood suggest possible gain control mechanism could involve regulation total membrane conductance output neurons paper neural model based idea used demonstrate activity levels pathways could control both gain baseline excitation target neuron
model hippocampus central element rat navigation presented simulations show both behaviour single cells navigation rat these compared single unit recordings behavioural data firing ca place cells simulated artificial rat moves environment input neuronal network whose output each cycle next direction travel rat cells number spikes time firing respect hippocampal rhythm learning occurs off synapses simultaneous preand post synaptic activity simulated rat successfully goals encountered more times during exploration open fields minute random exploration environment allows navigation newly presented goal novel starting positions limited number obstacles successfully avoided background experiments have shown hippocampus crucial spatial memory ability rat okeefe single unit recordings moving have revealed place cells fields ca ca hippocampus whose firing restricted small portions environment corresponding place fields okeefe see fig la addition cells have been found pre whose primary behavioural okeefe ii ii ii time figure typical ca place field max rate over second eeg rhythm shown rat runs through place field shows times firing place cell vertical immediately above below eeg mark positive negative zero crossings eeg define phase shows phase each spike okeefe head direction et al both navigation temporal well spatial aspects hippocampal region significant model hippocampal eeg rhythm best frequency hz occurs whenever rat making displacement movements recently place cell firing has been found have systematic phase relationship local eeg okeefe see fig lb finally rhythm has been found modulate long term potentiation synapses hippocampus et al
present theory cortico interaction learning region form new representations facilitate learning enhancing predictive stimuli stimulus stimulus cortical cerebellar regions sites long term acquire these new representations but me capable forming new representations themselves instantiated connectionist model theory accounts wide range trial level classical conditioning normal hippocampal animals several novel predictions investigated empirically theory implies region involved even simplest learning tasks although hippocampal animals may able use other strategies learn these tasks theory predicts they show consistently different patterns transfer generalization task demands change
so far has been general method relating extracellular electrophysiological measured activity neurons associative cortex underlying network cognitive states propose model such data using multivariate poisson hidden markov model demonstrate application approach temporal segmentation firing patterns characterization cortical responses external stimuli using such statistical model significantly discriminate two behavioral modes monkey characterize them different firing patterns well level their multi unit firing activity our study utilized measurements carried out behaving monkeys medical school university
information theoretic optimization principle infomax has previously been used unsupervised learning statistical regularities input ensemble principle states inputoutput mapping implemented processing stage should chosen so maximize average mutual information between input output patterns subject constraints presence processing noise present work show infomax applied class nonlinear input output mappings under certain conditions generate optimal filters have additional useful properties output activity each input pattern tends among relatively small number nodes filters sensitive higher order statistical structure beyond pairwise correlations input features localized filters receptive fields tend localized well multiresolution sets filters low spatial frequencies related pyramid coding wavelet representations emerge solutions certain types input ensembles
vestibulo ocular reflex vor compensatory eye movement images retina during head turns its magnitude gain modified visual experience during head movements possible learning mechanisms adaptation have been explored model oculomotor system based anatomical physiological constraints local correlational learning rules our model reproduce adaptation behavior vor under certain parameter conditions these conditions predictions time course adaptation learning sites made
present local learning rule hebbian learning conditional incorrect prediction reinforcement signal propose biological interpretation such framework display its utility through examples reinforcement signal cast delivery its target three examples presented illustrate framework applied development oculomotor system
switching between apparently coherent oscillatory stochastic episodes activity has been observed responses cat monkey visual cortex describe dynamics these phenomena two parallel approaches phenomenological rather microscopic hand analyze neuronal responses terms hidden state model parameters model extracted directly experimental spike trains they characterize underlying dynamics well coupling individual neurons network phenomenological model thus provides new framework experimental analysis network dynamics application method multi unit activities visual cortex cat existence oscillatory stochastic states switching behaviour assembly dynamics other hand start single spiking neuron derive equation time evolution assembly state represent phase density phase density dynamics exhibits two attractors limit cycle fixed point synaptic interaction nonlinear external fluctuations switch bistable system state other finally show two approaches mutually consistent therefore both explain detailed time structure data
new computational model addresses formation both topography ocular dominance presented motivated experimental evidence these phenomena may same mechanisms important aspect model ocular dominance segregation occur input activity both distributed positively correlated between eyes allows investigation dependence pattern ocular dominance degree correlation between eyes found increasing correlation leads experiments suggested test whether such behaviour occurs natural system
interpret time interval data obtained stimulated sensory neurons terms two simple dynamical systems driven noise embedded weak periodic function called signal bistable system defined two potential separated system implementation analog simulation circuits mimic dynamics given signal frequency our have two adjustable parameters signal noise intensities show experimental data obtained stimulated accurately approximated these simulations finally discuss stochastic resonance two models
formation propagating spiral waves studied randomly connected neural network composed integrate fire neurons recovery period connections using computer simulations network activity periodic stimulation single point results suggest spiral waves arise such network via sub critical hopf bifurcation
paper examines extends work self feature detectors concentrates visual processing system but infers weak assumptions made allow model used processing other sensory information claim examined here special attention auditory system much lower connectivity therefore more statistical variability line training obtain idea training times these compared time available pre mammals formation feature sensitive cells
paper presents neural network able control saccadic movements input network specification stimulation site motor map output time course eye position horizontal vertical angles units network exhibit neurons intermediate layer superior colliculus motor map brainstem oculomotor neurons simulations carried out network demonstrate its ability reproduce straightforward fashion many experimental observations
known biological data response patterns interneurons olfactory insects central importance coding olfactory signal propose analytically tractable model allows us relate distribution response patterns architecture network
information theory used derive simple formula amount information firing rate neuron about experimentally measured variable combination variables eg running speed head direction location animal etc derivation treats cell communication channel whose input measured variable whose output cells spike train applying formula find systematic differences information content hippocampal place cells different experimental conditions
network uses set recognition weights convert input vector into code vector uses set generative weights convert code vector into approximate reconstruction input vector derive objective function training based minimum description length mdl principle aim minimize information required describe both code vector reconstruction error show information minimized choosing code vectors stochastically according boltzmann distribution generafive weights define energy each possible code vector given input vector unfortunately code vectors use distributed representations exponentially expensive compute boltzmann distribution because involves possible code vectors show recognition weights used compute approximation boltzmann distribution approximation gives upper bound description length even bound poor used lyapunov function learning both generafive recognition weights demonstrate approach used learn factorial codes
although recurrent neural nets have been successful learning finite state machines continuous internal state dynamics neural net well matched discrete behavior describe architecture called allows discrete states evolve net learning progresses consists standard recurrent neural net trained gradient descent adaptive clustering technique state space based assumption finite set discrete internal states required task actual network state belongs set but has been corrupted noise due weights learns recover discrete state maximum probability noisy state simulations show leads significant improvement generalization performance over earlier neural net approaches induction
paper presents formulation unsupervised learning clusters reflecting multiple causal structure binary data unlike standard mixture model multiple cause model accounts observed data combining many hidden causes each varying degree subset observable dimensions crucial issue mixing function combining beliefs different cluster centers order generate data whose errors minimized both during recognition learning demonstrate inherent popular weighted sum followed sigmoid offer alternative form nonlinearity results presented demonstrating algorithms ability successfully discover coherent multiple causal representations noisy test data images printed characters
present new algorithm eliminating excess parameters improving network generalization after supervised training method principal pruning based principal component analysis node activations successive layers network simple cheap implement effective requires network does involve calculating full hessian cost function weight node activity correlation matrices each layer nodes required demonstrate efficacy method regression problem using polynomial basis functions economic time series prediction problem using two layer feedforward network
analyze simple hill algorithm previously shown outperform genetic algorithm ga simple load function analyze idealized genetic algorithm significantly faster than gives lower bound ga speed identify features give rise speedup discuss these features incorporated into real ga
selecting good model set input points cross validation computationally intensive process especially number possible models number training points high techniques such gradient descent helpful searching through space models but problems such local minima more importantly lack distance metric between various models reduce applicability these search methods technique finding good model data quickly bad models computational effort between better ones paper focuses special case leave out cross validation applied learning algorithms but argue applicable class model selection problems
show network architecture constructed connected oscillatory associative memory network modules employ selective attentional control synchronization direct flow communication computation within architecture solve grammatical inference problem previously have shown discrete time network algorithm implemented network completely described continuous ordinary differential equations time steps machine cycles system implemented rhythmic variation bifurcation parameter architecture oscillation amplitude codes information content activity module unit whereas phase frequency used network synchronized modules communicate amplitude information activity non modules contributes noise attentional control modeled special subset hidden modules affect resonant frequencies other hidden modules they control synchrony among other modules direct flow computation attention effect transitions between two state automaton system generate grammar internal noise used drive required random transitions automaton baird
learning recognize predict sequences using long term context has many applications practical theoretical problems found training recurrent neural networks perform tasks inputoutput dependencies span long intervals starting mathematical analysis problem consider compare alternative algorithms architectures tasks span inputoutput dependencies controlled results new algorithms show performance qualitatively superior obtained backpropagation
paper introduces evolutionary program induces recurrent neural networks structurally unconstrained contrast constructive algorithms employs population networks uses functions unsupervised feedback guide search through network space annealing used generating both gaussian weight changes structural modifications applying complex search collection task demonstrates system capable inducing networks complex internal dynamics
point matching distance measure invariant under translation rotation learn point set objects clustering noisy point set images unlike traditional clustering methods use distance measures operate feature vectors representation common most problem domains object based clustering technique employs distance measure specific type object within problem domain formulating clustering problem two nested objective functions derive optimization dynamics similar expectation maximization algorithm used mixture models
data clustering amounts combinatorial optimization problem reduce complexity data representation increase its precision central pairwise data clustering studied maximum entropy framework central clustering derive set equations minimization procedure yields optimal number clusters their centers their cluster probabilities meanfield approximation pairwise clustering used estimate assignment probabilities solution multidimensional scaling pairwise clustering derived yields optimal embedding clustering data points dimensional space
advantages supervised learning final error metric available during training classifiers algorithm directly reduce number training set unfortunately modeling human learning constructing classifiers autonomous robots labels often available expensive paper show labels making use structure between pattern distributions different sensory modalities show minimizing disagreement between outputs networks processing patterns these different modalities approximation minimizing number each leads similar results using vowel dataset show algorithm performs well finding appropriate placement vectors particularly classes different two modalities
real world learning tasks may involve high dimensional data sets arbitrary patterns missing data paper present framework based maximum likelihood density estimation learning such data sets use mixture models density estimates make two distinct em principle et al deriving learning algorithm em used both estimation mixture components missing data resulting algorithm applicable wide range supervised well unsupervised learning problems results classification benchmark data set presented
analyze data uncertain missing input features incorporated into training neural network general solution requires weighted integration over unknown uncertain input although computationally closed form solutions found certain gaussian basis function networks discuss cases solutions such mean unknown input
describe number learning rules used train supervised parallel feature extraction systems learning rules derived using gradient ascent quality function consider number quality functions rational functions higher order moments extracted feature values show system learns principle components correlation matrix principal component analysis systems usually optimal feature extractors classification therefore design quality functions produce feature vectors support unsupervised classification properties different systems compared help different artificially designed datasets database consisting color spectra
singular value decomposition svd important tool linear algebra used approximate matrices although many authors use svd eigenvector decomposition principal components transform important realize these other methods apply symmetric matrices while svd applied arbitrary matrices property important applications signal transmission control propose two new algorithms iterative computation svd given sample inputs outputs matrix although currently exist many algorithms eigenvector decomposition example these first true svd algorithms
present fast algorithm non linear dimension reduction algorithm builds local linear model data merging pca clustering based new distortion measure experiments speech image data indicate local linear algorithm produces encodings lower distortion than those built five layer auto associative networks local linear algorithm more than order magnitude faster train
approach presented learning high dimensional functions case learning algorithm affect generation new data local modeling algorithm locally weighted regression used represent learned function architectural parameters approach such distance metrics localized become function query point instead being global statistical tests given local model good enough sampling should moved new area our methods explicitly deal case prediction accuracy requirements exist during exploration gradually center exploration controlling speed shift local prediction accuracy goal directed exploration state space takes place along current data support until task goal achieved illustrate approach simulation results results real robot learning complex task
their nature memory based algorithms such knn parzen windows require computationally expensive search large database prototypes paper optimize searching process tangent distance simard improve speed performance closest prototypes found recursively searching included subsets database using distances increasing complexity done using hierarchy tangent distances increasing number tangent vectors its maximum multiresolution using wavelets each stage confidence level classification computed confidence high enough computation more complex distances avoided resulting algorithm applied character recognition close three orders magnitude faster than computing full tangent distance every prototypes
propose learning algorithm variable memory length markov process human communication whether given text handwriting speech has multi characteristic time scales short scales characterized mostly dynamics generate process whereas large scales more syntactic semantic information carried reason used fixed memory markov models cannot capture effectively complexity such structures other hand using long memory models uniformly practical even short memory four algorithm propose based minimizing statistical prediction error extending memory state length adaptively until total prediction error sufficiently small demonstrate algorithm learning structure natural english text applying learned model correction corrupted text using less than states models performance far superior fixed memory models similar number states show algorithm applied dna base prediction results comparable hmm based methods
four versions nearest neighbor algorithm locally adaptive introduced compared basic nearest neighbor algorithm knn locally adaptive knn algorithms choose value should used classify query results cross validation computations local neighborhood query local knn methods shown perform similar knn experiments commonly used data sets encouraging results three constructed tasks show local methods significantly outperform knn specific applications local methods line learning applications different regions input space patterns solving different sub tasks
paper shown conventional back propagation algorithm neural network regression robust data corrupted but outliers data corrupted robust model model error mixture normal distribution influence function mixture model calculated condition model robust outliers given em algorithm used estimate parameter usefulness model selection criteria discussed illustrative simulations performed
conventional bayesian justification backprop finds map weight vector paper shows find map function instead add correction term backprop term biases towards functions small description lengths particular favors kinds feature selection pruning weight sharing
drug activity prediction handwritten character recognition features extracted describe training example depend pose location orientation etc example handwritten character recognition best techniques addressing problem tangent distance method simard et al introduce new technique dynamic reposing addresses problem dynamic reposing iteratively learns neural network examples effort maximize predicted output values new models trained new poses computed until models poses converge paper compares dynamic reposing tangent distance method task predicting biological activity compounds fold cross validation comparison dynamic reposing tangent distance drug activity prediction dynamic reposing attains correct compared tangent distance method neural network standard poses nearest neighbor method
propose method improving performance network designed predict next value time series analyzing deviations networks predictions data training set carried out secondary network trained time series these residuals combined system two networks viewed new predictor demonstrate simplicity success method applying data small corrections secondary network regarded resulting taylor expansion complex network includes combined system find complex network more difficult train performs worse than two step procedure combined system
back propagation algorithm has been modified work without computations low resolution makes more attractive hardware implementation numbers represented floating point bit bits exponent states bit bit exponent gradients while bit fixed point numbers way computations executed shift add operations large over weights trained demonstrated same performance networks computed full precision estimate circuit implementation shows large network placed single chip reaching more than billion weight updates per second speedup obtained machine multiplication slower than shift operation
geometric data structures introduced provide efficient access collection functions euclidean space interest describe modified structure has been employed neural network classifier compare its performance several classification tasks against radial basis function networks standard layer perceptron
performance many nonparametric methods critically depends strategy along regression surface constrained topological mapping algorithm novel method achieves adaptive knot placement using neural network based kohonens self organizing maps present modification original algorithm provides knot placement according estimated second derivative regression surface
present new incremental radial basis function network suitable classification regression problems center positions continuously updated through soft competitive learning width radial basis functions derived distance topological neighbors during training observed error locally used determine next unit leads case classification problems placement units near class rather than near frequency peaks done most existing methods resulting networks need few training epochs seem generalize well demonstrated examples
extend optimal brain obs second order method pruning networks allow general error measures explore reduced computational storage implementation via dominant decomposition simulations nonlinear noisy pattern classification problems reveal ob does lead improved generalization performs favorably comparison optimal brain damage obd find required steps obd may lead inferior generalization result interpreted due noise back into system common technique stop training large network minimum validation error found test error could reduced even further means ob but obd pruning our results approximation used ob indicate why highly pruned network may lead inferior performance
present paper propose entropy method transform internal representation entropy function defined respect state hidden unit internal representation internal representation transformed changing parameter entropy function thus transformation referred transformation internal representation transformed according given problems transforming internal representation into minimum entropy representation obtain kernel networks smaller networks explicit interpretation other hand changing appropriately parameter obtain intermediate internal representations improved generalization applied entropy method obtaining kernel networks small internal entropy addition applied method frequency identification problem could obtain derived networks whose generalization performance significantly superior performance standard back propagation
present algorithm training feedforward recurrent neural networks detects internal representation uses these constructive manner add new neurons network advantages starting small network neurons required detecting internal early stage learning time reduced empirical results two real world problems faster learning speed applied training recurrent network well sequence recognition task grammar training times significantly less than previously reported
propose learning algorithm learning hierarchical models object recognition model architecture compositional hierarchy represents part whole relationships parts described local context substructures object focus report learning hierarchical models data ie inducing structure model prototypes observed exemplars object each node hierarchy probability distribution governing its parameters learned connections between nodes reflects structure object formulation substructures such their parts become conditionally independent resulting model interpreted bayesian belief network many similar stochastic visual grammar described
paper proposes practical optimization method layered neural networks optimal model parameter found simultaneously modify conventional information criterion into differentiable function parameters minimize while controlling back ordinary form discussed theoretically experimentally
study stop learning class feedforward networks networks linear outputs neuron fixed input weights they trained gradient descent algorithm finite number examples under general regularity conditions shown general three distinct phases generalization performance learning process particular network has better generalization performance learning stopped certain time before global minimum empirical error reached notion effective size machine defined used explain trade off between complexity machine training error learning process study leads naturally network size selection criterion turns out generalization criterion learning process shown stopping learning before global minimum empirical error has effect network size selection
study complexity problem artificial feedforward neural networks designed approximate real valued functions several real variables ie estimate number neurons network required ensure given degree approximation every function given function class indicate construct networks indicated number neurons evaluating standard activation functions our general theorem shows smoother activation function better rate approximation
training classifiers large databases computationally demanding desirable develop efficient procedures reliable prediction classifiers suitability implementing given task so resources assigned most promising candidates exploring new classifier candidates propose such practical principled predictive method practical because avoids costly procedure training poor classifiers whole training set principled because its theoretical foundation effectiveness proposed procedure demonstrated both multi layer networks
study feed forward nets arbitrarily many layers using standard sigmoid tanh our theorems complete knowledge output neural net arbitrary inputs uniquely specifies architecture weights thresholds many critical points error surface generic training problem neural nets originally introduced highly simplified models nervous system they widely used technology studied theoretically several they remain little understood mathematically feed forward neural net consists finite sequence positive integers do dl family real numbers defined family real numbers defined il sequence do called architecture neural net while called weights thresholds neural nets used compute non linear maps following construction ve begin fixing nonlinear function variable analogy nervous system suggests take asymptotic constants tends standard choice adopt throughout paper alternate address dept princeton university princeton nj given input oo define real numbers dt following induction tj xj known fixed set xj oj here dt interpreted outputs dt neurons th layer net output map net defined map tl practical applications tries pick neural net so output map approximates given map about have imperfect information main result paper under generic conditions perfect knowledge output map uniquely specifies weights thresholds neural net up obvious symmetries more precisely obvious symmetries follows let permutations de de let dr collection ls assume yt identity whenever easily neural nets oj do dl same output map set oj reflects neurons layer function odd nets called they related note particular isomorphic neural nets have same architecture our main theorem under generic conditions two neural nets same output map isomorphic discuss generic conditions impose neural nets have avoid obvious such suppose weights zero output map constant architecture thresholds neural net clearly uniquely determined fix suppose have therefore gives xj recovering feed forward net its output tl so through sum jj output depends output map does uniquely determine weights our hypotheses more than adequate exclude these specifically assume equal fraction form pq integers these conditions hold generic neural nets precise statement our main theorem follows two neural nets satisfy have same output nets isomorphic would interesting replace minimal hypotheses study functions other than tanh now sketch proof our main result accuracy simplicity after trivial reduction may assume do thus outputs nodes functions variable output map neural net key idea continue analytically complex values note read off structure net set singularities xj tanh poles points arithmetic progression ri leads two crucial observations poles form arithmetic progression every pole accumulation point poles xj merely fact immediate formula special case do obtain rn ri rn see fix assume has simple pole while analytic neighborhood lt ft obtain analytic neighborhood ls oj analytic neighborhood thus neighborhood poles solutions equation many solutions accumulating hence accumulation point poles proof hi view natural make following definitions natural domain neural net largest open subset complex plane output map zt analytically define th singular set setting complement natural domain set accumulation points these definitions made entirely terms output map without reference structure given neural net other hand sets contain nearly complete information architecture weights thresholds net allow us read off structure neural net analytic its output map see sets reflect structure net reason follows expect union over dt set poles together their accumulation points ignore here empty immediately read off depth neural net simply smallest empty induction ve need solve show union arithmetic nj therefore read off ii return point later
show randomly output classes various training data may used improve predictive accuracy classification algorithm present method calculating noise sensitivity signature learning algorithm based output classes signature used indicate good match between complexity classifier complexity data use noise sensitivity signatures different other schemes avoid such cross validation uses part training data various penalty functions data adaptive noise sensitivity signature methods use training data data adaptive non parametric they well suited situations limited training data
have recently shown widely known lms algorithm optimal estimator criterion has been introduced initially control theory literature means ensure robust performance face model uncertainties lack statistical information signals extend here our analysis nonlinear setting often encountered neural networks show backpropagation algorithm locally optimal fact provides theoretical justification widely observed excellent robustness properties lms backpropagation algorithms further discuss implications these results
paper efficiency recurrent neural network implementations state finite state machines explored specifically shown node complexity unrestricted case bounded above shown node complexity xm log weights thresholds restricted set fan restricted two matching lower bounds provided each these upper bounds assuming state encoded subset nodes size log mi
two layer networks sigmoidal hidden units generalization error shown bounded input dimension number training samples respectively represents expectation random number hidden units probability prior distribution weights corresponds gibbs regularizer relationship makes possible characterize explicitly regularization term affects networks bound obtained analytically large commonly used priors applied estimate expected network complexity er practice result provides quantitative explanation large networks generalize well
show randomly selected points probability such multi layer perceptron first hidden layer composed hi threshold logic units ment exactly zi such perceptron have units first hidden layer fully connected inputs implies maximal capacities sense cover input patterns per hidden unit input patterns per synaptic weight such networks both capacities achieved networks single hidden layer same single neuron comparing these results recent estimates vc dimension find contrast single neuron case sufficiently large hi vc dimension exceeds covers capacity
fundamental backpropagation bp algorithm training artificial neural networks cast deterministic gradient method under certain natural assumptions such series learning rates diverging while series their squares converging established every accumulation point online bp stationary point bp error function results presented cover serial parallel online bp modified bp momentum term bp weight decay
problem learning examples multilayer networks studied within framework statistical mechanics using replica formalism calculate average generalization error fully connected committee machine limit large number hidden units number training examples proportional number inputs network generalization error function training set size approaches finite value number training examples proportional number weights network find first order phase transitions discontinuous generalization error both binary continuous weights
neurons learning under unsupervised hebbian learning rule perform nonlinear generalization principal component analysis relationship between nonlinear pca nonlinear neurons reviewed stable fixed points neuron learning dynamics correspond maxima ic optimized under nonlinear pca order predict what neuron learns knowledge neuron dynamics required here correspondence between nonlinear pca neural networks breaks down shown simple model methods statistical mechanics used find optima objective function non linear pca determines what neurons learn order find solutions partitioned neurons solve dynamics
describe use smoothing spline analysis variance penalized log likelihood context learning estimating probability outcome given training set attribute vectors outcomes form pt vector attributes learned sum smooth functions attribute plus sum smooth functions two attributes etc smoothing parameters governing obtained iterative unbiased risk iterative method confidence intervals these estimates available
solvable models nonlinear machines me proposed learning artificial neural networks studied based theory ordinary differential equations learning algorithm constructed optimal parameter found without recursive procedure solvable models enable us analyze reason why experimental results error backpropagation often statistical learning theory
satisfiability random cnf formulae precisely variables per clause sat popular performance search algorithms formulae have clauses variables randomly keeping ratio mn fixed model has been proven have sharp threshold between formulae almost formulae almost never oc computer experiments carried out bell show similar threshold behavior each value finite size scaling theory critical point phenomena used statistical physics shown characterize size dependence near threshold annealed replica based mean field theories give good account results address tj research center ny usa portions work done while institute support foundation kirkpatrick tishby large scale computation without length scale increasingly possible model natural world computer matter physics has strategies complexities such calculations usually depending characteristic length example molecules finite ranged broken down into weakly smaller parts may use symmetry identify natural modes system whole even most difficult case continuous phase transitions correlated over wide range scales group provides way problem down its relevant parts providing generator behavior scales terms critical point itself but length scales much help organizing another sort large calculation examples include large rule based expert systems model complex industrial processes digital example has used network three more expert systems originally called check computer orders completeness internal consistency schedule production aid salesman needs detailed set tasks required rules deal parts ten years described employing nearly rules deal part numbers ten years moderate growth would valuable understand technical social factors have constrained many important commercial scientific problems without length scales computer modelling automatic classification lie within few decades size track distinct items kept stock banks credit specialized information building models what have might want next biology human currently described terms coupled through data similarly amino acid sequences known proteins deeper understanding computational cost these problems order needed see practical they simplified study style resolution search find obvious collective effects may heart its computational complexity threshold phenomena random sat properties randomly generated combinatorial structures often exhibit sharp threshold phenomena analogous phase transitions studied matter physics recently thresholds have been observed randomly generated boolean formulae mitchell et al consider satisfiability problem sat instance sat boolean formula normal form cnf ie conjunction logical clauses logical each contains exactly boolean variable equal probability its task determine whether assignment variables such clauses evaluate true here use denote number variables number clauses formula statistical mechanics satisfaction randomly generated sat instances has been shown analytically large ratio rt mn less than instances almost whereas ratios larger than almost instances unsatisfiable rigorous analysis has proven experimental evidence strongly suggests threshold sat mitchell et al main reasons studying randomly generated cnf formulae their use empirical evaluation combinatorial search algorithms cnf formulae good candidates evaluation such algorithms because determining their satisfiability np complete problem holds larger values satisfiability problem solved efficiently et al despite worst case complexity simple heuristic methods usually determine satisfiability random formulae computationally challenging test instances found generating formulae near threshold mitchell et al has made similar observation increased computational cost heuristic search boundary between two distinct phases behaviors combinatorial model provide precise characterization dependence threshold phenomena sat ranging employ finite size scaling method statistical physics direct observation width threshold critical region transition used characterize universal behavior quantities across entire critical region extending analysis combinatorial problems characterizes size model observed discussion applicability finite size scaling systems without metric see kirkpatrick thresholds sat sat sat sat sat mn fig fraction unsatisfiable formulae sat kirkpatrick tishby experimental data have generated extensive data satisfiability randomly generated formulae ranging fig shows fraction random sat formulae unsatisfiable function ratio example left most curve fig shows fraction formulae unsatisfiable random cnf formulae variables over range values each data point generated using randomly generated formulae giving accuracy used highly optimized implementation procedure procedure works best formulae smaller data obtained samples comparable computing cost fig ranging shows threshold each value except case curves cross single point up increasing between curves largest values seem converging single point well although curves smaller point formulae unsatisfiable thought computationally problems found mitchell al al point lies consistently right scale invariant point point curves cross each other shifts simple explanation rapid shift thresholds right increasing probability given clause satisfied random input configuration treat clauses independent probability clauses satisfied define entropy per input times log expected number satisfying configurations vanishing entropy gives estimate threshold identical upper bound derived several see log called annealed estimate ac because interactions between clauses annealed theories materials see average over many details disorder have marked aa each figures table results finite size scaling analysis fig clear threshold up larger values both threshold shift increasing slope curves fig accounted finite size scaling see kirkpatrick plot fraction samples unsatisfied against rescaled variable values rc derived experimental data first determined crossing point curves large fig determined make match up through critical region fig find these two parameters capture both threshold shift curves using see fraction statistical mechanics satisfaction scaled sat models annealed limit fig rescaled sat data using fig rescaled data sat approach annealed limit unsatisfiable formulae given fn invariant function fig description threshold shift follows immediately define etc fig find cs fit their data point function arbitrarily assuming leading correction they obtain two expressions differ few percent ranges obtained good results data other values table give critical parameters obtained analysis error bars subjective show range each parameter over best fits obtained note appears becomes increasingly good approximation etc increases success finite size scaling different strong evidence ie diverging correlations even absence length finally found similar shape fact combining various rescaled curves figure shows curves point tend limiting form obtained extending annealed arguments previous section define probability formula remains unsatisfied configurations curve similar form but shifted right other ones kirkpatrick tishby table critical parameters random sat outline statistical mechanics analysis space permits sketch our analysis model inputs binary may represent them vector spins xi each random formula written sum its clauses jl vector has non zero element input selects evaluates number clauses left unsatisfied particular configuration natural take value energy partition function inverse temperature factors into contributions each clause annealed approximation mentioned above consists simply taking trace over each individually their interactions construction expect both energy entropy extensive quantities proportional fig shows indeed case lines fig annealed predictions expressions energy obtained annealed theory used compare specific observed numerical experiments simple limit clauses do interact gives evidence supporting identification unsatisfied phase spin glass finally plausible phase spin glass like unsatisfied phase obtained solving st finite temperatures perform averaging over random clauses correctly requires introducing replicas see identical random formula defining overlap between expectation values spins two replicas new order parameter results appear capable accounting statistical mechanics satisfaction difference between experiment annealed predictions finite example approximation consider two replicas gives values table accounts rather closely average overlap found experimentally between pairs lowest energy states shown fig replica theory gives solution function gives lines fig defined table point maximum slope fig entropy function fig calculated replica theory vs experimental ground state overlaps pointing up pointing down ag conclusions have shown finite size scaling methods statistical physics used model threshold randomly generated sat problems given good fit our scaling analysis conjecture method give useful models phase transitions other combinatorial problems control parameter several authors have attempted relate np np completeness characteristics phase transitions models systems anderson see have proposed spin spin interactions random sign having inherent exponential complexity colleagues see first focus diverging correlation length seen continuous phase transitions root computational complexity fact both effects play important roles but sufficient may even necessary np complete problems eg salesman max clique lack phase boundary hard problems cluster thresholds phase transitions yet cost exploring largest cluster never exceeds steps exponential search cost sat comes random inputs require space repeatedly note satisfying kirkpatrick tishby input configuration sat determined its non existence proven polynomial time because reduced problem random directed graph spin glass studied anderson have form close our sat formulae but questions studied different finding input configuration minimum number clauses like finding ground state spin glass phase np hard even therefore both diverging correlations diverging size lengths defined random sign spin glass effects present expect local search like exponentially difficult average but these characteristics do imply np completeness references re linear time algorithm testing truth certain quantified boolean formulae process let vol computing taylor really hard problems proceedings cooperative solution constraint satisfaction problems science vol experimental results point satisfiability problems proc gets his side proc uses statistical mechanics computational complexity sciences complexity ed pp probabilistic analysis procedure solving satisfiability problem discrete applied vol threshold proc th foundations sc kirkpatrick statistical mechanics systems vol kirkpatrick evidence satisfiability threshold random cnf formulas proc ai np hard problems ca ma spin glass theory beyond world scientific mitchell hard easy distributions sat problems proc
prove except possibly small sets analog neural nets globally observable ie their corrupted pseudo computer simulations actually reflect true dynamical behavior network locally finite discrete boolean neural networks observable without
what correct theoretical description neuronal activity analysis dynamics globally connected network spiking neurons spike response model shows description mean firing rates possible active neurons fire firing occurs spatio temporal correlations spike structure neural code becomes relevant alternatively neurons gathered into local distributed ensembles assemblies description based mean ensemble activity principle possible but interaction between different assemblies becomes highly nonlinear description spikes should therefore preferred
most theoretical investigations large recurrent networks focus properties macroscopic order parameters such population averaged activities average overlaps memories statistics fluctuations local activities may important testing ground comparison between models observed cortical dynamics evaluated neuronal correlation functions stochastic network comprising excitatory inhibitory populations show network stationary state cross correlations relatively weak ie their amplitude relative auto correlations order ln being size interacting population holds except neighborhoods nonstationary states bifurcation point approached amplitude cross correlations grows becomes order decay behavior analogous phenomenon critical down systems thermal equilibrium near critical point near hopf bifurcation cross correlations exhibit oscillations
stochastic optimization algorithms typically use learning rate schedules behave asymptotically ot ensemble dynamics moody such algorithms provides easy path results mean squared weight error asymptotic apply approach stochastic gradient algorithms momentum show late times learning governed effective learning rate momentum parameter describe behavior asymptotic weight error give conditions insure optimal convergence speed finally use results develop adaptive form momentum achieves optimal convergence speed independent
presented framework describing two iteration performance attractor neural networks history dependent bayesian dynamics now extend analysis number directions input patterns applied small subsets neurons general connectivity architectures more efficient use history show optimal signal activation function has sigmoidal shape provide intuitive account activation functions non shape function model properties characteristic cortical neurons firing
motivated mathematical modeling analog implementation distributed simulation neural networks present definition asynchronous dynamics general ct dynamical systems defined ordinary differential equations based notions local times communication times provide preliminary results globally convergence asynchronous dynamics ct dynamical systems applying results neural networks obtain conditions ensure additive type neural networks
several recurrent networks have been proposed representations task formal language learning after training recurrent network recognize formal language predict next symbol sequence next logical step understand information processing carried out network researchers have extracting finite state machines internal state trajectories their recurrent networks paper describes sensitivity initial conditions discrete measurements trick these extraction methods return illusory finite state descriptions
biological neurons have variety intrinsic properties because large number voltage dependent currents control their activity neuromodulatory modify both balance conductances determine intrinsic properties strength synapses these mechanisms circuit dynamics suggest functional circuits exist environment they operate
intradendritic electrophysiological recordings reveal complex electrical spikes difficult conventional notions neuronal function paper argue such dendritic events expression more important mechanism proportional current amplifier whose primary task offset using example functionally important synaptic inputs layers reconstructed layer pyramidal neuron derive simulate properties conductances synaptic input current graded manner depends potassium conductance calcium conductances correspondence should addressed
based precise anatomical data olfactory system propose investigation possible mechanisms modulation control between two levels olfactory information processing lobe use simplified neurons but realistic architecture first conclusion feature extraction performed lobe interneurons central input fine tuning central input thus facilitates evolution fuzzy olfactory images layer towards more images upon odor presentation
using quasi realistic model feedback inhibition cells show weak inhibition sufficient maximally negligible effects total mn activity mn synchrony produce hz peak force power spectrum may cause instability feedback loops
maps orientation preference ocular dominance recorded macaque monkeys ranging agreement previous observations found basic features orientation ocular dominance maps well correlations between them present robust did observe changes strength ocular dominance signals well spacing ocular dominance bands both increased between latter finding suggests adult spacing ocular dominance bands depends cortical growth animals found corresponding increase spacing orientation preferences possibility orientation preferences cells change surface correlations between patterns orientation selectivity ocular dominance present visual system still seems more likely their development may process may require extensive visual experience
order best understand visual system should attempt characterize natural images processes images find these scenes possess ensemble scale invariance further they highly non gaussian nongaussian character cannot removed through local linear filtering find including simple gain control nonlinearity filtering process makes filter output quite gaussian meaning information maximized fixed channel variance finally use measured power spectrum place upper bound information about natural scenes array receptors
retina simulated its detailed biological properties study local preprocessing images direct visual pathway photoreceptors bipolar ganglion cells horizontal units well cells simulated computer program simulated analog non spiking transmission between photoreceptor bipolar cells between bipolar ganglion cells well gap junctions between horizontal cells release dopamine cells its diffusion extra cellular space photoreceptors retina containing units carried out retina displayed contour extraction effect adaptation simulation showed cells necessary ensure adaptation local
gradient descent algorithm parameter estimation similar those used continuous time recurrent neural networks derived hodgkin huxley type neuron models using membrane potential trajectories targets parameters maximal conductances thresholds activation curves time constants successfully estimated algorithm applied modeling slow non spike oscillation identified neuron ganglion model three currents trained experimental data revealed novel role current slow oscillation below
provide computational description function system circuit fish response sounds our simulations using backpropagation constrained feedforward network have generated hypotheses directly terms activity auditory nerve fibers principle cells system their associated inhibitory neurons
effort understand saccadic eye movements their relation visual attention other forms eye movements number other laboratories out large scale effort design build complete primate oculomotor system using analog cmos vlsi technology using technology low power compact multi chip system has been built works real time using real world visual inputs describe paper performance early version such system including array photoreceptors retina circuit computing mean location activity representing superior colliculus saccadic burst generator degree freedom platform models dynamic properties primate oculomotor plant
signal processing classification algorithms often have limited applicability resulting inaccurate model signals underlying structure present here efficient bayesian algorithm modeling signal composed superposition brief poisson distributed functions methodology applied specific problem modeling classifying extracellular neural waveforms composed superposition unknown number action potentials previous approaches have had limited success due largely problems determining spike shapes deciding many shapes distinct overlapping bayesian solution each these problems obtained inferring probabilistic model waveform approach uncertainty form number inferred ap shapes used obtain efficient method complex overlaps algorithm extract many times more information than previous methods facilitates extracellular investigation neuronal classes interactions within neuronal circuits bayesian modeling classification neural signals
do have good understanding theoretical principles learning realized neural systems address problem built computational model development sound localization system structure model drawn known experimental data while learning principles come recent work field brain style computation model accounts numerous properties sound localization system makes specific testable predictions future experiments provides theory developmental process
showed remarkable results suggest head motion barn owl controlled distinct circuits coding horizontal vertical components movement implies existence set orthogonal internal coordinates related meaningful coordinates external world coherent computational theory has yet been proposed explain finding have proposed simple model provides framework theory low level motor learning show theory predicts observed results barn owl model concept optimal unsupervised motor learning provides set criteria predict optimal internal representations describe two iterative neural network algorithms find optimal solution demonstrate possible mechanisms development internal representations animals
detail design construction analog vlsi model neural system responsible behaviors leech why leech biological network small relatively well understood silicon model therefore span three levels organization leech nervous system neuron ganglion system represents first comprehensive models leech operating real time circuit employs biophysically motivated analog neurons form multiple biologically inspired silicon ganglia these ganglia coupled using known connections thus model retains its biological counterpart though simplified output silicon circuit similar output leech central pattern generator model operates same spatial scale leech nervous system provide excellent platform explore real time adaptive locomotion leech other simple invertebrate nervous systems
transition point dynamic programming reinforcement learning direct dynamic programming approach adaptive optimal control reduce learning time memory usage required control continuous stochastic dynamic systems does so determining ideal set transition points specify control action changes necessary optimal control converges ideal set using variation learning assess merits adding removing states throughout state space applied track problem learned optimal control policy much than conventional learning able do so using less memory
recently found effective method control chaotic systems unstable fixed points using small control forces method based limited linear theory requires considerable knowledge dynamics system controlled paper use two radial basis function networks model unknown plant other controller controller trained recurrent learning algorithm minimize novel objective function such controller locate unstable fixed point drive system into fixed point priori knowledge system dynamics our results indicate neural controller offers many advantages over technique
paper describes routing algorithm packet routing reinforcement learning module embedded into each node switching network local communication used each node keep accurate statistics routing decisions lead minimal delivery times simple experiments involving node connected network routing superior algorithm based shortest paths able route efficiently even critical aspects simulation such network load allowed vary dynamically paper discussion tradeoff between discovering maintaining stable policies
consider problem learning inputoutput mappings through exploration eg learning kinematics dynamics robotic actions expensive computation cheap should explore selecting trajectory through input space gives us most amount information number steps discuss results field optimal experiment design may used guide such exploration demonstrate its use simple kinematics problem
describe relationship between certain reinforcement learning based dynamic programming class monte carlo methods solving systems linear equations proposed these methods solution linear system expected statistic suitably defined over sample paths markov chain significance our observations lies arguments these monte carlo methods scale better respect state space size than do standard iterative techniques solving systems linear equations analysis establishes convergence rate estimates because methods used rl systems approximating evaluation function fixed control policy approximate solutions systems linear equations connection these monte carlo methods establishes algorithms similar td algorithms sutton asymptotically more efficient precise sense than other methods policies further dp based rl methods have properties these monte carlo algorithms suggests although rl often perceived slow sufficiently large problems may fact more efficient than other known classes methods capable producing same results barto
reinforcement learning methods based approximating dynamic programming dp receiving increased attention due their utility forming reactive control policies systems embedded dynamic environments environments usually modeled controlled markov processes but environment model known priori adaptive methods necessary adaptive control methods often classified being direct indirect direct methods directly adapt control policy experience whereas indirect methods adapt model controlled process compute control policies based model our focus indirect adaptive dp based methods paper present convergence result indirect adaptive asynchronous value iteration algorithms case look up table used store value function our result implies convergence several existing reinforcement learning algorithms such adaptive real time dynamic programming barto singh prioritized sweeping moore atkeson although emphasis researchers studying dp reinforcement learning has been direct adaptive methods such learning watkins methods using td algorithms sutton clear these direct methods practice indirect methods such those analyzed paper barto
increasing attention has recently been algorithms based dynamic programming dp due suitability dp learning problems involving control stochastic environments system being controlled known unifying theoretical account these methods has been missing paper relate dp based learning algorithms powerful techniques stochastic approximation via new convergence theorem enabling us establish class convergent algorithms both tda learning belong
describe extension mixture experts architecture modelling controlling dynamical systems exhibit modes behavior extension based markov process model suggests recurrent network gating set linear non linear controllers new architecture demonstrated capable learning effective control strategies jump linear non linear multiple modes behavior
propose trajectory planning control theory continuous movements such connected cursive handwriting continuous natural speech its hardware based our previously proposed forward inverse relaxation neural network kawato computationally its optimization principle minimum criterion regarding representation level hard constraints satisfied trajectory represented set via points extracted handwritten character accordingly propose via point estimation algorithm estimates via points trajectory formation character via point extraction character experiments good quantitative agreement found between human handwriting data trajectories generated theory finally propose recognition schema based movement generation show result recognition schema applied handwritten character recognition extended phoneme timing estimation natural speech
paper describes algorithm verification signatures written input algorithm based novel artificial neural network called neural network network consists two identical sub networks their outputs during training two sub networks extract features two signatures while joining neuron measures distance between two feature vectors verification consists comparing extracted feature vector stored feature vector signatures closer stored representation than chosen threshold accepted other signatures
paper describes use convolutional neural network perform address block location machine printed mail pieces locating address block difficult object recognition problem because often large amount mail piece because address blocks vary dramatically size shape used convolutional network four outputs each trained find different corner address block simple set rules used generate candidates network output system performs well allowed five network tightly bound address delivery information cases
have developed artificial neural network based gaze tracking system individual users unlike other gaze normally require user use rest ensure head our system entirely non currently best gaze tracking systems accurate approximately degrees our experiments have been able achieve accuracy degrees while allowing head paper present empirical analysis performance large number artificial neural network architectures task
human genes continuous but rather consist short coding regions exons highly variable non coding regions introns apply hmms problem modeling exons introns detecting splice sites human our most interesting result so far detection particular oscillatory patterns minimal period roughly seem characteristic exon regions may have significant biological implications division biology california institute technology department psychology university exon splice site site splice site site consensus sequences ag figure structure genes scale introns typically much longer than exons
changes lighting conditions strongly effect performance reliability computer vision systems report face recognition results under drastically changing lighting conditions computer vision system concurrently uses contrast sensitive silicon retina conventional gain controlled ccd camera both input devices face recognition system employs elastic matching algorithm wavelet based features classify unknown faces assess effect analog chip preprocessing silicon retina ccd images have been bandpass filter adjust power spectrum silicon retina its ability adjust sensitivity increases recognition rate up percent these comparative experiments demonstrate preprocessing analog vlsi silicon retina generates image data object constant features
paper introduces new recognition based segmentation approach recognizing line cursive handwriting database english words original input stream coordinates encoded sequence uniform stroke descriptions processed six feed forward neural networks each designed recognize letters different sizes words recognized performing best first search over space possible results demonstrate method effective both writer dependent recognition error rate writer independent recognition error rate
developed system finding address blocks mail pieces process four images per second locating address block our system determines writing style handwritten machine printed moreover measures angle text lines noisy images layout analysis elements present image performed order distinguish text separate text address speed more than four images per second obtained modular hardware platform containing board two neural net chips processor board board digital signal processors system has been tested more than images its performance depends quality images lies between correct location noisy images over images
usually schedule their well advance optimize their pools activities many events such flight delays absence member require pool team change initial schedule paper show neural network comparison paradigm applied backgammon game tesauro sejnowski applied problem pool indeed both problems correspond choosing best solution set possible ones without ranking them called here best choice problem paper explains mathematical point view architecture learning strategy backpropagation neural network used best choice problem show learning phase network accelerated finally apply neural network model real problems
game go has high branching factor tree search approach used computer long range spatiotemporal interactions make position evaluation extremely difficult development conventional go programs their knowledge intensive nature demonstrate viable alternative training networks evaluate go positions via temporal difference td learning our approach based network architectures reflect spatial organization both input reinforcement signals go board training provide exposure though unlabelled play these techniques yield far better performance than networks trained alone network less than weights learned within games go position evaluation function enables primitive search commercial go program low level
online cursive handwriting recognition currently most challenges pattern recognition study presents novel approach problem composed two complementary phases first dynamic encoding writing trajectory into compact sequence discrete motor control symbols compact representation largely remove redundancy while preserving most its components second phase these control sequences used train adaptive probabilistic acyclic automata important ingredients writing trajectories eg letters present new efficient learning algorithm such stochastic automata demonstrate its utility spotting segmentation cursive our experiments show over letters correctly identified prior higher level language model moreover both training recognition algorithms efficient compared other modeling methods models line other
paper describes massively parallel simd computer easy program high performance low cost effective implementing highly parallel neural network architectures has bit serial processing elements each has bits memory interconnected switching network entire system single compatible computer using language class parallel processor terms fast arithmetic operators vectors variable precision integers
implemented using radial basis functions combination analog digital vlsi hybrid system uses custom analog circuits input layer digital signal board hidden output layers system combines advantages both analog digital low power consumption while minimizing overall system error analog circuits have been fabricated tested system has been built several applications have been executed system application provides significantly better results remote sensing problem than have been previously obtained using methods
present experimental results supervised learning dynamical features analog vlsi neural network chip recurrent network containing six continuous time analog neurons free parameters connection strengths thresholds trained generate time varying outputs approximating given periodic signals presented network chip implements stochastic algorithm error gradient along random directions parameter space error descent learning addition integrated learning functions generation pseudo random perturbations chip provides teacher forcing long term storage parameters network learns khz trajectory sec chip mm mm pm cmos process
recent physiological research has shown synchronization oscillatory responses striate cortex may code relationships between visual features objects vlsi circuit has been designed provide rapid phase locking synchronization multiple oscillators allow further exploration neural mechanism exploiting intrinsic random transistor mismatch devices subthreshold large groups phase locked oscillators readily partitioned into smaller phase locked groups multiple target binary images described utilizing phase locking architecture vlsi chip has been fabricated tested verify architecture chip employs pulse amplitude modulation encode output periphery system
paper describes low power analogue vlsi neural network called three layer perceptron multiplying synapses chip neurons fabricated cmos chip neurons variable gain per neuron lower than previous designs intended application chip intra classification part system measurements chip indicate per connection achievable part integrated system has been successfully trained loop parity morphology classification problems
use mean field theory methods statistical mechanics derive softmax nonlinearity discontinuous wta mapping give two simple ways implementing softmax network element these has number important network theoretic properties passive incrementally passive nonlinear resistive element content function having form entropy these properties should enable use element nonlinear networks such other elements resistive constraint boxes implement high speed analog optimization algorithms using minimum hardware
built high speed digital mean field boltzmann chip board general problems constraint satisfaction learning each chip has neural processors weight update processors supporting arbitrary topology up functional neurons chip learning theoretical maximum rate connection recall typical conditions chips high speed due parallel computation inner products limited but adequate precision weights activations bits fast mhz several design insights digital boltzmann vlsi constraint satisfaction learning
present neural network simulation implemented massively parallel connection machine contrast previous work simulator based biologically realistic neurons nontrivial single cell dynamics high connectivity structure modelled agreement biological data preservation temporal dynamics spike interactions simulate neural networks neurons coupled about synapses per neuron estimate performance much larger systems communication between neurons identified computationally most demanding task present novel method overcome bottleneck simulator has already been used study primary visual system cat
most commonly used neural network models well suited direct digital implementations because each node needs perform large number operations between floating point values ability learn examples generalize restricted networks type indeed networks each node implements simple boolean function boolean networks designed such way exhibit similar properties two algorithms generate boolean networks examples presented results show these algorithms generalize well class problems accept compact boolean network descriptions techniques described general applied tasks known have characteristic two examples applications presented image reconstruction hand written character recognition
present implementation intelligent electronic circuits realized first time using new functional device called neuron mos transistor vmos short simulating behavior biological neurons single transistor level search most data memory cell array instance automatically carried out hardware without software manipulation soft hardware named arbitrarily change its logic function real time external control signals without hardware modification implementation neural network chip self learning capability described through studies vmos intelligent circuit implementation interesting similarity architectures vmos logic circuitry biological systems
fast event driven software simulator has been developed simulating large networks spiking neurons synapses primitive network elements designed exhibit biologically realistic behaviors such spiking adaptation axonal delays summation post synaptic current pulses current inputs efficient event driven representation allows large networks simulated fraction time would required full compartmental model simulation corresponding analog cmos vlsi circuit primitives have been designed characterized so large scale circuits may simulated prior fabrication
introduce new approach line recognition handwritten words written unconstrained mixed style performs word level normalization fitting model word structure using em algorithm words coded into low resolution annotated images each pixel contains information about trajectory direction curvature recognizer convolution network spatially replicated network output hidden markov model produces word scores entire system globally trained minimize word level errors
present method learning tracking recognizing human hand gestures recorded conventional ccd camera without special other sensors view based representation used model aspects hand relevant trained gestures found using unsupervised clustering technique use normalized correlation networks dynamic time warping temporal domain distance function unsupervised clustering views computed space time dimensions distributed response combination these units characterizes input data low dimensional representation supervised classification stage uses labeled outputs spatio temporal units training data our system correctly classify gestures real time low cost image processing
propose computational model cortex shape depth texture model consists four stages extraction local spatial frequency frequency characterization detection texture compression normalization integration normalized frequency over space model accounts number psychophysical observations including experiments based novel random textures these textures generated white noise manipulated fourier domain order produce specific frequency spectra simulations range stimuli including real images show qualitative quantitative agreement human perception
feature correspondence problem classic visual object recognition concerned determining correct mapping between features measured image features expected model paper show determining good correspondences requires information about joint probability density over image features propose likelihood based correspondence matching general principle selecting optimal correspondences approach applicable non rigid models allows nonlinear perspective transformations optimally deal occlusions missing features experiments rigid non rigid hand gesture recognition support theory likelihood based techniques show almost decrease classification performance compared performance perfect correspondence knowledge
goal work investigate role primate mt neurons solving structure motion problem three types receptive field rf found area mt neurons et et correspond our analysis suggests th st nd order fuzzy space differential operators large radius ratio allows both smooth velocity fields detection boundaries objects model agreement recent psychophysical data surface interpolation suggest area mt partially information about object shape information about spatial relations necessary navigation manipulation
address problem optical flow reconstruction particular problem ambiguities near edges they occur due aperture problem ii occlusion problem pixels both sides intensity edge assigned same velocity estimates confidence these measurements correct side edge non our approach introduce field respect estimates confidence measures note confidence measures large intensity edges larger convex sides edges ie inside corners than concave side resolve ambiguities through local interactions via coupled markov random fields mrf result detection motion regions images large global convexity
present mean field theory method locating twodimensional objects have rigid transformations resulting algorithm form coarse fine correlation matching first consider problems matching synthetic point data derive point matching objective function tractable line segment matching objective function derived considering each line segment dense collection points approximating sum gaussians algorithm tested real images line segments extracted matched
recent work becker hinton becker hinton shows promising mechanism based maximizing mutual information assuming spatial coherence system itself learn visual abilities such binocular stereo introduce more general criterion based bayesian probability theory thereby demonstrate connection bayesian theories visual perception other organization principles early vision atick methods implementation using variants stochastic learning described special case linear filtering derive analytic expression output
short term memory processing time varying information artificial neural networks paper model linear memories presented ways include memories connectionist topologies discussed comparison drawn among different memory types what salient characteristic each memory model
spotting tasks require detection target patterns background richly varied non target inputs performance measure interest these tasks called figure fom detection rate target patterns false rate acceptable range new approach training presented computes fom gradient each input pattern directly maximizes fom using backpropagation eliminates need thresholds during training uses network resources model bayesian posteriori probability functions accurately patterns have significant effect detection accuracy over false rate interest fom training increased detection accuracy percentage points hybrid radial basis function rbf hidden markov model hmm credit speech corpus
have developed visual preprocessing algorithms extracting relevant features video image speaker provide speaker independent inputs automatic system visual features such mouth visible several shape descriptors mouth its motion rapidly computable manner quite insensitive lighting conditions formed hybrid system consisting two time delay neural networks video acoustic integrated their responses means independent bayesian optimal method given conditional independence seems hold our data hybrid system had error rate lower than acoustic alone five utterance speaker independent task indicating video used improve speech recognition
new classifier presented text independent speaker recognition new classifier called modified neural tree network mntn hierarchical classifier combines properties decision trees feed forward neural networks mntn differs standard new learning rule based discriminant learning used minimizes classification error opposed norm approximation error mntn uses leaf probability measures addition class labels mntn evaluated several speaker identification experiments compared multilayer percepttons mlps decision trees vector quantization vq classifiers vq classifier mntn demonstrate comparable performance perform significantly better than other classifiers task additionally mntn provides logarithmic retrieval time over vq classifier mntn vq classifiers compared several speaker verification experiments mntn found outperform vq classifier
progress has been made implementation speech production based physiological data inverse dynamics model speech system tile articulator trajectories emg signals modeled using tile acquired forward dynamics model temporal smoothness emg activation range constraints inverse dynamics model allows use faster speech motor control scheme applied phoneme synthesis via system dynamics future use speech recognition tile forward acoustic model mapping articulator trajectories tile acoustic parameters improved adding velocity information inputs distinguish acoustic parameter differences caused source characteristics
hybrid systems model time both using markov chain through properties connectionist network paper discuss nature time dependence currently employed our systems using recurrent networks feed forward multi layer perceptrons mlps particular introduce local into mlp produce enhanced input representation form adaptive gamma filter incorporates automatic approach learning temporal dependencies have phone recognition task using database results using gamma filtered input representation have shown improvement over baseline mlp system improvements have been obtained through merging baseline gamma filter models
previously had developed concept segmental neural net snn phonetic modeling continuous speech recognition kind neural network technology advanced state art large vocabulary employs hidden markov models hmm word resource management corpus more recently started neural net system larger more challenging corpus word wall street journal corpus during explored following research directions refine system training context dependent models regularization method ii training snn projection pursuit ii combining different models into hybrid system tested both development set independent test set resulting neural net system alone yielded performance level hmm system hybrid system achieved consistent word error reduction over hmm system paper describes our hybrid system emphasis optimization methods employed
although visual auditory systems share same basic tasks about its environment most connectionist work date has been different problem speech recognition believe most fundamental task auditory system analysis acoustic signals into components corresponding individual sound sources has called auditory scene analysis computational connectionist work auditory scene analysis reviewed outline general model includes these approaches described
consider problem cns learns control dynamics mechanical system using paradigm subjects hand virtual mechanical environment show learning control via composition model imposed dynamics properties computational elements cns model inferred through generalization capabilities subject outside training data
study explores extent network learns temporal relationships within between component features music account music theoretic psychological phenomena such hierarchy rhythmic predicted generated sequences recorded representation note learnt predictive recurrent network network learned transitions relations between within pitch timing components duration values development rhythmic metric structures training network developed response activation individual tones analysis hidden unit representation revealed musical sequences represented transitions between states hidden unit space
imagine you have designed neural network successfully learns complex classification task what relevant input features classifier relies these features combined produce classification decisions applications deeper insight into structure adaptive system thus into underlying classification problem may well important systems performance characteristics eg backpropagation based training scheme produces networks into equivalent concise set rules achieved penalty terms network parameters adapt network expressive power class rules thus during training simultaneously minimize classification transformation error real world tasks demonstrate our approach
variant encoder architecture units input output layers represent nodes graph applied task mapping locations sets neighboring locations degree internal ie hidden unit representations reflect global properties environment depends upon several parameters learning procedure architectural noise incremental learning shown important factors maintaining topographic relationships global scale
models analog retrieval require computationally cheap method estimating similarity between probe candidates large pool memory items vector dot product operation would ideal purpose possible encode complex structures vector representations such way similarity vector representations reflected underlying structural similarity paper describes such encoding provided reduced representations method encoding nested relational structures fixed width distributed representations conditions under structural similarity reflected dot product discussed
non linear complexities neural networks make network solutions difficult understand contribution analysis here extended analysis networks automatically generated learning algorithm because such networks have cross connections hidden layers standard analyses hidden unit activation patterns insufficient contribution defined product output weight associated activation unit whether unit input hidden unit multiplied sign output target current input pattern among contributions matrix contributions input patterns principal components analysis pca extract main features variation contributions such analysis applied three problems continuous xor arithmetic comparison distinguishing between two three cases technique yields useful insights into network solutions consistent across several networks
paper propose extension raam extension labeling raam encode labeled graphs cycles representing explicitly data encoded well content direct access content achieved transforming encoder network into analog hopfield network hidden units different access procedures defined depending access key sufficient conditions stability associated hopfield network briefly introduced
apply active exemplar selection white predicting chaotic time series given fixed set examples method chooses concise subset training fitting these exemplars results entire set being fit well desired algorithm incorporates method network complexity automatically adding exemplars hidden units needed fitting examples generated glass equation fractal dimension required about exemplars hidden units method requires order magnitude fewer floating point operations than training entire set examples significantly than two exemplar selection techniques suggests simpler active selection technique performs comparably
new neural network binary presented its use classifier demonstrated evaluated network feed forward type learns examples shot mode new neurons needed tested problem pixel classification performed well possible applications network associative memories
paper consider problem classifying eeg signals normal subjects subjects disorder eg disorder schizophrenia using class artificial neural networks multi layer perceptton shown multilayer perceptton capable classifying unseen test eeg signals high degree accuracy
almost models orientation direction selectivity visual cortex based feedforward connection schemes geniculate input provides excitation both pyramidal inhibitory neurons latter neurons response former non optimal stimuli anatomical studies show up excitatory synaptic input onto cortical cell provided other cortical cells massive excitatory feedback nature cortical circuits embedded canonical microcircuit here investigate analytically through biologically realistic simulations detailed model circuitry operating mode model weak geniculate input dramatically excitation while inhibition has dual role prevent early geniculate induced excitation null direction ii excitation ensure neurons fire stimulus their receptive field among insights gained possibility visual cortical function proposals short term memory strong limitations linearity tests use gratings properties visual cortical neurons compared detail model classical model direction selectivity does include excitatory cortico cortical connections model explain number features direction selective simple cells including small input conductance changes have been measured experimentally during stimulation null direction model allows us understand why velocity response curve area neurons different their lgn nonlinearities contrast response curve striate cortical neurons
propose computational framework understanding modeling human framework integrates many existing theoretical yet sufficiently concrete allow simulation experiments do attempt explain subjective experience but instead ask what differences exist within cognitive information processing system person represented information versus information central idea explore correspond temporally persistent states network computational modules three simulations described illustrating behavior persistent states models corresponds roughly behavior states people experience performing similar tasks our simulations show periodic settling persistent ie states improves performance up noise forcing decisions keep system track toward solution
biological sensorimotor systems static maps transform input sensory information into output motor behavior evidence many lines research suggests their representations experience dependent entities while plasticity essential flexible behavior presents nervous system challenges sensorimotor system adapts itself perform well under set circumstances perform poorly placed environment different demands negative transfer later experience dependent change benefits previous learning catastrophic interference explore first question separate paper volume et al here present psychophysical computational results explore question catastrophic interference context dynamic motor learning task under conditions subjects show evidence catastrophic interference under other conditions subjects appear its effects these results suggest motor learning undergo process modular neural networks well suited demands learning multiple inputoutput mappings incorporating notion slow changing connections into modular architecture able account psychophysical results
current understanding effects damage neural networks even though such understanding could lead important insights concerning motivated consideration present simple analytical framework estimating functional damage resulting focal structural lesions neural network effects focal lesions varying area shape number retrieval capacities spatially organized associative memory although our analytical results based approximations they correspond well simulation results study light important features characterizing clinical multi including strong association between number after stroke multiplicative interaction has been occur between disease multi dr department institute advanced computer studies university
based computational principles yet direct experimental validation has been proposed central nervous system cns uses internal model simulate dynamic behavior motor system planning control learning sutton barto kawato et al jordan rumelhart et al present experimental results simulations based novel approach investigates temporal propagation errors sensorimotor integration process our results provide direct support existence internal model
model short term memory ordered stimuli proposed implementation loop thought type memory model predicts presence time varying context signal coding timing items presentation addition store phonological information process serial items associated context nodes phonemes hebbian connections showing both short long term plasticity items activated phonemic input during presentation context phonemic feedback during output serial selection items occurs via winner take interaction items winner subsequently receiving inhibition approximate analysis error probabilities due gaussian noise during output presented model provides account probability error function serial position list length word length phonemic similarity temporal grouping item list proposed starting point model vocabulary acquisition
new model presented models between odor molecules receptor proteins activation second receptor proteins mathematical formulation reaction transformed into artificial neural network ann resulting feed forward network provides powerful means parameter fitting applying learning algorithms weights network corresponding chemical parameters trained presenting experimental data demonstrate simulation capabilities model experimental data neurons shown our model sufficient observed data simpler models able do task
spatial distribution time course electrical signals neurons have important theoretical practical consequences because difficult infer neuronal form affects electrical have developed quantitative yet intuitive approach analysis approach transforms architecture cell anatomical space using voltage distance metric describe theory behind approach illustrate its use
model hippocampus presented forms rapid self organized representations input arriving via path performs recall previous associations region ca performs comparison recall afferent input region ca comparison feedback regulation cholinergic modulation set appropriate dynamics learning new representations region ca ca network responds novel patterns increased cholinergic modulation allowing storage new self organized representations but responds familiar patterns decrease allowing recall based previous representations requires selectivity cholinergic suppression synaptic transmission regions ca ca has been demonstrated experimentally
biological neuron viewed device maps multidimensional temporal event signal dendritic postsynaptic activations into temporal event signal action potentials have designed network spatio temporal event mapping architecture learn perform mapping arbitrary biophysical models neurons such network appropriately trained called cell used place conventional compartmental model simulations transfer function important such network simulations cell offers advantages over compartmental models terms computational efficiency analytical framework vlsi implementations biological neurons
learn imitate song through auditory motor learning have developed theoretical framework song learning accounts response properties neurons have been observed many nuclei involved song learning specifically suggest pathway needed song production adult but essential song acquisition provides synaptic perturbations adaptive evaluations learning computer model based reinforcement learning constructed could real song accuracy based measure second generation model replicated song accuracy
neural network model self organization ocular dominance lateral connections binocular input presented self organizing process results network afferent weights each neuron organize into smooth hill shaped receptive fields primarily retinas neurons common eye preference form connected patches lateral connections primarily link regions same eye preference similar self organization cortical structures has been observed experimentally model shows lateral connections cortex may develop based correlated activity explains why lateral connection patterns follow receptive field properties such ocular dominance
auditory system barn owl contains several spatial maps barn optical over their eyes these auditory maps shifted stay visual map suggesting visual input imposes frame reference auditory maps optic first site convergence visual auditory information site plasticity shift auditory maps plasticity occurs instead inferior colliculus contains auditory map into optic explored model owl global reinforcement signal whose delivery controlled visual hebb learning rule gated reinforcement learned appropriately adjust auditory maps addition reinforcement learning adjusted weights inferior colliculus owl brain even though weights allowed change throughout auditory system observation possibility site learning does have specified but could determined learning procedure network architecture pouget sejnowski optic inferior external nucleus field cochlea visual system figure view auditory pathways barn owl
macaque lateral geniculate nucleus lgn exhibits pattern changes through nucleus point small due blind spot retina present three dimensional model local cell interactions cause wave development neuronal receptive fields propagate through nucleus establish two distinct patterns examine interactions between wave localized singularities due find induce change pattern explore critical factors determine general lgn organization
accumulating data neurophysiology have suggested two information processing roles prefrontal cortex short term active memory inhibition present new behavioral task computational model developed parallel task developed probe both these prefrontal functions simultaneously produces rich set behavioral data act constraints model model implemented continuous time thus providing natural framework study temporal dynamics processing task show model used examine behavioral consequences specifically use model make novel testable predictions regarding behavioral performance who suffer reduced tone brain area
implement study computational model stevens theory schizophrenia theory onset schizophrenia associated reactive synaptic occurring brain regions receiving temporal lobe projections such area frontal cortex model frontal module associative memory neural network whose input synapses represent incoming temporal projections analyze face external input projections compensatory internal synaptic connections increased noise levels maintain memory capacities generally preserved schizophrenia these compensatory changes lead spontaneous biased retrieval stored memories corresponds occurrence without apparent external trigger their tendency concentrate few central our results explain why these symptoms tend schizophrenia progresses why delayed leads much slower response
parietal cortex thought represent positions objects particular coordinate systems propose alternative approach spatial perception objects parietal cortex perspective sensorimotor transformations responses single parietal neurons modeled gaussian function retinal position multiplied sigmoid function eye position form set basis functions show here these basis functions used generate receptive fields either retinotopic head centered coordinates simple linear transformations possibility parietal cortex does attempt compute positions objects particular frame reference but instead computes general purpose representation retinal location eye position transformation synthesized direct projection representation predicts produced parietal lesions should confined coordinates but should observed multiple frames reference single patients prediction supported several experiments pouget sejnowski
many cells part medial superior temporal mst area visual cortex respond selectively spiral flow patterns specific combinations rotation motions previous have suggested these cells may represent self motion spiral patterns generated relative motion observer particular object mst cell may account portion complex flow field set active cells could encode entire flow manner mst effectively segments moving objects such grouping operation essential interpreting scenes containing several independent moving objects observer motion describe model based hypothesis selective tuning mst cells reflects grouping object components undergoing coherent motion inputs model generated sequences images simulated realistic motion situations combining observer motion eye movements independent object motion input representation modeled after response properties neurons area mt provides primary input area mst after applying unsupervised learning algorithm units tuned patterns coherent motion results match many known properties mst cells consistent recent studies indicating these cells process object motion information richard sejnowski
institute theoretical computer science mail abstract investigate computational power formal model networks spiking neurons both assumption timing precision case limited timing precision prove upper lower bounds number examples needed train such networks
derive global optimal training algorithms neural networks these algorithms guarantee smallest possible prediction error energy over possible fixed energy therefore robust respect model uncertainties lack statistical information signals ensuing estimators infinite dimensional sense updating weight vector estimate requires knowledge previous weight certain finite dimensional approximation these estimators backpropagation algorithm explains local optimality backpropagation has been previously demonstrated
novel class locally excitatory globally inhibitory oscillator networks proposed model each oscillator corresponds standard relaxation oscillator two time scales network exhibits mechanism selective gating whereby oscillator up its active phase rapidly oscillators stimulated same pattern while others up show analytically selective gating mechanism network rapidly achieves both synchronization within blocks oscillators stimulated connected regions between different blocks computer simulations demonstrate networks promising ability segmenting multiple input patterns real time model physical foundation oscillatory correlation theory feature binding may provide effective computational framework scene segmentation segregation
present new method obtaining response function its average most properties learning generalization linear perceptrons derived first known results thermodynamic limit infinite perceptron size show explicitly self averaging limit discuss extensions our method more general learning scenarios anisotropic teacher space priors input distributions weight decay terms finally use our method calculate finite corrections order discuss corresponding finite size effects generalization learning dynamics important spin off observation results obtained thermodynamic limit often directly relevant systems fairly real world sizes
discuss model consistent learning additional restriction probability distribution training samples target concept hypothesis class show model provides significant improvement upper bounds sample complexity ie minimal number random training samples allowing selection hypothesis accuracy confidence further show model has potential providing finite sample complexity even case infinite vc dimension well sample complexity below vc dimension achieved linking sample complexity average number implementable training sample rather than maximal size sample ie vc dimension
learning continuous valued functions using neural network ensembles committees give improved accuracy reliable estimation generalization error active learning ambiguity defined variation output ensemble members averaged over unlabeled data so disagreement among networks discussed use ambiguity combination cross validation give reliable estimate ensemble generalization error type ensemble cross validation sometimes improve performance shown estimate optimal weights ensemble members using unlabeled data generalization query committee finally shown ambiguity used select new training data labeled active learning scheme
random errors databases limit performance classifier trained applied database paper propose method estimate limiting performance classifiers imposed database demonstrate technique task predicting failure paths
neural network learning paradigm based information theory proposed way perform fashion reduction among elements output layer without loss information sensory input model developed performs nonlinear decorrelation up higher orders results probabilistically independent components output layer means need gaussian distribution neither input nor output theory presented related unsupervised leaming theory proposes redundancy reduction goal cognition nonlinear units used nonlinear principal component analysis obtained case nonlinear manifolds reduced minimum dimension manifolds such units used network performs generalized principal component analysis sense non gaussian distributions linearly higher orders correlation taken into account basic structure architecture involves general transformation volume therefore entropy yielding map without loss information minimization mutual information among output neurons eliminates redundancy between outputs results statistical decorrelation extracted features known factorial learning
using statistical mechanical formalism calculate evidence generalisation error consistency measure linear perceptton trained tested set examples generated non linear teacher teacher said because student never model without error our model allows us interpolate between known case linear teacher nonlinear teacher comparison hyperparameters maximise evidence those performance measures reveals non linear case evidence procedure guide performance finally explore extent evidence procedure unreliable find despite being sub optimal circumstances might useful method fixing hyperparameters
paper presents rigorous characterization general nonlinear learning machine generalizes during training process trained random sample using gradient descent algorithm based reduction training error shown particular best generalization performance occurs general before global minimum training error achieved different roles played complexity machine class complexity specific machine class during learning precisely
present here analysis stochastic neural network composed three state neurons described equation outer product representation equation employed representation extension analysis two three state neurons easily performed apply formalism approximation schemes simple three state network compare results monte carlo simulations
present statistical method pac learns class stochastic perceptrons arbitrary monotonic activation function weights wi probability distribution generates input examples member family call blocking distributions such distributions represent important step beyond case each input variable statistically independent blocking family contains markov distributions order stochastic perceptron mean perceptron upon presentation input vector outputs probability fi because same algorithm works monotonic activation function boolean domain handles well studied cases usual radial basis functions
supervised learning learning queries rather than random examples improve generalization performance significantly study performance query learning problems student cannot learn teacher perfectly occur frequently practice prototypical scenario kind consider linear perceptron student learning binary perceptron teacher two kinds queries maximum information gain ie minimum entropy investigated minimum student space entropy queries appropriate teacher space unknown minimum teacher space entropy queries used teacher space assumed known but student simpler form has been chosen find queries structure student space determines efficacy query learning whereas queries lead higher generalization error than random examples due lack feedback about progress student way queries selected
consider effect combining several least squares estimators expected performance regression problem computing exact bias variance curves function sample size able quantitatively compare effect combination bias variance separately thus expected error sum two our exact calculations demonstrate combination estimators particularly useful case data set small noisy function learned large data sets single estimator produces superior results finally show splitting data set into several independent parts training each estimator different subset performance cases significantly improved key words bias variance least squares combination
performance line algorithms learning studied line learning number examples equivalent learning time each example presented once learning curve generalization error function depends schedule learning rate target perceptton rule learning curve perceptton algorithm decrease fast schedule optimized target realizable perceptton perceptton algorithm does generally converge solution lowest generalization error case due simple output noise propose new line algorithm perceptron yielding learning curve approach optimal generalization error fast generalize perceptron algorithm class thresholded smooth functions learning target class well input distributions algorithm converges optimal solution its learning curve decrease fast
paper discusses use artificial neural networks dynamic modelling time series argue prediction more appropriate capture dynamics underlying dynamical system because iterated model show method implemented recurrent ann trained trajectory learning show select trajectory length train iterated predictor case chaotic time series experimental results proposed method
propose novel rigorous approach analysis unsupervised hebbian learning network behavior model determined underlying nonlinear dynamics parameterized set parameters hebbian rule arbor density synapses these parameters determine presence absence specific receptive field referred connection pattern fixed point attractor model paper perform qualitative analysis underlying nonlinear dynamics over parameter space determine effects system parameters emergence various receptive fields predict precisely within parameter regime network have potential develop specially connection pattern particular approach first time crucial role played synaptic density functions provides complete precise parameter space defines relationships among different receptive fields our theoretical predictions confirmed numerical simulations pan
estimate number training samples required ensure performance neural network its training data matches obtained data applied network existing estimates higher orders magnitude than practice indicates work seeks narrow gap between theory practice transforming problem into determining distribution random field space weight vectors turn application recent technique called poisson heuristic
study asymptotic properties sequence weight vector estimates obtained training multilayer feedforward neural network basic gradient descent method using fixed learning constant batch processing case exact analysis establishes existence limiting distribution gaussian general general case small learning constant approximation permits application results theory random matrices again establish existence limiting distribution study first few moments distribution compare contrast results our analysis those techniques stochastic approximation
increasing attention has been reinforcement learning algorithms recent years partly due theoretical analysis their behavior markov environments markov assumption removed neither generally algorithms nor analyses continue propose analyze new learning algorithm solve certain class non markov decision problems our algorithm applies problems environment markov but learner has restricted access state information algorithm involves monte carlo policy evaluation combined policy improvement method similar markov decision problems guaranteed converge local maximum algorithm operates space stochastic policies space yield policy performs considerably better than deterministic policy although space stochastic policies continuous even discrete action space our algorithm computationally tractable jaakkola singh jordan
widely accepted use more compact representations than lookup tables crucial scaling reinforcement learning rl algorithms real world problems unfortunately almost theory reinforcement learning assumes lookup table representations paper address issue combining function approximation rl present function approximator based simple extension state aggregation commonly used form compact representation namely soft state aggregation theory convergence rl arbitrary but fixed soft state aggregation novel intuitive understanding effect state aggregation online rl new heuristic adaptive state aggregation algorithm finds improved compact representations exploiting non discrete nature soft state aggregation preliminary empirical results presented
straightforward approach curse dimensionality reinforcement learning dynamic programming replace lookup table generalizing function approximator such neural net although has been successful domain backgammon guarantee convergence paper show combination dynamic programming function approximation robust even cases may produce entirely wrong policy introduce grow support new algorithm divergence yet still benefits successful generalization
reinforcement learning addresses problem learning select actions order maximize ones performance unknown environments scale reinforcement learning complex real world tasks such typically studied ai ultimately able discover structure world order abstract away details operate more tractable problem spaces paper presents skills algorithm skills discovers skills partially defined action policies arise context multiple related tasks skills whole action sequences into single operators they learned minimizing compactness action policies using description length argument their representation empirical results simple grid navigation tasks illustrate successful discovery structure reinforcement learning
semi decision problems continuous time generalizations discrete time markov decision problems number reinforcement have been developed recently solution markov decision problems based ideas asynchronous dynamic programming stochastic approximation among these td real time dynamic programming after semi decision problems optimality equation context propose those named above adapted solution semi markov decision problems demonstrate these algorithms applying them problem determining optimal control simple system conclude discussion circumstances under these algorithms may applied
prove convergence algorithm equivalent learning construction its equivalence achieved encoding values within policy value function actor critic algorithm novel two ways updates critic most probable action executed given state rewards actor using criteria depend relative probability action executed
basic paradigm learning neural networks learning examples training set input output examples used network target function learning hints generalization learning examples additional information about target function incorporated same learning process such information come common sense rules special financial market applications training data noisy use such hints have advantage demonstrate use hints exchange trading us versus german mark japanese over period explain general method learning hints applied other markets learning model method restricted neural networks
paper discusses linearly weighted combination estimators weighting functions dependent input show weighting functions derived either evaluating input dependent variance each estimator estimating likely given estimator has seen data region input space close input pattern latter solution closely related mixture experts approach show learning rules mixture experts derived theory about learning missing features presented approaches modular weighting functions easily modified more estimators added furthermore easy incorporate estimators derived data such expert systems algorithms
introduce recurrent architecture having modular structure formulate training procedure based em algorithm resulting model has similarities hidden markov models but supports recurrent networks processing style allows exploit supervised learning paradigm while using maximum likelihood estimation
propose statistical mechanical framework modeling discrete time series maximum likelihood estimation done via boltzmann learning dimensional networks tied weights call these networks boltzmann chains show they contain hidden markov models hmms special case our framework new architectures address particular hmms look two such architectures parallel chains model feature sets time scales networks model long term dependencies between hidden states these networks show implement boltzmann learning rule exactly polynomial time without simulated mean field annealing necessary computations done exact procedures statistical mechanics
data collection costly much gained actively selecting particularly informative data points sequential way bayesian decision theoretic framework develop query selection criterion explicitly takes into account intended use model predictions markov chain monte carlo methods necessary quantities approximated desired precision number data points grows model complexity modified bayesian model selection strategy properties two versions criterion demonstrated numerical experiments
visualizing pairwise dissimilarity data difficult combinatorial optimization problems known multidimensional scaling pairwise data clustering algorithms embedding dissimilarity data set space clustering these data actively selecting data support clustering process discussed maximum entropy framework active data selection provides strategy discover structure data set efficiently partially unknown data
new learning algorithm derived performs online stochastic gradient ascent mutual information between outputs inputs network absence priori knowledge about signal noise components input propagation information depends network non detailed higher order moments input density functions mutual information between outputs well their individual network input into independent components example application have achieved near perfect separation ten mixed speech signals our simulations lead us believe our network performs better blind separation than network reflecting fact derived mutual information objective bell sejnowski
between nodes competitive learning network achieved through competition basis neural activity simple inhibitory mechanisms limited sparse representations while decorrelation factorization schemes support distributed representations computationally neural plasticity competitive interaction instead obtain alternatives fully distributed representations use technique simplify improve our binary information gain optimization algorithm feature extraction sejnowski same approach could used improve other learning algorithms
existing recurrent net learning algorithms introduce conceptual framework viewing recurrent training matching vector fields dynamical systems phase space reconstruction techniques make hidden states explicit reducing temporal learning feed forward problem short propose viewing iterated prediction best way training recurrent networks deterministic signals using framework train multiple trajectories insure their stability design arbitrary dynamical systems
dynamic cell structures dcs represent family artificial neural architectures suited both unsupervised supervised learning they belong recently introduced class topology representing networks build perfectly topology preserving feature maps dcs employ modified learning rule conjunction competitive hebbian learning kohonen type learning rule serves adjust synaptic weight vectors while hebbian learning establishes dynamic lateral connection structure between units reflecting topology feature manifold case supervised learning ie function approximation each neural unit implements radial basis function additional layer linear output units adjusts according delta rule dcs first rbf based approximation scheme attempting concurrently learn utilize perfectly topology preserving map improved performance simulations selection cmu benchmarks indicate dcs idea applied growing cell structure algorithm leads efficient elegant algorithm conventional models similar tasks
although artificial neural networks have been applied variety real world scenarios remarkable success they have often been exhibiting low degree human techniques compact sets symbolic rules out artificial neural networks offer promising perspective overcome obvious deficiency neural network representations paper presents approach extraction rules artificial neural networks its key mechanism validity interval analysis generic tool extracting symbolic knowledge propagating rule like knowledge through backpropagation style neural networks empirical studies robot arm domain illustrate appropriateness proposed method extracting rules networks real valued distributed representations
have determined capacity information efficiency associative net configured brain like way partial connectivity noisy input cues recall theory used calculate capacity pattern recall achieved using winners strategy transforming dendritic sum according input activity unit usage greatly increase capacity associative net under these conditions sparse patterns maximum information efficiency achieved low connectivity levels corresponds level connectivity commonly seen brain brain connected most information efficient way
radial basis function rbf networks known networks locally tuned processing units see well known their ease use most algorithms used train these types networks require fixed architecture number units hidden layer determined before training starts rce training algorithm introduced cooper see its probabilistic extension rce algorithm take advantage growing structure hidden units introduced necessary nature these algorithms allows training reach stability much faster than case gradient descent based methods unfortunately rce networks do adjust standard deviation their prototypes individually using global value parameter paper introduces dynamic decay adjustment algorithm utilizes constructive nature rce algorithm together independent adaptation each prototypes decay factor addition radial adjustment class dependent distinguishes between different shown networks trained presented algorithm perform substantially better than common rbf networks
present new algorithm finding low complexity networks high generalization capability algorithm searches large connected regions so called fiat minima error function weight space environment fiat minimum error remains approximately constant using mdl based argument fiat minima shown correspond low expected overfitting although our algorithm requires computation second order derivatives has order complexity experiments feedforward recurrent nets described application stock market prediction method outperforms conventional backprop weight decay optimal brain
product units provide method automatically learning higher order input combinations required efficient learning neural networks show problems encountered using backpropagation train networks containing these units paper examines these problems proposes heuristics improve learning using these heuristics constructive method introduced solves well problems significantly less neurons than previously reported secondly product units implemented candidate units cascade correlation system resulted smaller networks trained faster than using sigmoidal gaussian units
present deterministic annealing variant em algorithm maximum likelihood parameter estimation problems our approach em process problem minimizing thermodynamic free energy using principle entropy statistical mechanics analogy unlike simulated annealing approaches minimization performed moreover derived algorithm unlike conventional em algorithm obtain better estimates free initial parameter values
paper studies problem diffusion markovian models such hidden markov models hmms makes difficult task learning long term dependencies sequences using results markov chain theory show problem diffusion reduced transition probabilities approach under condition standard hmms have limited modeling capabilities but inputoutput hmms still perform interesting computations
introduce novel algorithm learning motivated segmentation problems computational vision underlying factors correspond clusters highly correlated input features algorithm new kind competitive clustering model cluster generators explain each feature data set explain each input example rather than competing examples features traditional clustering algorithms natural extension algorithm hierarchical models data generated multiple unknown categories each different multiple causal structure several simulations demonstrate power approach
paper presents alternating minimization am algorithm used training radial basis function linear networks algorithm modification small step point method used solving linear programs algorithm has convergence rate dl iterations measure network size measure resulting solutions accuracy two results presented specify two steps am may ensure convergence each step alternating minimization
self organizing neural network sequence classification called described analyzed experimentally extends kohonen feature map architecture activation retention decay order create unique distributed response patterns different sequences yields extremely dense yet representations sequential input few training iterations network has proven successful mapping arbitrary sequences binary real numbers well phonemic representations english words potential applications include isolated spoken word recognition cognitive science models sequence processing
paper studies convergence properties well known means clustering algorithm means algorithm described either gradient descent algorithm slightly extending em algorithm hard threshold case show means algorithm actually minimizes quantization error using fast algorithm
develop principled strategy sample function optimally function approximation tasks within bayesian framework using ideas optimal experiment design introduce objective function incorporating both bias variance measure degree approximation potential utility data points towards optimizing objective show general strategy used derive precise algorithms select data two cases learning unit step functions polynomial functions particular investigate whether such active algorithms learn target fewer examples obtain theoretical empirical results suggest case
understanding knowledge representations neural nets has been difficult problem principal components analysis pca contributions products activations connection weights has yielded valuable insights into knowledge representations but much work has focused correlation matrix contributions present work shows analyzing variance covariance matrix contributions yields more valid insights taking account weights
neural network weights symbolic terms crucial interpreting explaining behavior network additionally domains symbolic description may lead more robust generalization present principled approach symbolic rule extraction based notion weight templates parameterized regions weight space corresponding specific symbolic expressions appropriate choice representation show template parameters may efficiently identified instantiated yield optimal match units actual weights depending requirements application domain our method accommodate arbitrary ok complexity simple expressions ok complexity more general class recursive expressions ok complexity number inputs unit our method rule extraction offers several benefits over alternative approaches literature simulation results variety problems demonstrate its effectiveness
many real world learning problems best characterized interaction multiple independent causes factors discovering such causal structure data focus paper based cooperative vector quantizer architecture unsupervised learning algorithm derived expectation maximization em framework due combinatorial nature data generation process exact step computationally intractable two alternative methods computing step proposed gibbs sampling mean field approximation promising empirical results presented
incremental network model introduced able learn important topological relations given set input vectors means simple hebb like learning rule contrast previous approaches like neural gas method model has parameters change over time able continue learning adding units connections until performance criterion has been met applications model include vector quantization clustering interpolation
propose alternative model mixtures experts uses different parametric form gating network modified model trained em algorithm comparison earlier models trained either em gradient ascent need select learning report simulation experiments show new architecture yields faster convergence apply new model two problem domains piecewise nonlinear function approximation combination multiple previously trained classifiers
most common techniques estimating conditional probability densities inappropriate applications involving periodic variables paper introduce three novel techniques such problems investigate their performance using synthetic data apply these techniques problem extracting distribution wind vector directions radar data gathered remote sensing
introduce study methods synaptic noise into dynamically driven recurrent neural networks show applying controlled amount noise during training may improve convergence generalization addition analyze effects each noise parameter additive vs multiplicative cumulative vs non cumulative per time step vs per string predict best overall performance achieved additive noise each time step extensive simulations learning dual parity grammar temporal strings these predictions
hinton proposed generalization artificial neural nets should improve nets learn represent domains underlying regularities hints work shows outputs backprop net used inputs through information given net extend these ideas showing backprop net learning many related tasks same time use these tasks inductive bias each other thus learn better identify five mechanisms backprop improves generalization give empirical evidence backprop generalizes better real domains
present graph based method rapid accurate search through prototypes transformation invariant pattern classification our method has theory same recognition accuracy other recent methods based tangent distance simard et al uses same categorization rule nevertheless ours significantly faster during classification because far fewer tangent distances need computed crucial success our system novel graph architecture transformation constraints geometric relationships among prototypes encoded during learning improved graph search criterion used during classification these architectural insights applicable wide range problem domains here demonstrate handwriting recognition task basic implementation our system requires less than half computation euclidean sorting method
paper derive classifiers winner take wta approximations bayes classifier gaussian mixtures class conditional densities derived classifiers include clustering based algorithms like lvq means propose constrained rank gaussian mixtures model derive wta algorithm our experiments two speech classification tasks indicate constrained rank model wta approximations improve performance over unconstrained models
present efficient algorithms dealing problem missing inputs incomplete feature vectors during training recall our approach based approximation input data distribution using parzen windows recall obtain closed form solutions arbitrary feedforward networks training show backpropagation step incomplete pattern approximated weighted averaged backpropagation step complexity solutions training recall independent number missing features verify our theoretical results using classification regression problem
many different discrete time recurrent neural network architectures have been proposed has been effort compare these experimentally paper review many these architectures compare they perform various classes simple problems including grammatical inference nonlinear system identification
prior constraints imposed upon learning problem form distance measures prototypical point sets graphs learned clustering point matching graph matching distance measures point matching distance measure invariant under affine rotation scale permutations operates between noisy images missing spurious points graph matching distance measure operates weighted graphs invariant under permutations learning formulated optimization problem large so formulated million variables efficiently minimized using combination optimization techniques algebraic transformations iterative projective scaling clocked deterministic annealing
paper explores application temporal difference td learning sutton forecasting behavior dynamical systems outputs opposed game like situations performance td learning comparison standard supervised learning depends amount noise present data paper use deterministic chaotic time series low noise laser task direct five step ahead predictions our experiments show standard supervised learning better than td learning td algorithm viewed linking adjacent predictions similar effect obtained sharing internal representation network thus compare two architectures both paradigms first architecture separate hidden units consists individual networks each five direct multi step prediction tasks second shared hidden units has single larger hidden layer finds representation five predictions next five steps generated data set do find significant difference between two architectures paper wi colors time
analogue vlsi neural network has been designed tested perform morphology classification tasks analogue techniques chosen meet strict power area requirements system robustness neural network architecture reduces impact noise drift offsets inherent analogue approaches network multi layer perceptron chip digital weight storage input feed network has winner take circuit output network trained loop included commercial signal processing path system has successfully distinguished different patients better than true positive true negative rhythms cannot detected present chip implemented cmos less than maximum average power area
present silicon model axon shows promise building block pulse based neural computations involving correlations pulses across both space time circuit shares number features its biological counterpart including excitation threshold brief refractory period after pulse completion pulse amplitude restoration pulse width restoration provide simple explanation circuit operation present data chip fabricated standard rn cmos process through mos implementation service mosis restoration width pulse time stable propagation axons
describe analog vlsi implementation art algorithm prototype chip has been fabricated standard low cost double single poly cmos process has die area chip modified version original art architecture such modification has been shown preserve computational properties original algorithm while being more appropriate vlsi chip implements art network nodes nodes therefore cluster binary pixels input patterns into up different categories modular system possible array chips without extra circuitry resulting layer xn nodes layer xm nodes pattern classification performed less than means equivalent computing power connections connection updates per second although internally chip analog nature interfaces outside world through digital signals thus having true digital behavior experimental chip test results available have been obtained through test digital chips
novel two terminal device consisting layer between exhibits non analogue memory action device stores synaptic weights ann chip replacing previously used dynamic weight storage two different synapse designs discussed results presented
present analog vlsi chip parallel analog vector quantization mosis double poly cmos chip contains array charge based distance estimation cells implementing mean absolute difference metric operating input analog vector field analog template vectors distance cell including dynamic template storage measures xm additionally chip features winner take wta output circuit linear complexity global positive feedback fast settling single winner output experimental results complete vq system demonstrate correct operation analog input dynamic range cycle time power dissipation
localization orientation various novel interesting events environment critical sensorimotor ability animals mammals superior colliculus sc plays major role behavior deeper layers exhibiting mapped responses visual auditory somatosensory stimuli sensory information arriving different modalities should represented same coordinate frame auditory cues particular thought computed head based coordinates transformed retinal coordinates paper analog vlsi implementation auditory localization plane described extends architecture proposed barn owl primate eye movement system further transformation required transformation intended model projection primates auditory cortical areas deeper layers primate superior colliculus system analog vlsi based saccadic eye movement system being constructed our laboratory
consider problem decoding block coded data using physical dynamical system sketch out algorithm fractal block codes show implement recurrent neural network using physically simple but highly nonlinear analog circuit models neurons synapses nonlinear system has many fixed points but have our procedure choose parameters such way solution desired solution stable partial proof concept present experimental data small system neuron analog cmos chip fabricated analog well process chip operates subthreshold regime each choice parameters converges unique stable state each state exhibits qualitatively fractal shape
have our study parallel learning method et al implications its implementation analog vlsi our new results indicate most cases single parallel perturbation per pattern presentation function parameters weights neural network theoretically best course true certain problems may generally true faced issues implementation such limited precision these cases multiple parallel perturbations may best indicated our previous results
paper describes way neural hardware implementation analog digital mixed mode neural chip full custom neural vlsi artificial neural used implement speech recognition system multi layer perceptron linear neurons trained successfully under limited accuracy computations network large frame input layer tested recognize spoken words forward retrieval hardware module suggested eight chips more extended performance capacity ii song lee
describe single transistor silicon synapses compute learn provide non memory retention single transistor synapses simultaneously perform long term weight storage compute product input weight value update weight value according hebbian backpropagation learning rule memory accomplished via charge storage floating gates providing long term retention synapses efficiently use physics silicon perform weight updates weight value increased using weight value decreases using injection small size low power operation single transistor synapses allows development dense synaptic arrays describe design fabrication characterization modeling array single transistor synapses steady state source current used representation weight value both functions proportional power source current synaptic array fabricated standard pro double poly analog process available mosis
deciding appropriate representation use modeling human auditory processing critical issue auditory science while have successfully performed many single speaker tasks spectrogram methods more difficult problems need richer representation paper describes powerful auditory representation known shows non linear representation converted back into sound loss important information interesting because plausible representation sound paper shows improved methods spectrogram inversion conventional pattern inversion cochlear model inversion representation
paper consider speech coding problem speech modelling particular prediction speech over short time segments performed using hierarchical mixture experts hme jordan jacobs hme gives two advantages over traditional non linear function approximators such multi layer perceptton mlp statistical understanding operation predictor information about performance predictor form likelihood information local error bars these two issues examined both toy real world problems regression time series prediction speech coding context extend principle combining local predictions via hme vector quantization scheme fixed local combined line each observation
system translates hand gestures speech through adaptive interface hand gestures mapped continuously control parameters parallel formant speech mapping allows hand act artificial produces speech real time gives vocabulary addition direct control fundamental frequency volume currently best version uses several input devices including space parallel formant speech neural networks gesture speech task divided into vowel consonant production using gating network weight outputs vowel consonant neural network gating network consonant network trained examples user vowel network implements fixed user defined relationship between hand position vowel sound does require training examples user volume fundamental frequency stop produced fixed mapping input devices subject has trained he slowly speech quality similar text speech but far more natural pitch variations hinton
paper presents ongoing work speaker independent visual speech recognition system work presented here builds previous research efforts area explores potential use simple hidden markov models limited vocabulary speaker independent visual speech recognition task hand recognition first four english digits task possible applications car phone images modeled mixtures independent gaussian distributions temporal dependencies captured standard left right hidden markov models results indicate simple hidden markov models may used successfully recognize relatively image sequences system achieved performance levels equivalent humans asked recognize first four english digits
paper incorporate hierarchical mixtures experts hme method probability estimation developed jordan into continuous speech recognition system resulting system thought continuous density hmm system but instead using gaussian mixtures hme system employs large set organized but relatively small neural networks perform probability density estimation hierarchical structure reminiscent decision tree except two important differences each expert neural net performs soft decision rather than hard decision unlike ordinary decision trees parameters neural nets hme automatically using em algorithm report results word word wall street journal corpus using hme models
paper presents rapid speaker normalization technique based neural network spectral mapping neural network used front end continuous speech recognition system hmm based normalize input acoustic data new speaker spectral difference between speakers reduced using limited amount new acoustic data rich sentences recognition error phone units acoustic phonetic continuous speech corpus decreased ratio used local basis networks gaussian kernels recursive allocation units line optimization parameters model application model included linear term results compare favorably multivariate linear mapping based constrained orthonormal transformations
speech provide good performance most users but error rate often increases dramatically small percentage talkers who different those talkers used training expensive solution problem more training data attempt sample these outlier users second solution explored paper artificially number training talkers transforming speech existing training talkers approach similar training set ocr digit recognition warping training digit images but more difficult because continuous speech has much larger number dimensions eg linguistic phonetic style temporal spectral differ across talkers explored use simple linear spectral warping training data base used word spotting average detection rate overall increased percentage points male speakers percentage points female speakers increase small but similar obtained amount training data
present unifying view discrete time operator models used context finite word length linear signal processing comparisons made between recently presented gamma operator model delta operator models performing nonlinear system identification prediction using neural networks new model based adaptive bilinear transformation generalizes above models presented
describe framework learning saccadic eye movements using representation target points natural scenes representation takes form high dimensional vector responses spatial filters different orientations scales first demonstrate use response vector task locating previously points scene subsequently use property strategy derive adaptive motor map accurate saccades
describe system track hand sequence video frames recognize hand gestures user independent manner system hand each video frame determines hand open closed tracking system able track hand within pixels its correct location test set containing video sequences different individuals captured different room environments gesture recognition network correctly determines hand being open closed frames test set system has been designed operate real time existing hardware
describe framework real time tracking facial expressions uses neurally inspired correlation interpolation methods distributed view based representation used characterize facial state computed using replicated correlation network ensemble response set view correlation scores input network based interpolation method maps perceptual state motor control states simulated face model activation levels motor state correspond muscle activations derived model integrating fast robust processing models obtain system able quickly track interpret complex facial motions real time
perceptual learning defined fast improvement performance retention learned ability over period time set psychophysical experiments demonstrated perceptual learning occurs discrimination direction stochastic motion stimuli here model learning using two approaches clustering model learns accommodate motion noise averaging model learns ignore noise simulations models show performance similar psychophysical results
paper outlines dynamic theory development adaptation neural networks feedback connections given input ensemble connections change strength according associative learning rule approach stable state neuronal outputs apply theory primary visual cortex examine implications dynamical decorrelation activities orientation selective cells connections theory gives unified quantitative explanation psychophysical experiments orientation contrast orientation adaptation using parameter achieve good between theoretical predictions experimental data
paper presents new method image compression neural networks first show use neural networks pyramidal framework yielding so called pca present image compression method based pca pyramid similar laplace pyramid wavelet transform experimental results real images reported finally present method combine quantization step learning pca pyramid
paper presents unsupervised learning scheme objects their projected images scheme exploits auto associative networks ability encode each view single object into representation indicates its view direction propose two models employ different classification mechanisms first model selects auto associative network whose recovered view best matches input view second model based modular architecture whose additional network classifies views splitting input space demonstrate effectiveness proposed classification models through simulations using wire frame objects
fundamental open problem computer vision determining pose correspondence between two sets points space solved novel robust easily implementable algorithm technique works noisy point sets may unequal sizes may differ non rigid transformations variation calculates pose between point sets related affine transformation translation rotation scale variation calculates translation rotation objective describing problem derived mean field theory objective minimized clocked em like dynamics experiments both handwritten synthetic data provide empirical evidence method
problem interpolating between specified images image sequence simple but important task model based vision describe approach based abstract task manifold learning present results both synthetic real image sequences problem development combined reading speech recognition system
efficiency image search greatly improved using coarse fine search strategy multi resolution image representation resolution so low objects have few distinguishing features search becomes difficult show performance search such low resolutions improved using context information ie objects visible low resolution objects interest but associated them networks given explicit context information inputs they learn detect context objects case user does have aware their existence use integrated feature represent high frequency information low resolutions use multiresolution search techniques allows us combine information about appearance objects many scales efficient way natural fom exemplar selection arises these techniques illustrate these ideas training hierarchical systems neural networks find clusters
simard showed performance nearest neighbor classification schemes handwritten character recognition improved incorporating invariance specific transformations underlying distance metric so called tangent distance resulting classifier prohibitively slow memory intensive due large amount prototypes need stored used distance comparisons paper develop rich models representing large subsets prototypes these models either used singly per class basic building blocks conjunction means clustering algorithm work performed while member statistics data analysis research group bell laboratories hill nj simard
paper presents results first use neural networks real time feedback control high temperature fusion experiment currently principal experimental device research into magnetic approach controlled fusion temperatures up million confined strong magnetic fields accurate control position shape boundary requires real time feedback control magnetic field structure time scale few tens software simulations have demonstrated neural network approach give significantly better performance than linear technique currently used most experiments practical application neural network approach requires high speed hardware fully parallel implementation multilayer perceptron using hybrid digital analogue technology has been developed smith
construct mixture locally linear generative models collection pixel based images digits use them recognition different models given digit used capture different writing new images classified evaluating their log likelihoods under each model use em based algorithm step computationally straightforward principal components analysis pca incorporating tangent plane information about expected local deformations requires adding tangent vectors into sample covariance matrices pca improves performance
theory optimal unsupervised motor learning shows network discover reduced order controller unknown nonlinear system representing most significant modes here extend theory apply command sequences so most significant components discovered network correspond motion primitives combinations these primitives used produce wide variety different movements demonstrate applications human handwriting decomposition synthesis well analysis electrophysiological experiments movements resulting stimulation spinal cord
study integrated neural network control architecture nonlinear dynamic systems presented most recent emphasis neural network control field has error feedback control input lack adaptation problem integrated architecture paper combines feed forward control error feedback adaptive control using neural networks paper reveals different internal functionality these two kinds neural network controllers certain input eg state feedback error feedback error feedback neural network controllers learn gains respect error feedback producing error driven adaptive systems results demonstrate two kinds control scheme combined realize their individual advantages testing added plant shows good tracking adaptation integrated neural control architecture
each year people huge amount time typing text people type typically contains amount redundancy due word usage patterns texts structure paper describes neural network system call typing predicts what next displays most likely subsequent word who accept single instead typing its multi layer perceptron heart adapts its predictions likely subsequent text users word usage pattern characteristics text currently being increases typing speed typing english typing code have been demonstrated using system suggesting potential time savings more than per user per year addition increasing typing speed reduces number user type similar amount english computer programs savings has potential significantly reduce frequency repeated caused typing most common office environment
text neural predictor network used approximate conditional probability distribution possible next characters given previous characters ps outputs fed into standard coding algorithms generate short codes characters high predicted probability long codes highly characters tested short german our method outperforms widely used algorithms used functions such
experiments demonstrated sigmoid multilayer perceptron mlp networks provide slightly better risk prediction than conventional logistic regression used predict risk stroke failure patients who operations mlp networks hidden layer networks hidden layer trained using stochastic gradient descent early stopping mlp networks logistic regression used same input features evaluated using bootstrap sampling roc areas predicting using input features logistic regression mlp networks regularization provided early stopping important component improved performance simplified approach generating confidence intervals mlp risk predictions using auxiliary confidence mlp developed confidence mlp trained reproduce confidence intervals generated during training using outputs mlp networks trained different bootstrap samples
system has been used early predict breast cancer patient outcome attempt increase prognostic accuracy many prognostic factors have been identified because stage model accommodate these new factors factors breast cancer has lead clinical what required new prognostic system test prognostic factors integrate predictive factors variables order increase prognostic accuracy using area under curve receiver operating characteristic compare accuracy following predictive models terms five year breast cancer specific survival system principal component analysis classification regression trees logistic regression cascade correlation neural network conjugate gradient descent neural probabilistic neural network backpropagation neural network several statistical models significantly more ac than system logistic regression backpropagation neural network most accurate prediction models predicting five year breast cancer specific survival
paper presents program learns play final outcome games learns board evaluation functions represented artificial neural networks integrates inductive neural network learning temporal variant explanation based learning performance results illustrate strengths approach
diagnosis human disease machine fault missing data problem many variables initially unknown additional information needs obtained joint probability distribution data used solve problem model mixture models whose parameters estimated em algorithm gives benefit missing data database itself handled correctly new information refine diagnosis performed using maximum utility principle system based learning domain independent less intensive than expert systems probabilistic networks example using heart disease database presented
remote sensing applications ground truth data often used basis training pattern recognition algorithms generate maps detect objects interest practical situations experts may visually examine images provide subjective noisy estimate truth reliability bias expert non trivial problem paper discuss our recent work topic context detecting small images empirical results using expectation maximization procedure suggest accounting subjective noise quite significant terms quantifying both human algorithm detection performance
paper present connectionist system writer independent large vocabulary line cursive handwriting recognition system combines robust input representation preserves dynamic writing information neural network architecture so called multi state time delay neural network ms tdnn integrates recognition segmentation single framework our preprocessing transforms original coordinate sequence into still temporal sequence feature vectors combine strictly local features like curvature writing direction like representation coordinates proximity ms tdnn architecture well suited handling temporal sequences provided input representation our system tested both writer dependent writer independent tasks vocabulary sizes ranging up words example word vocabulary achieve word recognition rates up writer dependent writer independent without using language models
machines perform classification tasks such speech character recognition appropriately handling patterns key achieving high performance authors presents new type classification system adaptive input field neural network includes simple pre trained neural network elastic input field attached input layer using iterative method determine optimal translation elastic input field compensate original deformations convergence algorithm shown applied handwritten recognition consequently originally patterns correctly total performance improved without modifying neural network
multi class classification problems efficiently solved partitioning original problem into sub problems involving two classes each pair classes potentially small neural network trained using data these two classes show combine outputs two class neural networks order obtain posterior probabilities class decisions resulting probabilistic pairwise classifier part handwriting recognition system currently applied check reading present results real world data bases show practical point view these results compare favorably other neural network approaches
experiments performed reveal computational properties human motor memory system show humans practice reaching movements while interacting novel mechanical environment they learn internal model inverse dynamics environment subjects show recall model testing after initial practice representation internal model memory such interference attempt learn new inverse dynamics map immediately after mapping learned suggest interference same computational elements used encode first inverse dynamics map being used learn second mapping predict leads initially learned
fundamental properties both neural networks central nervous system share ability learn generalize examples while property has been studied extensively neural network literature has been explored human perceptual motor learning have chosen coordinate transformation system visuomotor map transforms visual coordinates into motor coordinates study generalization effects learning new input output pairs using paradigm computer controlled visual feedback have studied generalization visuomotor map subsequent both local context dependent local two input output pairs induced significant global yet change visuomotor map suggesting representation map composed units large functional receptive fields our study context dependent indicated single point visual space mapped two different locations depending context variable starting point movement furthermore context varied shift between two consistent two visuomotor modules being learned gated smoothly context
additive clustering treats similarity two stimuli weighted additive measure their common features inspired recent work unsupervised learning multiple cause models propose new statistically well motivated algorithm discovering structure natural stimulus classes using model substantial gains conceptual simplicity practical efficiency solution quality over earlier efforts present preliminary results artificial data two classic similarity data sets
have recently developed theory spatial representations position object encoded particular frame reference but instead involves neurons computing basis functions their sensory inputs type representation able perform nonlinear sensorimotor transformations consistent response properties parietal neurons now ask whether same theory could account behavior human patients parietal lesions these lesions induce deficit known characterized lack reaction stimuli located contralateral simulated basis function representation found three most important aspects models failed cross lines line experiments ii deficit affected multiple frames reference iii could object centered these results strongly support basis function hypothesis spatial representations provide computational theory single cell level
significant limitation neural networks representations they learn usually humans present novel algorithm extracting symbolic representations trained neural networks our algorithm uses queries induce decision tree approximates concept represented given network our experiments demonstrate able produce decision trees maintain high level fidelity their respective networks while being accurate unlike previous work area our algorithm general its applicability scales well large networks problems high dimensional input spaces
consideration attention means goal directed behavior non stationary environments argue dynamics attention should satisfy two demands long term quick transition these two characteristics within linear domain propose near bifurcation behavior sigmoidal unit self connection candidate dynamical mechanism satisfies both these demands further show simulations tasks near saddle node bifurcation behavior recurrent networks emerge functional property survival nonstationary environments
choice input representation neural network have impact its accuracy classifying novel instances neural networks typically computationally expensive train making difficult test large numbers alternative representations paper introduces fast quality measures neural network representations allowing quickly accurately estimate collection possible representations problem best show our measures ranking representations more accurate than previously published measure based experiments three difficult real world pattern recognition problems
essential feature intelligent sensory processing ability focus part signal interest against background signals able direct focus paper problem auditory scene segmentation considered model early stages process proposed behaviour model shown agreement number well known psychophysical results principal contribution model lies demonstrating might result interactions between patterns activity input signals traces previous activity feedback influence way subsequent signals processed
have analyzed relationship between correlated spike count peak cross correlation spike trains pairs simultaneously recorded neurons previous study area mt macaque monkey et al conclude common input responsible creating peaks order ten milliseconds wide spike train cross responsible creating correlation spike count observed two second time scale trial argue both common excitation inhibition may play significant roles establishing correlation
while generally neurons transmit information about their synaptic inputs through spike trains code information transmitted well understood upper bound information encoded obtained precise timing each spike information here develop general approach quantifying information carried spike trains under hypothesis apply leaky integrate fire model neuronal dynamics formulate problem terms probability distribution pt interspike intervals isis assuming spikes detected arbitrary but finite temporal resolution absence added noise variability isis could encode information information rate simply entropy isi distribution ht times spike rate ht thus provides exact expression information rate methods developed here used determine experimentally information carried spike trains even lower bound information rate provided stimulus reconstruction method tight preliminary series experiments have used these methods estimate information rates hippocampal neurons slice response current injection these experiments suggest information rates high bitsspike information rate spike trains cortical neurons use spike trains communicate other neurons output each neuron stochastic function its input other neurons interest know much each neuron other neurons about its inputs much information does spike train provide about signal consider noise nt added signal st produce total input yt st nt passed through possibly stochastic functional produce output spike train yt zt assume information contained spike train represented list spike times extra information contained properties such spike width note many characteristics spike train such mean instantaneous rate stevens zador derived representation such derivative property turns out relevant formulation specialized appropriately interested mutual information zt between input signal ensemble st output spike train ensemble zt defined terms entropy hs signal entropy hz spike train their joint entropy hs hs note mutual information symmetric joint entropy hs hz note signal st spike train zt completely independent mutual information joint entropy sum individual hs hz completely line our intuition case spike train provide information about signal information estimation through stimulus reconstruction bialek colleagues bialek et al have used reconstruction method obtain strict lower bound mutual information experimental setting method based expression mathematically equivalent eq involving conditional entropy signal given spike train upper bound conditional entropy obtained reconstruction signal entropy estimated second order statistics reconstruction error et st maximum entropy property gaussian upper bound intuitively first equation says information gained about spike train observing stimulus initial uncertainty signal absence knowledge spike train minus uncertainty remains about signal once spike train known second equation says second uncertainty greater particular estimate than optimal estimate information estimation through spike train reliability have adopted different approach based equivalent expression mutual information first term hz entropy spike train while second conditional entropy spike train given signal intuitively like inverse spike train given repeated applications same signal eq has advantage spike train deterministic function input permits exact calculation mutual information follows important difference between conditional entropy term here eq whereas has both deterministic stochastic component has stochastic component thus absence added noise discrete entropy eq reduces hz isis independent hz simply expressed terms entropy discrete isi distribution pt log pt information through spiking neuron hz number spikes here vt probability spike interval assumption finite timing precision potential information finite advantage considering isi distribution pt rather than full spike train distribution former while latter multivariate estimating former requires much less data under what conditions isis independent correlations between isis arise either through stimulus spike generation mechanism itself below guarantee correlations do arise spike generator considering integrate fire model information about previous spike next spike further limit temporally uncorrelated stimuli ie stimuli drawn white noise ensemble isis independent eq applied presence noise evaluated give conditional entropy isi given signal jl probability obtaining isi tj response particular stimulus presence noise nt conditional entropy thought reliability spike generating mechanism average trial trial variability spike train generated response repeated applications same stimulus maximum spike train entropy what follows useful compare information rate neuron limiting case exponential isi distribution has maximum entropy point process given rate provides upper bound information rate possible spike train given spike rate temporal precision let ft exponential distribution mean spike rate assuming temporal precision ht log rate ht example hz sec gives bitsspike hz spike train into msec bins possible transmit more than reduce size two fold rate increases log bitsspike while double bits get bits note different firing rate eg hz size still increases but because spike rate twice high becomes increase information rate model now consider functional describing leaky model spike generation suppose add noise nt signal st yt nt st threshold sum produce spike train zt st nt specifically suppose voltage vt neuron obeys yt membrane time constant both nt have white gaussian distributions yt has mean variance er voltage reaches threshold time neuron spike time initial condition stevens zador language neurobiology model thought limiting case neuron leaky spike generating mechanism receiving many excitatory inhibitory synaptic inputs note input yt white correlations spike train induced signal neuron after each spike correlations induced mechanism thus isis independent eq applied estimate mutual information between ensemble input signals ensemble outputs model isis independent construction need evaluate ht determine pt distribution isis conditional distribution isis ensemble signals note pt corresponds first time distribution process neuron model considering has two regimes determined relation asymptotic membrane potential absence threshold threshold threshold crossings occur even signal variance zero er subthreshold regime threshold crossings occur limit et ie mean firing rate low compared integration time constant occur subthreshold regime isi distribution exponential its coefficient variation unity cf low rate regime firing mean distinguish more usual usage neuron stochastic situation instantaneous firing rate parameter probability firing over interval depends stimulus ie cr st present case exponential isi distribution arises deterministic mechanism between these regimes threshold equal asymptotic potential have explicit exact solution entire isi distribution et al art special case absence fluctuations membrane potential subthreshold its neurophysiological interpretation excitatory inputs balance inhibitory inputs so neuron firing information rates noisy noiseless signals here compare information rate neuron balance point maximum entropy spike train simplicity consider zero noise case ie nt fig shows information per spike function firing rate calculated eq varied changing signal variance er assume spikes resolved temporal resolution msec ie isi distribution has bins msec wide dashed line shows theoretical upper bound given exponential distribution limit approached neuron operating far below threshold limit both model upper bound information per spike decreasing function spike rate model almost achieves upper bound mean isi equal membrane time constant model information saturates low firing rates but exponential distribution information increases without bound high firing rates information goes zero firing rate fast individual isis resolved temporal resolution fig lb shows information rate information per second neuron balance point goes through information through spiking neuron maximum firing rate increases maximum occurs lower firing rate than exponential distribution dashed line bounding information rates stimulus reconstruction construction eq gives exact expression information rate model therefore compare lower bound provided stimulus reconstruction method eq bialek et al assess tight lower bound provides fig shows lower bound provided reconstruction solid line reliability dashed line methods function firing rate firing rate increased increasing mean input stimulus yt noise set low firing rates two estimates nearly identical but high firing rates reconstruction method substantially information rate amount underestimate depends model parameters decreases noise added stimulus bound therefore empirical question while bialek colleagues show under conditions their experiments underestimate less than factor two clear potential underestimate under different conditions different systems greater discussion while generally spike trains encode information about neurons inputs clear information encoded idea mean firing rate alone encodes signal variability about mean effectively noise alternative view variability itself encodes signal ie information encoded precise times spikes occur view information expressed terms interspike interval isi distribution spike train encoding scheme yields much higher information rates than mean rate over interval longer than typical isi considered here have quantified information content spike trains under latter hypothesis simple neuronal model consider model construction isis independent so information rate computed directly information per spike bitsspike spike rate information per spike turn depends temporal precision spikes resolved precision infinite information content would infinite well message could example encoded expansion precise arrival time single spike reliability spike transduction mechanism entropy isi distribution itself low firing rates neuron subthreshold limit isi distribution close theoretically maximal exponential distribution much recent interest information theoretic analyses neural code work bialek colleagues bialek et al rieke et al who measured information rate sensory neurons number systems present results broad agreement those deweese who considered information rate linear filtered threshold crossing lftc model deweese developed functional expansion first term describes limit spike times isis independent second term correction correlations lftc model differs present model mainly does reset after each spike consequently natural lftc model gaussian signal noise filter times resulting waveform crosses threshold called spikes stevens zador representation spike train lftc model sequence firing times while model natural representation sequence ti isis choice two representations equivalent two models complementary lftc model results obtained colored signals noise while such conditions model model contrast class highly correlated spike trains considered lftc model isi condition required model less restrictive than independent spike condition lftc model spikes independent iff isis isi distribution pt exponential particular high firing rates isi distribution far exponential therefore spikes far independent even isis themselves independent because have assumed input st white its entropy infinite mutual information grow without bound temporal precision spikes resolved improves nevertheless spike train minute fraction total available information signal thereby saturates capacity spike train while clear whether real neurons actually behave implausible typical cortical neuron receives many synaptic inputs information rate each input same target information rate upon target fold greater synaptic could decrease substantially than its capacity preliminary series experiments have used reliability method estimate information rate hippocampal neuronal spike trains slice response current injection stevens zador under these conditions isis appear independent so method developed here applied these experiments information rates high bitsspike observed references bialek rieke de van reading neural code science deweese optimization principles neural code advances neural information processing systems vol mit press cambridge ma probability random variables stochastic processes nd hill rieke de van bialek neural coding mit press highly irregular firing cortical cells temporal integration random neuroscience moore solutions stochastic model neuronal spike production mathematical
topographic maps primary areas mammalian cerebral cortex result behavioural training nature seems consistent behaviour competitive neural networks has been demonstrated past computer simulation model training hand representation primate somatosensory cortex using neural field theory his colleagues expressions changes both receptive field size factor derived consistent owl monkey experiments make prediction goes beyond them
vestibulo ocular reflex vor images retina during rapid head motions gain vor ratio eye head rotation velocity typically around eyes focused distant target stabilize images accurately vor gain vary context eye position eye head translation first describe kinematic model vor relies solely sensory information available head rotation head translation neural eye position angle propose dynamical model compare eye velocity responses measured monkeys dynamical model observed amplitude time course modulation vor suggests way combine required neural signals within cerebellum brain makes predictions responses neurons multiple inputs head rotation translation eye position etc oculomotor system
extended version dual constraint model motor presented includes activity dependent independent competition supported wide range recent neurophysiological evidence indicates strong relationship between synaptic efficacy survival computational model justified molecular level its predictions match developmental behaviour real synapses
poisson neuron model output rate modulated poisson process miller time varying rate parameter rt instantaneous function stimulus rt poisson neuron rt gives instantaneous firing rate instantaneous probability firing output stochastic function input part because its great simplicity model widely used usually addition refractory period especially vivo single unit electrophysiological studies st usually taken value sensory stimulus integrate fire neuron model contrast output filtered thresholded function input input passed through low pass filter determined membrane time constant integrated until membrane potential vt reaches threshold point vt reset its initial value contrast poisson model integrate fire model deterministic function input although integrate fire model real neural dynamics captures many qualitative features often used starting point biophysical behavior single neurons here show slightly modified poisson model derived integrate fire model noisy inputs yt st nt modified model transfer function sigmoid whose shape determined noise variance understanding equivalence between dominant vivo simple neuron models may help links between two levels cf stevens zador
computational model song learning song learns different song song uses categorization train itself reproduce song model crucial gap computational explanation learning exploring organization perception shows competitive learning may lead organization specific nucleus brain replicates song production results previous model doya sejnowski demonstrates perceptual learning guide production through reinforcement learning
analyse geometry eye rotations particular using basic lie group theory differential geometry various rotations related through unifying mathematical treatment transformations between co systems computed using formula next describe law means lie algebra so enables us demonstrate direct connection law showing eye orientations restricted space latter equivalent sphere exactly space gaze directions our analysis provides mathematical framework studying oculomotor system could extended investigate geometry multi joint arm movements
detection essential localization external sounds requires auditory signal processing high temporal precision present integrate fire model spike processing auditory pathway barn owl shown temporal precision range achieved neuronal time constants least magnitude longer important feature our model unsupervised hebbian learning rule leads temporal fine tuning neuronal connections temporal coding range model barn owl auditory pathway
selective suppression transmission feedback synapses during learning proposed mechanism combining associative feedback self organization feedforward synapses experimental data demonstrates cholinergic suppression synaptic transmission layer feedback synapses lack suppression layer iv feedforward synapses network feature uses local rules learn mappings linearly separable during learning sensory stimuli desired response simultaneously presented input feedforward connections form self organized representations input while suppressed feedback connections feedforward connectivity during recall suppression removed sensory input self organized representation activity generates learned response
present hypothesis about cerebellum could movement presence significant feedback delays without forward model motor plant show simplified cerebellar model learn control nonlinear mass system realistic delays both afferent pathways models operation involves prediction but instead predicting sensory input directly movement fashion input patterns include delayed sensory feedback
because distance between brain their different eeg data collected point human scalp includes activity generated within large brain area spatial eeg data volume conduction does involve significant time delays suggesting independent component analysis ica algorithm bell sejnowski suitable performing blind source separation eeg data ica algorithm separates problem source identification source localization first results applying ica algorithm eeg event related potential erp data collected during sustained auditory detection task show ica training insensitive different random ica may used obvious eeg components line muscle noise eye movements other sources ica capable overlapping eeg phenomena including spatially separable erp components separate ica channels eeg behavioral state using ica via changes amount residual correlation between ica filtered output channels bell sejnowski
despite structural differences visual systems different species whether vertebrate invertebrate share certain functional properties center surround receptive field mechanism represents such example here analogous shown formed artificial neural network learns localize contours edges luminance difference furthermore input pattern corrupted background noise hidden units becomes broader decrease signal noise ratio snr same kind snr dependent plasticity present real visual neurons bipolar cells retina shown here experimentally well large cells fly eye described others analogous plasticity shown present responses these artificial biological visual systems thus spatial temporal filtering properties wide variety see world appear optimized changes space time
paper problem learning appropriate domain specific bias addressed shown achieved learning many related tasks same domain theorem given bounding number tasks learnt theorem tasks known possess common internal representation preprocessing number examples required per task good learning tasks simultaneously scales like bound minimum number examples learn single task bound number examples required learn each task independently experiment providing strong qualitative support theoretical results reported
statistical theory proposed analysis treats realizable stochastic neural networks trained loss asymptotic case shown asymptotic gain generalization error small perform early stopping even have access optimal stopping time considering cross validation stopping answer question what ratio examples should divided into training testing sets order obtain optimum performance non asymptotic region cross validated early stopping always decreases generalization error our large scale simulations done cm nice agreement our analytical findings
study characteristics learning ensembles solving exactly simple model ensemble linear students find surprisingly rich behaviour learning large ensembles use under regularized students actually over fit training data globally optimal performance obtained choosing training set sizes students appropriately smaller ensembles optimization ensemble weights yield significant improvements ensemble generalization performance individual students subject noise training process choosing students wide range regularization parameters makes improvement robust against changes unknown level noise training data
paper shows neural networks use continuous activation functions have vc dimension least large square number weights result long standing open question namely whether well known ow log bound known hard threshold nets held more general sigmoidal nets implications number samples needed valid generalization discussed
recurrent perceptron classifiers generalize classical perceptron model they take into account those correlations among input coordinates arise linear digital filtering paper provides tight bounds sample complexity associated fitting such models experimental data
has unknown whether principle carry out reliable digital computations networks biologically realistic models neurons article presents rigorous constructions simulating real time arbitrary given boolean circuits finite automata arbitrarily high reliability networks noisy spiking neurons addition show help inhibition even networks unreliable spiking neurons simulate real time neuron threshold gate therefore multilayer perceptron threshold circuit reliable manner these constructions provide possible explanation fact biological neural systems carry out quite complex computations msec turns out assumption these constructions require about shape behaviour noise surprisingly weak
paper examine perceptron learning task task realizable provided another perceptron identical architecture both perceptrons have nonlinear sigmoid output functions gain output function determines level nonlinearity learning task observed high level nonlinearity leads overfitting give explanation rather surprising observation develop method avoid overfitting method has two possible interpretations learning noise other cross validated early stopping learning rules examples property makes feedforward neural nets interesting many practical applications their ability approximate functions given examples feed forward networks least hidden layer nonlinear units able approximate each continuous function dimensional hypercube arbitrarily well while existence neural function approximators already established still lack knowledge about their practical major problems good realization like overfitting need better understanding work study overfitting layer perceptron model model allows good theoretical description while exhibits already qualitatively similar behavior multilayer perceptron layer perceptron has input units output unit between input output has layer adjustable weights wi output possibly nonlinear function weighted sum inputs xi ie realizable learning task exhibits overfitting quality function approximation measured difference between correct output nets output averaged over possible inputs supervised learning scheme trains network using set examples correct output known learning task minimize certain cost function measures difference between correct output nets output averaged over examples using mean squared error suitable measure difference between outputs define training error et generalization error eg ec development both errors function number trained examples given learning curves training done gradient theoretical purposes useful study learning tasks provided second network so called teacher network concept allows more transparent definition difficulty learning task monitoring training process becomes always possible compare student network teacher network directly suitable quantities such comparison perceptron case following order parameters wi wi wi both have transparent interpretation normalized overlap between weight vectors teacher student norm students weight vector these order parameters used multilayer learning but their number increases number possible permutations between hidden units teacher student learning task here concentrate case student perceptton has learn mapping provided another perceptton choose identical networks teacher student both have same sigmoid output function ie gh gh identical network architectures teacher student realizable tasks principle student able learn task provided teacher exactly tasks learnt exactly remains always finite error use distributed random inputs weights weighted sum assumed gaussian distributed express generalization error order parameters tanh gaussian measure oo vr exp equation see student learns gain output function adjusts norm its weights gain plays important role allows tune function between linear function highly nonlinear function now want determine learning curves task emergence overfitting explicit expression weights below storage capacity perceptton ie minimum training error et zero zero training error implies every example has been learnt exactly thus weights minimal norm fulfill condition given see et al note weights completely independent output function gh gh they same simplest realizable case linear perceptron learns linear perceptron statistical mechanics calculation order parameters done method statistical mechanics applies commonly used replica method details about replica approach see et al solution continuous perceptron problem found bss et al results statistical mechanics calculations exact thermodynamic limit ie oo variable more natural measure defined fraction number patterns over system size ie thermodynamic limit infinite but still finite normally reasonable system sizes such already well described theory usually concentrates zero temperature limit because implies training error et accepts its absolute minimum every number presented examples corresponding order parameters case linear perceptron learns linear student zero temperature limit called exhaustive training student net trained until absolute minimum et reached small high gains ie levels nonlinearity exhaustive training leads overfitting means generalization error should decreasing reason overfitting training follows strongly examples critical gain determines whether generalization error increasing decreasing function small values determined linear approximation small both order parameters small students tanh function approximated linear function simplifies equation following expression function has upper bound ie vr critical gain reached numerical solution gives higher slope positive small following considerations use always gain example intermediate level nonlinearity realizable learning task exhibits overfitting figure learning curves problem tanh perceptron learns different values gain even realizable case exhaustive training lead overfitting gain high enough understand emergence overfitting here evaluation generalization error dependence order parameters helpful fig shows function between between exhaustive training realizable cases follows always line independent actual output function means training guided training error generalization error gain higher than line eg starts lower slope than results overfitting avoid overfitting fig guess already increases fast compared ratio between better during training process so have develop description training process first training process found already order parameters finite temperatures statistical mechanics approach good description training process learning task bss so use finite temperature order parameters task these again taken task linear perceptton learns linear perceptton aa temperature dependent variable local min local figure contour plot defined generalization error function two order parameters starting minimum eg contour lines eg given dotted lines dashed line corresponds eg solid lines parametric curves order parameters certain training strategies straight line illustrates exhaustive training lower ones optimal training explained fig here gain zero temperature limit corresponds show now decrease temperature dependent parameter oo describes evolution order parameters during training process training process natural parameter number parallel training steps each parallel training step patterns presented once weights updated fig shows evolution order parameters parametric curves exhaustive learning curve defined parameter solid line each training ends curve dotted lines illustrate training process runs infinity simulations training process have shown theoretical curve good description least after training steps now use description training process definition optimized training strategy optimal temperature optimized training strategy chooses corresponding temperature but value ie temperature minimizes generalization error lower solid curve indicating parametric curve value chosen every minimizes function has two minima between solid line indicates always absolute minimum parametric curves corresponding local minima given double dashed dash dotted lines note optimized value always related optimized temperature through equation but parameter related number training steps realizable learning task exhibits overfitting local min min local min simulation li figure training process order parameters parametric curves parameters straight solid line corresponds exhaustive learning ie dotted lines describe training process fixed iterative training reduces parameter examples given lower solid line optimized learning curve achieve curve value chosen minimizes eg between error ec has two minima double dashed dash dotted lines indicate second local minimum ec compare fig see absolute local minimum naive early stopping procedure ends always minimum smaller first minimum during training process see simulation indicated early stopping fig fig together indicate earlier stopping training process avoid overfitting but order determine stopping point has know actual generalization error during training cross validation tries provide approximation real generalization error cross validation error defined like et see set examples used during training here calculate optimum using real generalization error given determine optimal point early stopping lower bound training finite cross validation sets preliminary tests have shown already small cross validation sets approximate real quite well training stopped eg increases resulting curve given fig indicate standard deviation simulation averaged over trials fig same results shown learning curves see clearly early stopping strategy avoids overfitting summary paper have shown overfitting emerge realizable learning tasks calculation critical gain contour lines fig imply eg local min min local min simulation figure learning curves corresponding parametric curves fig upper solid line shows again exhaustive training optimized finite temperature curve lower solid line exhaustive optimal training lead identical results see simulation early stopping finds first minimum ec reason overfitting nonlinearity problem network adjusts slowly nonlinearity task have developed method avoid overfitting interpreted two ways training finite temperature reduces overfitting realized trains noisy examples other interpretation learns without noise but training earlier early stopping guided cross validation observed early stopping completely simple lead local minimum generalization error should aware possibility before applies early stopping multilayer perceptrons built nonlinear perceptrons same effects important multilayer learning study large scale simulations et al has shown overfitting occurs realizable multilayer learning tasks acknowledgments would like thank discussions hints concerning presentation references bss avoiding overfitting finite temperature learning crossvalidation international conference artificial neural networks vol bss generalization ability percepttons continuous outputs
stability criterion dynamic parameter adaptation given case learning rate backpropagation class stable algorithms presented studied including convergence proof
new nearest neighbor method described estimating bayes risk multiclass pattern classification problem sample data eg classified training set although assumed classification problem accurately described sufficiently smooth class conditional distributions neither these distributions nor corresponding prior probabilities classes required thus method applied practical problems underlying probabilities known method illustrated using two different pattern recognition problems
paper recursive estimation algorithms dynamic modular networks developed models based gaussian rbf networks gating network considered two stages first simply time varying scalar second based state mixture local experts scheme resulting algorithm uses kalman filter estimation model estimation gating probability estimation both hard soft competition based estimation schemes developed former most probable network adapted latter networks adapted appropriate weighting data
linear threshold elements basic building blocks artificial neural networks linear threshold element computes function sign weighted sum input variables weights arbitrary integers actually they big integers exponential number input variables practice difficult implement big weights present literature distinction made between two extreme cases linear threshold functions polynomial size weights opposed those exponential size weights main contribution paper up gap further separation namely prove class linear threshold functions polynomial size weights divided into according degree polynomial fact prove more general result exists minimal weight linear threshold function arbitrary number inputs weight size prove those results have developed novel technique constructing linear threshold functions minimal weights
describe use modern analytical techniques solving dynamics symmetric recurrent neural networks near saturation these explicitly take into account correlations between post synaptic potentials thereby allow reliable prediction transients
fourier transform boolean functions has come play important role proving many important learnability results aim demonstrate fourier transform techniques useful practical algorithm addition being powerful theoretical tool describe more changes have introduced algorithm ones crucial without performance algorithm would benefits present confidence level each prediction measures likelihood prediction correct
propose way using boolean circuits perform real valued computation way naturally extends their boolean multiple fan threshold gates model shown mimic hardware implementation continuous neural networks vapnik chervonenkis dimension sample size analysis systems performed giving best known sample sizes real valued neural network experimental results confirm conclusion sample sizes required networks significantly smaller than sigmoidal networks
process machine learning considered two stages model selection parameter estimation paper technique presented constructing dynamical systems desired qualitative properties approach based fact rt dimensional nonlinear dynamical system decomposed into gradient rt systems thus model selection stage consists choosing gradient portions appropriately so certain behavior estimate parameters convergent learning rule presented algorithm has been proven converge desired system trajectory initial conditions system inputs technique used design neural network models guaranteed solve trajectory learning problem
recent experiments show neural codes work wide range share common features first these observations seem show these features arise naturally linear filtered threshold crossing lftc model set threshold maximize transmitted information maximization process requires neural adaptation signal level conventional light dark adaptation but statistical structure signal noise distributions present new approach calculating mutual information between neurons output spike train aspect its input signal does require reconstruction input signal formulation valid provided correlations spike train small provide procedure assumption paper based joint work deweese preliminary results lftc model previous proceedings deweese conclusions reached time have been further analysis model
present statistical method exactly learns class constant depth perceptron networks weights taken arbitrary thresholds distribution generates input examples member family product distributions these networks known perceptron networks read once formulas over weighted threshold basis loop free neural nets each node has arbitrary high probability learner able exactly identify connectivity target perceptron network using new statistical test exploits strong property sums independent random variables
propose active learning method hidden unit reduction devised specially multilayer perceptrons mlp first review our active learning method point out many fisher information based methods applied mlp have critical problem information matrix may singular solve problem derive singularity condition information fix propose active learning technique applicable mlp its effectiveness verified through experiments
analyze compare well known gradient descent algorithm new algorithm called exponentiated gradient algorithm training single neuron arbitrary transfer function both algorithms easily generalized larger neural networks generalization gradient descent standard back propagation algorithm paper prove loss bounds both algorithms single neuron case local minima make difficult prove worst case bounds gradient based algorithms use loss function formation spurious local minima define such matching loss function strictly increasing differentiable transfer function prove worst case loss bound such transfer function its corresponding matching loss example matching loss identity function square loss matching loss logistic sigmoid entropic loss different structure bounds two algorithms indicates new algorithm out performs gradient descent inputs contain large number irrelevant components dp kivinen warmuth
show single neuron logistic function transfer function number local minima error function based square loss grow exponentially dimension
adaptive back propagation algorithm studied compared gradient descent standard back propagation line learning two layer neural networks arbitrary number hidden units within statistical mechanics framework both numerical studies rigorous analysis show adaptive back propagation method results faster training breaking symmetry between hidden units more efficiently providing faster convergence optimal generalization than gradient descent
topographic mappings occur frequently brain popular approach understanding structure such mappings map points representing input features space few dimensions points dimensional space using selforganizing algorithm argue more general approach may useful similarities between features constrained geometric distances objective function topographic matching chosen rather than being specified implicitly self organizing algorithm investigate analytically example more general approach applied structure mappings such pattern ocular dominance columns primary visual cortex
dynamics complex neural networks modelling process cortical maps include aspects long short term memory behaviour network such characterized equation neural activity fast phenomenon equation synaptic modification slow part neural system present quadratic type lyapunov function flow competitive neural system fast slow dynamic variables show consequences stability analysis neural net parameters
examine issue evaluation model specific parameters modified vc formalism two examples analyzed dimensional homogeneous perceptron dimensional higher order neuron both models solved theoretically their leaming curves compared against true learning curves shown formalism has potential generate variety leaming curves including ones phase transitions
present bayesian framework inferring parameters mixture experts model based ensemble learning variational free energy bayesian approach avoids over fitting noise level under estimation problems traditional maximum likelihood inference demonstrate these methods artificial problems time series prediction
paper consider probabilities different convergent algorithm hopfield type neural network treating case unbiased random patterns show failed results total memory
theory early stopping applied linear models presented backpropagation learning algorithm modeled gradient descent continuous time given training set validation set weight vectors found early stopping lie certain surface usually given training set candidate early stopping weight vector validation sets have least squares weights certain plane latter fact exploited estimate probability stopping given point along trajectory initial weight vector weights derived training set estimate probability training goes extending theory nonlinear models discussed
given recurrent neural network discrete time model may have asymptotic dynamics different related continuous time model paper consider discrete time model continuous time leaky model study its parallel sequential dynamics symmetric networks provide sufficient necessary many cases conditions model have same cycle free dynamics corresponding continuous time model symmetric networks
introduce analyze mixture model supervised learning probabilistic devise online learning algorithm efficiently infers structure estimates parameters each model mixture theoretical analysis comparative simulations indicate learning algorithm tracks best model arbitrarily large possibly infinite pool models present application model inducing noun phrase recognizer
paper introduce approach training estimation posterior probabilities using recursive algorithm reminiscent em based forward backward algorithm estimation sequence likelihoods although general method developed context statistical model transition based speech recognition using artificial neural networks ann generate probabilities hidden markov models hmms new approach use local conditional posterior probabilities transitions estimate global posterior probabilities word sequences although still use anns estimate posterior probabilities network trained targets themselves estimates local posterior probabilities initial experimental result shows significant decrease error rate comparison baseline system
paper propose recurrent networks feedback into input units handling two types data analysis problems hand scheme used static data input variables missing other hand used sequential data input variables missing available different frequencies unlike case probabilistic models eg gaussian missing variables network does attempt model distribution missing variables given observed variables instead more discriminant approach missing variables purpose minimizing learning criterion eg minimize output error
family discovery task learning dimension structure parameterized family stochastic models especially appropriate training examples partitioned into episodes samples drawn single parameter value present three family discovery algorithms based surface learning show they significantly improve performance over two alternatives parameterized classification task
nearest neighbor classification class conditional probabilities locally constant suffers bias high dimensions propose locally adaptive form nearest neighbor classification try curse dimensionality use local linear discriminant analysis estimate effective metric computing neighborhoods determine local decision boundaries centroid information neighborhoods directions orthogonal these local decision boundaries them parallel boundaries neighborhood based classifier employed using modified neighborhoods propose method global dimension reduction combines local dimension information indicate these techniques extended regression problem
propose new learning method generalized learning vector quantization reference vectors updated based steepest descent method order minimize cost function cost function so obtained learning rule satisfies convergence condition prove kohonens rule used lvq does satisfy convergence condition thus degrades recognition ability experimental results printed character recognition reveal superior lvq recognition ability
investigate effectiveness stochastic baseline evaluating performance genetic algorithms gas combinatorial function particular address two problems gas have been applied literature problem problem demonstrate simple stochastic methods able achieve results comparable superior those obtained gas designed address these two problems further illustrate case problem insights obtained formulation stochastic algorithm lead improvements encoding used ga
statistically independent features extracted finding factorial representation signal distribution principal component analysis pca linear correlated gaussian distributed signals independent component analysis ica extracts features case linear statistical dependent but necessarily gaussian distributed signals nonlinear component analysis finally should find factorial representation nonlinear statistical dependent distributed signals paper proposes task novel feed forward information nonlinear map explicit transformations solves problem non gaussian output distributions considering single coordinate higher order statistics
bayesian kullback learning scheme called machine proposed based two complement but equivalent bayesian representations joint density their kullback divergence scheme existing major supervised unsupervised including classical maximum likelihood least square learning maximum information preservation em em algorithm information geometry recent popular helmholtz machine well other learning methods new variants new results but scheme provides number new learning models
natural artificial neural circuits capable specific state space trajectories natural approach problem learn relevant trajectories examples unfortunately gradient descent learning complex trajectories networks suggest possible approach trajectories realized combining simple oscillators various modular ways contrast two regimes fast slow oscillations cases show banks oscillators bounded frequencies have universal approximation properties open questions discussed briefly
derive smoothing regularizer recurrent network models requiring robustness prediction performance perturbations training data regularizer viewed generalization first order dynamic models closed form expression regularizer covers both time simultaneous recurrent nets feedforward nets linear nets special cases have successfully tested regularizer number case studies found performs better than standard quadratic weight decay
currently considerable interest developing general nonlinear density models based latent hidden variables such models have ability discover presence relatively small number underlying causes acting combination give rise apparent complexity observed data set unfortunately train such models generally requires large computational effort paper introduce novel latent variable algorithm retains general non linear capabilities previous models but uses training procedure based em algorithm demonstrate performance model toy problem data flow multi phase
present framework learning hidden markov models distributed state representations within framework derive learning algorithm based expectation maximization em procedure maximum likelihood estimation analogous standard baum welch update rules step our algorithm exact solved analytically due combinatorial nature hidden state representation exact step intractable simple tractable mean field approximation derived empirical results set problems suggest both mean field approximation gibbs sampling viable alternatives computationally expensive exact algorithm
new boosting algorithm schapire used improve performance decision trees constructed using information ratio criterion algorithm boosting algorithm iteratively constructs series decision trees each decision tree being trained pruned examples have been filtered previously trained trees examples have been classified previous trees ensemble higher probability give new probability distribution next tree ensemble train results optical character recognition ocr knowledge discovery data mining problems show comparison single trees trees trained independently trees trained subsets feature space boosting ensemble much better
develop refined mean field approximation inference learning probabilistic neural networks our mean field theory unlike most does assume units behave independent degrees freedom instead exploits principled way existence large substructures computationally tractable illustrate advantages framework show incorporate weak higher order interactions into first order hidden markov model treating corrections but first order structure within mean field theory
have already shown extracting long term dependencies sequential data difficult both deterministic dynamical systems such recurrent networks probabilistic models such hidden markov models hmms inputoutput hidden markov models practice avoid problem researchers have used domain specific priori knowledge give meaning hidden state variables representing past context paper propose use more general type priori knowledge namely temporal dependencies structured implies long term dependencies represented variables long time scale principle applied recurrent network includes delays multiple time scales experiments confirm advantages such structures similar approach proposed hmms
study bayesian networks continuous variables using nonlinear conditional density estimators demonstrate useful structures extracted data set self organized way present sampling techniques belief update based markov conditional density models
conventional binary classification trees such cart either split data using axis aligned hyperplanes they perform computationally expensive search continuous space hyperplanes unrestricted orientations show limitations former overcome without latter every pair training data points hyperplane orthogonal line joining data points line such hyperplanes plausible candidates splits comparison datasets found method generating candidate splits outperformed standard methods particularly training sets small
bayesian analysis neural networks difficult because simple prior over weights implies complex prior distribution over functions paper investigate use gaussian process priors over functions permit predictive bayesian analysis fixed values hyperparameters carried out exactly using matrix operations two methods using optimization averaging via hybrid monte carlo over hyperparameters have been tested number challenging problems have produced excellent results
sigmoid type belief networks class probabilistic neural networks provide natural framework representing probabilistic information variety unsupervised supervised learning problems often parameters used these networks need learned examples unfortunately estimating parameters via exact probabilistic calculations ie em algorithm intractable even networks fairly small numbers hidden units propose avoid step bounding likelihoods instead computing them exactly introduce extended complementary representations these networks show estimation network parameters made fast reduced quadratic optimization performing estimation either alternative domains complementary networks used continuous density estimation well
neural network ensembles have been shown accurate classification techniques previous work has shown effective ensemble should consist networks highly correct but ones make their errors different parts input space well most existing techniques indirectly address problem creating such set networks paper present technique called uses genetic algorithms directly search accurate diverse set trained networks works first creating initial population uses genetic operators create new networks keeping set networks accurate possible while each other much possible experiments three dna problems show able generate set trained networks more accurate than several existing approaches experiments show able effectively incorporate prior knowledge available improve quality its ensemble
compare two regularization methods used improve generalization capabilities gaussian mixture density estimates first method uses bayesian prior parameter space derive em expectation maximization update rules maximize posterior parameter probability second approach apply ensemble averaging density estimation includes bagging recently has been found produce impressive results classification networks
following study growth logical function complexity network two overlapping waves pruning other hebbian reinforcement connections results indicate significant spatial gradient appearance both linearly separable non linearly separable functions two inputs network cells much their slope appearance sensitive parameters highly non linear way
recently several researchers have reported encouraging experimental results using gaussian like activation functions multilayer percepttons networks type usually require fewer hidden layers units often learn much faster than typical sigmoidal networks explain these results consider hyper ridge network simple perceptron hidden units activation function interested points dimensions into two classes limit approaches infinity capacity hyper ridge perceptton identical show usual case practice ratio hyper ridge perceptron approaches
backpropagation learning algorithms typically networks structure into single vector weight parameters optimized suggest their performance may improved utilizing structural information instead introduce framework each weight accordingly model activation error signals treated approximately independent random variables characteristic scale weight changes matched residuals allowing structural properties such nodes fan fan out affect local learning rate backpropagated error model permits calculation upper bound global learning rate batch updates turn leads different update rules bias rs non bias weights approach yields performance family relations benchmark multi layer network both batch learning momentum delta delta algorithm convergence optimal learning rate up more than order magnitude
propose hierarchical scheme rapid learning context dependent skills based recently introduced parameterized selforganizing map underlying idea first learning effort system into rapid learner more restricted range contexts carried out prior learning stage during system set basis mappings skills set prototypical contexts adaptation new context achieved interpolating space basis mappings thus extremely rapid demonstrate potential approach task visuomotor map robot two includes forward backward robot kinematics end coordinates retina coordinates joint angles after phase transformation learned new camera set up single observation
has recently been shown gradient descent learning algorithms recurrent neural networks perform poorly tasks involve long term dependencies paper explore problem class architectures called networks have powerful representational capabilities previous work reported gradient descent learning more effective networks than recurrent networks hidden states show although networks do problem long term dependencies they greatly improve performance such problems present experimental results show networks often information two three times long conventional recurrent networks
present two hierarchical mixture experts hme architecture applying likelihood splitting criteria each expert hme grow tree adaptively during training secondly considering most probable path through tree may prune branches away either they become redundant demonstrate results growing path pruning algorithms show significant speed more efficient use parameters over standard fixed structure discriminating between two classifying bit parity patterns
new learning algorithm developed design statistical classifiers minimizing rate misclassification method based ideas information theory statistical physics assigns data classes probability distributions chosen minimize expected classification error while simultaneously enforcing classifiers structure level randomness measured entropy classifier structure quantified associated cost constrained optimization problem equivalent minimization helmholtz free energy resulting optimization method basic extension deterministic annealing algorithm explicitly structural constraints assignments while reducing entropy expected cost temperature limit low temperature error rate minimized directly hard classifier structure obtained learning algorithm used design variety classifier structures approach compared standard methods radial basis function design demonstrated substantially outperform other design methods several benchmark examples while often retaining design complexity comparable greater than strict descent based methods miller rao
practical method bayesian training feed forward neural networks using sophisticated monte carlo methods presented evaluated reasonably small amounts computer time approach outperforms other state art methods tasks real world domains
introduce constructive incremental learning system regression problems models data means locally linear experts contrast other approaches experts trained independently do data during learning prediction query required do experts their individual predictions each expert trained minimizing penalized local cross validation error using second order methods way expert able find local distance metric adjusting size shape receptive field its predictions valid detect relevant input features adjusting its bias importance individual input dimensions derive asymptotic results our method variety simulations properties algorithm demonstrated respect interference learning speed prediction accuracy feature detection task oriented incremental learning
paper relates computational power recurrent cascade correlation rcc architecture finite state automata fsa while recurrent networks fsa equivalent rcc paper presents theoretical analysis rcc architecture form proof describing large class fsa cannot realized rcc
report our development high performance system neural network other signal processing applications have designed implemented vector attached processor conventional present performance comparisons commercial neural network backpropagation training ii system demonstrates significant over extensively code running
new technique termed softassign applied first time two classic combinatorial optimization problems traveling salesman problem graph partitioning softassign has emerged recurrent neural physics framework two way assignment constraints without use penalty terms energy functions softassign generalized two way winner take constraints multiple membership constraints required graph partitioning softassign technique compared softmax glass within statistical physics framework softmax penalty term has been widely used method enforcing two way constraints common within many combinatorial optimization problems benchmarks present evidence softassign has clear advantages accuracy speed algorithmic simplicity over softmax penalty term optimization problems two way constraints
investigate optimization neural networks governed general objective functions practical formulations such notoriously difficult solve common problem poor local result applied methods paper novel framework introduced solution largescale optimization problems assumes little about objective function applied general nonlinear non convex functions thousand variables thus efficiently minimized combination techniques deterministic annealing multiscale optimization attention mechanisms region optimization methods
paper investigates learning context learning addresses situations learner faces whole stream learning tasks such scenarios provide transfer knowledge across multiple learning tasks order generalize more accurately less training data paper several different approaches learning described applied object recognition domain shown across board learning approaches generalize consistently more accurately less training data their ability transfer knowledge across learning tasks
many classification problems have property costly part obtaining examples class label paper suggests simple method using distribution information contained unlabeled examples labeled examples supervised training framework empirical tests show technique described paper significantly improve accuracy supervised learner learner well below its asymptotic accuracy level
introduce new algorithm designed learn sparse percepttons over input representations include high order features our algorithm based hypothesis boosting method able pac learn relatively natural class target concepts moreover algorithm appears work well practice set three problem domains algorithm produces classifiers utilize small numbers features yet exhibit good generalization performance perhaps most importantly our algorithm generates concept descriptions easy humans understand
wake sleep algorithm hinton dayan frey relatively efficient method fitting multilayer stochastic generative model high dimensional data addition top down connections generafive model makes use bottom up connections approximating probability distribution over hidden units given data trains these bottom up connections using simple delta rule use variety synthetic real data sets compare performance wake sleep algorithm monte carlo mean field methods fitting same generative model compare other models less powerful but easier fit
analog electronic cochlear models need exponentially scaled filters cmos compatible lateral bipolar transistors create exponentially scaled currents biased using resistive line voltage difference between both ends line these independent cmos threshold voltage current sources implemented much better matched than current sources created mos transistors weak inversion measurements integrated test chips shown verify improved matching
both vertebrate invertebrate retinas highly extracting contrast independent background intensity over five more decades has been possible adaptation operating point background intensity while maintaining high gain transient responses properties retina allows system extract information edges image silicon retina models adaptation properties receptors properties cells invertebrate retina outer layer vertebrate retina illustrate spatio temporal responses silicon retina moving bars chip has pixels die fabricated rn well technology
unique architecture winner search hardware has been developed using novel neuron like high functionality device called neuron mos transistor vmos short key circuit element circuits developed work find location maximum minimum signal among number input data continuous time basis thus enabling real time winner tracking well fully parallel sorting multiple input data have developed two circuit schemes ensemble selecting vmos ring oscillators finding winner oscillating node other ensemble variable threshold receiving common voltage competitive excitation data sorting conducted through consecutive winner search actions test circuits fabricated double cmos process their operation has been experimentally verified
present integrated analog processor real time wavelet decomposition reconstruction continuous temporal signals covering audio frequency range processor performs complex harmonic modulation gaussian filtering parallel channels each clocked different rate producing multiresolution mapping logarithmic frequency scale our implementation uses mixed mode analog digital circuits techniques filters achieve wide linear dynamic range while maintaining compact circuit size low power consumption include experimental results processor characterize its components separately measurements single channel test chip
developing special purpose low power analog digital speech music applications feature analog circuit models biological process audio signal before conversion paper describes our most recent design working system uses several chip compute multiple representations sound analog input multi representation system demonstrates plausibility implementing auditory scene analysis approach sound processing
dimensional model primate smooth pursuit mechanism has been implemented tm cmos vlsi model negative feedback model positive feedback scheme produce smooth pursuit system velocity target retina furthermore system uses current eye motion predictor future target motion analysis stability biological correspondence system discussed implementation focal plane local correlation based visual motion detection technique used velocity measurements ranging over orders magnitude variation provides input smooth pursuit system system performed successful velocity tracking high contrast scenes circuit design performance complete smooth pursuit system presented
systems process sensory data frequently model matching stage class hypotheses combined recognize complex entity introduce new model single function multiple data model appropriate stage functionality added small hardware expense certain existing simd architectures incremental addition programming model adding simd machine allow faster model matching but increase its flexibility general purpose machine its performing initial stages sensory processing
describe two parallel analog vlsi architectures integrate optical flow data obtained arrays elementary velocity sensors estimate direction time contact direction computation performed simulations evaluate most important qualitative properties optical flow field determine best functional operators implementation architecture time contact exploited divergence theorem integrate data velocity sensors present architecture average out possible errors
technique segmenting sounds using processing based mammalian early auditory processing presented technique based features sound neuron spike recording suggests detected cochlear nucleus sound signal each signal processed enhance onsets offsets onset offset signals compressed clustered both time across frequency channels using network fire neurons onsets offsets spikes timing these spikes used segment sound background traditional speech interpretation techniques based fourier transforms spectrum hidden markov model neural network interpretation stage have limitations both continuous speech interpreting speech presence noise has led interest front ends modelling biological auditory systems speech interpretation systems meyer cosi et al auditory modelling systems use similar early auditory processing used biological systems mammalian auditory processing uses two incoming signal filtered first external car before causes membrane vibration passed through middle ear window cochlea inside cochlea wave causes pattern vibration occur membrane appears active process using both inner outer hair cells movement detected inner hair cells turned into neural neurons spiral ganglion these pass down auditory nerve arrive various parts cochlear nucleus nerve fibres other areas lateral medial nuclei superior ls smith inferior colliculus example see pickles modern sound speech interpretation systems use form bandpass filtering following biology far cochlea most use fourier transforms perform calculation energy each band over time period usually between ms what cochlea does auditory modelling front ends differ extent length they follow animal early auditory processing but term generally implies least wideband filters used high temporal resolution initial stages means use filtering techniques rather than fourier transforms bandpass stage such filtering systems have been implemented patterson holdsworth patterson holdsworth placed directly silicon lazzaro lazzaro et al liu et al van auditory models have moved beyond cochlear filtering inner hair cell has been modelled either simple smith has been based work example patterson holdsworth cosi lazzaro has silicon version processing lazzaro others such ct al et al meyer smith have considered early brainstem nuclei their possible contribution based neurophysiology different cell types pickles et al auditory model based systems have yet find their way into speech recognition systems cosi work presented here uses auditory modelling up onset cells cochlear nucleus adds temporal neural network clean up segmentation produced part has been smith though system has biological plausibility aim effective data driven segmentation technique implement able silicon techniques used sound applied auditory front end patterson holdsworth sound into channels each bandwidth hz centre frequency khz band moore these modelling effect inner hair cells signals produced auditory nerve real system has far more channels each nerve channel carries spike coded information coding here models signal population neighboring auditory nerve fibres onset offset filter signal present auditory nerve stronger near onset tone than later pickles effect much more certain cell types cochlear nucleus these fire strongly after onset sound band they sensitive emphasis onsets modelled signal each band filter computes two averages more recent less recent less recent more recent biologically possible justification consider neuron receiving same driving input twice other excitatory input has shorter time constant than inhibitory input both exponentially weighted averages averages formed using filter have been smith but former place much emphasis most recent part signal making latter more effective onset based sound segmentation filter output input signal ot ft determine rise fall times pulses system sensitive used so sd gaussians ms ms filter has positive peak crosses ms negative these values system sensitive energy falls occur sounds positive onset offset signal implies signal increasing intensity negative onset offset signal implies decreasing intensity convolution used sound analog difference gaussians operator used extract edges images marr hildreth smith performed segmentation directly signal onset offset signal onset offset signal divided into two positive going signals onset signal consisting positive going part offset signal consisting negative going part both compressed taken increases dynamical system models biological effects compressed onset signal models output population onset cells technique producing onset signal related et al cosi integrate fire neural network segment sound using onset offset signals they need integrated across frequency bands across time temporal clustering achieved using network integrate fire units integrate fire unit its weighted input over time activity mit initially according da input neuron dissipation describes integration reaches threshold unit fires ie pulse reset after firing period input called refractory period such neurons discussed eg integrate fire neuron used per neuron received input either single set adjacent channels equal positive weighting output each neuron fed back set adjacent neurons again fixed positive weight time step here ms later because leaky nature accumulation activity excitatory input neuron arriving its activation threshold has effect next firing time than excitatory input arriving activation lower thus similar input applied set neurons adjacent channels effect inter neuron connections first fires its neighbors fire almost immediately allows network such neurons cluster onset offset signals producing sharp burst spikes across number channels providing onsets offsets external internal weights network adjusted so onset offset input alone allowed neurons fire while internal input alone enough ls smith cause firing refractory period used set ms onset system ms offset system onset system effect produce sharp onset firing responses across adjacent channels response increase energy channels thus grouping onsets both temporally appropriate onsets these generally brief clearly marked output stage call onset map offsets tend more due physical effects example sound start element starts move but die away slowly vibration see discussion even vibration does stop sound die away more slowly due thus cannot reliably mark offset sound instead reduce refractory period offset neurons produce train pulses duration offset call output stage offset map results technique entirely data driven applied sound source has been applied both speech musical sounds figure shows effect applying techniques discussed short piece speech fig shows neural network integrates onset across channels allowing these onsets used segmentation simplest technique up continuous speech each onset ensure onset single channel does system onsets occur near each other do result short segments segmentation boundary have least onsets inside period ms minimum segment length set ms utterance neural information processing systems has phonetic representation segmented into following segments la arm el pro am same text spoken more slowly over rather than has phonetic representation segmenting using technique gives following segments el pr ro uj st am although phonemes broken between segments system provides effective segmentation relatively insensitive speech rate system effective finding speech inside certain types noise such motor noise seen fig le system has been used segment sound single musical instruments these have clear breaks between straightforward smith correct segmentation achieved directly onset offset signal but achieved sounds change smoothly visible figure onsets here clear using network segmentation produced based sound segmentation figure offset maps author neural information processing systems rapidly envelope original sound onset map channels hz khz onset filter parameters text neuron per channel interconnection neuron refractory period ms but network has input applied adjacent channels internal feedback channels offset map produced similarly refractory period ms envelope say nice noise background lines mark utterance onset offset maps perfect best results obtained here input network spread across channels conclusions further work effective data driven segmentation technique based onset feature detection using integrate fire neurons has been demonstrated relatively broadband noise segmentation end itself effectiveness technique depend application ls smith figure sound vertical lines showing boundary between onsets found using single neuron per channel interconnection but internal feedback each channel adjacent channels offsets found refractory period ms segmentation currently information bands onsets propose extend work combining segmentation described here work bands sharing same frequency amplitude modulation aim extract sound segments subset bands allowing segmentation run concurrently acknowledgements many due members centre cognitive computational university references meyer speech analysis means physiologically based model cochlear nerve nucleus visual representations speech signals eds modelling intermediate auditory system applied biology cc classification unit types cochlear nucleus pst histograms regularity analysis neurophysiology onset based sound segmentation meyer model processing auditory nerve cochlear nucleus proceedings computational auditory scene analysis cs department computing science university et al challenge spoken language systems research directions ieee trans speech audio processing cosi use auditory models speech technology intelligent perceptual models van linear predictive coding speech signal using analog cochlear model internal report center neuro systems what world do approach auditory event perception psychology do so responses neurons auditory nerve fibres cats am pure tones analysis spectrum research lazzaro silicon modelling pitch perception sciences usa lazzaro silicon auditory processors computer ieee trans neural networks may theory pitch perception liu ag analog cochlear model multiresolution speech analysis advances neural information processing systems jd lee eds morgan kaufmann marr hildreth theory edge detection proc society simulation auditory neural transduction further studies soc am moore suggested formulae calculating auditory filter excitation patterns soc re synchronization pulse coupled biological oscillators patterson holdsworth
application interconnected self organizing maps handwritten digit recognition presented lateral connections learn correlations activity between units map resulting excitatory connections focus activity into local patches inhibitory connections redundant activity map map thus forms internal representations easy recognize eg perceptron network recognition rate subset nist database higher than regular self organizing map som front end higher than recognition raw input directly these results form promising starting point building pattern recognition systems map front end
paper describes training recurrent neural network letter posterior probability estimator hidden markov model off line handwriting recognition system network estimates posterior distributions each series frames representing sections handwritten word supervised training algorithm backpropagation through time requires target outputs provided each frame three methods deriving these targets presented novel method based upon algorithm found result recognizer lowest error rate
method incorporating context dependent phone classes connectionist hmm hybrid speech recognition system introduced modular approach adopted single layer networks discriminate between different context classes given phone class acoustic data context networks combined context independent ci network generate context dependent phone probability estimates experiments show average reduction word error rate ci system word word tasks respectively due improved modelling decoding speed system more than twice fast ci system
new line learning algorithm minimizes statistical dependency among outputs derived blind separation mixed signals dependency measured average mutual information mi outputs source signals mixing matrix unknown except number sources gram expansion instead expansion used evaluating mi natural gradient approach used minimize mi novel activation function proposed line learning algorithm has property easily implemented neural network like model validity new learning algorithm verified computer simulations
hybrid contextual radial basis function markov model off line handwritten word recognition system presented task assigned radial basis function networks estimation emission probabilities associated markov states model contextual because estimation emission probabilities takes into account left context current image segment represented its sequence new system does outperform previous system without context but acts differently
completely parallel object recognition np complete achieving recognizer feasible complexity requires compromise between parallel sequential processing system selectively focuses parts given image after another successive generated sample image these samples processed generate temporal context results integrated over time computational model based partially recurrent feedforward network proposed made testing rem world problem recognition handwritten digits encouraging results
paper describes tm ocr module two neural network algorithms its cc first network trained find individual characters field while performs classification both networks gabor projections original pixel images high rates greater compared its purely art version system module system has beta implemented specialized parallel hardware allows run has been vehicle its overall rate character level without into field rate fields system achieves character field success rate
define gamma multi layer perceptton mlp mlp usual synaptic weights replaced gamma filters proposed de de associated gain terms throughout layers derive gradient descent update equations apply model recognition speech phonemes find both inclusion gamma filters layers inclusion synaptic gains improves performance gamma mlp compare gamma mlp tdnn back mlp back mlp architectures local approximation scheme find gamma mlp results substantial reduction error rates
matching feature point sets lies core many approaches object recognition present framework non rigid matching begins module affine point matching integrates multiple features improve correspondence develops object representation based spatial regions model local transformations algorithm feature matching iteratively updates transformation parameters correspondence solution each turn mapping solved closed form permits its use data dimension correspondence set via method two way constraint satisfaction called softassign has recently emerged neural physics complexity non rigid matching algorithm multiple features same affine point matching algorithm results synthetic real world data provided point sets data multiple types features parts
intermediate higher vision processes require selection subset available sensory information before further processing usually selection implemented form spatially region visual field so called focus attention scans visual scene dependent input attentional state subject here present model control focus attention primates based saliency map mechanism expected model functionality biological vision but essential understanding complex scenes machine vision
sensory system constructs model environment its input might need verify models accuracy method verification multivariate time series prediction good model could predict near future activity its inputs much good scientific theory predicts future data such predicting model would require top down connections compare predictions input feedback could improve models performance two ways internal activity toward expected patterns generating specific error signals predictions fail proof concept model event driven computationally efficient layered network incorporating cortical features like excitatory synapses local inhibition constructed make near future predictions simple moving stimulus after unsupervised learning network contained units tuned obvious features stimulus like contour orientation motion but contour end stopping illusory contours
visual occlusion events constitute major source depth information paper presents self organizing neural network learns detect represent predict relationships arise during occlusion events after period exposure motion sequences containing occlusion events network develops two parallel channels chains lateral excitatory connections every motion trajectory channel chain visible chain activated moving stimulus visible other channel off chain chain carries persistent representation predicts motion visible stimulus becomes due occlusion learning rule uses chain trigger learning off chain off chain neurons learn separate associations object depth ordering results closely related recent discovery neurons macaque monkey posterior parietal cortex respond selectively inferred motion stimuli
facial action coding system devised provides objective means measuring facial muscle involved facial expression paper approach automated facial expression analysis detecting classifying facial actions generated database over image sequences subjects performing over distinct facial actions action combinations compare three different approaches classifying facial actions these images spatial analysis based principal components images explicit measurement local image features such template matching motion flow fields dataset containing six individual actions subjects these methods had performances respectively generalization novel subjects combined performance improved
visual cognition depends critically ability make rapid eye movements known saccades over targets interest visual scene saccades known pattern muscle activation target location computed prior movement visual feedback despite these distinctive properties has been general model saccadic strategy employed human visual system during visual search natural scenes paper proposes model saccadic uses scene representations derived oriented spatial filters multiple scales visual search proceeds coarse fine fashion largest scale filter responses being compared first model empirically tested comparing its performance actual eye movement data human subjects natural visual search task preliminary results indicate substantial agreement between eye movements predicted model those recorded human subjects
model human motion perception presented model contains two stages direction selective units first stage contains broadly tuned units while second stage contains units tuned model accounts motion through adapting units first stage inhibitory interactions second stage model explains two populations moving slightly different directions perceived single population moving direction vector sum two populations moving strongly different directions perceived transparent motion model explains why motion both cases appears non transparent motion
neural network model perception presented builds upon theory boundary contour contour system grossberg colleagues early ratio encoding retinal ganglion neurons well psychophysical results across different background used provide functional constraints theory suggest contrast hypothesis states ratio measures between regions given more weight determination respective regions simulations model address data perception including ratio hypothesis cross
finite sample sufficient determine density therefore entropy signal directly assumption about either functional form density about its smoothness necessary both amount prior over space possible density functions far most common approach assume density has parametric form contrast derive differential learning rule called optimizes entropy way kernel density estimation entropy its derivative calculated sampling density estimate resulting parameter update rule surprisingly simple efficient show used detect correct magnetic resonance images mri application beyond existing parametric entropy models
have developed gesture recognition system runs unconstrained office environment active camera using vision previously implemented environment determine spatial location salient body parts user guide active camera obtain images gestures expressions hidden state reinforcement learning paradigm used implement visual attention attention module selects targets based goal successful recognition uses new multiple model learning formulation given set target gestures our system learn maximally discriminate particular gesture
neurally inspired visual object recognition system described called whose goal identify common objects large known set independent angle distance non rigid distortion database consists objects rigid non rigid cord articulated statistical complex scenes recognition results obtained using set color shape feature channels within simple feedforward network architecture response test set novel test views each object presented individually color video images identified object correctly time chance using nearest neighbor classifier similar levels performance obtained subset non rigid objects generalization behavior reveals emergence striking natural category structure explicit input feature dimensions
present neural network based face detection system connected neural network examines small windows image whether each window contains face system between multiple networks improve performance over single network use bootstrap algorithm training adds false into training set training progresses eliminates difficult task manually selecting non face training examples chosen span entire space non face images comparisons another state art face detection system presented our system has better performance terms detection false positive rates
central performance improvement committee relative individual networks error correlation between networks committee investigated methods achieving error independence between networks training networks different resampling sets original training set methods tested artificial task real world problems cancer breast cancer
exploratory behavior within environment vehicle cognitive during time practice perfect sensorimotor patterns become behavioral modules more complex actions paper explores development such primitive learning systems using light weight hand used being developed mit artificial intelligence primitive grasping procedures learned sensory inputs using connectionist reinforcement algorithm while two sensory data recognize objects detect using competitive learning back propagation algorithm strategies respectively system consistent quick during initial learning stage but new situations after training
learning adjust opponents position critical success having intelligent agents towards specific tasks environments paper describes our work memory based technique choose action based continuous valued state attribute indicating position investigate question agent performs variations training situations our experiments indicate random variations fall within bound initial training agent performs better initial training rather than
report development modular neural system visual guidance robot pick place actions several neural networks integrated single system visually recognizes human hand pointing gestures stereo pairs color video images output hand recognition stage processed set color sensitive neural networks determine cartesian location target object pointing gesture finally information used guide robot target object put another location specified second pointing gesture accuracy current system allows identify location target object accuracy cm workspace area cm our current environment sufficient pick place arbitrarily target objects within workspace system consists neural networks perform tasks image segmentation estimation hand location estimation pointing direction object recognition necessary coordinate transforms drawing use learning algorithms functions network modules created data examples
state art speech processors cochlear perform channel selection using spectral maxima strategy strategy lead high frequency features needed discriminate between sounds present paper novel channel selection strategy based upon pattern recognition allows channel made proposed strategy implemented using multi layer perceptrons trained labelled speech database input network energy coefficients energy channels output system indices selected channels compare performance our proposed system spectral maxima strategy show our strategy produce significantly better results
most current methods prediction protein secondary structure use small window protein sequence predict structure central amino acid describe new method prediction non local structure called consists two more connected often widely separated protein chain network two windows introduced after training set proteins network predicts well but many false positives using global energy function prediction combined local prediction three secondary structures fi energy function minimized using simulated annealing give final prediction
present results use neural network based act novelty anomaly detectors detect motor trained reconstruct spectra obtained motor laboratory tests have demonstrated trained has small reconstruction error measurements recorded but larger error those recorded motor fault have designed built motor monitoring system using anomaly detection process testing system three industrial commercial sites
report here changes normalized eeg cross spectrum used conjunction feedforward neural networks monitor changes operators continuously near real time previously have shown eeg spectral amplitudes changes changes behavioral error rate auditory detection task here report first time increases frequency detection errors task patterns increased decreased spectral coherence several frequency bands eeg channel pairs relationships between eeg coherence performance vary between subjects but within subjects their topographic spectral profiles appear stable session session changes changes correlations among eeg waveforms recorded different scalp sites neural networks estimate correlation changes spontaneous eeg signals
paper propose memory based learning algorithm called predictive routing pq routing adaptive traffic control attempt address two problems encountered routing boyan littman namely inability fine tune routing policies under low network load inability learn new optimal policies under decreasing load conditions unlike other memory based reinforcement learning algorithms memory used keep past experiences increase learning speed pq routing best experiences learned them predicting traffic effectiveness pq routing has been verified under various network topologies traffic conditions simulation results show pq routing superior routing terms both learning speed
recent years interest has shifted asset allocation portfolio management exploit growing dynamics markets paper asset allocation decision problem optimized applying programming reinforcement learning based algorithms using artificial exchange rate asset allocation strategy optimized reinforcement learning learning shown equivalent policy computed dynamic programming approach tested task german stock market here neural networks used value function approximators resulting asset allocation strategy superior heuristic benchmark policy further example demonstrates applicability neural network based reinforcement learning problem setting high dimensional state space
paper discusses use multilayer feedforward neural networks predicting stocks excess return based its exposure various technical fundamental factors demonstrate effectiveness approach portfolio consists equally long short positions constructed its returns against returns index
paper describes neural network based controller capacity network system proposed order overcome real time response constraint two basic architectures evaluated feedforward network heuristic feedforward network recurrent network these architectures compared against linear programming benchmark used teacher label data samples feedforward neural network training algorithm found systems able provide traffic throughput respectively throughput obtained linear solution once trained neural network based solutions found fraction time required
current environmental monitoring systems assume particles spherical do attempt classify them laser based system developed university aims classifying particles through generation two dimensional profiles performances template matching two types neural network semi linear units compared image classification neural network approach shown capable comparable recognition performance while number advantages over template matching
paper discusses robot learn goal directed navigation tasks using local sensory inputs emphasis such learning tasks could formulated embedding problem dynamical systems desired trajectories task space should embedded into adequate sensory based internal state space so unique mapping internal state space motor command could established paper shows recurrent neural network self organizing such adequate internal state space temporal sensory input our experiments using real robot laser range sensor robot robustly achieving dynamical coherence environment shown such coherence becomes structurally stable global attractor self organized coupling internal environmental dynamics
paper describes policy iteration algorithm optimizing performance harmonic function based controller respect user defined index value functions represented potential distributions over problem domain being control policies represented gradient fields over same domain intermediate policies intrinsically ie collisions during adaptation process algorithm has efficient implementation parallel simd architectures potential application travel distance minimization illustrates its usefulness
control standard method performing fine manipulation tasks like grasping assembly but requires estimation state contact soc between robot arm objects involved here present method learn model movement measured data method requires little prior knowledge resulting model explicitly estimates soc current soc viewed hidden state variable discrete hmm control dependent transition probabilities between states modeled parametrized functions measurement show their parameters estimated measurements same time parameters movement each soc learning algorithm variant em procedure step computed exactly solving step exactly possible general here gradient ascent used produce increase likelihood
neural network based approach presented controlling two distinct types nonlinear systems first corresponds nonlinear systems parametric uncertainties parameters occur second corresponds systems control structures cannot determined proposed neural controllers shown result closed loop system stability under certain conditions
paper describes application reinforcement learning rl difficult real world problem elevator elevator domain poses combination challenges seen most rl research date elevator systems operate continuous state spaces continuous time discrete event dynamic systems their states fully observable they nonstationary due changing arrival rates addition use team rl agents each responsible controlling elevator car team receives global reinforcement signal appears noisy each agent due effects actions other agents random nature arrivals incomplete observation state these show results simulation best heuristic elevator control algorithms aware these results demonstrate power rl large scale stochastic dynamic optimization problem practical utility
scheduling important task interested particular task scheduling processing space program paper summarizes our previous work formulating task solution reinforcement learning algorithm tda previous work its hand input features paper shows extend time delay neural network tdnn architecture apply irregular length schedules experimental tests show tdnn tda network match performance our previous hand system tests show both neural network approaches significantly outperform best previous non learning solution problem terms quality resulting schedules number search steps required construct them
paper examine practical use hardware neural networks autonomous mobile robot have developed hardware neural system based around custom vlsi chip ii designed specifically embedded hardware neural applications present here demonstration application autonomous mobile robot flexibility system robot gains basic few training epochs using rule training methodology
consider solution large stochastic control problems means methods rely compact representations variant value iteration algorithm compute approximate go functions while such methods known unstable general identify new class problems convergence well error bounds guaranteed class involves linear cost go function together assumption dynamic programming operator respect euclidean norm applied functions parameterized class provide special case assumption satisfied relies locality transitions state space other cases discussed full length version paper
describe reinforcement learning problem motivate algorithms seek approximation function present new convergence results two such algorithms
performing policy iteration dynamic programming should require knowledge relative rather than absolute measures utility actions what baird calls advantages actions states nevertheless most existing methods dynamic programming including compute form absolute utility function smooth problems advantages satisfy two differential consistency conditions including requirement they free show enforcing these lead appropriate policy improvement solely terms advantages
paper introduce new algorithms optimizing noisy each experiment expensive algorithms build global non linear model expected output same time using bayesian linear regression analysis locally weighted polynomial models local model answers queries about confidence noise gradient use them make automated decisions similar those made response surface methodology global local models combined naturally locally weighted regression examine question whether global model really help optimization extend case time varying functions compare new algorithms highly tuned higher order stochastic optimization algorithm randomly generated functions simulated task note significant improvements total regret time converge final solution quality
continuous time continuous state version temporal difference td algorithm derived order facilitate application reinforcement learning real world control tasks neurobiological modeling optimal nonlinear feedback control law derived using derivatives value function performance algorithms tested task up pendulum limited both critic specifies paths upright position actor works nonlinear feedback controller successfully implemented radial basis function rbf networks
present new algorithm associative reinforcement learning algorithm based upon idea matching networks output probability probability distribution derived environment reward signal probability matching algorithm shown perform faster less local minima than previously existing algorithms use probability matching train mixture experts networks architecture other reinforcement learning rules fail converge reliably even simple problems architecture particularly well suited our algorithm compute arbitrarily complex functions yet calculation output probability simple
following investigates use single neuron learning algorithms improve performance text retrieval systems accept natural language queries retrieval process explained transforms natural language query into query real retrieval system initial query expanded using statistical techniques used document ranking binary classification results experiments suggest kivinen exponentiated gradient descent learning algorithm works significantly better than previous approaches
although td major machine learning has led similar impressive temporal difference learning other applications even other games able success td developing competitive evaluation function parameter feed forward neural network without using back propagation reinforcement temporal difference learning methods instead apply simple hill relative environment these results further analysis suggest surprising success program had more do co evolutionary structure learning task dynamics backgammon game itself
present connectionist method representing images explicitly addresses their hierarchical nature data neuroscience about whole object viewpoint sensitive cells cortex attentional basis field modulation ideas about hierarchical descriptions based resulting model makes critical use bottom up top down pathways analysis synthesis illustrate model simple example representing information about faces hierarchical models images objects constitute important paradigm case representational hierarchy such faces consist parts such eyes mouths representation manipulation part whole hierarchical information fixed hardware heavy around connectionist has consequently been many interesting proposals such raam turned primate visual system cortex appears construct representations visually presented objects mouths faces both objects so require fully representations presumably level probably using different possibly partially overlapping sets cells natural way represent part whole relationship between mouths faces have neuronal hierarchy connections bottom up mouth units face units so information about mouth used help recognize analyze image face connections top down face units mouth units generarive synthetic knowledge face scene usually mouth little thank hinton olshausen poggio pouget discussions comments dayan empirical support against such neuronal hierarchy but seems extremely unlikely correct set levels classes objects seems impossible recent evidence activities cells intermediate areas visual processing hierarchy such influenced visual attention suggests alternative strategy representing part whole information interaction subject attentional control between top down generative bottom up recognition processing version our example activating units represent particular face leads through top down generafive model pattern activity lower areas closely related pattern activity would seen entire face viewed activation lower areas turn provides bottom up input recognition system bottom up direction attentional signal controls aspects activation actually processed example activity reflecting lower part face should recognized case mouth units recognize restricted pattern activity being particular sort mouth therefore have provided way visual system represent part whole relationship between faces mouths describes many instance attentional control could mainly active during top down phase instead would create indeed intermediate areas activity corresponding lower portion face first place focus attention need so spatial overall scheme based hierarchical top down synthesis bottom up analysis model visual processing helmholtz machine note hierarchy here refers processing hierarchy rather than part whole hierarchy discussed above synthetic model forming effective map object attentional eye position image shown form figure image probabilities over activities units various levels system would caused seeing aspect object selected placing focus scale attention appropriately use generafive model during synthesis way described above hierarchical description particular image use statistical inverse synthetic model way analyzing images determine what objects they inversion process clearly sensitive attentional eye position actually determines nature object scene but way depicted ie its parameters reflected attentional eye position particular bottom up analysis model exists connections leading viewpoint selective image cells reported logothetis et al form population codes represented images mouths etc top down synthesis model exists connections leading reverse direction generalizations our scheme may course necessary generate image way down map specifies top down computational task like bottom up addressed using controlled synaptic matrix model neural models part whole hierarchies layer attentional figure model top down generative direction model generates images faces eyes mouths based attentional eye position selection single top layer unit bottom up recognition direction inverse map response neurons middle layer modulated illustrated graphs shown inside representing neurons middle layer attentional eye position see section more details olshausen et al our solution emerges control attentional eye position various levels processing most modulating activity equivalent modulation parietal cortex based actual rather than attentional eye position has been characterized pouget sejnowski terms basis fields they showed these basis fields used solve same tasks model but neuronal rather than synaptic multiplicative modulation fact eye position modulation almost occurs many levels system possibly including our scheme clearly requires modulating attentional eye position able become spatial eye position et al collected evidence part hypothesis although coordinate systems modulation entirely clear their data bottom up top down mappings learned taking eye position modulation into account experiments below used version wake sleep algorithm its conceptual computational simplicity requires learning bottom up model generated imagery during sleep learning model assigned explanations during observation real input during wake current version simplicity eye position set correctly during recognition but interested exploring automatic ways doing results have developed simple model illustrates feasibility scheme presented above context recognizing generating face its parts recognition involves taking image face part mouth nose eyes arbitrary position retina dayan face mouth eye figure recognition left column each pair shows stimuli right shows resulting activations top layer ordered face mouth nose eye stimuli faces random positions retina recognition performed setting attentional eye position image setting attentional scale creates window attention around position shown circle corresponding size position generation each panel shows output generative pathway randomly chosen attentional eye position activating each top layer units turn focus attention marked circle whose size reflects attentional scale name object whose neuronal representation top layer activated shown above each panel setting appropriate top level unit remaining units zero generation involves imaging either whole face its parts selected active unit top layer arbitrary position retina model figure consists three layers lowest layer retina recognition direction retina into layer hidden units these project top layer has four neurons generative direction connectivity network fully connected both directions activity each neuron based input recognition following layer generation linear function weight matrices recognition generative direction attentional eye position influences activity through multiplicative modulation neuronal responses hidden layer linear response ri ri each neuron middle layer based bottom up top down connections multiplied es tuning curves each dimension attentional eye position es coding scale focus attention respectively thus activity mi hidden neuron have mi recognition pathway mi generative pathway tuning curves chosen sigmoid random centers ci random directions di sc other implementations have used gaussian tuning functions fact requirement regarding shape tuning functions through superposition them construct functions show dependence attentional eye position recognition direction attentional eye position has influence activity input layer defining window attention implemented using gaussian window centered attentional eye position its size given attentional scale allow system learn models parts based experience images whole faces train model employ variant unsupervised wake sleep algorithm algorithm generative pathway trained during wake phase neural models part whole hierarchies stimuli input layer retina our case cause activation neurons network through recognition pathway providing error signal train generarive pathway using delta rule conversely sleep phase random activation top layer unit conjunction randomly chosen attentional eye position leads via generarive connections generation activation middle layer consequently image input layer used adapt recognition weights again using delta rule although delta rule wake sleep fine recognition direction leads poor generarive model our simple case generation much more difficult than recognition solution therefore train generarive weights using back propagation uses activity top layer created recognition pathway input retinal activation pattern target signal hence learning still unsupervised except appropriate attentional eye positions always set during recognition have system weights weights between layers trained model training could done standard wake sleep algorithm ie using local delta rule both sets weights figure shows several examples performance recognition pathway different stimuli after iterations network able recognize stimuli accurately different positions visual field figure shows several examples output generarive model illustrating its capacity produce images faces their parts arbitrary locations imaging whole face focusing attention eg area around its center nose unit through recognition pathway relationship eg nose part face established straightforward way discussion representing hierarchical structure key problem visual images offer canonical example seems possible underlying neural mechanisms theory based view object selective cells attentional eye position modulation firing cells these work context analysis synthesis recognition generative models such part whole hierarchy object such face contains eyes contain etc generative direction choosing view object through different effective eye position recognition direction allowing real attentional eye positions activate view selective cells scheme related auto associative memory raam system tm raam provides way representing tree structured information instance learn object whose structure standard auto associative net would leading pattern hidden unit activations would learn leading finally leading would itself representation whole object compression operation its expansion inverse required explicit methods tree structure our scheme representing hierarchical information similar raam using notion attentional eye position perform its compression expansion dayan operations whereas raam normally constructs its own codes intermediate levels trees fed here images faces real available those instance their associated mouths changes learning task but notion direct recognition without repeated parts various aspects our scheme require way eye position affects recognition coding different instances objects use top down information during bottom up recognition variants scheme objects big challenging fit go into single image hierarchical objects other than images working more probabilistically correct version taking advantage statistical helmholtz machine eye position information ubiquitous visual processing areas including lgn well parietal cortex further revealed having dramatic effect perception et tm study form two eyes normally aligned but eye much study showed even image retina eye so fixed retinal coordinates least component percept moves eye moves information about eye position dramatically effects visual processing manner consistent model presented here shifts based modulation required et theory perceptual stability across essentially builds up scene exactly form mapping general many instances object eg many different faces general case top level would implement distributed code identity parameters objects currently investigating methods implementing form representation into model key feature model interaction synthesis analysis pathways part whole hierarchies interaction between two pathways aid system performing image analysis integrating information across hierarchy raam extra feature required hierarchy short term memory raam memory stores information about various separate sub trees have already been decoded encoded our system memory required during generative force whole activity lower layers even after activity upper layers has free these upper units recognize part memory during recognition necessary marginal cases information across separate parts well whole solution hierarchical representation gives up computational simplicity naive neuronal hierarchical scheme described
order process incoming sounds efficiently auditory system adapted statistical structure natural auditory scenes first step investigating relation between system its inputs study low order statistical properties several sound ensembles using filter analysis focusing amplitude phase different frequency bands find simple parametric descriptions their distribution power spectrum valid different types sounds particular amplitude distribution has exponential tail its power spectrum exhibits modified power law behavior self similarity long range temporal correlations furthermore statistics different bands within given ensemble identical suggesting translation invariance along cochlear axis these results show natural sounds highly redundant have possible implications neural code used auditory system
employed white noise velocity signal study dynamics response single neurons cortical area mt visual motion responses quantified using reverse correlation optimal linear reconstruction filters reconstruction signal noise ratio snr snr lower bound estimates information rate lower than expected percent information transmitted below hz highest lower bound bit rate simulated motion energy poisson spike statistics able out perform mt neurons temporal integration window measured reverse correlation half width ranged ms window stimulus moved faster but did change temporal frequency held constant
poggio proposed view based model object recognition accounts several psychophysical properties certain recognition tasks model predicted existence view tuned view invariant units later found logothetis et al logothetis et al cortex monkeys trained views specific objects model does specify inputs view tuned units their internal organization paper propose model these view tuned units consistent physiological data single cell responses
binocular rivalry alternating percept result two eyes see different scenes recent psychophysical evidence supports account component binocular rivalry similar other bistable test hypothesis generated competition between cortical explanations inputs rather than direct competition between inputs recent neurophysiological evidence shows binocular neurons modulated changing percept others even they selective between stimuli presented eyes extend our model hierarchy address these effects
train recurrent networks control computer model elegant model presented based closely body mechanics behavioral analyses neurophysiology elegans each constraints relevant information processing simulated moving autonomously simulated chemical environments display variety strategies similar those biological
encoding random time varying stimuli single spike trains electrosensory neurons weakly electric fish investigated using methods statistical signal processing first stage electrosensory system spike trains found encode faithfully detailed time course random stimuli while second stage neurons specifically features temporal waveform stimulus therefore stimulus information processed second stage electrosensory system extracting temporal features faithfully preserved image environment sampled first stage
introduce plausible model contour integration visual inputs individual oriented edges model composed interacting excitatory neurons inhibitory interneurons receives visual inputs via oriented receptive fields rfs like those rf centers distributed space each location finite number cells tuned orientations spanning model cortical interactions modify neural activities produced visual inputs selectively activities edge elements belonging smooth input contours elements within contour produce synchronized neural activities show analytically empirically contour enhancement neural synchrony increase contour length smoothness observed experimentally model gives testable predictions addition introduces feedback mechanism allowing higher visual centers enhance segment contours
paper develops arguments family temporal log linear models represent spatio temporal correlations among spiking events group neurons models represent pairwise correlations but correlations higher order methods discussed inferring existence absence correlations estimating their strength frequentist bayesian approach correlation detection compared frequentist method based statistic estimates obtained via max principle bayesian approach markov chain monte carlo model composition mc algorithm applied search over connectivity structures method used approximate their posterior probability performance methods tested synthetic data methods applied experimental data obtained fourth author means measurements carried out behaving monkeys medical school university neural connectivity structures need neither hierarchical nor learning quasi synchronization patterns among spiking neurons
biophysical modeling studies have previously shown cortical pyramidal cells driven strong nmda type synaptic currents andor containing dendritic voltage dependent ca channels respond more strongly synapses activated several spatially clustered groups optimal size comparison same number synapses activated about dendritic arbor nonlinear intradendritic interactions giving rise cluster sensitivity property akin layer virtual nonlinear hidden units dendrites implications cellular basis learning memory certain classes nonlinear sensory processing present study show single neuron access excitatory inputs off center cells lgn exhibits principal nonlinear response properties complex cell primary visual cortex namely orientation tuning coupled translation invariance contrast conjecture type intradendritic processing could explain complex cell responses absence oriented simple cell input mel ruderman
recently nature pp demonstrated stimulation beyond classical receptive field crf modulate but change neurons response oriented stimuli they revealed patch suppressed cells stimulated orientations inside outside their crf strongly respond stimuli oriented orthogonal their preferred orientation here analyze emergence such complex response patterns simple model primary visual cortex show observed sensitivity orientation contrast explained between local isotropic interactions long range connectivity between distant orientation domains particular demonstrate observed properties might arise without specific connections between sites cross oriented crfs
coarse codes widely used throughout brain encode sensory motor variables methods designed interpret these codes such population vector analysis either ie variance estimate much larger than smallest possible variance biologically implausible like maximum likelihood moreover these methods attempt compute scalar vector estimate encoded variable neurons faced similar estimation problem they read out responses presynaptic neurons but contrast they typically encode variable further population code rather than scalar show non linear recurrent network used perform these estimation optimal way while keeping estimate coarse code work suggests lateral connections cortex may involved up uncorrelated noise among neurons representing similar variables
linear architectural model cortical simple cells presented model mutual inhibition occurring through synaptic coupling functions distributed space possible basis wide variety spario temporal simple cell response properties including direction selectivity velocity tuning while spatial asymmetries included explicitly structure inhibitory interconnections temporal asymmetries specific mutual inhibition scheme considered extensive simulations supporting model reported
change mean firing rate neuron but its pattern firing therefore reliable neural coding scheme whether rate coding spike time based coding robust dynamic neuromodulatory environment common observation cholinergic modulation leads reduction spike frequency adaptation implies modification spike timing would make neural code based precise spike timing difficult maintain paper effects cholinergic modulation studied test hypothesis precise spike timing serve reliable neural code using whole cell patch technique rat neocortical slice preparation compartmental modeling techniques show cholinergic modulation surprisingly preserved spike timing response inputs resembles vivo conditions result suggests vivo spike timing may much more changes than previous physiological studies have am sejnowski
parameter space neural networks has riemannian metric structure natural riemannian gradient should used instead conventional gradient former denotes true steepest descent direction loss function riemannian space behavior stochastic gradient learning algorithm much more effective natural gradient used present paper studies information geometrical structure perceptrons other networks prove line learning method based natural gradient asymptotically efficient optimal batch algorithm adaptive modification learning constant proposed analyzed terms riemannian measure shown efficient natural gradient finally applied blind separation independent signal sources
paper shows large neural network used pattern classification problem learning algorithm finds network small weights has small squared error training patterns generalization performance depends size weights rather than number weights more specifically consider layer feed forward network sigmoid units sum magnitudes weights associated each unit bounded misclassification probability converges error estimate closely related squared error training set rate ignoring log factors number training patterns input dimension constant may explain generalization performance neural networks particularly number training examples considerably smaller than number weights supports heuristics such weight decay early stopping attempt keep weights small during training
new method calculate full training process neural network introduced sophisticated methods like replica trick used results directly related actual number training steps results presented here like maximal learning rate exact description early stopping necessary number training steps further problems addressed approach
study number hidden layers required multilayer neural network threshold units compute function td dimension characterized functions computable hidden layer under assumption multiple intersection point defined compact set consider restriction neighborhood multiple intersection point infinity give necessary sufficient conditions locally computable hidden layer show adding these conditions assumptions sufficient ensure global hidden layer exhibiting new non local configuration critical cycle implies computable hidden layer
new regression technique based concept support vectors introduced compare support vector regression svr committee regression technique bagging based regression trees ridge regression done feature space basis these experiments expected svr have advantages high dimensionality space because svr optimization does depend dimensionality input space
convergence properties gradient descent algorithm case linear perceptton may obtained response function derive general expression response function apply case data simple input correlations found correlations may slow down learning explains success pca method reducing training time motivated finding furthermore propose transform input data removing mean across input variables well examples decrease correlations numerical findings medical classification problem fine agreement theoretical results
propose new method compute prediction intervals especially small data sets width prediction interval does depend variance target distribution but accuracy our estimator mean target ie width confidence interval confidence interval follows variation ensemble neural networks each them trained stopped bootstrap replicates original data set second improvement use residuals validation patterns instead training patterns estimation variance target distribution illustrated synthetic example our method better than existing methods regard extrapolation interpolation data regimes limited amount data yields prediction intervals actual confidence levels closer desired confidence levels statistical intervals paper consider feedforward neural networks regression tasks estimating underlying mathematical function between input output variables based finite number data points possibly corrupted noise given set pairs assumed generated according denotes noise zero mean trained such regression task output network given new input vector real world computing snn foundation neural networks practical confidence prediction intervals interpreted estimate regression ie mean target distribution given input sometimes interested reliable estimate regression many applications important quantify accuracy our statements regression problems distinguish two different aspects accuracy our estimate true regression accuracy our estimate respect observed output confidence intervals deal first aspect ie consider distribution quantity prediction intervals latter ie treat quantity see prediction interval necessarily corresponding confidence interval method somewhat similar ours introduced estimate both mean variance target probability distribution based assumption sufficiently large data set ie their risk overfitting neural network finds correct regression practical applications limited data sets such assumptions strict paper propose new method estimates estimator through bootstrap resampling tendency considering residuals validation patterns rather than those training patterns bootstrapping early stopping bootstrapping based idea available data set but particular realization unknown probability distribution instead sampling over true probability distribution impossible defines empirical distribution so called naive bootstrapping empirical distribution sum delta peaks available data points each probability content bootstrap sample collection pdata patterns drawn replacement empirical probability distribution bootstrap sample but our training set patterns do occur training set definition part validation set large probability pattern becomes part validation set training neural network particular bootstrap sample weights adjusted order minimize error training data training stopped error validation data starts increase so called early stopping procedure popular strategy prevent overfitting neural networks viewed alternative regularization techniques such weight decay context bootstrapping procedure generate training validation set similar fold cross validation each bootstrap replicates train stop single neural network output network input vector written oi estimate our ensemble networks regression take average output run il so called estimator shown proper balancing network outputs yield even better results heskes confidence intervals confidence intervals provide way quantify our confidence estimate regression ie have consider probability distribution true regression given our estimate our line reasoning goes follows see assume our ensemble neural networks yields more less unbiased estimate ie distribution centered around truth neural networks biased estimators example neural networks trained finite number examples always have tendency almost other model sharp peak data introduces bias arrive asymptotically correct confidence intervals should taken into account would possible compute such bias correction should do first place arrive better estimator our working hypothesis here bias component confidence intervals negligible comparison variance component do exist methods claim give confidence intervals second see eg discussion order correct ie up including terms order after do know handle bias component such precise confidence intervals require amount bootstrap samples our purposes first order correct intervals up including terms order always symmetric derived assuming gaussian distribution variance distribution estimated variance outputs networks bootstrap method see eg distribution gaussian so inverse distribution find regression randomly drawing data sets consisting pdata data points according true distribution inputs corresponding targets best do define empirical distribution explained before estimate distribution yields estimate so following bootstrap procedure arrive confidence intervals rem depends desired confidence level factors taken table percentage points students distribution number degrees freedom equal number bootstrap runs nrun more direct alternative choose such more than nrun network predictions paper assume both inputs outputs stochastic case deterministic input variables other bootstrapping techniques see eg more appropriate statistical intervals resulting naive bootstrapping may practical confidence prediction intervals prediction intervals confidence intervals deal accuracy our prediction regression ie mean target probability distribution prediction intervals consider accuracy predict targets themselves ie they based estimates distribution propose following method two noise components rn independent variance first component has been estimated our bootstrap procedure arrive confidence intervals remaining task estimate noise inherent regression problem assume noise more less gaussian such again compute its variance may depend input mathematical symbols course interested prediction intervals new points do know targets suppose had left set test patterns had never used training nor our neural networks could try estimate model fit remaining residuals using minus loglikelihood error measure exp course leaving out these test patterns data our bootstrap procedure offers alternative each pattern about bootstrap runs part training set let us write pattern validation set run otherwise each pattern use average instead average rn get close possible unbiased estimate residual independent test patterns without training data so suggest find function minimizes error yet leaving out test patterns would data nor using training data would underestimate error but exploiting information about residuals validation patterns once have found function compute both mean deviation combined prediction interval again factor found students table chosen such more than pdata patterns st function may modelled separate neural network similar method proposed exponential instead linear transfer function output unit ensure variance always positive heskes input input input input figure prediction intervals synthetic problem training set crosses true regression solid line network prediction dashed line validation residuals crosses training residuals true variance solid line estimated variance based validation residuals dashed line based training residuals dash dotted line width standard error bars more advanced method dashed line simpler procedure dash dotted line what should solid line prediction intervals solid line network prediction dashed line test points illustration consider synthetic problem similar used example demonstrate incorporate regression estimator prediction intervals inputs drawn interval probability density px ie more examples drawn boundary than middle targets generated according regression solid line figure la variance target distribution solid line figure lb following obtain training set pdata data points crosses figure la train ensemble networks each having hidden units tanh transfer function linear output unit average network output dashed line practical confidence prediction intervals figure la following compare two methods arrive prediction intervals more advanced method described section ie taking into account uncertainty estimator correcting tendency training data simpler procedure similar both effects compute squared validation residuals crosses ion lb based runs pattern part validation set training residuals based runs pattern part training set validation residuals most time somewhat larger than training residuals our more advanced method uncertainty our model validation residuals other procedure simply training residuals estimate variance target distribution obvious distribution residuals figure lb does allow complex model here take feedforward network hidden unit xx exp parameters found through minimization error both advanced method dashed line simpler procedure line variance target distribution estimated step function former being based validation residuals minus uncertainty estimator slightly more than latter being based training residuals both estimates far truth solid line especially yet considering such limited amount noisy residuals hardly expect better figure considers width standard error bars ie prediction intervals error level simpler procedure width prediction interval dash dotted line figure follows directly estimate variance target distribution our more advanced method adds uncertainty estimator arrive dashed line correct width prediction interval ie width would include targets particular input given solid line prediction intervals obtained through more advanced procedure displayed figure together set test points visualizing probability distribution inputs corresponding targets method proposed section has several advantages prediction intervals advanced method include test points figure close desired confidence level simpler procedure actual confidence level difference mainly due use validation residuals instead training residuals incorporation uncertainty estimator important regions input space few training data example density training data affects both extrapolation interpolation ix prediction intervals obtained advanced method become wider wider whereas those obtained through simpler procedure remain more less constant prediction interval dashed line near result relatively large variance network predictions region shows our method incorporates effect density training data has accuracy interpolation heskes conclusion discussion have presented novel method compute prediction intervals applications limited amount data uncertainty estimator itself has been taken into account computation confidence intervals explains qualitative improvement over existing methods regimes low density training data usage residuals validation instead training patterns yields prediction intervals better price have computation time have train ensemble networks about different bootstrap replicates other good reasons resampling averaging over networks improves generalization performance early stopping natural strategy prevent overfitting would interesting see our frequentist method compares bayesian alternatives see eg prediction intervals used detection outliers regard training set straightforward point out targets prediction interval error level say wide prediction interval new test pattern indicates test pattern lies region input space low density training data making prediction completely unreliable weak point our method assumption computation confidence intervals assumption makes confidence intervals general discussed such bootstrap methods tend perform better than other alternatives based computation hessian matrix partly because they incorporate variability due random initialization furthermore model prediction interval function input extent deficiency but still incorporating even somewhat inaccurate confidence interval ensures never our accuracy regions input space have never been before references regression input dependent noise bayesian treatment these proceedings bagging predictors machine learning
study generalization capability mixture experts learning examples generated another network same architecture number examples smaller than critical value network shows symmetric phase role experts specialized upon crossing critical point system continuous phase transition symmetry breaking phase gating network partitions input space effectively each expert assigned appropriate subspace find mixture experts multiple level hierarchy shows multiple phase transitions
results study worst case learning curves particular class probability distribution input space mlp hard threshold hidden units presented shown particular thermodynamic limit scaling number connections first hidden layer although true learning curve behaves its vc dimension based bound trivial its vc entropy bound trivial shown bounds following true learning curve derived formalism based density error patterns
paper apply method complexity regularization derive estimation bounds nonlinear function estimation using single hidden layer radial basis function network our approach differs previous complexity regularization neural network function learning schemes operate random covering numbers metric entropy making possible consider much broader families activation functions namely functions bounded variation constraints previously imposed network parameters way network trained means complexity regularization involving empirical risk minimization bounds expected risk terms sample size obtained large class loss functions rates convergence optimal loss derived
study mistake driven variant line bayesian learning algorithm similar studied variant updates its state learns trials makes mistake algorithm makes binary using linear threshold classifier runs time linear number attributes seen learner have been able show theoretically simulations algorithm performs well under assumptions quite different those prior original bayesian algorithm handle situations do know handle linear time bayesian algorithms expect our techniques useful deriving analyzing other algorithms
exhibit novel nets networks noisy spiking neurons coding furthermore shown networks noisy spiking neurons coding strictly power nets number units
introduce model noise robust analog computations time flexible enough cover most important concrete cases such computations noisy analog neural nets networks noisy spiking neurons show presence arbitrarily small amounts analog noise reduces power analog computational models finite automata prove new type upper bound vc dimension computational models analog noise
present algorithm expected bayes optimal predictions large feed forward networks based mean field methods developed within statistical mechanics systems give derivation single layer perceptton show algorithm provides leave out cross validation test predictions simulations show excellent agreement theoretical results statistical mechanics
stochastic line learning faster than batch learning late times learning rate annealed remove noise present stochastic weight updates annealing phase tile convergence rate mean square best proportional number input alternative increase batch size remove noise paper explore convergence lms using small but fixed batch sizes adaptive batch size show best adaptive batch schedule exponential has rate convergence same annealing ie best proportional
shown conventional computers faster than planar hopfield networks although planar hopfield networks take exponential time converge stable state arbitrary planar hopfield network found conventional computer polynomial time theory completeness gives strong evidence such separation unlikely hopfield networks demonstrated case several restricted classes hopfield networks including those who interconnection graphs class bipartite graphs graphs degree dual graph neighbor hypercube connected cycles exchange graph
paper investigates stationary points hebb learning rule sigmoid nonlinearity show mathematically input has low information content measured inputs variance learning rule suppresses learning forces weight vector converge zero vector information content exceeds certain value rule automatically begin learn feature input our analysis suggests under certain conditions first principal component learned weight vector length remains bounded provided variance input finite simulations confirm theoretical results derived
given computational resources best use criterion minimal expected generalisation error select model determine its parameters may generalisation performance higher learning speed method quantifying sub optimality set out here so choice made furthermore method applicable broad class models including fast memory based methods such added benefit providing first time means analyse generalisation properties such models bayesian framework
study effect noise regularization line gradient descent learning scenario general two layer student network arbitrary number hidden units training examples randomly drawn input vectors labeled two layer teacher network arbitrary number hidden units examples corrupted gaussian noise affecting either output model itself examine effect both types noise weight decay regularization dynamical evolution order parameters generalization error various phases learning process
given multidimensional data set model its density consider define optimal interpolation between two points done assigning cost each path through space based two competing goals interpolate through regions high density other minimize arc length path functional derive lagrange equations motion given two points desired interpolation found solving boundary value problem show interpolation done efficiently high dimensions gaussian dirichlet mixture models
analyse online learning finite training sets learning rates extension statistical mechanics methods obtain exact results time dependent generalization error linear network large number weights find example small training sets size larger learning rates used without asymptotic generalization performance convergence speed optimal settings less importantly weight decay given final learning time generalization performance online learning essentially good learning
support vector sv method recently proposed estimating regressions constructing multidimensional splines solving linear operator equations vapnik presentation report results applying sv method these problems
learning properties universal approximator normalized committee machine adjustable biases studied line back propagation learning within statistical mechanics framework numerical studies show model has features do exist previously studied two layer network models without adjustable biases eg attractive suboptimal symmetric phases even realizable cases noiseless data
neural networks wide class weight priors shown limit infinite number hidden units prior over functions tends gaussian process paper analytic forms derived covariance function gaussian processes corresponding networks sigmoidal gaussian hidden units allows predictions made efficiently using networks infinite number hidden units shows somewhat may easier compute infinite networks than finite ones
consider microscopic equations learning problems neural networks fields example obtained fields fields example absent learning process energy landscape assume density local minima obey exponential distribution yielding macroscopic properties first step replica symmetry breaking solution microscopic equations provide learning algorithm results higher stability than conventional algorithms
consider problem prediction stationary time series using architecture known mixtures experts here suggest mixture several autoregressive models study focuses theoretical foundations prediction problem context more precisely demonstrated model universal approximator respect learning unknown prediction function statement upper bounds mean squared error established based these results possible compare other families models eg neural networks state dependent models shown degenerate version fact equivalent neural network number experts architecture plays similar role number hidden units latter model
classifier called consistent respect given set points correctly classifies set consider defined local propose algorithms consistent classifier reduction expected complexities proposed algorithms derived along expected classifier sizes particular proposed approach yields consistent reduction nearest neighbor classifier performs classification assigning each new object class regardless data structure proposed reduction method suggests notion soft classification allowing respect objects supported data performances proposed classifiers predicting stock behavior compared achieved nearest neighbor method
full bayesian method applying neural networks prediction problem set up structure net perform necessary integrals these integrals tractable analytically markov chain monte carlo mcmc methods slow especially parameter space high dimensional using gaussian processes approximate weight space integral analytically so small number hyperparameters need integrated over mcmc methods have applied idea classification problems obtaining excellent results real world problems investigated so far
most regression problem assumed distribution target data described deterministic function inputs together additive gaussian noise having constant variance use maximum likelihood train such models corresponds minimization sum squares error function many applications more realistic model would allow noise variance itself depend input variables use maximum likelihood train such models would give highly biased results paper show bayesian treatment allow input dependent variance while overcoming bias maximum likelihood
self organizing map som algorithm has been extensively studied has been applied considerable success wide variety problems algorithm derived heuristic ideas leads number significant limitations paper consider problem modelling probability density data space several dimensions terms smaller number latent hidden variables introduce novel form latent variable model call algorithm generarive topographic mapping allows general non linear transformations latent space data space trained using em expectation maximization algorithm our approach overcomes limitations som while introducing significant demonstrate performance algorithm simulated data flow multi phase
power sampling methods bayesian reconstruction noisy signals well known extension sampling temporal problems discussed efficacy sampling over time demonstrated visual tracking
problem assigning points dimensional real space clusters formulated determining centers such sum distances each point nearest center minimized distance used problem formulated minimizing piecewise linear concave function set shown equivalent bilinear program minimizing bilinear function set fast finite median algorithm consisting solving few linear programs closed form leads stationary point bilinear program computational testing number realworld databases carried out breast cancer database median training set correctness comparable mean algorithm its testing set correctness better additionally prognostic breast cancer database distinct important survival curves extracted median algorithm whereas mean algorithm failed obtain such distinct survival curves same database
support vector learning machines svm finding application pattern recognition regression estimation operator inversion ill posed problems against general methods improving generalization performance improving speed test phase svms increasing interest paper combine two such techniques pattern recognition problem method improving generalization performance virtual support vector method does so incorporating known invariances problem method achieves error rate nist test digit images method improving speed reduced set method does so approximating support vector decision surface apply method achieve factor speedup test phase over virtual support vector machine combined approach yields machine both times faster than original machine has better generalization performance achieving error virtual support vector method applicable svm problem known invariances reduced set method applicable support vector machine
describe notion equivalent kernels suggest provides framework comparing different classes regression models including neural networks both parametric non parametric statistical techniques unfortunately standard techniques break down faced models such neural networks more than layer adjustable parameters propose algorithm overcomes limitation estimating equivalent kernels neural network models using data perturbation approach experimental results indicate networks do use maximum possible number degrees freedom these controlled using regularisation techniques equivalent kernels learnt network vary both size shape different regions input space
supervised learning usually clear distinction between inputs outputs inputs what you measure outputs what you predict those measurements paper shows distinction between inputs outputs simple features more useful extra outputs than inputs using feature output get more than case values but learn mapping other inputs feature many features mapping may more useful than feature value itself present two regression problems classification problem performance improves features could have been used inputs used extra outputs instead result surprising feature used output used during testing
paper developed two parts discuss new approach self organization single layer linear feed forward network first two novel algorithms self organization derived two layer linear associative network performing classification trained constrained least mean squared classification error criterion second two adaptive algorithms derived these selforganizing procedures compute principal generalized eigenvectors two correlation matrices two sequences random vectors these novel adaptive algorithms implemented single layer linear feed forward network give rigorous convergence analysis adaptive algorithms using stochastic approximation theory example consider problem online signal detection digital mobile communications
work investigates representational inductive capabilities time delay neural networks general two tdnn those delays inputs those include delays hidden units both architectures capable representing same class languages definite memory machine languages but delays hidden units helps outperform problems composed repeated features over short time windows
globally convergent method defined capable sequentially producing large numbers stationary points multi layer perceptron mean squared error surface using algorithm large subsets stationary points two test problems found shown empirically mlp neural network appears have extreme ratio saddle points compared local minima even small neural network problems have extremely large numbers solutions
describe criterion attempts minimize error learner minimizing its estimated squared bias describe experiments locally weighted regression two simple problems observe bias approach outperforms more common variance exploration approach even presence noise
many optimization problems structure solutions reflects complex relationships between different input parameters example experience may us certain parameters closely related should explored independently similarly experience may establish subset parameters take particular values search cost landscape should take advantage these relationships present mimic framework analyze global structure optimization landscape novel efficient algorithm estimation structure derived use knowledge structure guide randomized search through solution space turn refine our estimate structure our technique obtains significant speed gains over other randomized optimization procedures
modification described use mean field approximations step em algorithms data latent structure models described ghahramani among others modification involves second order taylor approximations expectations computed step potential benefits method illustrated using simple latent profile models
paper describes new framework relational graph matching starting point recently reported bayesian consistency measure structural differences using hamming distance main contributions work demonstrate discrete components cost function second contribution show cost function used locate matches using continuous nonlinear optimisation finally show resulting graph matching algorithm relates standard quadratic assignment problem
limitations using self organizing maps som either quantization vq multidimensional scaling mds being discussed recent empirical findings relevant theory remaining ability doing both vq mds same time new combined technique online means clustering plus mapping cluster som shown perform significantly worse terms quantization error recovering structure clusters preserving topology comprehensive empirical study using series multivariate normal clustering problems
real valued random hidden variables useful modelling latent structure explains correlations among observed variables propose simple unit adds zero mean gaussian noise its input before passing through sigmoidal function such units produce variety useful behaviors ranging deterministic binary stochastic continuous stochastic show slice sampling used inference learning top down networks these units demonstrate learning two simple problems
propose novel approach automatically growing pruning hierarchical mixtures experts constructive algorithm proposed here enables large hierarchies consisting several hundred experts trained effectively show trained our automatic growing procedure yield better generalization performance than traditional static balanced hierarchies evaluation algorithm performed vowel classification within hybrid version janus speech recognition system using subset large vocabulary speaker independent continuous speech recognition database
compare different methods combine predictions neural networks trained different bootstrap samples regression problem these methods introduced here call balancing based analysis ensemble generalization error into ambiguity term term incorporating generalization performances individual networks show estimate these individual errors residuals validation patterns weighting factors different networks follow quadratic programming problem real world problem concerning prediction sales figures well known boston housing data set balancing clearly outperforms other recently proposed alternatives bagging bumping early stopping bootstrapping stopped training popular strategy prevent overfitting neural networks complete data set split up into training validation set through learning weights adapted order minimize error training data training stopped error validation data starts increasing final network depends subdivision training validation set often usually random initial weight configuration chosen minimization procedure other words early stopped neural networks highly unstable small changes data different initial conditions produce large changes estimate argued unstable estimators ie apply same procedure several times using different training validation set perhaps starting different initial real world computing snn foundation neural networks balancing between bagging bumping configurations neural network literature resampling often referred training ensembles neural networks paper discuss methods combining outputs networks obtained through such procedure first have choose generate training validation sets options among others fold cross validation bootstrapping paper consider bootstrapping based idea available data set but particular realization probability distribution principle would like do inference true yet unknown probability distribution natural do define empirical distribution so called naive bootstrapping empirical distribution sum delta peaks available data points each probability content pdata pdata number patterns bootstrap sample collection pdata patterns drawn replacement empirical probability distribution data points occur once twice even more than twice bootstrap sample bootstrap sample taken training set patterns do occur particular bootstrap sample constitute validation set large pdata probability pattern becomes part validation set pdata pdata advantage bootstrapping over other resampling techniques most statistical theory resampling based bootstrap using naive bootstrapping generate nrun training validation sets out our complete data set pdata input output combinations paper restrict regression problems output variable keep track matrix components indicating whether part validation set run training set each subdivision train stop neural network layer hidden units output network weight vector wi input tanh jl use definition validation error run written pi pi yt pdata number validation patterns run error network pattern after training left nrun networks practice quite different performances complete data set should combine these outputs get best possible performance new data combining estimators several methods have been proposed combine estimators see eg review paper consider estimators same architecture heskes but trained stopped different data training validation sets recently two such methods have been suggested estimators bagging bootstrap aggregating bumping meaning bootstrap model parameters bagging prediction newly arriving input vector average over network predictions bagging completely performance individual networks data used training stopping bumping other hand away networks except lowest error complete data set following describe intermediate form due here call balancing theoretical analysis implications idea found suppose after training receive new set test patterns do know true targets but calculate network output each network give each network weighting factor ci define prediction networks pattern weighted average goal find weighting factors ai subject constraints run ai yielding smallest possible generalization error problem course our about targets iv bagging simply takes ci nrun networks whereas bumping implies ai ji pdata pdata write generalization error form ij ij last term depends network outputs thus calculated ambiguity term favors networks outputs first part idea behind bumping more general involved than discussed here interested referred paper consider its naive version balancing between bagging bumping containing generalization errors individual networks depends targets iv thus unknown favors networks themselves already have low generalization error next section find reasonable estimates these generalization errors based network performances validation data once have obtained these estimates finding optimal weighting factors ai under constraints straightforward quadratic programming problem estimating generalization error first good estimate generalization error network could performance validation data included during training validation error strongly depends subdivision training validation set example few outliers pure part validation set validation error relatively large training error relatively small correct bias result random subdivision introduce expected validation error run first define number runs pattern part validation set error averaged over these runs validation expected validation error follows validation pdata pi ratio between observed expected validation error indicates whether validation error network relatively high low our estimate generalization error network ratio multiplied overall scaling factor being estimated average generalization error pdata validation note implicitly make assumption bias introduced stopping minimal error validation patterns negligible ie validation patterns used stopping network considered new network completely independent test patterns simulations compare following methods combining neural network outputs individual average individual generalization error ie generalization error get average decide perform run serves reference other methods compared bumping generalization network lowest error data available training stopping heskes unfair unfair bumping bagging ambiguity balancing bumping balancing store store store store store store mean table decrease generalization error relative average individual generalization error result several methods combining neural networks trained predict sales figures several stores bagging generalization error take average network outputs our prediction ambiguity generalization error weighting factors chosen maximize ambiguity ie taking identical estimates individual generalization errors networks expression balancing generalization error weighting factors chosen minimize our estimate generalization error unfair bumping smallest generalization error individual error ie result bumping had indeed chosen network smallest generalization error unfair balancing lowest possible generalization error could obtain had perfect estimates individual generalization errors last two methods unfair bumping unfair balancing serve kind reference never used practice applied these methods real world problem concerning prediction sales figures several department stores each store networks hidden units trained stopped bootstrap samples about patterns test set performances various methods combination measured consists about patterns inputs include weather conditions day previous sales figures results summarized table give decrease generalization error relative average individual generalization error seen table bumping hardly improves performance reason error data used training stopping predictor generalization error amount overfitting generalization performance obtained through bagging ie first averaging over outputs proven always better than average individual generalization error balancing between bagging bumping number replicates number replicates figure decrease generalization error relative average individual generalization error function number bootstrap replicates different combination methods bagging star ambiguity dotted star bumping dashed star balancing solid star unfair bumping dashed circle unfair balancing solid circle shown mean left standard deviation right decrease networks trained tested boston housing database these data bagging better than bumping but worse than maximizing ambiguity cases except store maximization ambiguity slightly better balancing clear winner among methods last column table shows much better get could find more accurate estimates generalization errors individual networks method balancing most networks ie solution quadratic programming problem under constraints yields few weighting factors different zero average about set simulations balancing thus indeed compromise between bagging taking networks into bumping keeping network compared these methods well known boston housing data set concerning median housing price several based mainly economic predictor variables see eg more information left out available cases assessment generalization performance other cases used training stopping neural networks hidden units average individual mean squared error over bootstrap runs comparable mean squared error reported study performance depends number bootstrap replicates randomly sets bootstrap replicates out our ensemble replicates applied combination methods these sets each did times figure shows mean decrease generalization error relative average individual generalization error its standard deviation again balancing comes out best especially larger number bootstrap replicates seems beyond say replicates both bumping bagging hardly more runs whereas both maximization ambiguity balancing still increase their performance bagging fully taking into account network pre heskes yields smallest variation bumping keeping them far largest balancing maximization ambiguity combine several predictions thus yield variation between conclusion discussion balancing compromise between bagging bumping attempt arrive better performances regression problems obtain reasonable estimates quality different networks incorporate these estimates calculation proper weighting factors see similar ideas related work context generalization obtaining several estimators computationally expensive instability feedforward neural networks hardly leaves us choice furthermore ensemble neural networks used approximate confidence prediction intervals see eg estimate relevance input fields so has been argued combination several estimators structure may present single estimator having hardly structure neural networks do seem have lot they challenge show ensemble neural networks does give more accurate predictions but reveals more information than single network references bagging predictors machine learning
neural unit learning rules problem independent component analysis ica blind source separation introduced these new algorithms every ica neuron develops into finds independent components learning rules use simple constrained hebbian learning feedback may added speed up convergence these stochastic gradient descent rules novel computationally efficient fixed point algorithm introduced
develop recursive node elimination formalism efficiently approximating large probabilistic networks constraints set network topologies yet formalism integrated exact methods whenever they applicable approximations use controlled they maintain consistently upper lower bounds desired quantities times show boltzmann machines sigmoid belief networks combination ie chain graphs handled within same framework accuracy methods verified experimentally
obtain classification systems both good generalization performance efficiency space time propose learning method based combinations weak classifiers weak linear classifiers percepttons do little better than making random randomized algorithm proposed find weak classifiers they combined through majority demonstrated through systematic experiments method developed able obtain combinations weak classifiers good generalization performance fast training time variety test problems real applications
study time series model viewed decision tree markov temporal structure model intractable exact calculations thus utilize variational approximations consider three different distributions approximation markov calculations performed exactly layers decision tree decision tree calculations performed exactly time steps markov chain viterbi like assumption made pick out single most likely state sequence present simulation results artificial data
present paper propose method unify information maximization minimization hidden units information maximization minimization performed two different levels collective individual level thus two kinds information collective individual information defined maximizing collective information minimizing individual information simple networks generated terms number connections number hidden units obtained networks expected give better generalization improved interpretation internal representations method applied inference maximum onset principle artificial language problem shown individual information minimization collective information maximization addition experimental results confirmed improved generalization performance because over training significantly suppressed
unsupervised algorithms based convex conic proposed find closest convex conic combination basis vectors input learning algorithms produce basis vectors minimize reconstruction error convex algorithm develops locally linear models input while conic algorithm discovers features both algorithms used model handwritten digits compared vector quantization principal component analysis neural network implementations involve feedback connections project reconstruction back input layer
introduce arc new algorithm improvement ann performance measures importance patterns network output errors several artificial benchmark problems algorithm compares favorably other combine techniques
multilayer architectures such those used bayesian belief networks helmholtz machines provide powerful framework representing learning higher order statistical relations among inputs because exact probability calculations these models often intractable much interest finding approximate algorithms present algorithm efficiently discovers higher order structure using em gibbs sampling model interpreted stochastic recurrent network ambiguity lower level states resolved through feedback higher levels demonstrate performance algorithm benchmark problems
tasks source separation density estimation extracting local geometrical structure distributions obtained mixtures statistically independent sources our modifications self organizing map som algorithm results purely digital learning rules perform non parametric histogram density estimation non parametric nature separation allows source separation non linear mixtures anisotropic coupling introduced into our om role network locally independent component contours approach provides exact verification condition source separation prior source distributions
dimension reducing feature extraction neural network techniques preserve relationships data have traditionally been exclusive domain kohonen self maps recently introduced novel dimension reducing feature extraction process topographic based upon radial basis function architecture has been observed generalisation performance system broadly insensitive model order complexity other smoothing factors such kernel widths contrary intuition derived supervised neural network models paper provide effective demonstration property give theoretical justification apparent self behaviour architecture feed forward neural network topographic transformation recently important class topographic neural network based feature extraction approaches related traditional statistical methods mappings multidimensional scaling have been introduced these novel alternatives kohonen like approaches topographic feature extraction possess several interesting properties instance architecture has empirically observed property generalisation does seem depend critically model order complexity contrary intuition based upon knowledge its supervised counterparts paper presents evidence their self behaviour provides explanation terms curvature trained models now provide brief
belief network aim obtain junction tree minimum state space according searching optimal triangulation cast search over permutations graphs vertices our approach discrete set permutations convex continuous domain suitably extending cost function over solving nonlinear optimization task obtain good triangulation respect cost paper presents two ways embedding triangulation problem into continuous domain shows they perform well compared best known heuristic
combining set learned models form improved estimator issue redundancy set models addressed progression existing approaches their limitations respect redundancy discussed new approach pcr based principal components regression proposed address these limitations evaluation new approach collection domains reveals pcr most robust combination method redundancy learned models increased redundancy could handled without eliminating learned models principal components learned models provided continuum regularized weights pcr could choose
address statistical classifier design given mixed training set consisting small labelled feature set generally larger set situation arises eg medical images although training features may expensive required extract their class labels propose classifier structure learning algorithm make effective use unlabelled data improve performance learning based maximization total data likelihood ie over both labelled unlabelled data subsets two em learning algorithms proposed em formalism applied unlabelled data classifier based joint probability model features labels mixture experts structure equivalent radial basis function rbf classifier but unlike likelihood based training application new method greatly extended observation test data new data classify fact additional unlabelled combined operation much akin what done image segmentation whenever new data classify experiments data sets uc database demonstrate new learning algorithms structure achieve substantial performance gains over alternative approaches
paper propose method learning bayesian belief networks data method uses artificial neural networks probability estimators thus avoiding need making prior assumptions nature probability distributions governing relationships among variables new method has potential being applied domains containing both discrete continuous variables arbitrarily distributed compare learning performance new method performance method proposed cooper experimental results show although learning scheme based use ann estimators slower learning accuracy two methods comparable category algorithms architectures
smoothing regularizers radial basis functions have been studied extensively but general smoothing regularizers projective basis functions such widely used sigmoidal have been proposed derive new classes simple rn order smoothing regularizers networks form fw uj general basis functions these global form jl al form these regularizers bound corresponding rn order smoothing integral denotes network weights weighting function dimensional input space global local cases distinguished different choices simple algebraic forms enable direct smoothness without need monte carlo new regularizers shown yield better generalization errors than weight decay implicit assumptions latter wrong unlike weight decay new regularizers distinguish between roles input output weights capture interactions between them address centre computer architecture university moody
separation generalization error into two types bias variance bienenstock leads notion error reduction averaging over committee classifiers committee performance decreases both average error constituent classifiers increases degree correlated across committee here method reducing correlations introduced uses winner take procedure similar competitive learning drive individual networks different minima weight space respect training set such correlations generalization performance reduced thereby reducing committee error
adaptive line algorithm extending learning learning idea proposed theoretically motivated gradient flow information applied learning continuous functions distributions even explicit loss function given hessian available its efficiency demonstrated non stationary blind separation task acoustic signals
present algorithm fast stochastic gradient descent uses nonlinear adaptive momentum scheme optimize late time convergence rate algorithm makes effective use curvature information requires storage computation convergence rates close theoretical optimum demonstrate technique linear large nonlinear backprop networks improving stochastic search learning algorithms perform gradient descent cost function formulated either stochastic line batch form stochastic version takes form current weight estimate pt learning rate minus instantaneous gradient estimate xt input time obtains corresponding batch mode learning rule taking constant averaging over stochastic learning provides several advantages over batch learning large datasets batch average expensive compute stochastic learning eliminates averaging stochastic update regarded noisy estimate batch update intrinsic noise reduce likelihood poor local optima assume inputs iid achieved random sampling replacement training data using curvature information fast stochastic search noise reduced late training allow weights converge after settling within basin local optimum learning rate annealing allows convergence weight error well known expected squared weight error its maximal rate oc annealing schedule furthermore achieve rate have smallest eigenvalue hessian references finally optimal io gives lowest possible value multiple dimensions optimal learning rate matrix lt hessian local optimum incorporating curvature information into stochastic learning difficult two reasons first hessian available point stochastic learning perform averages over training data second even hessian available optimal learning requires its inverse prohibitively expensive compute primary result paper achieve algorithm behaves optimally ie had incorporated inverse full hessian without storage computational algorithm requires storage computation number weights network uses adaptive momentum parameter extending our earlier work fully non linear problems demonstrate performance several large back networks trained large datasets implementations stochastic learning typically use constant learning rate during early part training what moody call search phase obtain exponential convergence towards local optimum switch annealed learning called converge phase use adaptive search converge algorithm determine point switch annealing originally means insure during annealed phase compare its performance adaptive momentum well provide comparison conjugate gradient optimization momentum stochastic gradient descent adaptive momentum algorithm propose suggested earlier work convergence rates annealed learning constant momentum section summarize relevant results work extending include momentum leaves learning rule momentum parameter constrained so analysis dynamics expected squared weight error learning rate annealing shows late times learning proceeds algorithm without momentum but scaled effective learning rate result consistent earlier work momentum learning small constant same result holds proposed algorithm optimizing convergence rate estimates hessian time averaging finite differences gradient learning rate inverse its extension multiple dimensions would require storage time inversion both large models allow effective learning rate matrix following our comments
softassign quadratic assignment algorithm has recently emerged effective strategy variety optimization problems pattern recognition combinatorial optimization while effectiveness algorithm demonstrated thousands simulations known proof convergence here provide proof convergence most general form algorithm
paper compares three penalty terms respect efficiency supervised learning using second order learning algorithms our experiments showed reasonably adequate penalty factor combination squared penalty term second order learning algorithm drastically improves convergence performance more than times over other combinations same time about better generalization performance
hint piece side information about target function learned consider monotonicity hint states function learned monotonic input variables application monotonicity hints demonstrated two real world credit application task problem medical diagnosis measure monotonicity error candidate function defined objective function monotonicity derived bayesian principles report experimental results show using monotonicity hints leads statistically significant improvement performance both problems
present new algorithms parameter estimation hmms adapting framework used supervised learning construct iterative algorithms maximize likelihood observations while attempting stay close current estimated parameters use bound relative entropy between two hmms distance measure between them result new iterative training algorithms similar em baum welch algorithm training hmms proposed algorithms composed step similar expectation step baum welch new update parameters replaces maximization re estimation step algorithm takes more time per iteration approximated version uses same expectation step baum welch evaluate experimentally new algorithms synthetic natural speech pronunciation data sparse models ie models relatively small number non zero parameters proposed algorithms require significantly fewer iterations use numbers name states hmm state special initial state state special final state state sequence denoted starts initial state but never returns ends final state observations symbols numbers observation sequences denoted discrete output hidden markov model hmm parameterized two matrices first matrix dimension denotes probability moving state state second matrix dimension probability symbol state set parameters hmm denoted initial state distribution vector represented first row hmm probabilistic generator sequences starts initial state iteratively does following until final state reached current state next state chosen according transition probabilities out current state row matrix after arriving state symbol output according output probabilities state row matrix let px denote probability likelihood hmm generates observation sequence path starting state state px omit conditions throughout paper assume hmms absorbing every state path final state singer warmuth non zero probability similar parameter estimation algorithms derived hmms absorbing hmms induce probability over state observation sequences ie px likelihood observation sequence obtained summing over possible hidden paths state sequences px es px obtain likelihood set observations simply likelihood values individual sequences seek hmm maximizes likelihood given set observations equivalently maximizes log likelihood xx simplify our notation denote generic parameter oi ranges total number parameters might less zero denote total number parameters leave fixed correspondence between oi entries indices naturally partitioned into classes corresponding rows matrices denote class parameters belongs ill vector oj st both oi oj parameters same row two matrices whenever clear context use denote both class parameters row number ie state associated class now rewrite px so xs number times parameter used along path observation sequence note value does depend actual parameters next compute partial derivatives likelihood log likelihood using notation sl px xx xx px px px xx xi here es expected number corresponds oi over paths produce these values calculated expectation step expectation maximization em training algorithm hmms known baum welch algorithm next sections use additional following expectations io note summation here over legal arbitrary length expected number times state visited entropic distance functions hmms our training algorithms based following framework kivinen warmuth iterative updates assume have already done number iterations our current parameters assume further set observations processed current iteration batch case set never changes line case typically single observation new parameters should stay close incorporates knowledge obtained past iterations but should maximize log likelihood current date set thus instead maximizing loglikelihood maximize see further motivation training algorithms hidden markov models here measures distance between old new parameters trade off factor maximizing usually difficult both distance function loglikelihood depend approximate log likelihood first order taylor expansion around add lagrange multipliers constraints parameters each class sum commonly used distance function relative entropy calculate relative entropy between two hmms need sum over possible hidden state sequence leads following definition deal px sl es px above divergence difficult calculate convex function avoid computational difficulties non convexity upper bound relative entropy using log sum inequality px px px sl xs xs xs px sl xs px ln xs note distance function hmm viewed joint distribution between observation sequences hidden state sequences further simplify bound relative entropy using following lemma proof omitted lemma absorbing hmm gives following new formula ii eq equation still difficult solve depend new set known therefore approximate distance function ii new parameter updates now would like use functions discuss previous section first derive our mn update using function done replacing setting derivatives resulting wrt gives following set equations ai ai singer warmuth now solve ti replace ai normalization factor ensures sum parameters above re estimation rule entropic update hmms now derive alternate update mixture weights approximate original mixture weights lead state dependent learning rate parameters class computation time limited see discussion below expectations approximated values readily available possible choice use sample based expectations approximation these weights needed calculating gradient evaluated expectation step baum welch let approximation leads following distance function ix results update call approximated entropic update hmms oi exp ji oj given current set parameters learning rate obtain new set parameters iteratively evaluating right hand side entropic update approximated entropic update calculate expectations ix done expectation step baum welch weights obtained averaging fix lets us evaluate right hand side approximated entropic update entropic update slightly more involved requires additional calculation recall expected number times state visited data compute these expectations need sum over possible sequences state observation pairs probability possible symbols given state sum calculating reduces evaluating probability reaching state each possible time sequence length absorbing hmms approximated efficiently using dynamic programming compute summing probabilities legal state sequences up length cn typically proved sufficient obtain accurate approximations therefore time complexity calculating depends number states regardless dimension output vector training data subtle improvement possible over update treating transition probabilities output probabilities differently first transition probabilities updated based state probabilities based new parameters possible state probabilities depend transition probabilities output probabilities finally output probabilities updated used place il training algorithms hidden markov models relation em convergence properties first show em algorithm hmms derived using our framework do so approximate relative entropy distance see dx use distance approximate re re de il minimizing last version distance function following same derivation steps approximated entropic update arrive what call approximated update hmms xx setting results update ix maximization re estimation step em algorithm although omitted paper due lack space shown entropic updates update improve likelihood each iteration therefore these updates belong family generalized em algorithms guaranteed converge local maximum given additional conditions furthermore using analysis second order approximation likelihood function local maximum similar shown approximated update mapping close local maximum exists learning rate results faster rate convergence than using experiments ith artificial anti natural data order test actual convergence rate algorithms compare them baum welch created synthetic data using hmms our experiments mainly used sparse models models many parameters zero previous work eg might suggest entropic updates perform better sparse models indeed used dense models generate data algorithms showed almost same performance training algorithms started randomly chosen dense model comparing algorithms used same initial model due different trajectories parameter space each algorithm may converge different local maximum presentation show here results cases updates converged same maximum often occur hmm generating data sparse enough examples typically tens observations per non zero parameter tested both entropic updates updates learning rates greater than speed up convergence two entropic updates converge almost equally fast synthetic data generated hmm natural data entropic update converges slightly faster than approximated version update benefits learning rates larger than update need used does necessarily ensure non new parameters problems data generated hmm therefore used entropic updates our experiments natural data order have comparison did tune learning rate set figure give comparison entropic update approximated entropic update baum welch left figure using hmm generate random observation sequences but parameters average each vector parameters singer warmuth hmm non zero performance entropic update approximated entropic update practically same both updates clearly outperform baum welch reason performance two entropic updates same observations indeed generated hmm case approximating expectations sample based expectations seems reasonable these results suggest valuable alternative using baum welch sparse potentially biased hmm large number parameters zero instead suggest starting full model let entropic updates find relevant parameters approach demonstrated right part figure example data generated sparse hmm states possible output symbols hmms parameters nonzero three log likelihood curves given figure log likelihood achieved baum welch those parameters non zero hmm generating data initialized random non zero values other two log likelihood entropic update baum welch parameters initialized randomly curves show entropic update its inferior initialization less than iterations see horizontal line figure point requires more iterations converge compared baum welch given prior knowledge non zero parameters contrast baum welch started full model its convergence much slower than entropic update ox entropic update entropic update em baum welch iteration iteration figure comparison entropic updates baum welch next tested updates speech pronunciation data natural speech word might differently different speakers common practice construct set stochastic models order capture variability possible alternative given word problem studied previously using state merging algorithm hmms using subclass probabilistic finite automata purpose experiments discussed here compare above algorithms entropic updates but rather compare entropic updates baum welch nevertheless resulting hmm pronunciation models usually sparse typically two three phonemes have non zero output probability given state average number states practice follow states about therefore entropic updates may provide good alternative algorithms presented used instruments mit database database contains acoustic waveforms continuous speech phone labels alphabet phones constitute temporally aligned phonetic transcription words purpose building pronunciation models acoustic data ignored partitioned phonetic labels according words data data filtered partitioned so words occurring between times dataset used training evaluation according following partition each word used training data learning algorithm remaining used evaluation built each word three pronunciation models training fully connected hmm whose number states set times sample denoted models evaluated calculating training algorithms hidden markov models log likelihood averaged over different random parameter each hmm phonetic transcription each word test set table give negative log likelihood achieved test data together average number iterations needed training overall differences log likelihood small means results should interpreted nevertheless entropic update obtained highest likelihood test data while least number iterations approximated entropic update baum welch achieve similar results test data but latter requires more iterations resulting models reveals reason why entropic update achieves higher likelihood values namely does better setting irrelevant parameters zero does faster negative log likelihood iterations states nm welch entropic update table comparison entropic updates baum welch speech pronunciation data conclusions future research paper have showed framework kivinen warmuth used derive parameter updates algorithms hmms view hmm joint distribution between observation sequences hidden state sequences use bound relative entropy distance between new old parameter settings approximate relative entropy distance replace exact state expectations sample based approximation fix learning rate framework yields alternative derivation em algorithm hmms em update uses sample based estimates state expectations hard use line setting contrast line versions our updates easily derived using observation sequence time alternative gradient descent based methods estimating parameters hmms such methods usually employ exponential parameterization such soft max parameters see case learning set mixture coefficients exponential parameterization led algorithm slower convergence rate compared algorithms derived using entropic distances clear whether still case hmms our future goals perform comparative study different updates emphasis line versions acknowledgments thank showing us simple derivative calculations used paper thank interesting discussions references smooth line learning algorithms hidden markov models neural computation le baum statistical inference probabilistic finite state mathematical cover elements ap nm maximum likelihood incomplete data via em algorithm journal statistical society schapire singer warmuth comparison new old algorithms mixture estimation problem learning theory pages kivinen warmuth exponentiated gradient versus gradient descent linear predictors computation appear
paper discusses probabilistic model based approach clustering sequences using hidden markov models hmms problem generalization standard mixture model approach clustering feature space two primary issues addressed first novel parameter initialization procedure proposed second more difficult problem determining number clusters data investigated experimental results indicate proposed techniques useful hidden cluster structure data sets sequences
algorithm described article based obs algorithm main disadvantage obs its high complexity obs needs calculate inverse hessian weight thus much time prune big net better algorithm should use matrix remove more than weight because calculating inverse hessian takes most time obs algorithm algorithm called unit obs described article method overcome disadvantage algorithm needs calculate inverse hessian once remove whole unit thus drastically reducing time prune big nets further advantage unit obs used do feature extraction input data helpful understanding unknown problems
seek analyze two factors call style content underlying set observations fit training data bilinear models explicitly represent two factor structure these models adapt easily during testing new content allowing us solve three general tasks extrapolation new style unobserved content classification content observed new style translation new content observed new style classification bilinear models probabilistic framework separable mixture models generalizes earlier work mixture models significant performance improvement benchmark speech dataset shows benefits our approach
optimal brain damage obd method reducing number weights neural network obd estimates increase cost function weights pruned valid approximation learning algorithm has converged into local minimum other hand often desirable learning process before local minimum reached early stopping paper show obd estimates increase cost function network local minimum show obd extended such used connection early stopping call new approach early brain damage allows already pruned weights demonstrate improvements achieved using three publicly available data sets
pouget present theoretical framework population codes generalizes naturally important case population provides information about whole probability distribution over underlying quantity rather than single value use framework analyze two existing models suggest evaluate third model encoding such probability distributions
two dimensional image motion detection neural networks have been implemented using general purpose analog neural computer neural circuits perform spatiotemporal feature extraction based cortical motion detection model neural computer provides neurons synapses synaptic time constants required realize model vlsi hardware results show visual motion estimation implemented simple sum neural hardware temporal computational capabilities neural circuits compute general visual motion real time
many popular learning rules formulated terms continuous analog inputs outputs biological systems use action potentials digital amplitude events encode analog information inter event interval action potential representations now being used advantage neuromorphic vlsi systems well report simple learning rule based equation described kohonen modified action potential neuronal outputs demonstrate learning rule analog vlsi chip uses storage synaptic weights show our time dependent learning rule sufficient achieve approximate weight normalization detect temporal correlations spike trains spike based learning neuron analog vlsi
use constant statistics constraint array sensors contains gain offset variations algorithm has been mapped analog hardware designed fabricated cmos technology measured results chip show system achieves invariance gain offset variations input signal
dimensional visual tracking chip has been implemented using neuromorphic analog vlsi techniques model selective visual attention control saccadic smooth pursuit eye movements chip incorporates focal plane processing compute image saliency winner take circuit select feature tracking target position direction motion reported target moves across array demonstrate its functionality closed loop system performs saccadic smooth pursuit tracking movements using dimensional mechanical eye
major problem has practical application analog neuro has been poor accuracy due analog device characteristics inherent each device result paper proposes dynamic control architecture allows analog silicon neural networks compensate device characteristics adapt change input level have applied architecture compensate input offset voltages analog cmos wta winner take chip have fabricated experimental data show effectiveness architecture
describe implementation hidden markov model state decoding system component speech recognition system key specification state decoder design power dissipation requirement led analog circuit implementation characterize operation word state state decoder test chip
propose neuromorphic architecture real time processing acoustic transients analog vlsi show normalization time frequency signal allows elegant robust implementation correlation algorithm algorithm uses binary instead analog analog multiplication removes need analog storage analog multiplication simulations show resulting algorithm has same out sample classification performance correct baseline template matching algorithm
detection amplitude modulation major step determination pitch sound article present silicon model uses spiking neurons extract fundamental frequency sound based observation so called mammalian cochlear nucleus well certain rates amplitude modulation depending cells intrinsic frequency our silicon model uses three different circuits ie artificial cochlea inner hair cell circuit spiking neuron circuit
humans use visual well auditory speech signals recognize spoken words variety systems have been investigated performing task main purpose research systematically compare performance range dynamic visual features task have found normalization images eliminate variation due translation scale planar rotation yielded substantial improvements generalization performance regardless visual representation used addition dynamic information difference between successive frames yielded better performance than optical flow based approaches compression local low pass filtering surprisingly better than global principal components analysis pca these results examined possible explanations explored
paper discusses fairly general adaptation algorithm standard neural network increase its recognition accuracy specific user basis algorithm output neural network characteristic input even output incorrect exploit characteristic output using output adaptation module maps output into correct user dependent confidence vector simplified resource network constructs radial basis functions line applied construct writer adaptive character recognition system line characters decreases word error rate test set average while creating basis functions each writer test set
paper presents new approach speech recognition hybrid technology while standard approach hybrid systems based use neural networks posterior probability estimators new approach based use mutual information neural networks trained special learning algorithm order maximize mutual information between input classes network its resulting sequence firing output neurons during training shown paper such neural network optimal neural vector quantizer discrete hidden markov model system trained maximum likelihood principles main advantages approach fact such neural networks easily combined hmms complexity context dependent capabilities shown resulting hybrid system achieves high recognition rates now already same level best conventional hmm systems continuous parameters capabilities mutual information neural networks yet entirely exploited
time series prediction major applications neural networks after short
reduce computational complexity classification systems using tangent distance et al developed algorithm devise rich models representing large subsets data computes automatically best associated tangent subspace proposed discriminant modular classification system based several multilayer percepttons use tangent distance error reconstruction measure propose gradient based constructive learning algorithm building tangent subspace model discriminant capabilities combines several advantages both devised tangent models hold discriminant capabilities space requirements improved respect our algorithm discriminant thus needs fewer prototype models dimension tangent subspace determined automatically constructive algorithm our algorithm able learn new transformations
prediction estimation smoothing fundamental signal processing perform these tasks given noisy data form time series model process generates data taking noise system explicitly into account kalman discussed involve dual process estimating both model parameters underlying state system review several established methods linear case propose several extensions utilizing dual kalman filters forward backward filters applicable neural networks methods compared several simulations noisy time series include example nonlinear noise reduction speech
paper investigates number ensemble methods improving performance phoneme classification use speech recognition system two ensemble methods described boosting mixtures experts both combination results presented two speech recognition databases isolated word database large vocabulary continuous speech database these results show principled ensemble methods such boosting mixtures provide superior performance more naive ensemble methods such averaging
future rd ca have combined artificial neural network ann character classifier context driven search over character segmentation word segmentation word recognition hypotheses provide robust recognition hand printed english text new models computers present training use anns character classifiers word recognition including normalized output error frequency balancing error emphasis negative training stroke warping reducing priori biases emerges discussed
field has suggested neurons line edge found primary visual cortex cats monkeys form sparse distributed representation natural scenes has such responses should emerge unsupervised learning algorithm attempts find factorial code independent visual features show here non linear infomax applied ensemble natural scenes produces sets visual filters localised oriented these filters gabor like resemble those produced sparseness maximisation network olshausen field addition outputs these filters independent possible infomax network able perform independent components analysis ica compare resulting ica filters their associated basis functions other filters produced principal components analysis pca zero phase filters ica filters have more distributed outputs natural scenes they resemble receptive fields simple cells visual cortex suggests these neurons form information theoretic co system images
paper describes new technique object recognition based learning appearance models image decomposed into local regions described new texture representation called generalized second moments derived output multiscale filter banks class characteristic local texture features their global composition learned hierarchical mixture experts architecture jordan jacobs technique applied vehicle database consisting general car categories van back van without back old difficult problem considerable class variation new technique has misclassification rate compared images give misclassification rate nearest neighbors give misclassification rate
paper propose model lateral connectivity orientation selective cells visual cortex based considerations study properties input signal visual cortex find new statistical structures have been processed geniculate pathway applying idea system optimizes representation incoming signals derive lateral connectivity achieve set local orientation selective patches well complete spatial structure layer such patches compare results various physiological measurements
study spatiotemporal correlation natural time varying images explore hypothesis visual system concerned optimal coding visual representation through spatiotemporal decorrelation input signal based measured spatiotemporal power spectrum transform needed input signal derived analytically compared actual processing observed psychophysical experiments
local disparity information often sparse noisy creates two demands estimating disparity image region need spatially average get accurate estimate problem averaging over discontinuities have developed network model disparity estimation based neurons such those found early stages processing visual cortex model accurately estimate multiple region may caused transparency occlusion real images random dot stereograms use selection mechanism selectively integrate reliable local disparity estimates results superior performance compared standard back propagation cross correlation approaches addition representations learned selection mechanism consistent recent neurophysiological results friedman poggio cells cortical visual area combining multi scale biologically plausible image processing power mixture experts learning algorithm represents promising approach yields both high performance new insights into visual system function selective integration model disparity estimation
self organizing architecture developed image region classification system consists utilizes multiscale filtering competition cooperation diffusion compute vector image boundary surface properties notably texture properties vector inputs system incrementally learns noisy multidimensional mappings their probabilities architecture applied difficult real world image classification problems including classification synthetic aperture radar natural texture images outperforms recent state art system classifying natural textures
paper describes early visual process contour using em algorithm underlying computational representation based fine spline according our em approach adjustment spline parameters iterative weighted least squares fitting process expectation step our em procedure computes likelihood data using mixture model defined over set spline these splines limited their spatial extent using gaussian functions maximisation likelihood leads set linear equations spline parameters solve weighted least squares problem evaluate technique road structures red images
simple mathematical model large scale circuitry primary visual cortex introduced shown basic cortical architecture recurrent local excitation lateral inhibition account quantitatively such properties orientation tuning model account such local effects cross orientation suppression shown state dependent coupling between similar orientation patches added model reproduce such effects non local orientation suppression non local enhancement following account given perceptual phenomena involving object segmentation such direct indirect
compare generalization performance three distinct representation schemes facial using single classification strategy neural network face images presented represented full face projections dataset onto their eigenvectors similar projection constrained eye mouth areas finally projection eye mouth areas onto eigenvectors obtained random image patches dataset latter system achieves generalization novel face images individuals networks trained drawn database human subjects consistently identify single face
have investigated possibility rapid processing visual system could achieved using order firing different code rather than more conventional firing rate schemes using neural net simulator based integrate fire input layer function delay have modeled initial stages visual processing initial results extremely promising even activity retinal output cells limited spike per neuron per image effectively out form rate coding sophisticated processing based asynchronous activation possible
central computational vision research has been realization reliable estimation local scene properties requires propagating measurements across image many authors have therefore suggested solving vision problems using architectures locally connected units updating their activity parallel unfortunately convergence traditional relaxation methods such architectures has proven slow general they do guarantee stable point global minimum paper show architecture bayesian beliefs about image properties propagated between neighboring units yields convergence times several orders magnitude faster than traditional methods avoids local minima particular our architecture non iterative sense marr every time step local estimates given location optimal given information has already been propagated location illustrate algorithms performance real images compare several existing methods theory our approach shown figure figure la shows prototypical ill posed problem interpolation function sparse data figure lb shows traditional relaxation approach problem dense array units represents value interpolated function sampled points activity unit updated based local data those points data available activity neighboring points discussed below local update rule interpreting images propagating bayesian beliefs figure prototypical ill posed problem traditional relaxation approach dense array units represent value interpolated function units update their activity based local information activity neighboring units bayesian belief propagation bbp approach units transmit probabilities combine them according probability calculus two non interacting streams defined such network converges state activity each unit corresponds value globally optimal interpolating function figure shows bayesian belief propagation bbp approach problem traditional approach function represented activity dense array units units transmit probabilities rather than single estimates their neighbors combine probabilities according probability calculus formalize above discussion let represent activity unit location let noisy samples true function typical interpolation problem would minimize yi have defined grid points data points data quadratic local update direction gradient converge optimal estimate yields updates sort relaxation algorithms differ their choice corresponds relaxation corresponds successive over relaxation sor method choice such problems derive bbp update rule problem note minimizing equivalent maximizing posterior probability given assuming following generative model yi rl ratio plays role similar original cost functional advantage considering cost functional posterior enables us use methods hidden markov models bayesian belief nets optimal estimation derive local update rules cf denote posterior markovian property allows us factor into three terms depending local data another depending data left third depending data right thus denotes normalizing constant now conditional weiss written terms denotes another normalizing constant symmetric equation written suggests propagation scheme units represent probabilities given left hand side equations updates right hand side ie activities neighboring units specifically gaussian generating process probabilities represented their mean variance thus denote pi similarly performing integration gives kalman filter like update parameters wi he update rules he parameters analogous so far have considered continuous estimation problems bu identical issues arise labeling problems he estimate label discrete values denote he label value zero otherwise typically minimizes form relation labeling algorithms minimize his cost he form again labeling differ choice linear sum followed gives discrete hopfield network updates linear sum followed threshold gives he continuous mean field hop field updates nd yet another form gives relaxation labeling algorithm see review relaxation labeling methods derive bbp algorithm ce cn again rewrite posterior markov generating process process gives he same expressions equations he integral replaced linear sum he probabilities here gaussian he pi represented mean nd but rather vector length thus he update rule similarly mi sequence labels minimizes those should do rather than propagation interpreting images propagating bayesian beliefs figure first frame sequence hand translated left contour extracted using standard methods convergence equations mathematical identities hence possible show after iterations activity units pi converge correct posteriors maximal distance between two units architecture iteration refers update units furthermore have been able show after iterations activity unit pi guaranteed represent probability hidden state location given data within distance guarantee significant light distinction made regarding local propagation rules scheme units communicate their neighbors obvious limit fast information reach given unit ie after iterations unit know about information within distance thus minimal number iterations required data reach units distinguished between two types iterations those needed allow information reach units versus those used refine estimate based information has already arrived significance guarantee pi shows bbp uses first type iteration iterations used allow more information reach units once information has arrived pi represents correct posterior given information further iterations needed refine estimate moreover have been able show schemes do propagate probabilities such those equations general represent optimal estimate given information has already arrived summarize both traditional relaxation updates equation bbp updates equations give simple rules updating units activity based local data activities neighboring units fact bbp updates based probability calculus guarantees units activity optimal given information has already arrived gives rise qualitative difference between convergence these two types schemes next section demonstrate difference image interpretation problems results figure shows first frame sequence hand translated left figure shows bounding contour hand extracted using standard techniques motion propagation along contours local measurements along contour insufficient determine motion hildreth suggested overcome local ambiguity minimizing following weiss figure local estimate velocity along contour performance sor gradient descent bbp function time bbp converges orders magnitude faster than sor motion estimate sor after iterations motion estimate bbp after iterations cost functional dt dx dt denote spatial temporal image derivatives denotes velocity point along contour functional analogous interpolation functional eq derivation relaxation bbp updates analogous figure shows estimate motion based solely local information estimates wrong due aperture problem figure shows performance three propagation schemes gradient descent sor bbp gradient descent converges so slowly improvement its estimate plot sor converges much faster than gradient descent but still has significant error after iterations bbp gets correct estimate after iterations here subsequent iteration refers update units network due fact after iterations estimate location optimal given data interval case enough data every such interval along contour correctly estimate motion figure shows estimate produced sor after iterations even simple visual estimate quite wrong figure shows correct estimate produced bbp after iterations direction figure propagation extracted contour figure bounds dark light region direction figure dof eg refers these two regions figure ground local cue dof convexity given three neighboring points along contour dof makes angle defined those points interpreting images propagating bayesian beliefs figure local estimate dof along contour performance descent relaxation labeling bbp function time bbp method converges global minimum dof estimate hopfield net after convergence dof estimate bbp after convergence rather than figure shows results using local cue hand contour local cue sufficient overcome local ambiguity minimizing cost functional takes into account dof neighboring points addition local convexity denote dof point along contour define jl rn determined angle location figure shows performance four propagation algorithms task three traditional relaxation labeling algorithms hop field et al constrained gradient descent bbp three traditional algorithms converge local minimum while bbp converges global minimum figure shows local minimum reached hop field network figure shows correct solution reached bbp algorithm recall section bbp guaranteed converge correct posterior given data extensions previous two examples ambiguity reduced combining information other points same contour exist cases information should propagated points image unfortunately such propagation problems correspond markov random field mrf generative models calculation posterior cannot done efficiently his weiss colleagues have recently shown mrfs approximated hierarchical multi resolution models current work have been using multi resolution generative model derive local bbp rules case bayesian beliefs propagated between neighboring units pyramidal representation image although work still preliminary stages find encouraging results comparison traditional relaxation schemes discussion update rules equations differ slightly those derived pearl quantities conditional probabilities hence normalized sum unity using original algorithm sequences long ones considering lead messages become small our update rules differ slightly forward backward algorithm hmms ours based assumption states equally likely hence updates symmetric finally equation seen variant equation addition these minor differences context use update rules different while hmms kalman filters updates seen calculations toward calculating posterior use these updates parallel network local units interested estimates units network improve function iteration have shown architecture bayesian beliefs according probability calculus yields orders magnitude improvements convergence over traditional schemes do propagate probabilities thus image interpretation provides important example task bayesian acknowledgments thank dayan comments versions mi jordan discussions introducing me bayesian nets supported training grant references applied optimal estimation mit press hildreth measurement visual motion mit press li markov random field modeling computer vision mark efficient multiscale regularization application computation optical flow ieee transactions image processing vision freeman co pearl probabilistic reasoning intelligent systems networks plausible inference morgan kaufmann speech recognition scene labeling relaxation operations ieee transactions systems intermediate level visual representations construction surface perception journal cognitive neuroscience
has been suggested long range intrinsic connections striate cortex may play role contour extraction et al number physiological psychophysical studies have examined possible role long range connections modulation contrast detection thresholds et al various pre detection tasks field et al have developed network based anatomical connectivity striate cortex well temporal dynamics neuronal processing able reproduce observed experimental results network has been tested real images has applications terms identifying salient contours automatic image processing systems
present algorithm identifying linear patterns twodimensional lattice based concept orientation selective cell concept neurobiology vision constructing multi layered neural network fixed architecture implements orientation selectivity define output elements corresponding different orientations allow us make selection decision algorithm takes into account lattice well presence noise method applied sample data collected detector order identify leave linear pattern signals segmented two dimensional representation relevant part detector used algorithm performs well given its architecture system becomes good candidate fast pattern recognition parallel processing devices
paper presents method combinations traffic accepted packet data link so quality service constraints met method uses samples results different load conditions build neural network decision function previous similar approaches problem have significant bias bias likely occur real system results targets orders magnitude preprocessing data either remove bias provide confidence level method applied sources based difficult analyze data traces data method produces accurate access control function dramatically outperforms analytic alternatives interestingly results depend away more than data
artificial neural networks used predict future returns stocks order take financial decisions should build separate network each stock share same network stocks paper explore other alternatives layers shared others shared prediction future returns different stocks viewed different tasks sharing parameters across stocks form multi task learning series experiments stocks obtain returns more than above various benchmarks
paper shows option financial markets sequentially means extended kalman filter algorithm consider call put option pairs identical price time two output nonlinear system approach popular literature radial basis functions neural network used modelling nonlinear system generating these observations show both these systems may identified recursively using ekf algorithm present results simulations index options data discuss implications viewing problem sequential manner
data traditionally analyzed simple techniques flexible models such neural networks have potential discover features data useful flexible models have effective control overfitting paper reports comparative study predictive quality neural networks other flexible models applied real artificial data results suggest major complex features real data demonstrate bayesian neural network methodology provides effective control overfitting while retaining ability discover complex features artificial data
related cancer substantially reduced through early detection treatment current detection techniques such fail achieve concurrently high sensitivity vivo technique quickly quantitatively probes biochemical changes occur pre tissue rbf ensemble algorithms based such spectra provide automated near realtime implementation pre cancer detection results more reliable direct accurate than those achieved either human experts multivariate statistical algorithms
present mixture experts me approach interpolate sparse spatially correlated science data interpolation method uses global model estimated data take account spatial dependence data based close relationship between radial basis function rbf network use mixture generalized rbf networks partition input space into statistically correlated regions learn local model data each region applying me approach simulated real world data show able achieve good partitioning input space learn local models improve generalization
high frequency exchange data decomposed into three components effect component information news component regular information component presence effect news make analysis trends due diffusion information regular information component difficult propose neural net based independent component analysis separate high frequency exchange data into these three components our empirical results show our proposed multi effect decomposition reveal intrinsic price behavior
dynamic programming learning other discrete markov decision process applied continuous dimensional state spaces state space into array boxes often problematic above two dimensions coarse quantization lead poor policies fine quantization expensive possible solutions variable resolution discretization function approximation neural nets third option has been little studied learning literature interpolation coarse grid paper study interpolation techniques result vast improvements online behavior resulting control systems multilinear interpolation interpolation algorithm based interesting regular triangulation dimensional space adapt these under three reinforcement learning paradigms offline value iteration known model ii learning iii online value iteration previously unknown model learned data describe empirical results resulting implications practical learning continuous non linear dynamic control grid based interpolation techniques reinforcement learning algorithms generate functions map states cost values dealing continuous state spaces these functions approximated following approximators frequently used fine grids may used two dimensions above two dimensions fine grids expensive value functions discontinuous see lead even fine discretization two dimensions neural nets have been used conjunction td sutton learning watkins high dimensional spaces tesauro barto while promising always clear they produce accurate value functions might needed fine control dynamic systems most commonly used methods applying value iteration policy iteration neural net value function often unstable boyan moore interpolation over points coarse grid another potentially useful approximator value functions has been little studied reinforcement learning paper attempts interpolation schemes may particularly attractive because they local convergence has been proven such cases offline value iteration gordon interpolation methods discussed here split state space into regular grid dimensional boxes data points associated centers corners resulting boxes value given point continuous state space computed weighted average neighboring data points multilinear interpolation using multilinear interpolation data points corners grids boxes interpolated value within box appropriately weighted average datapoints corners weighting scheme global continuity interpolated surface guarantees interpolated value grid corner matches given value corner dimensional space multilinear interpolation simply involves piecewise linear between data points higher dimensional space recursive though efficient implementation described follows pick arbitrary axis project query point along axis each two opposite faces box containing query point use two dimensional over datapoints each these two faces calculate values both these projected points linearly interpolate between two values generated previous step multilinear interpolation processes data points every query becomes prohibitively expensive increases simplex based interpolation possible interpolate over data points given query time still achieve continuous surface fits datapoints exactly each box broken into according kuhn triangulation moore assume box unit hypercube corner xx opposite corner each simplex kuhn triangulation corresponds possible set points satisfying equation each box into manner generates two elements dimensional surface common have entire faces common ensures continuity across element boundaries interpolating use kuhn triangulation interpolation follows scale coordinate system box containing query point unit hypercube let new coordinate query point us simplex use sorting algorithm rank through kuhn triangulation query point triangulation interpolation reinforcement learning express cc convex combination coordinates relevant corners use coefficients determined previous step weights weighted sum data values stored corresponding corners point do explicitly represent different above steps performed time except second done log time using conventional sorting problem domains car hill hillcar domain goal car near top dimensional hill hill enough needs back up order enough speed get goal state space two dimensional see moore atkeson further details but note our formulation harder than usual formulation goal region restricted narrow range velocities around trials start random states task specified reward action taken outside goal region inside goal used two actions available maximum maximum acrobot acrobot two link planar robot acting vertical plane under weak its joint joint goal hand least links above sutton state space four dimensional two angular positions two angular velocities trials always start stationary position straight down task formulated same way car actions allowed two extreme applying interpolation three cases case offline value iteration known model first effect taking each possible action each state corresponding grid suggested gordon use these calculations derive completely discrete mdp taking action state mdp results possible states number datapoints used per interpolation without interpolation multilinear interpolation simplex based interpolation calculate optimal policy derived mdp offline using value iteration because value iteration performed completely discrete mdp calculations much less computationally expensive than they would have been many other kinds function approximators value iteration gives us values datapoints our grid may use interpolate values other states during online control hillcar results value iteration known model tested two interpolation methods variety quantization levels first performing value iteration offline starting car random states averaging number steps taken goal those states recorded number backups required before convergence well execution time required entire value iteration mhz see figure results steps goal values means expected error steps grid size interpolation method none steps goal backups time sec steps goal backups time sec simplex steps goal backups time sec figure hillcar value iteration known model grid size interpolation method none steps goal backups time sec steps goal backups time sec simplex steps goal backups time sec figure acrobot value iteration known model interpolated functions require more backups convergence but dramatic improvement policy surprisingly both interpolation methods provide improvements even extremely high grid resolutions grid datapoints along each axis better than interpolated grids datapoints along each axis acrobot results value iteration known model used same value iteration algorithm acrobot domain case our test trials always same start state but ran tests larger set grid sizes figure grids different resolutions place grid cell boundaries different locations these boundary locations appear important problem performance varies grid resolution changes cases interpolation necessary arrive solution without interpolation value iteration often failed converge relatively coarse grids may trajectory goal passes through grid box more than once would immediately algorithm constant value over entire grid box controllers using multilinear interpolation consistently better than those employing simplex based interpolation smoother value function provided multilinear interpolation seems help value iteration interpolation about twice fast multilinear interpolation higher dimensions speed ratio increase triangulation interpolation reinforcement learning case ii learning under second reinforcement learning paradigm do use model rather learn function directly maps state action pairs long term rewards watkins does interpolation help here implementation exploration function zero everywhere after sufficient distance our last decision point perform single changing grid point values according perceptron like update rule select action interpolated function highest current state hillcar results learning used learning grid size figure shows learning curves three learners using three different interpolation techniques both interpolation methods provided significant improvement both initial final online performance learner without interpolation achieved final average performance about steps goal multilinear interpolation simplex based interpolation note these significant improvements over corresponding results value iteration known model interpolated functions often cause controllers enter cycles because learning backups being performed online learning controller escape these control cycles values such cycles acrobot results learning used same algorithms acrobot domain grid size results shown figure figure left cumulative performance learning hillcar grid multilinear interpolation comes out top interpolation bottom right learning acrobot grid two come out top nearly identical performance each learner axis shows sum rewards trials date better average performance gradient gradients always negative because each state transition before reaching goal results reward both learners using interpolation improved rapidly eventually reached goal relatively small number steps per trial learner using multilinear interpolation eventually achieved average steps goal per trial learner using simplex based interpolation achieved steps per trial other hand learner using interpolation much worse taking average more than steps per trial controller chooses actions randomly typically takes about same number steps reach goal simplex based interpolation provided line performance close provided multilinear interpolation but roughly half computational cost case iii value iteration model learning here use mode system but do assume have start instead model system interact assume model adequate calculate value function via same algorithms would use true mode approach may particularly tasks data expensive computation cheap here models learned using simple grid based function approximators without interpolation both reward transition functions model same grid resolution used value function grid model approximator strongly exploration model so every state initially assumed absorbing state zero reward while making transitions through state space update model use prioritized sweeping moore atkeson concentrate backups relevant parts state space stop effects actions under updated model run value iteration convergence fairly time consuming done rather rarely rely updates performed prioritized sweeping guide system figure left cumulative performance model learning hillcar grid right acrobot grid both cases multilinear interpolation comes out top while interpolation up bottom hillcar results value iteration learned model used algorithm described above grid average about two prioritized sweeping backups performed per transition complete performed every steps throughout first two every steps figure shows results first trials over first learner using simplex based interpolation much better than learner using interpolation its performance shown close learner using multilinear interpolation taking average steps goal per while learner using multilinear interpolation took learner using interpolation did significantly worse than others these later trials taking steps per trial triangulation interpolation reinforcement learning model learners performance improved more quickly than learners over first few trials other hand their final performance significantly worse learners acrobot results value iteration learned model used same algorithm grid acrobot domain time performing complete every steps through first two trials every figure shows results case learner using interpolation took so much time per trial experiment early after trials still taking average more than steps reach goal learners using interpolation much better learner using multilinear interpolation converged solution taking steps per trial learner using simplex based interpolation averaged about steps again graphs show these three learners initially improve significantly faster than did learners using similar grid sizes conclusions have shown two interpolation schemes based weighted average points square cell other triangulation may used three reinforcement learning paradigms optimal policy computation known model learning online value iteration while learning model each case our empirical studies demonstrate interpolation decreasing quantization level necessary solution future extensions research explore use variable resolution grids multiple low dimensional place high dimension interpolation manner reminiscent cmac memory based approximators more intelligent exploration research part national science foundation research moore references hill boyan moore boyan moore generalization reinforcement learning approximating value function neural information processing systems barto barto improving elevator performance using reinforcement learning touretzky mozer editors neural information processing systems gordon gordon stable function approximation dynamic programming proceedings th international conference machine learning morgan kaufmann moore atkeson moore atkeson prioritized sweeping reinforcement learning less data less real time learning moore atkeson moore atkeson game algorithm variable resolution reinforcement learning multidimensional state spaces machine learning moore moore generation applications report university
new reinforcement learning architecture nonlinear control proposed direct feedback controller actor trained value gradient based controller architecture enables both efficient use value function simple computation real time implementation good performance verified multi dimensional nonlinear control tasks using gaussian softmax networks
general procedures determining bayes optimal adaptive controls markov decision processes mdps require amount computation optimal learning problem intractable paper proposes approximate approach bandit processes used model certain local sense given mdp bandit processes constitute important subclass mdps have optimal learning strategies defined terms indices computed relatively efficiently thus scheme achieving approximately optimal learning general mdps proceeds taking actions suggested strategies optimal respect local bandit models
closed loop control relies sensory feedback usually assumed free but sensing cost may take sequences actions open loop mode describe reinforcement learning algorithm learns combine open loop closed loop control sensing cost although assume reliable sensors use open loop control means actions sometimes taken current state controlled system uncertain special case hidden state problem reinforcement learning cope our algorithm relies short term memory main result paper rule significantly limits exploration possible memory states pruning memory states estimated value information greater than its cost prove rule allows convergence optimal policy
reinforcement learning methods discrete semi markov decision problems such real time dynamic programming generalized controlled diffusion processes optimal control problem reduces boundary value problem fully nonlinear second order differential equation bellman type numerical analysis provides methods kind equation case learning control systems equations various grid levels obtained using observed information transitions local cost ensure consistency special attention needs directed toward type time space discretization during observation algorithm multi grid observation proposed multi grid algorithm demonstrated simple problem
now widely accepted learning task scratch ie without prior knowledge humans rarely attempt learn scratch they extract initial biases well strategies approach learning problem instructions andor other humans learning control paper investigates learning demonstration applied context reinforcement learning consider priming function value function policy model task dynamics possible areas speed up learning general nonlinear learning problems model based reinforcement learning shows significant speed up after demonstration while special case linear quadratic problems methods demonstration implementation pole balancing complex robot arm demonstrate facing complexities real signal processing model based reinforcement learning offers most robustness problems using suggested methods robot learns pole balancing single trial after second long demonstration human
model learning combined dynamic programming has been shown effective learning control continuous state dynamic systems simplest method assumes learned model correct applies dynamic programming but many approximators provide uncertainty estimates fit they exploited paper addresses case system having catastrophic during learning propose new algorithm adapted dual control literature use bayesian locally weighted regression models dynamic programming common reinforcement learning assumption exploration should paper addresses case system has exploration algorithm illustrated dimensional simulated control problem
have calculated analytical expressions bias variance estimators provided various temporal difference value estimation algorithms change offline updates over trials absorbing markov chains using lookup table representations illustrate classes learning curve behavior various chains show manner td sensitive choice its eligibility trace parameters
probability models used predict outcomes compensate missing data but even perfect model cannot used make decisions unless utility outcomes preferences between them provided arises many real world problems such medical diagnosis cost test well expected improvement outcome considered relatively little work has been done learning outcomes optimal decision making paper show temporal difference reinforcement learning td used determine decision theoretic within context mixture model apply new approach problem medical diagnosis learning reduces number tests have done achieve same level performance compared probability model alone results significant cost savings increased efficiency
present monte carlo simulation algorithm real time policy improvement adaptive controller monte carlo simulation long term expected reward each possible action statistically measured using initial policy make decisions each step simulation action maximizing measured expected reward taken resulting improved policy our algorithm easily has been implemented parallel have obtained promising initial results applying algorithm domain backgammon results reported wide variety initial policies ranging random policy td extremely strong multi layer neural network each case monte carlo algorithm gives substantial reduction much factor more error rate base algorithm potentially useful many other adaptive control applications possible simulate environment
present new results about temporal difference learning algorithm applied approximating cost go function markov chain using linear function approximators algorithm analyze performs line updating parameter vector during single trajectory finite state markov chain results include convergence probability characterization limit convergence bound resulting approximation error addition establishing new stronger results than those previously available our analysis based new line reasoning provides new intuition about dynamics temporal difference learning furthermore discuss implications two examples significance line updating linearly parameterized function approximators
propose analyze algorithm approximates solutions problem optimal stopping discounted markov chain scheme involves use linear combinations fixed basis functions approximate function weights linear combination incrementally updated through iterative process similar learning involving simulation underlying markov chain due space limitations provide proof convergence probability bounds approximation error first theoretical result establishes algorithm combined arbitrary linear function approximators solve sequential decision problem though paper focuses case finite state spaces results extend naturally continuous unbounded state spaces addressed full length paper
have developed neural network architecture implements theory attention learning trans cortical communication based adaptive synchronization hz hz oscillations between cortical areas here present specific higher order cortical model attentional networks rhythmic interaction primary cortical levels processing accounts mismatch auditory erp results psychological experiments jones showing auditory stream segregation depends rhythmic structure inputs timing mechanisms model allow us explain relative timing information such relative order events between streams lost streams formed model suggests theories auditory perception attention jones may
novel neural network model pre attention processing tasks presented using displays line orientations taken experiments study hypothesis distinction between parallel versus serial processes arises global information internal representations visual scene model operates two phases first visual displays compressed via principal component analysis second compressed data processed target detector module order identify existence target display our main finding targets displays found experimentally processed parallel detected system while targets experimentally serial displays cannot fundamental difference explained via variance analysis compressed representations providing numerical criterion distinguishing parallel serial displays our model yields mapping response time similar search surface providing explicit formulation their intuitive notion feature similarity presents neural realization processing may classical explanations visual search parallel versus serial processing computational study visual search
human subjects known adapt their motor behavior shift visual field brought about over their eyes have studied analog effect speech using device feed back transformed speech signals real time exposed subjects their own speech feedback found speakers learn adjust their production vowel compensate feedback change perceived phonetic identity moreover effect generalizes across consonant contexts different
singular value decomposition svd viewed method unsupervised training network two classes events linear connections through single hidden layer svd used learn represent relations among large numbers words large numbers natural text they result dimensional semantic spaces trained newly added word could represented vector similarities measured cosine contained angle between vectors good accuracy simulating human behaviors has been demonstrated performance multiple choice vocabulary domain knowledge tests expert evaluations several other ways examples given kind knowledge extracted method applied
motivated findings modular structure association cortex study multi modular model associative memory successfully store memory patterns different levels activity show segregation synaptic conductances into intra modular linear inter modular nonlinear ones considerably enhances networks memory retrieval performance compared conventional single module associative memory network multi modular network has two main advantages less damage input its response consistent cognitive data category specific
dual route connectionist single route models reading have been over claims correct explanation reading process recent dual route models predict subjects should show increased naming latency irregular words earlier word eg slower than prediction has been confirmed human experiments would appear effect left right reading process claim single route parallel connectionist models cannot account claim presented here consisting network models do show interaction along neighborhood statistics explain effect
accounts often damage specific functional pathway brain farah has proposed alternative class explanations involving partial damage multiple pathways explore explanation optic aphasia disorder severe performance observed patients asked name visually presented objects but surprisingly performance relatively normal naming objects auditory cues gesturing appropriate use visually presented objects model highly specific deficit through partial damage two pathways maps visual input semantics other maps semantics naming responses effect damage superadditive meaning tasks require pathway other show little performance deficit but damage task requires both pathways ie naming visually presented objects our model explains other phenomena associated optic aphasia makes testable experimental predictions study cognition resulting damage functional systems brain generally accounts neuropsychological damage particular functional system between systems farah suggested alternative class explanations neuropsychological partial damage multiple systems through interactions among damage explore explanation neuropsychological disorder optic aphasia optic aphasia arising left posterior lesions including cortex corpus marked deficit naming visually presented objects referred visual naming farah patients demonstrate recognition visually presented objects example gesturing appropriate use object sorting visual items into their proper categories visual gesturing patients name objects cues such definition typical sounds made objects auditory naming highly specific nature deficit rules out explanation terms damage single pathway standard model visual naming figure suggesting more complex model required involving theory optic aphasia figure standard box model visual naming boxes denote levels representation denote pathways mapping level representation another although optic aphasia cannot explained damage vision semantics pathway semantics naming pathway farah proposed explanation terms partial damage both pathways xs semantic multiple semantic systems multiple pathways visual naming more parsimonious account suggested farah optic aphasia might arise partial lesions two pathways standard model those connecting visual input semantics semantics naming effect damage these pathways superadditive meaning tasks require these pathways eg visual gesturing auditory naming relatively whereas tasks requiring both pathways eg visual naming show significant deficit model superadditive present computational model superadditive theory optic aphasia architecture figure architecture has four pathways visual input semantics auditory input semantics semantics naming semantics gesturing each pathway acts associative memory critical property pathway required explain optic aphasia speed accuracy trade off initial output pathway appears rapidly but may inaccurate quick guess refined over time pathway output asymptotically converges best interpretation input implement pathway using architecture suggested mozer architecture inputs mapped their best interpretations means two stage process figure first quick shot mapping performed multilayer feedforward connectionist network transform input directly its corresponding output followed slower iterative clean up process carried out recurrent attractor network architecture shows speed accuracy trade off virtue assumption feedforward mapping network does have capacity produce exactly right output every input especially inputs corrupted noise otherwise incomplete consequently clean up stage required produce interpretation noisy output mapping network fully distributed attractor networks have been used similar purposes eg simplicity adopt attractor network layer state units layer radial basis function rbf units rbf unit per attractor each rbf attractor unit measures distance current state attractor represents activity attractor unit figure connectionist implementation processing pathway pathway consists feedforward mapping network followed recurrent attractor network denote connectionist processing units denote connections between units between layers units way output clean up network mapping network pathway input mozer farah exp state unit activity vector time vector location attractor strength attractor strength determines region state space over attractor its rate state converge attractor state units receive input mapping network attractor units updated follows activity state unit time ith output mapping net ith element attractor dis given hi bit linear threshold function bounds activity between weighted time average ith output mapping net simulations ct activity state units governed two forces external input feedforward net first term equation attractor unit activities second term parameter di acts kind attentional mechanism relative influence these two forces basic idea input coming mapping net changing system should input should yet concerned interpreting input case input straight through state units hence di should have value close input begins stabilize focus shifts interpreting input following dynamics attractor network shift corresponds being zero weighted time average update rule di what allows smooth transition function its new value certain constructions function mozer preparation have proven convergence algorithm attractor apart speed accuracy trade off these dynamics have another important consequence present model particularly respect pathways pathway into pathway such feeding into state unit activities act input because these activities change over time state approaches well formed state dynamics pathway quite complex deal unstable input property important explaining several phenomena associated optic aphasia pattern generation patterns constructed each five representational spaces visual auditory input semantic name gesture responses each representational space arbitrarily made dimensional generated binary valued patterns each space correspond known entities representational domain visual auditory semantic spaces patterns partitioned into similarity clusters per cluster patterns chosen randomly subject two constraints patterns different clusters had least apart had between apart because similarity patterns name gesture spaces irrelevant our modeling did impose similarity structure these spaces superadditive theory optic aphasia instead generated patterns these spaces random subject constraint every pattern had least every other after generating patterns each representational spaces established arbitrary correspondences among patterns such visual pattern auditory pattern semantic pattern name pattern gesture pattern represented same concept appropriate response visual naming task visual pattern would semantic pattern name pattern training procedure feedforward networks four pathways independently trained associations using back propagation each these networks contained single hidden layer units units network used symmetric activation function give activities range amount training chosen such performance training examples perfect usually several elements output would ie have wrong sign others would exactly correct ie done architectural assumption feedforward net does have capacity map every input exactly right output hence clean up process required training required clean up network due representation attractors clean up network trivial hand wire each clean up net attractors its domain along rest state attractor attractor strengths initialized same value except rest state attractor rest state attractor required lower strength so even weak external input would sufficient attractor network out rest state simulation methodology after each pathway had been trained model damaged removing fraction connections mapping networks connections chosen random equal fraction removed two pathways clean up nets damaged architecture damaged total different times creating simulated patients who tested each four tasks input patterns task results report come averaging across simulated patients input patterns responses determined after system had been given sufficient time relax into name gesture attractor taken response each response classified following mutually exclusive response types correct response same produced three immediately trials visual error visual pattern corresponding incorrect response visual pattern corresponding correct response semantic error error other error priming mechanism priming increased recently experienced stimuli has been found across wide variety tasks normal subjects included priming our model increasing parameter recently visited attractors see mozer details becker related approach damaged model mechanism often gave rise results have examined models behavior varied amount damage quantified parameter report performance simulated patients intermediate amount damage yielded effects produced error rates visual naming task range roughly median performance patients literature mozer farah table error rate damaged model various tasks task error rate auditory gesturing auditory naming visual gesturing visual naming table presents error rates model four tasks pattern errors shows qualitative fit human patient data model produced errors auditory gesturing task because two component pathways relatively few errors made auditory naming visual gesturing tasks each involved damaged pathway because clean up nets able compensate damage error rate visual naming task quite large due damage both its component pathways error rate visual naming cannot accounted summing effects damage two component pathways because sum error rates auditory naming visual gesturing each involves two partially damaged pathways nearly four times smaller rather effects damage these pathways interact their interaction leads superadditive visual pattern presented model mapped damaged pathway into corrupted semantic representation up while sufficiently minor clean up eventually clean up process considerably during period time semantic clean up network searching correct attractor corrupted semantic representation fed into damaged pathway combined effect initially noisy semantic representation input damaged pathway leads naming representation past point up properly interactions architecture merely consequence arbitrary assumption built into our model argue point consider two modifications architecture might eliminate interaction damaged model first allowed pathway relax into well formed state before feeding its output into pathway would little interaction effects damage would additive cortical pathways do operate sequentially stage its computation next stage moreover brain such processing strategy partial results pathway next speed processing without
rich body data exists showing recollection specific information makes important contribution recognition memory distinct contribution adequately captured existing memory models furthermore neuropsychological evidence indicates recollection hippocampus present model based largely known features hippocampal anatomy physiology accounts following key characteristics recollection false recollection rare ie participants rarely claim having studied items increasing interference leads less recollection but apparently does compromise quality recollection ie extent information reflects events study
given set objects visual field does visual system learn particular object interest while ignoring rest occlusions background clutter so discounted recognizing familiar object paper attempt answer these questions context kalman filter based model visual recognition has previously proved useful explaining certain neurophysiological phenomena such related extra classical receptive field effects visual cortex using results field robust statistics describe extension kalman filter model handle multiple objects visual field resulting robust kalman filter model demonstrates certain forms attention viewed property interaction between top down expectations bottom up signals model suggests functional interpretations certain effects have been observed visual cortical neurons experimental results provided help demonstrate ability model perform robust segmentation recognition objects image sequences presence varying degrees occlusions clutter
recently researchers have derived formal complexity analysis analog computation setting discrete time dynamical systems empirical training recurrent neural networks produces systems analog mechanisms previous work showed learn process simple context free language cfl counting extend work show learn harder cfl simple organizing its resources into symbol sensitive counting solution provide dynamical systems analysis demonstrates network count but store counting information
present study concerned word recognition rates degraded documents compare human machine reading capabilities series experiments explores interaction word recognition word frequency non words degradation level study influence character segmentation compare human performance our artificial neural network model reading found proposed computer model uses word context efficiently humans but performs slightly worse pure character recognition task
known humans make finer between familiar sounds eg than between ones eg different noise segments here show corresponding enhancement present early auditory processing stages based previous work demonstrated natural sounds had robust statistical properties could quantified auditory system exploits those properties construct efficient neural codes test hypothesis measure information rate carried auditory spike trains narrow band stimuli whose amplitude modulation has naturalistic characteristics compare information rate stimuli non naturalistic modulation find naturalistic inputs significantly enhance rate transmitted information indicating neural responses matched characteristics natural auditory scenes natural scene statistics neural code primary goal research understand complex sounds occur natural scenes processed auditory system natural sounds difficult describe quantitatively complexity auditory responses they makes hard gain insight into their processing hence most studies auditory physiology restricted pure tones noise stimuli resulting limited understanding auditory encoding paper novel approach study natural sound encoding auditory spike trains our corresponding author mail mail attias schreiner hi iii figure left amplitude modulation stimulus drawn naturalistic stimulus set evoked spike train inferior colliculus neuron right amplitude modulation non naturalistic set evoked spike train same neuron method consists measuring statistical characteristics natural auditory scenes incorporating them into simple stimuli systematic manner thus creating naturalistic stimuli enable us study encoding natural sounds controlled fashion first stage program has been described attias schreiner second reported below fig shows two segments long stimuli corresponding spike trains same neuron elicited pure tones amplitude modulated these stimuli while both stimuli appear random have same mean both spike trains have same firing rate may observe high low amplitudes more likely occur stimulus left indeed these stimuli drawn two stimulus sets different statistical properties our present study auditory coding focuses efficiency neural code given stimulus set well animal reconstruct input sound discriminate between similar sound segments based evoked spike train those abilities affected changing stimulus statistics quantify discrimination capability auditory neurons inferior colliculus cat using concepts information theory bialek et al rieke et al leads issue optimal coding atick theoretically given auditory scene particular statistical properties possible design encoding scheme would exploit those properties resulting neural code optimal scene but consequently less efficient other scenes here investigate hypothesis auditory system uses code adapted natural auditory scenes question addressed comparing discrimination capability auditory neurons between sound segments drawn naturalistic stimulus set non naturalistic set statistics natural sounds first step investigating relation between neural responses auditory inputs studied quantified temporal statistics natural auditory scenes attias schreiner well known different locations basal membrane respond selectively different frequency components incoming sound xt eg pickles hence frequency corresponds spatial coordinate analogy retinal location vision therefore analyzed large database sounds including speech music animal background sounds using various filter banks comprising khz each frequency band amplitude phase band limited signal xt cost extracted amplitude probability distribution pa auto correlation coding naturalistic stimuli auditory midbrain neurons music music speech cat sound curves give correspond he low he he curve function computed well those instantaneous frequency dt those statistics found nearly identical bands across examined sounds particular distribution log amplitude log normalized have zero mean unit variance could well fitted form pa exp normalization constants should large amplitude several examples displayed fig log amplitude distribution corresponds mathematically amplitude distribution musical instruments found pa known laplace distribution speech signal processing well background sounds pa shown band amplitude distribution gaussian signal power spectra fourier transform found have modified form together results those findings show natural sounds distinguished arbitrary ones robust characteristics present paper explore what extent auditory system exploits them constructing efficient neural codes another important point made attias schreiner well ruderman bialek regarding visual signals natural inputs often gaussian eg unlike signals used conventional system identification methods often applied nervous system paper use non gaussian stimuli study auditory coding measuring rate information transfer experiment based our results temporal statistics natural auditory scenes construct naturalistic stimuli starting simple signal systematically incorporate successively more complicated characteristics natural sounds into attias schreiner use narrow band stimuli consisting amplitude modulated sound frequencies khz phase modulation focusing point amplitude statistics constructed white naturalistic amplitude choosing exponential distribution ac cr pa ac each time point independently using modulation frequency hz ie fourier transform used non naturalistic stimulus set chosen uniform distribution pa adjusted so both stimulus sets had same mean short segment each set shown fig two distributions right stimuli min duration played cats minimize adaptation effects between two sets using sec long segments single unit recordings made inferior colliculus ic auditory processing stage eg pickles each ic unit responds best narrow range sound frequencies center called its best frequency neighboring units have similar topographic frequency organization auditory system each unit stimuli frequency most hz away units used firing rates response those stimuli between hz stimulus electrode signal recorded sampling rate khz after detecting spikes extracting stimulus amplitude both amplitude spike train down sampled khz analysis order assess ability discriminate between different inputs based observed spike train computed mutual information ir between spike train response rt ti ti spike times stimulus amplitude st consists two terms hs hs stimulus entropy log number different stimuli entropy stimulus conditioned response log number different stimuli could given response thus could based response averaged over responses our approach generally follows ideas bialek et al rieke et al simplify calculation first modified stimuli st get st function chosen so gaussian hence exponential stimuli uniform stimuli inverse error function gaussianization has two advantages first expression mutual information ir now simpler being given frequency dependent signal noise ratio snrf see below hs depends power spectrum st second more importantly noise distribution observed become closer gaussian following transformation compute bound above calculation requires conditional distribution note these variables complex hence joint real parts latter approximated gaussian mean srf variance variance fact power spectrum noise define st computing mutual information those gaussian distributions straightforward provides lower bound coding naturalistic stimuli auditory midbrain neurons figure left signal noise ratio snrf vs modulation frequency naturalistic stimuli right normalized noise distribution solid line amplitude distribution stimuli dashed line stimuli dashed dotted line true its log snrf signal noise ratio given snrf spectrum stimulus averaging performed over responses main object here estimate stimulus elicited spike train would optimally given conditional mean dp each gaussian estimator generally non linear becomes linear given wiener filter our distributions approximately gaussians used conditional mean obtained kernel estimate gaussian kernel rf spectrum spike train indexes data points obtained computing fft using window scaling reflects assumption distributions differ their variance enables us use data points frequencies estimate given our estimate produced slightly higher snrf than wiener estimate used bialek et al rieke et al others information naturalistic stimuli snrf exponential stimuli shown fig left our units ic neurons have preferred modulation frequency eg pickles about hz unit generally snrf equality stimulus response completely independent thus stimulus components frequencies higher than hz effectively cannot estimated spike train stimulus amplitude distribution shown fig right dashed line together noise distribution normalized have unit variance solid line nearly gaussian attias oo figure left signal noise ratio snrf vs modulation frequency stimuli solid line compared naturalistic stimuli dotted line right normalized noise distribution solid line amplitude distribution stimuli dashed line compared naturalistic stimuli dotted line stimuli dashed dotted line using obtain information rate spike rate measured unit translates into averaging across units have naturalistic stimuli although information rate computed using conditional mean estimator interesting examine wiener filter ht provides optimal linear estimator stimulus discussed previous section filter displayed fig solid line has temporal width several tens milliseconds information non naturalistic stimuli snrf uniform stimuli shown fig left solid line same unit fig significantly lower than corresponding snrf exponential stimuli comparison dashed line mutual information rate obtain amounts averaging across units have non naturalistic stimuli stimulus amplitude distribution shown fig right dashed line together exponential distribution dotted line comparison well noise distribution normalized have unit variance noise case less gaussian than exponential stimuli suggesting our calculated bound may lower uniform stimuli fig shows stimulus reconstruction filter dashed line has similar time course filter exponential stimuli but decay significantly slower its temporal width more than msec conclusion measured rate auditory neurons carry information simple stimuli naturalistic amplitude modulation found higher than stimuli non naturalistic modulation result along same lines obtained rieke et al using gaussian signals whose spectrum shaped according call spectrum similarly work vision field atick ruderman bialek atick suggests visual receptive field properties consistent optimal coding predictions based characteristics natural images future work explore coding stimuli more complex natural statistical characteristics coding naturalistic stimuli auditory midbrain neurons figure response wiener reconstruction filter naturalistic stimuli solid line non naturalistic stimuli dashed line extend higher processing stages acknowledgements thank bialek miller useful discussions miller read experimental support supported office research foundation references jj atick towards theory early visual processing neural jj atick could information theory provide theory sensory processing network attias ce schreiner temporal low order statistics natural sounds advances neural information processing systems mit press bialek rieke de van reading neural code science jj atick temporal decorrelation theory non responses lateral geniculate nucleus network field relations between statistics natural images response properties cortical cells soc am sm statistical signal processing estimation theory new simple coding procedure enhances neurons information capacity pickles
relationship between neurons refractory period precision its response identical stimuli investigated constructed model spiking neuron combines probabilistic firing refractory period realistic model closely both average firing rate response precision retinal ganglion cell model based free firing rate exists absence function may better description spiking neurons response than stimulus time histogram
conditioning experiments probe ways animals make predictions about rewards use those predictions control their behavior standard model conditioning paradigms involve many conditioned stimuli suggests individual predictions should added together various key results show model fails circumstances motivate alternative model attentional selection between different available stimuli new model form mixture experts has close relationship other existing psychological statistically well
while understanding functional role different classes neurons awake primary visual cortex has been extensively studied time our understanding feature selectivity functional role neurons primary auditory cortex much complete moving bars have long been recognized optimal stimulus many visual cortical neurons finding has recently been confirmed extended detail using reverse correlation methods jones et al et al study recorded neurons primary auditory cortex awake primate used novel reverse correlation technique compute receptive fields preferred stimuli both multiple frequency components ongoing time these receptive fields make clear neurons primary auditory cortex primary visual cortex typically show considerable structure their feature processing properties often including multiple excitatory inhibitory regions their receptive fields these neurons sensitive stimulus edges frequency composition time sensitive stimulus transitions such changes frequency these neurons show strong responses selectivity continuous frequency modulated stimuli analogous visual gratings
current challenges understanding neural information processing biological systems code carried large populations neurons acting parallel present algorithm automated discovery stochastic firing patterns large ensembles neurons algorithm helmholtz machine family attempts predict observed spike patterns data model consists observable layer directly activated input spike patterns hidden units activated through connections input layer hidden unit activity propagated down observable layer create prediction data pattern produced hidden units added incrementally their weights adjusted improve fit between predictions data increase bound probability data given model greedy strategy globally optimal but computationally tractable large populations neurons show benchmark data artificially constructed spike trains promising early results neurophysiological data collected our multi electrode cortical
pattern eye movement characterized smooth rotations eye direction rapid rotations opposite direction reset eye position periodic alternating pan form has been described unstable but amplitude limited oscillation pan has been observed previously subjects vestibulo cerebellar damage describe results pan produced normal subjects rotation propose new model neural circuits control eye movement inherently unstable but instability kept check under normal circumstances cerebellum circumstances cerebellar such damage plasticity due rotation lead pan
provide model standard task more challenging task involving novel platform locations exhibit trial learning after few training model uses hippocampal place cells support reinforcement learning integrated manner build use coordinates
initial activity independent formation topographic map system has long been thought rely matching molecular cues expressed gradients retina direct experimental evidence existence such gradients has emerged new data has discussion new set models experimental literature here capabilities these models analyzed gradient shapes they predict vivo derived
developing nervous system gradients target derived factors play important role guiding axons appropriate targets paper shape such gradient might have calculated function distance target time start factor production using estimates relevant parameter values experimental literature spatiotemporal domain growth cone could detect such gradient derived large times value maximum guidance range about mm obtained value fits well experimental data smaller times analysis predicts guidance over longer ranges may possible prediction remains tested
propose model early visual processing primates model consists population linear spatial filters interact through non linear excitatory inhibitory statistical estimation theory used derive human psychophysical thresholds responses entire population units model able reproduce human thresholds contrast orientation discrimination tasks predict contrast thresholds presence varying orientation spatial frequency
paper present new method studying auditory systems based sequences method allows us study linear response system presence various other stimuli such speech sinusoidal allows construct linear kernels receptive fields same time other stimuli being presented using method calculate modulation transfer function single units inferior colliculus cat different operating points discuss nonlinearities response
normal vision inputs two eyes integrated into single percept images presented two eyes perceptual integration gives way between monocular inputs phenomenon called binocular rivalry although recent evidence indicates binocular rivalry involves modulation neuronal responses cortex basic mechanisms responsible differential processing stimuli remain using neural network models mammalian early visual system demonstrate here firing cortical like neurons first receive inputs two eyes results activity patterns later stages visual pathway contrast synchronization firing among these cells such competition temporal coordination cortical activity its effects neural competition emerge naturally network connectivity its dynamics these results suggest input related differences relative spike timing early stage visual processing may give rise phenomena both perceptual integration rivalry binocular vision
here analyze synaptic transmission information theoretic perspective derive closed form expressions lower bounds capacity simple model cortical synapse under two explicit coding paradigms under signal estimation paradigm assume signal encoded mean firing rate poisson neuron performance optimal linear estimator signal provides lower bound capacity signal estimation under signal detection paradigm presence absence signal has detected performance optimal spike detector allows us compute lower bound capacity signal detection find single synapses empirically measured parameter values transmit information poorly but significant improvement achieved small amount redundancy
proposed complex cells visual cortex driven pool simple cells same preferred orientation but different spatial phases wide variety experimental results over past two decades have pure hierarchical model primarily demonstrating many complex cells receive input lgn cells do depend simple cell input recently showed using detailed biophysical model nonlinear interactions among synaptic inputs dendritic tree could provide nonlinear computations complex cell responses mel ruderman work extends result case complex cell binocular disparity tuning demonstrating isolated model pyramidal cell disparity tuning resolution much finer than overall dimensions cells receptive field systematically shifted optimal disparity values pairs light dark bars both good agreement published reports freeman our results potential importance intradendritic computation binocular visual processing particular cortical neurophysiology general account binocular disparity tuning
macaque cortex neurons have been found respond selectively complex shapes while showing broad tuning invariance respect stimulus transformations such translation scale changes limited tuning rotation depth training monkeys novel like objects logothetis et al could investigate whether these invariance properties due experience many transformed instances object mechanisms allow cells show response invariance previously unseen instances object they found object selective cells exhibited limited invariance various transformations after training single object views while previous models accounted tuning cells rotations depth their selectivity specific object relative population objects model described here attempts explain biologically plausible way additional properties translation size invariance using same stimuli experiment find model neurons exhibit invariance properties closely parallel those real neurons simulations show model capable unsupervised learning view tuned neurons thank dayan logothetis useful discussions comments poggio
discuss solution problem separating waveforms produced multiple cells extracellular neural recording take explicitly probabilistic approach using latent variable models varying describe distribution waveforms produced single cell models range single gaussian distribution waveforms each cell mixture hidden markov models overall statistical structure approach allowing details generative model chosen depend specific neural preparation
have studied application independent component analysis ica approach identification possible removal artifacts meg recording statistical technique separates components according kurtosis their amplitude distributions over time thus distinguishing between strictly signals occurring signals many artifacts belong last category order assess effectiveness method controlled artifacts produced included saccadic eye movements increased due presence digital inside room results demonstrate capability method identify clearly produced artifacts
model responses cells visual area during natural vision our model consists classical energy mechanism whose output divided gain control texture contrast mechanisms apply model review stimulus sequence replicates stimulation cell receives during free viewing natural images data collected three cells using five different review model fit separately data each energy mechanism alone find but significant correlations between model data these correlations improved somewhat allow surround effects case inclusion delayed surround dramatically improves fit data modifying time course models response
prove canonical distortion measure optimal distance measure use nearest nn classification show reduces squared euclidean distance feature space function classes expressed linear combinations fixed set features pac like bounds given required learn experiment presented neural network learnt japanese ocr environment used do nn classification
introduce new boolean computing element related linear threshold element boolean version neuron instead sign function computes arbitrary many transitions boolean function weighted sum its inputs call new computing element ltm element linear threshold multiple transitions paper consists following main contributions related our study ltm circuits efficient designs ltm circuits addition multiple number integers product two integers particular show compute addition integers single layer ltm elements ii proof area vlsi layout reduced lt circuits ltm circuits inputs symmetric boolean functions iii characterization computing power ltm relative lt circuits
recent theoretical results pattern classification thresholded real valued functions such support vector machines sigmoid networks boosting give bounds misclassification probability do depend size classifier hence considerably smaller than bounds follow vc theory paper show these techniques more widely applied representing other boolean functions two layer neural networks thresholded convex combinations boolean functions example show high probability decision tree depth more than consistent training examples has misclassification probability more than log class node decision functions thought effective number leaves becomes small distribution leaves induced training data gets far uniform bound qualitatively different vc bound considerably smaller use same technique give similar results formulae author correspondence should addressed bartlett lee
simple linear averaging outputs several networks eg bagging seems follow naturally decomposition sum squared error sum squared error average model quadratic function weighting factors assigned networks ensemble suggesting quadratic programming algorithm finding optimal weighting factors interpret output network probability statement sum squared error corresponds minus loglikelihood kullback leibler divergence linear averaging outputs logarithmic averaging probability statements logarithmic pool paper whole about model averaging quadratic programming find optimal weighting factors specific error but applies combination probability statements kind logarithmic pool long kullback leibler divergence plays role error measure examples treat model averaging classification models under cross entropy error measure models estimating variances
derive first order approximation density maximum entropy continuous random variable given number simple constraints results density expansion somewhat similar classical polynomial density expansions gram using approximation density approximation differential entropy derived approximation entropy both more exact more robust against outliers than classical approximation based polynomial density expansions without being computationally more expensive approximation has applications example independent component analysis projection pursuit
present new approximate learning algorithm boltzmann machines using systematic expansion gibbs free energy second order weights linear response correction correlations given hessian gibbs free energy computational complexity algorithm number neurons compare performance exact bm learning algorithm first order weiss mean field theory second order tap mean field theory learning task consists fully connected spin glass model neurons conclude method works well problems tap correction gives significant improvement over weiss mean field theory both spin glass problems inclusion diagonal weights improves weiss approximation problems but spin glass problems
study line generalized linear regression multidimensional outputs ie neural networks multiple output nodes but hidden nodes allow final layer transfer functions such softmax function need consider linear activations output neurons use distance functions certain kind two completely independent roles deriving analyzing line learning algorithms such tasks use distance function define matching loss function possibly multidimensional transfer function allows us generalize earlier results dimensional multidimensional outputs use another distance function tool measuring progress made line updates shows previously studied algorithms such gradient descent exponentiated gradient fit into common framework evaluate performance algorithms using relative loss bounds compare loss line best off line predictor relevant model class thus completely eliminating probabilistic assumptions about data
generalization ability neural network sometimes improved dramatically regularization analyze improvement needs more refined results than asymptotic distribution weight vector here study simple case dimensional linear regression under quadratic regularization ie ridge regression study random design case derive expansions optimal regularization parameter ensuing improvement possible construct examples best use regularization
employ both equation order parameter approaches analyze asymptotic dynamics line learning different learning rate annealing schedules examine relations between results obtained two approaches obtain new results optimal decay coefficients their dependence number hidden nodes two layer architecture
present method determining globally optimal line learning rule soft committee machine under statistical mechanics framework work previous results locally optimal rules rate change generalization error considered maximize total reduction generalization error over whole learning process show resulting rule significantly outperform locally optimal rule
perceptron decision trees known linear machine etc order data dependent structural risk applied data dependent analysis performed indicates choosing maximal margin decision nodes improve generalization analysis uses novel technique bound generalization error terms margins individual nodes experiments performed real data sets confirm validity approach
derive correspondence between regularization operators used regularization networks hilbert kernels support vector machines more specifically prove functions associated regularization operators suitable support vector kernels equivalent regularization properties product show large number radial basis functions namely conditionally positive definite functions may used support vector kernels
simple but powerful modification standard gaussian distribution studied variables gaussian constrained nonnegative enabling use energy functions two multimodal examples competitive cooperative distributions illustrate power gaussian cooperative distribution represent translations pattern demonstrates potential gaussian modeling pattern manifolds
online learning most common forms neural network training present analysis online learning finite training sets non linear networks namely soft committee machines theory more realistic learning scenarios dynamical equations derived appropriate set order parameters these exact limiting case either linear networks infinite training sets preliminary comparisons simulations suggest theory captures effects finite training sets but may yet account correctly presence local minima
apply general algorithm merging prediction strategies aggregating algorithm problem linear regression square loss our main assumption response variable bounded turns out particular problem aggregating algorithm resembles but slightly different wellknown ridge estimation procedure general results about aggregating algorithm guaranteed bound difference between our algorithms performance best sense linear regression functions performance show aa attains optimal constant our bound whereas constant attained ridge regression procedure general times worse
demonstrate problem training neural networks small average squared error computationally intractable consider data set points xi xi input vectors real outputs work class neural networks yi relative error occurs tries fit data set prove several classes neural networks achieving relative error smaller than fixed positive threshold independent size data set np hard
study storage capacity fully connected committee machine large number hidden nodes storage capacity obtained analyzing geometrical structure weight space related internal representation examining asymptotic behavior order parameters limit large storage capacity ac found proportional up leading order result satisfies mathematical bound given whereas replica symmetric solution conventional approach bound
inverse fisher information matrix used natural gradient descent algorithm train single layer multi layer perceptrons have discovered new scheme represent fisher information matrix stochastic multi layer perceptron based scheme have designed algorithm compute natural gradient input dimension much larger than number hidden neurons complexity algorithm order confirmed simulations natural gradient descent learning rule efficient but robust
bayesian learning neural networks typically based either local gaussian approximations mode posterior weight distribution markov chain monte carlo simulations third approach called ensemble learning introduced hinton van aims approximate posterior distribution minimizing kullback leibler divergence between true posterior parametric approximating distribution derivation deterministic algorithm use gaussian approximating distribution diagonal covariance matrix so unable capture posterior correlations between parameters paper show ensemble learning approach extended gaussian distributions while remaining computationally tractable extend framework deal hyperparameters leading simple re estimation procedure initial results standard benchmark problem encouraging
bayesian methods have been successfully applied regression classification problems multi layer percepttons present novel application bayesian techniques radial basis function networks developing gaussian approximation posterior distribution fixed basis function widths analytic parameters setting regularization constants crossvalidation single optimal parameter estimate treat issue assigning prior distributions these constants adapted light data under simple re estimation formula
recently model supervised learning probabilistic represented suffix trees introduced algorithm tends build large trees requiring large amounts computer memory paper propose new more compact model shares parameters distributions associated contexts yielding similar conditional output distributions illustrate advantages proposed algorithm comparative experiments inducing noun phrase recognizer
exact inference densely connected bayesian networks computationally intractable so considerable interest developing effective approximation schemes approach has been adopted bound log likelihood using mean field approximating distribution while leads tractable algorithm mean field distribution assumed factorial hence paper demonstrate feasibility using richer class approximating distributions based mixtures mean field distributions derive efficient algorithm updating mixture parameters apply problem learning sigmoid belief networks our results demonstrate systematic improvement over simple mean field theory number mixture components increased
study several statistically biologically motivated learning rules using same visual environment made up natural scenes same single cell neuronal architecture allows us concentrate feature extraction neuronal coding properties these rules included these rules kurtosis maximization quadratic form learning rule single cell ica using structure removal method demonstrate receptive fields developed using these rules depend small portion distribution find quadratic form rule behaves manner similar kurtosis maximization rule distribution contains directions although modification equations computationally simpler iv iv cooper
derive analyse robust optimization schemes noisy vector quantization basis deterministic annealing starting cost function central clustering incorporates distortions channel noise develop soft topographic vector quantization algorithm based maximum entropy principle performs maximum likelihood estimate em fashion annealing temperature parameter leads phase transitions existing code vector representation during process calculate critical temperatures modes function eigenvectors eigenvalues covariance matrix data transition matrix channel noise whole family vector quantization algorithms derived among them deterministic annealing scheme kohonens self organizing map som algorithm call applied vector quantization image data via noisy binary symmetric channel algorithms performance compared those while naturally superior does take into account channel noise its results compare well those computationally much more demanding
many applications such credit default prediction medical image recognition test inputs available addition labeled training examples propose method incorporate test inputs into learning our method results solutions having smaller test errors than simple training solution especially noisy problems small training sets
paper considers problem learning ranking set alternatives based upon incomplete information eg limited number observations describe two algorithms hypothesis ranking their application probably approximately correct pac expected loss el learning criteria empirical results provided demonstrate effectiveness these ranking procedures both synthetic datasets real world data design optimization problem
many applications desirable order rather than classify instances here consider problem learning order given feedback form preference ie statements effect instance should ranked ahead another outline two stage approach first learns conventional means preference function form indicates whether rank before new instances ordered so maximize learned preference function show problem finding ordering agrees best preference function np complete even under restrictive assumptions nevertheless describe simple greedy algorithm guaranteed find good approximation discuss line learning algorithm based algorithm finding good linear combination ranking experts use ordering algorithm combined line learning algorithm find combination search experts each domain specific query expansion strategy search engine present experimental results demonstrate merits our approach
paper discuss regularisation learning algorithms environments data sequentially techniques such cross validation achieve regularisation model selection possible further bootstrapping determine confidence level practical these problems minimum variance estimation approach makes use extended kalman algorithm training multi layer perceptrons employed novel contribution paper show theoretical links between extended kalman filtering variable learning rate algorithms bayesian estimation framework doing so propose algorithms overcome need heuristic choices initial conditions noise covariance matrices kalman approach
classification finite sequences without explicit knowledge their statistical nature fundamental problem many important applications propose new information theoretic approach problem based following ingredients sequences similar they likely generated same source ii cross estimated via universal compression iii markovian sequences asymptotically optimally these ingredients design method classification discrete sequences whenever they compressed introduce method illustrate its application hierarchical clustering languages estimating similarities protein sequences
first describe hierarchical generafive model viewed non linear generalisation factor analysis implemented neural network model performs perceptual inference probabilistically consistent manner using top down bottom up lateral connections these connections learned using simple rules require locally available information show incorporate lateral connections into generafive model model extracts sparse distributed hierarchical representation depth simplified random dot stereograms localised disparity detectors first hidden layer form topographic map presented image patches natural scenes model develops local feature detectors
learning techniques classification tasks work indirectly first trying fit full probabilistic model observed data whether good idea depends robustness respect deviations model study question experimentally restricted yet non trivial interesting case consider conditionally independent attribute model single binary valued hidden variable other attributes ie target observables depend model finding most likely value variable given known values others reduces testing linear function observed values learn two techniques standard em algorithm new algorithm develop based covariances compare these fashion against algorithm version winnow attempts find good linear classifier directly our conclusions help using model classification once data model performance quickly degrades below directly learned linear classifier
discuss strategy classification involves estimating class probabilities each pair classes coupling estimates together coupling model similar method paired comparisons study nature class probability estimates arise examine performance procedure simulated datasets classifiers used include linear discriminants nearest neighbors application support vector machines briefly described
adaptive line algorithm proposed estimate hierarchical data structures non stationary data sources approach based principle minimum cross entropy derive decision tree data clustering employs idea learning learn adapt changes data characteristics its efficiency demonstrated grouping non stationary data hierarchical segmentation images
address problem learning structure nonlinear markov networks continuous variables viewed non gaussian multidimensional density estimation exploiting certain conditional variables markov networks graphical way describing conditional well suited model relationships do exhibit natural causal ordering use neural network structures model quantitative relationships between variables main focus paper learning structure purpose insight into underlying process using two data sets show interesting structures found using our approach inference briefly addressed
active data clustering novel technique clustering proximity data utilizes principles sequential experiment design order data generation data analysis proposed active data sampling strategy based expected value information concept statistical decision theory considered important step towards analysis largescale data sets because offers way overcome inherent data sparseness proximity data present applications unsupervised texture segmentation computer vision information retrieval document databases
present computationally efficient algorithm function approximation piecewise linear sigmoidal nodes hidden layer network constructed node time using method fitting residual task fitting individual nodes accomplished using new algorithm best fit solving sequence quadratic programming problems approach offers significant advantages over derivative based search algorithms eg backpropagation its extensions unique characteristics algorithm include finite step convergence simple stopping criterion deterministic methodology good local minima good scaling properties robust numerical implementation
new class classification techniques have recently been developed statistics machine learning literature technique method takes standard classifier such lda trees into algorithm produce new classifier standard classifier known classifier these methods often produce large improvements over using single classifier paper investigate these methods give motivation its success
map network simple learning algorithm combines self organization capability self organizing map som probabilistic generative topographic mapping simulations suggest algorithm has stronger tendency self organize random initial configuration than map algorithm further simplified employ pure hebbian learning without changing qualitative behaviour network
multiple instance learning variation supervised learning task learn concept given positive negative instances each bag may contain many instances but bag labeled positive even instances falls within concept bag labeled negative instances negative describe new general framework called diverse density solving multiple instance learning problems apply framework learn simple description person series images containing person stock selection problem drug activity prediction problem
applications gaussian mixture models occur frequently fields statistics artificial neural networks key issues arising mixture model estimate optimum number mixture components paper extends reversible jump markov chain monte carlo mcmc algorithm case multivariate spherical gaussian mixtures using hierarchical prior model using method number mixture components longer fixed but becomes model estimate reversible jump mcmc algorithm capable moving between parameter subspaces correspond models different numbers mixture components result sample full joint distribution unknown model parameters generated technique demonstrated simulated example well known vowel dataset
paper introduces probability model mixture trees account sparse dynamically changing dependence relationships present family efficient algorithms use em minimum spanning tree algorithm find ml map mixture trees variety priors including dirichlet mdl priors
several effective methods improving performance single learning algorithm have been developed recently general approach create set learned models repeatedly applying algorithm different versions training data combine learned models predictions according prescribed scheme little work has been done combining predictions collection models generated many learning algorithms having different representation andor search strategies paper describes method uses strategies correspondence analysis model relationship between learning examples way they classified collection learned models nearest neighbor method applied within resulting representation classify previously unseen examples new algorithm consistently performs well better than other combining techniques data sets
propose diffusion networks type recurrent neural network probabilistic dynamics models learning natural signals continuous time space give formula gradient log likelihood path respect drift parameters diffusion network gradient used optimize diffusion networks regime wide variety problems techniques have engineering fields such system identification state estimation signal filtering aspect work particular interest computational neuroscience hardware design suitable choice activation function eg quasi linear sigmoidal gradient formula local space time
consider general problem learning multi category classification labeled examples present experimental results nearest neighbor algorithm actively selects samples different pattern classes according rule instead priori class probabilities amount improvement query based approach over passive batch approach depends complexity bayes rule principle algorithm based general enough used learning algorithm permits model selection criterion error rate classifier terms complexity model
existing proofs demonstrating computational limitations recurrent cascade correlation similar networks mozer explicitly limit their results units having sigmoidal hard threshold transfer functions et al proof given here shows finite discrete transfer function used units rcc network finite state automata fsa network cannot model matter many units used proof applies continuous transfer functions finite number fixed points such sigmoid radial basis functions
globally high dimensional data has locally low dimensional distributions perform local dimensionality reduction before further processing data paper examine several techniques local dimensionality reduction context locally weighted linear regression possible candidates derive local versions factor analysis regression principle component regression principle component regression joint distributions partial least squares regression after statistical bases these methods perform monte carlo simulations evaluate their robustness respect their statistical assumptions surprising outcome locally weighted partial least squares regression offers best average results thus outperforming even factor analysis theoretically most appealing our candidate techniques regression tasks involve mapping dimensional continuous input vector onto dimensional output vector they form ubiquitous class problems found fields including process control sensorimotor control coordinate transformations various stages information processing biological nervous systems paper focus spatially localized learning techniques example kernel regression gaussian weighting functions local learning offer advantages real time incremental learning problems due fast convergence considerable robustness towards problems negative interference large tolerance model selection atkeson moore schaal schaal atkeson press local learning usually based interpolating data local neighborhood around query point high dimensional learning problems suffers caused fact high dimensions neighborhoods local they almost empty whereas neighborhood empty local global learning methods such sigmoidal feedforward networks do face vijayakumar atkeson problem they do employ neighborhood relations although they require strong prior knowledge about problem hand order successful assuming local learning high dimensions necessarily being globally high dimensional does imply data remains high dimensional viewed locally example control robot arms biological arms have shown estimating inverse dynamics arm globally dimensional space reduces average dimensions locally vijayakumar schaal local learning system robustly exploit such locally low dimensional distributions should able avoid curse dimensionality pursuit question what context local regression method perform local dimensionality reduction paper derive compare several candidate techniques under perfectly statistical eg gaussian noise gaussian input distributions perfectly linear data ii less perfect conditions eg non gaussian distributions slightly quadratic data incorrect guess dimensionality true data distribution focus nonlinear function approximation locally weighted linear regression allows us adapt variety global linear dimensionality reduction techniques has found widespread application several local learning systems atkeson moore schaal jordan jacobs jordan hinton particular derive investigate locally weighted principal component regression lwpcr locally weighted joint data principal component analysis lwpca locally weighted factor analysis lwfa locally weighted partial least squares lwpls section briefly outline these methods their theoretical foundations while section empirically evaluate robustness these methods using synthetic data sets increasingly statistical assumptions techniques methods reduction assume our regression data generating process two sets observables inputs outputs characteristics process ensure functional relation fi both obtained through measurement device adds independent mean zero noise different magnitude each observable such simplicity focus dimensional output data functions either linear slightly quadratic these cases most common nonlinear function approximation locally linear models locality regression weighting error each data point weight gaussian kernel denotes query point positive semi definite distance metric size shape neighborhood regression atkeson et al parameters determined framework nonparametric statistics schaal atkeson press parametric maximum likelihood estimations et al present study they determined manually their secondary results paper without loss generality our data sets set zero vector compute weights input data such weighted mean xi wi zero output data equally translated mean zero mean zero data necessary most techniques considered below translated input data summarized rows matrix corresponding translated outputs elements vector corresponding weights diagonal matrix cases need joint input output data denoted zx local reduction factor analysis lwfa factor analysis technique dimensionality reduction most appropriate given generating process our regression data assumes observed data produced mean zero independently distributed dimensional vector factors transformed matrix contaminated mean zero independent noise diagonal covariance matrix ex both normally distributed parameters obtained iteratively expectation maximization algorithm em linear regression problem assumes generated denotes vector regression coefficients linear model identity matrix after calculating em joint data space formulated estimate derived conditional probability distributions assumed normal expected value mean conditional distribution locally weighted version lwfa obtained together estimate factors joint weighted covariance matrix xn denotes expectation operator matrix coefficients involved estimating factors note unless noise zero estimated different true tries average out noise data joint space principal component analysis lwpca alternative way determining parameters reduced space employs locally weighted principal component analysis lwpca joint data space defining largest kl principal components weighted covariance matrix eigenvectors unit length matrix inversion theorem provides means derive efficient estimate our dimensional output case dimensional row vector evaluation does require matrix inversion but rather division assumes normal distributions variables lwfa lwpca special case lwfa noise covariance spherical ie same magnitude noise observables under these circumstances subspaces spanned both methods same regression coefficients lwpca different those lwfa unless noise level zero lwfa optimizes coefficients according noise data equation thus normal distributions correct guess lwpca always expected perform worse than lwfa vijayakumar atkeson partial least squares lwpls partial least squares friedman recursively computes orthogonal projections input data performs single variable regressions along these projections residuals previous iteration step locally weighted version partial least squares lwpls proceeds shown equation below single variable regressions ordinary training lookup least squares lwpls makes same statistical assumption ordinary linear regressions ie output variables do have additive noise but input variables less choice projections introduces element lwpls remains dr still friedman though interestingly exists strong way projections chosen cascade correlation lwpls inputs previous step against projected inputs pi order ensure orthogonality projections lwpls chooses projections powerful way optimal function fits single projections ie certain input distributions address issue our empirical evaluations comparing step lwpls step lwpls principal component regression lwpcr although optimal computationally efficient techniques dimensionality reduction linear regression principal component regression lwpcr inputs projected onto largest principal components weighted covariance matrix input data matrix regression coefficients thus calculated equation inexpensive evaluate after projecting becomes diagonal matrix easy lwpcr assumes inputs have additive spherical noise includes zero noise case during dimensionality reduction lwpcr does take into account output data clipping input dimensions low variance nevertheless have important contribution regression output statistical point view less likely low variance inputs have significant contribution linear regression confidence bands regression coefficients increase variance associated input input data has non spherical noise lwpcr focus regression irrelevant projections monte carlo evaluations order evaluate candidate methods data sets inputs output randomly generated each data set consisted training points test points distributed either uniformly unit hypercube outputs local dimensionality reduction generated either linear quadratic function dimensional input space projected into dimensional space randomly chosen distance preserving linear transformation finally gaussian noise various magnitudes added both dimensional inputs dimensional output test sets additive noise outputs omitted each regression technique localized gaussian kernel equation dimensional distance metric di manually chosen ensure gaussian kernel had sufficiently many data points data areas kernel precise experimental conditions followed closely those suggested friedman kinds linear functions ii kinds quadratic functions xs ii kinds noise conditions each sub conditions output noise low noise local ratio high noise ii equal noise inputs outputs low noise ex high noise ex iii unequal noise inputs outputs low noise nn high noise kinds input distributions uniform unit hyper ii uniform unit hyper data points activate gaussian weighting function di more than forms hyper shaped distribution every algorithm ran times each combinations conditions additionally complete test repeated three further conditions varying di called factors lwfa algorithms assumed true dimensionality dimensional data ie few correct many factors average results summarized figure figure show summary results three factor conditions averaging over trials per condition each mean these averages over two input distribution conditions linear quadratic function condition these four cases frequently observed statistical assumptions nonlinear function approximation locally linear models figure lb number factors equals underlying dimensionality problem algorithms essentially performing equally well perfectly gaussian distributions random variables shown separately assumptions perfectly achieves best results almost closely followed lwpls unequal noise condition two pca based techniques lwpca lwpcr perform worst expected they choose suboptimal projections statistical assumptions lwfa parts its advantages such summary results become fairly balanced figure quality function fitting changes significantly correct number factors illustrated figure ac few factors figure lwpcr performs worst because randomly principle components input data without respect important regression second worse lwfa according its assumptions signal cannot model noise leading degraded estimate subspace consequently degraded regression results lwpls has clear lead test closely followed lwpca except lwfa methods evaluate data set non iterative calculations lwfa em maximally iterations until log likelihood increased less than le iteration vijayakumar atkeson many factors than necessary figure now lwpca degrades effect due its extracting noise contaminated projection strongly influences recovery regression parameters equation other algorithms perform almost equally well lwfa lwpls taking small lead output equal noise unequal noise noise inputs outputs inputs outputs io regression results factors results factors regression results factors oi summary results figure average summary results monte carlo experiments each primarily divided into three major noise conditions each noise condition four further subdivision coefficients linear quadratic model equal low added noise ii like high added noise iii coefficients linear quadratic model different low noise added iv like iii high added noise refer text descriptions monte carlo studies further explanations local reduction summary conclusions figure summarizes monte carlo experiments final average plot except lwpls every other technique showed least clear our robustness tests particularly incorrect number factors made these apparent high dimensional regression problems local dimensionality ie number factors clearly defined number but rather varying quantity depending way generating process operates usually process does need generate locally low dimensional distributions often chooses do so instance human ann movements follow patterns despite they could generate arbitrary ones thus local dimensionality reduction needs find autonomously appropriate number local factor locally weighted partial least squares turned out surprisingly robust technique purpose even outperforming statistically appealing probabilistic factor analysis principal component analysis number factors easily controlled based variance threshold input space friedman while factor analysis usually requires expensive cross validation techniques simple variance based control over number factors actually improve results lwpca lwpcr practice shown figure lwpcr more robust towards number factors while lwpca more robust towards interested dynamically growing number factors while obtaining already good regression results few factors lwpca especially lwpls seem appropriate should noted well factor lwpls already performed figure conclusion locally weighted partial least squares equally robust local weighted factor analysis towards additive noise both input output data moreover superior number factors seems most favorable technique local dimensionality reduction high dimensional regressions acknowledgments authors hinton them least squares work supported human information processing research laboratories support includes german research association foundation german foundation vijayakumar supported japanese science atkeson air force office scientific research grant national science foundation references atkeson moore schaal locally weighted learning artificial intelligence review pp atkeson moore schaal locally weighted learning control artificial intelligence review pp regression data sources new york
explore methods incorporating prior knowledge about problem hand support vector learning machines show both invariances under group transformations prior knowledge about locality images incorporated constructing appropriate kernel functions
boosting general method improving performance learning algorithm consistently generates classifiers need perform slightly better than random recently proposed promising boosting algorithm adaboost has been applied great success several benchmark machine learning problems using rather simple learning algorithms decision trees paper use adaboost improve performances neural networks compare training methods based sampling training set weighting cost function our system achieves about error data base online handwritten digits more than adaptive boosting multi layer network achieved error uci letters error uci data set
monotonicity constraint arises many application domains present machine learning model monotonic network monotonicity exactly ie virtue functional form straightforward method implementing training monotonic network described monotonic networks proven universal approximators continuous differentiable monotonic functions apply monotonic networks real world task rating prediction compare them other approaches
paper technique previously used supervised learning applied unsupervised learning specifically used non parametric multivariate density estimation combine finite mixture model kernel density estimators experimental results both simulated data real world data sets clearly demonstrate density estimation outperforms other strategies such choosing single best model based cross validation combining uniform weights even single best model chosen looking data used independent testing
similarity based fault tolerant retrieval neural associative memories has lead applications drawback efficient model sparse patterns high asymptotic information capacity little practical use because high cross noise arising retrieval finite sizes here new iterative retrieval method model presented called cb retrieval providing enhanced performance discuss its asymptotic capacity limit analyze first step compare experiments model applying efficient cb memory model either information retrieval systems functional model cortico cortical pathways requires more than robustness against random noise input our experiments show segmentation ability cb retrieval addresses containing superposition provided even high memory load
nonlinear dimensionality reduction formulated here problem trying find euclidean feature space embedding set observations preserves closely possible their intrinsic metric distances between points observation manifold measured along paths our feature mapping procedure isomap able reliably recover low dimensional nonlinear structure realistic perceptual data sets such manifold face images conventional global mapping methods find local minima recovered map provides canonical set globally meaningful features allows perceptual transformations such interpolation extrapolation analogy highly nonlinear transformations original observation space computed simple linear operations feature space
our aim paper develop bayesian matching hierarchical relational models goal make discrete label assignments so global cost function information concerning consistency match different levels hierarchy our bayesian development naturally distinguishes between intra level inter level constraints allows impact match assessed its own level representation but upon its parents children hierarchy
blind source separation fisher information matrix used riemannian metric tensor parameter space steepest descent algorithm maximize likelihood function riemannian parameter space becomes serial updating rule property algorithm further simplified using asymptotic form fisher information matrix around equilibrium
asynchronous pulse density modulating digital neural network system has been developed our laboratory consists thousand neurons physically interconnected via million bit synapses solve thousand simultaneous nonlinear first order differential equations fully parallel continuous fashion performance system measured winner take network thousand neurons although magnitude input network parameters identical each competing neuron them milliseconds processing speed amounts billion connections per second broad range neural networks including spatiotemporal filtering feedforward feedback networks run appropriate network parameters system
have developed analog vlsi system models coordination neurobiological segmental oscillators have implemented tested system consists chain pattern generating circuits coupled their nearest neighbors each pattern generating circuit implemented two silicon neurons connected inhibitory network discuss mechanisms oscillations two cell network explore system behavior based isotropic anisotropic coupling frequency gradients along chain oscillators
describe design fabrication test results analog cmos vlsi neural network prototype chip intended phase based machine vision algorithms chip implements image filtering operation similar gabor filtering because gabor filters output complex valued used define phase every pixel image phase used robust algorithms disparity estimation binocular stereo control stereo vision image motion analysis chip reported here takes input image generates two outputs every pixel corresponding real parts output
present method analysis nonstationary time series multiple operating modes particular possible detect model both switching dynamics less time consuming drift mode another achieved two steps first unsupervised training method provides prediction experts inherent dynamical modes trained experts used hidden markov model allows model application physiological data demonstrates analysis modeling real world time series improved drift paradigm taken into account
discuss problem catastrophic fusion multimodal recognition systems problem arises systems need different channels non stationary environments practice shows recognition modules within each tested contexts their assumptions their influence product tends increase catastrophic results explore principled solution problem based upon bayesian ideas competitive models inference each sensory channel provided simple white noise context models perceptual hypothesis context jointly estimated context deviations interpreted changes white noise contamination strength automatically adjusting influence module approach tested fixed automatic speech recognition problem good results
hidden markov models hmms automatic speech recognition rely high dimensional feature vectors summarize properties speech correlations between features arise speech signal non stationary corrupted noise investigate model these correlations using factor analysis statistical method dimensionality reduction factor analysis uses small number parameters model covariance structure high dimensional data these parameters estimated expectation maximization em algorithm embedded training procedures hmms evaluate combined use mixture densities factor analysis hmms recognize strings holding total number parameters fixed find these methods properly combined yield better models than either method its own
apply information maximization maximum likelihood blind source separation complex valued signals mixed complex valued nonstationary matrices case arises communications signals incorporate known source signal distributions adaptation thus making algorithms less blind results reduction amount data needed successful convergence adaptation rapidly changing signal mixing conditions such mobile communications becomes now feasible demonstrated simulations
paper present novel hybrid architecture continuous speech recognition systems consists continuous hmm system extended arbitrary neural network used takes several frames feature vector input produce more discriminative feature vectors respect underlying hmm system hybrid system extension state art continuous hmm system fact first hybrid system really capable these standard systems respect recognition accuracy experimental results show relative error reduction about achieved good recognition system based continuous hmms resource management word continuous speech recognition task
observed distribution natural images far uniform contrary real images have complex important structure exploited image processing recognition analysis have been many proposed approaches principled statistical modeling images but each has been limited either complexity models complexity images present non parametric multi scale statistical model images used recognition image de mode synthesize high quality textures
paper describes new approach extracting perspective structure point sets novel feature unify tasks estimating transformation geometry identifying matches constructing mixture model over bi graph representing correspondence match optimisation using em algorithm according our em framework probabilities structural correspondence gate contributions expected likelihood function used estimate maximum likelihood perspective pose parameters provides means rejecting structural outliers
image intensity variations result several different object surface effects including shading dimensional object surface itself essential problem vision people solve naturally attribute proper physical cause eg surface observed image addressed problem approach combining psychophysical bayesian computational methods assessed human performance set test images found people made fairly consistent surface properties our computational model assigned simple prior probabilities different explanations image solved most probable interpretation bayesian framework ratings test images our algorithm compared surprisingly well mean ratings our subjects
image often represented set detected features get compression representing images way furthermore get representation little affected small amounts noise image features typically chosen ad hoc manner show good set features obtained using sufficient statistics idea sparse data representation naturally arises treat dimensional dimensional signal reconstruction problem make our ideas concrete
model motion detection presented model contains three stages first stage selective contrast next two stages work parallel phase insensitive stage pools across different contrast through spatiotemporal filter thus detect first second order motion phase sensitive stage contrast separate each filtered through spatiotemporal filter thus first order motion detected differential phase sensitivity therefore account detection first second order motion phase insensitive detectors correspond cortical complex cells phase sensitive detectors simple cells
neural network approach presented based effects simple disparity estimators fast scheme within single network structure dense disparity map associated validation map additionally view scene available network operations based simple biological plausible circuitry algorithm fully parallel non iterative
implement model obstacle avoidance insects small monocular robot result system capable rapid navigation through dense obstacle field key system use behavior body during movement shown behavior blind spot surrounding focus expansion normally found systems without behavior system models cooperation several behaviors ocular response similar vor response field computation mapping motor system resulting system neurally plausible simple should easily avlsi hardware
converging evidence has shown human object recognition depends images object further greater similarity between objects stronger dependence object appearance more important twodimensional image information becomes these findings do rule out use structural information recognition degree information used visual memory important issue liu showed model restricted rotations image plane independent templates could account human performance discriminating novel object views now present results models generalized radial basis functions nearest neighbor matching allows affine transformations bayesian statistical estimator integrates over possible affine transformations performance human observers relative each models better novel views than familiar template views suggesting humans generalize better novel views template views bayesian estimator yields optimal performance transformations independent templates therefore models matching operations independent templates unlikely account human recognition performance
scale invariance fundamental property ensembles natural images their non gaussian properties less well understood but they indicate existence rich statistical structure work present detailed study marginal statistics variable related edges images numerical analysis shows exhibits extended self similarity scaling property stronger than self similarity its moments expressed power given more interesting predicted terms multiplicative log poisson process same model used recently predict correct structure functions these results allow us study underlying singularities particular find most singular structures dimensional most singular manifold consists sharp edges category visual processing
ability rely similarity metrics invariant image transformations important issue image classification tasks such face character recognition analyze invariant metric has performed well latter tangent distance study its limitations applied regular images showing most significant among these convergence local minima drastically reduced computing distance multiresolution setting leads multiresolution tangent distance exhibits significantly higher invariance image transformations easily combined robust estimation procedures
many real world tasks small fraction available inputs important particular time paper presents method relevance inputs exploiting temporal coherence method proposed paper dynamically relevance inputs using expectations their future values model task learned model simultaneously extended create task specific predictions future values inputs inputs either relevant therefore accounted model those contain noise predicted accurately these inputs de turn new improved model task created techniques presented paper have yielded significant improvements vision based autonomous control vehicle vision based hand tracking cluttered scenes detection faults
new algorithm presented approximates perceived visual similarity between images images initially transformed into feature space captures visual structure texture color using tree filters similarity inverse distance perceptual feature space using algorithm have constructed image database system perform example based retrieval large image databases using constructed target sets limit variation single visual characteristic retrieval rates quantitatively compared those standard methods
pixel general purpose vision chip spatial focal plane processing presented size configuration processing receptive field programmable chips architecture allows photoreceptor cells small densely performing computation read out away array addition raw intensity image chip outputs four processed images parallel presented application chip line segment orientation detection found retinal receptive fields
capable rapidly detecting integrating visual motion information relevant ways first stage visual motion processing retinotopic array functional units known elementary motion detectors several decades colleagues developed correlation based model motion detection described behavior these neural circuits have implemented variant model tin analog cmos vlsi process result low power continuous time analog circuit integrated photoreceptors responds motion real time responses circuit sinusoidal gratings qualitatively resemble temporal frequency response spatial frequency response direction selectivity motion sensitive neurons observed insects addition its possible engineering applications circuit could potentially used building block constructing hardware models higher level insect motion integration
multi scale neural network system producing style variations given system four part variation after being trained music pieces like unlike earlier approaches learning structure system able learn reproduce high order structure like harmonic motif phrase structure sequences achieved using mutually interacting feedforward networks operating different time scales combination kohonen networks classify recognize musical structure results style their quality has been experts comparable experienced human
severe contamination eeg activity eye movements muscle heart line noise serious problem eeg interpretation analysis rejecting contaminated eeg segments results considerable loss information may impractical clinical data many methods have been proposed remove eye movement artifacts eeg recordings often regression time frequency domain performed simultaneous eeg recordings derive parameters characterizing appearance spread artifacts eeg channels records contain brain signals so out activity involves portion relevant eeg signal each recording well regression cannot used remove muscle noise line noise these have reference channels here propose new generally applicable method removing wide variety artifacts eeg records method based extended version previous independent component analysis ica algorithm performing blind source separation linear mixtures independent source signals either sub gaussian super gaussian distributions our results show ica effectively detect separate remove activity eeg records wide variety sources results comparing favorably those obtained using regression based methods extended ica removes artifacts eeg recordings
present novel generic approach problem event related potential identification classification based competitive neural net architecture network weights converge embedded signal patterns resulting formation matched filter network performance analyzed via simulation study exploring identification robustness under low snr conditions compared expected performance information theoretic perspective classifier applied real event related potential data recorded during classic odd type paradigm first time variable signal patterns automatically identified strong limiting requirement priori stimulus related selective grouping recorded data
have constructed inexpensive video based tracking system learns track head uses real time graphical user inputs auxiliary detector signals train convolutional neural network inputs neural network consist normalized luminance images motion information frame differences images used provide scale invariance during online training phase neural network rapidly adjusts input weights depending upon reliability different channels surrounding environment quick adaptation allows system robustly track head even other objects moving within cluttered background
work tackle problem time series modeling video traffic different existing methods model timeseries time domain model wavelet coefficients wavelet domain strength wavelet model includes unified approach model both long range short range dependence video traffic simultaneously computationally efficient method developing model generating high quality video traffic feasibility performance analysis using model
integrated service communication networks important problem call admission control routing so optimally use network resources problem naturally formulated dynamic programming problem complex solved exactly use methods reinforcement learning rl together decomposition approach find call admission control routing policies performance our policy network approximately different feature configurations compared commonly used heuristic policy
program execution speed computers sensitive factor two more order instructions presented processor realize potential execution efficiency optimizing employ heuristic algorithm scheduling such algorithms hand expensive time consuming show cast scheduling problem learning task obtaining heuristic scheduling algorithm automatically our focus problem scheduling straight line code called basic blocks instructions our empirical results show few features adequate quite good performance task real processor several supervised learning methods perform nearly optimally respect features used
paper enhances learning algorithm optimal asset allocation proposed new formulation simplifies approach using value function many allows model free policy iteration after testing new algorithm real data possibility risk management within framework markov decision problems analyzed proposed methods allows construction multi period portfolio management system takes into account costs risk preferences several constraints allocation
rapid expansion computer networks during past few years security has become crucial issue modern computer systems good way detect use through monitoring user activity methods detection based hand coded rule sets predicting commands line build reliable paper proposes new way applying neural networks detect believe user leaves using system neural network used learn identify each user much like use place people scenes users behavior does match system possible security backpropagation neural network called neural network detector trained identification task tested experimentally system users system accurate detecting activity false rate these results suggest learning user profiles effective way detecting
paper propose technique incorporate contextual information into object classification real word cases identity object ambiguous due noise measurements based classification should made helpful reduce ambiguity utilizing extra information referred context our case identities accompanying objects technique applied white cell classification comparisons made against context approach demonstrates superior classification performance achieved using context our particular application significantly reduces false rate thus greatly reduces cost due expensive clinical tests author correspondence incorporating contextual information white cell identification
describe system learning rules musical harmony these rules learned examples expressed rule based neural networks rules applied realtime generate new accompanying harmony live real time functionality imposes constraints learning processes including limitations types information system use input amount processing system perform demonstrate algorithms generating musical rules examples meet these constraints describe method including priori knowledge into rules yields significant performance gains describe techniques applying these rules generate new music real time conclude paper analysis experimental results
paper reports about application bayes inferred neural network classifiers field automatic sleep reason using bayesian learning task two fold first bayesian inference known regularization automatically second side effect bayesian learning leads larger variance network outputs regions without training data results well known effects used detect outliers fold cross validation experiment full bayesian solution found hybrid monte carlo algorithm better than single maximum posteriori map solution found evidence approximation second experiment studied properties both solutions rejecting classification movement experiences bayesian learning real world application
discuss development multi layer perceptron neural network classifier use between mean squared error sufficient make correct objective about performance neural classifier concepts sensitivity introduced combined receiver operating characteristic curves based objective observations such criteria color imaging results markers neural network able make reliable predictions discriminating comparable experienced
paper presents new approach problem modelling using neural networks first model conditional distributions amounts such way model itself determines order process time dependent shape scale conditional distributions after integrating over particular weather patterns able extract variations long term trends
explain training data separated into clean information noise analogous data neural network separated into time invariant structure used forecasting noisy part propose unified theory connecting optimization algorithms learning together algorithms control data noise parameter noise combined algorithm allows data driven local control network parameters therefore improvement generalization approach proven useful task forecasting german market
prioritized sweeping model based reinforcement learning method attempts focus agents limited computational resources achieve good estimate value environment states choose effectively costly planning step classic prioritized sweeping uses simple heuristic focus computation states likely have largest errors paper introduce generalized prioritized sweeping principled method generating such estimates representation specific manner allows us extend prioritized sweeping beyond explicit state based representation deal compact representations necessary dealing large state spaces apply method generalized model approximators such bayesian networks describe preliminary experiments compare our approach classical prioritized sweeping
paper describes interactions model learning algorithms planning algorithms have found exploring model based reinforcement learning paper focuses local trajectory used effectively learned nonparametric models find trajectory fully consistent learned model often have difficulty finding reasonable plans early stages learning trajectory balance learned model minimizing cost maximizing reward often do better even plan fully consistent learned model
new policy iteration algorithm partially observable markov decision processes presented simpler more efficient than earlier policy iteration algorithm key representation policy finite state controller representation makes policy evaluation straightforward papers contribution show dynamic programming update used policy improvement step interpreted transformation finite state controller into improved finite state controller new algorithm consistently outperforms value iteration approach solving infinite horizon problems
initial experiments described here directed toward using reinforcement learning rl develop automated recovery system high aircraft outer loop flight control system designed bring aircraft range out control states level flight minimum time while satisfying physical physiological constraints here report results simple version problem involving single axis pitch simulated through simulated control experience using medium fidelity aircraft simulation rl system approximates optimal policy pitch inputs produce minimum time transitions straight level flight unconstrained cases while avoiding ground rl system able acceleration constraint while simulated automated aircraft recovery via reinforcement learning
paper concerned problem reinforcement learning rl continuous state space time stochastic control problems state bellman equation satisfied value function use finite difference method designing convergent approximation scheme propose rl algorithm based scheme prove its convergence optimal solution
propose local error estimates together algorithms adaptive posteriori grid time reinforcement learning consider deterministic system continuous state time infinite horizon discounted cost functional grid follow procedure numerical methods bellman equation time propose new criterion based consistency estimates discrete solutions demonstrate optimal ratio time space discretization crucial optimal learning rates accuracy approximate optimal value function
present new approach reinforcement learning policies considered learning process constrained hierarchies partially specified machines allows use prior knowledge reduce search space provides framework knowledge across problems component solutions solve larger more complicated problems our approach seen providing link between reinforcement learning behavior based reactive approaches control present provably convergent algorithms problem solving learning hierarchical machines demonstrate their effectiveness problem several thousand states
planning precup richard sutton university massachusetts ma abstract planning learning multiple levels temporal abstraction key problem artificial intelligence paper summarize approach problem based mathematical framework markov decision processes reinforcement learning current model based reinforcement learning based step models cannot represent common sense higher level actions such going grasping object paper generalizes prior work temporally abstract models sutton extends prediction setting include actions control planning introduce more general form temporally abstract model multi time model establish its suitability planning learning virtue its relationship bellman equations paper summarizes theoretical framework multi time models illustrates their potential advantages gridworld planning task need hierarchical abstract planning fundamental problem ai see eg et al dayan hinton model based reinforcement learning offers possible solution problem integrating planning real time learning decision making williams moore atkeson sutton barto current model based reinforcement learning based step models cannot represent common sense higher level actions modeling such actions requires ability handle different levels temporal abstraction new approach modeling multiple time scales introduced sutton based prior work singh dayan sutton approach enables models environment different temporal scales producing temporally abstract models work concerned predicting environment paper summarizes extension approach including actions control environment precup sutton particular generalize usual notion multi time models temporally abstract planning primitive step action abstract action arbitrary closed loop policy whereas prior work modeled behavior agent environment system under single given policy here learn different models set different policies each possible way behaving agent learns separate model what happen planning choose between these overall policies well between primitive actions illustrate kind advance trying make consider example shown figure standard grid world primitive actions move grid cell neighboring cell imagine learning agent repeatedly given new tasks form new goal locations travel rapidly possible agent plans level primitive actions its plans many actions long take relatively long time compute planning could much faster abstract actions could used plan moving room room rather than cell cell each room agent learns two models two abstract actions traveling efficiently each adjacent room do address paper question such abstract actions could discovered without help instead focus mathematical theory abstract actions particular define general semantics property seems required order them used general kind planning typically used markov decision processes end paper illustrate theory example problem showing room room abstract actions substantially speed planning unreliable primitive actions up left right fail time down abstract actions each figure example task natural abstract actions move room room reinforcement learning mdp framework reinforcement learning learning agent environment discrete lowest level time scale each time step agent state environment st basis chooses primitive action response each primitive action environment produces step later numerical reward next state st agents objective learn policy mapping states probabilities taking each action maximizes expected discounted future reward each state rate parameter denotes expectation implicitly conditional policy being followed quantity vs called value state under policy called value function policy value under optimal policy denoted vs max planning reinforcement learning refers use models effects actions compute value functions particularly precup sutton assume states discrete form finite set st viewed theoretical limitation ideas present assumption allows us alternatively denote value functions column vectors each having components contain values states general vector use notation refer its component model action whether primitive abstract has two components matrix predicting state result action each state other vector predicting cumulative reward received along way case primitive action matrix step transition probabilities environment times denotes column pt these predictions corresponding state st denotes unit basis vector corresponding st reward prediction primitive action contains expected immediate rewards rt sa vs stochastic policy similarly define its step model pr ps st vs suitability planning conventional planning step models used compute value functions via bellman equations prediction control vector notation prediction control bellman equations pv pv respectively max function applied component wise control equation planning these turned into updates eg converge value functions thus bellman equations usually used define compute value functions given models actions following sutton here reverse roles take value functions given use bellman equations define compute models new abstract actions particular model used planning stable consistent bellman equations useful define special terms consistency each bellman equation let denote arbitrary model vector matrix model said valid policy sutton pv valid model used compute via iteration algorithm pv direct sense validity model implies suitable planning introduce here parallel definition consistency control bellman equation model said non nop has positive elements pv relation holds component wise nop model added inside max operator control bellman equation condition ensures true value state thus model does promise more than multi time models temporally abstract planning achievable serve option planning purposes step models primitive actions nop due similarly straightforward show step model policy nop purposes more convenient write model single matrix say model has been put homogeneous coordinates vectors corresponding value functions put into homogeneous coordinates adding initial element always using notation new models combined using two basic operations composition averaging two models composed matrix multiplication yielding new model set models mi averaged weighted set diagonal matrices di such yi di yield new model yi sutton showed set models valid policy closed under composition averaging enables models acting different time scales mixed together resulting model still used compute have proven set nop models closed under composition averaging precup sutton these operations permit richer variety combinations nop models than they do valid models because nop models combined need correspond particular policy multi time models validity nop model do imply each other precup sutton nevertheless believe good model should both valid nop would like describe class models sense includes interesting models valid non expressive enough include common sense notions abstract action these goals have led us notion multi time model simplest example multi step model called step model policy predicts step truncated return state steps into future times different models same policy averaged result called mixture model mixtures valid non due properties established previous section kind mixture suggested sutton allows exponential decay weights over time controlled parameter figure two markov environments mixture models expressive enough capturing properties environment order get intuition about expressive power model should have let us consider example figure interested state attained two environments presented should characterized significantly different models step models ny linear mixture step models cannot achieve goal order problem models should average differently over different trajectories possible through state space full model sutton precup sutton distinguish between these two situations model more general form mixture model different parameter associated each state state viewed probability trajectory through state space ends state although models seem have more expressive power they cannot describe step models would like have more general form model both classes goal achieved accurate multi time models multi time models defined respect policy step model policy defined define accurate multi time model sequence random weights such wt wt weights random variables chosen according distribution depends states visited before time weight wt measure importance given th state trajectory particular wt state has weight associated wt io wi remaining weight along trajectory given state effect state outcome state trajectory random weights along each trajectory make general form model necessary constraint weights depend previously visited states particular choose weighting sequences generate types multi step models described sutton weighting variables such wt vt obtain step models weighting sequence form wt parameter associated state visited time step describes full model main result multi time models they satisfy two criteria defined previous section accurate multi time model nop valid proofs these results long include here example order illustrate way multi time models used practice let us return gridworld example figure cells grid correspond states environment state agent perform four primitive actions up down probability actions cause agent move cell corresponding direction unless would take agent into wall case same state probability agent instead moves other three directions unless takes into wall course penalty bumping into each room defined two abstract actions going each adjacent each abstract action has set input states states room two outcome states target hallway corresponds successful outcome state adjacent other hallway corresponds failure agent has out room each abstract action given its complete model optimal policy getting into target hallway weighting variables along trajectory have value outcome states everywhere multi time models temporally abstract planning iteration ii ii iteration iteration iteration iteration iteration figure value iteration using primitive abstract actions goal state have arbitrary position but illustration let us suppose goal two steps down right hallway value goal state rewards along way factor performed planning according standard value iteration method states except goal state starts experiment ranged over primitive actions other ranged over set including both primitive abstract actions using primitive actions values propagated step away each iteration after six iterations instance states most six steps away goal non zero values models abstract actions produce significant speed up propagation values each step figure shows value function after each iteration using both primitive abstract actions planning area circle drawn each state proportional value state first three iterations identical case primitive actions used once values propagated first hallway states adjacent hallway receive values well states room containing goal these values correspond performing abstract action getting into right hallway following optimal primitive actions get goal point path goal known each state right half environment even path optimal states after six iterations optimal policy known states environment models abstract actions do need given priori they learned experience fact abstract models used experiment have been learned during step random walk environment starting point precup sutton learning represented outcome states each abstract action along associated these states used learning watkins learn optimal state action value function associated each abstract action ub greedy policy respect policy associated abstract action ub same time used model learning algorithm presented sutton compute model corresponding policy learning algorithm completely online incremental its complexity comparable regular step models abstract actions built while agent acting environment without additional effort such models used planning process they would represent primitive actions more efficient learning planning especially goal changing over time acknowledgments authors thank helpful discussions comments paper research supported part nsf grant barto richard sutton grant barto richard sutton precup support foundation references dayan improving generalization temporal difference learning representation neural computation dayan hinton learning advances neural information processing systems volume pp san mateo ca morgan kaufmann hierarchical learning stochastic domains preliminary results proceedings international conference machine learning icml pp san mateo ca morgan kaufmann learning solve problems searching macro operators ltd ps chunking anatomy general learning mechanism machine learning moore atkeson prioritized sweeping reinforcement learning less data less real time machine learning williams efficient learning planning within dyna framework adaptive behavior precup sutton multi time models reinforcement learning icml role models reinforcement learning structure plans behavior ny singh scaling reinforcement learning learning variable temporal resolution models proceedings international conference machine learning icml pp san mateo ca morgan kaufmann sutton td models modeling world mixture time scales proceedings international conference machine learning icml pp san mateo ca morgan kaufmann sutton barto reinforcement learning
paper show discounted mdps factor asymptotic rate convergence learning ot otherwise provided state action pairs sampled fixed probability distribution here ratio minimum maximum state action frequencies results extend convergent line learning provided now become minimum maximum state action frequencies corresponding stationary distribution
learning system composed linear control modules reinforcement learning modules selection modules hybrid reinforcement learning system proposed fast learning real world control problems selection modules choose appropriate control module dependent state hybrid learning system applied control type biped robot learned control more quickly than usual reinforcement learning because did need learn control fiat linear control module control robot trained step learning during first learning step selection module trained training procedure controlled linear controller learned control more quickly average number trials about so small learning system applicable real robot control
based computational principles concept internal model adaptive control has been divided into forward inverse model yet little evidence learning control cns through adaptation other here examine two adaptive control architectures based inverse model other based combination forward inverse models show reaching movements hand novel force fields learning forward model results key characteristics performance match kinematics human subjects contrast adaptive control system relies inverse model fails produce kinematic patterns observed subjects despite fact more stable our results provide evidence learning control novel dynamics via formation forward model
benchmark task spiral problem well known neural networks unlike previous work learning approach problem generic perspective does involve learning point out spiral problem intrinsically connected problem generic solution both problems proposed based oscillatory correlation using time delay network our simulation results qualitatively consistent human performance interpret human limitations terms synchrony time delays both biologically plausible special case our network without time delays always distinguish these figures regardless shape position size orientation
despite fact mental arithmetic based few hundred basic simple algorithms humans have difficult time subject even experienced individuals make mistakes associative multiplication process doing multiplication memory without use rules algorithms especially problematic humans exhibit certain characteristic phenomena performing associative both type error error frequency propose model process associative multiplication compare its performance both these phenomena data normal humans model proposed anderson et al
humans demonstrate remarkable ability generate accurate appropriate motor behavior under many different often uncertain conditions paper describes new modular approach human motor learning control based multiple pairs inverse controller forward predictor models architecture simultaneously learns multiple inverse models necessary control well select inverse models appropriate given environment simulations object manipulation demonstrates ability learn multiple objects appropriate generalization novel objects inappropriate activation motor programs based visual cues followed line correction seen size weight
connectionist systems have representing complex structures system composed simple neuron like computing elements encode complex relations recently researchers have representations extend both time space many researchers have proposed synchronous firing units encode complex representations identify limitations approach present asynchronous model binding effectively represents complex structures asynchronous model extends synchronous approach argue our cognitive architecture utilizes similar mechanism
learning many visual perceptual tasks has been shown specific stimuli while new stimuli require re learning scratch here demonstrate generalization using novel paradigm motion discrimination learning has been previously shown specific trained subjects discriminate directions moving verified previous results learning does transfer trained direction new tracking subjects performance across time new direction found their rate learning therefore learning generalized task previously considered difficult generalization replicated second experiment transfer following training easy stimuli perceptual learning between learning easy vs difficult tasks involve different learning processes operating different visual cortical areas here show interpret these results terms signal detection theory assumption limited computational resources obtain observed phenomena direct transfer change learning rate increasing levels appears human generalization expected behavior generic discrimination system
structure visual scene described many levels coarse level scene composed objects finer level each object made up parts parts work propose simple principle such hierarchical structure extracted visual scenes regularity relations among different parts object weaker than internal structure part principle applied recursively define part whole relationships among elements scene principle does make use object models categories other sorts higher level knowledge rather part whole relationships established based statistics set sample visual scenes illustrate model performs unsupervised decomposition simple scenes model account results human learning experiment relationships
consider problem learning concepts small numbers positive examples humans perform but computers rarely capable machine learning cognitive science present both theoretical analysis empirical study human subjects simple task learning concepts corresponding axis aligned rectangles multidimensional feature space existing learning models applied task cannot explain subjects generalize few examples concept propose principled bayesian model based assumption examples random sample concept learned model gives precise fits human behavior simple task provides qualitative insights into more complex realistic cases concept learning
recent experimental data indicate synaptic connections between neurons depends relative timing preand postsynaptic action potentials hebbian synaptic modification rule based these data leads stable state excitatory inhibitory inputs neuron balanced producing irregular pattern firing has been proposed neurons vivo operate such mode
contrast response function crf many neurons primary visual cortex saturates shifts towards higher contrast values following presentation high contrast visual stimuli using recurrent neural network excitatory spiking neurons adapting synapses show both effects could explained fast slow component synaptic adaptation fast synaptic depression leads saturation crf phase advance cortical response high contrast stimuli ii slow adaptation synaptic release probability derived such mutual information between input output cortical neuron maximal component given infomax learning rule explains contrast adaptation averaged membrane potential component well surprising experimental result stimulus modulated component component cortical cells membrane potential adapts weakly based our results propose new experiment estimate strength effective excitatory feedback cortical neuron suggest relatively simple experimental test our synaptic mechanism contrast adaptation
visually guided arm reaching movements produced distributed neural networks within parietal frontal regions cerebral cortex experimental data indicate single neurons these regions broadly tuned parameters movement appropriate commands populations neurons coordinated action neurons using neuronal population vector npv npv provides estimate movement parameters direction velocity may even fail reflect parameters movement arm changed designed model cortical motor command investigate relation between desired direction movement actual direction movement direction npv motor cortex model two layer self organizing neural network combines broadly tuned cartesian visual information calculate angular motor commands initial part movement two link arm network trained motor positions simulations showed network produced appropriate movement direction over large part workspace small deviations actual trajectory desired trajectory workspace these deviations large deviations npv both trajectories these results suggest npv does give image cortical processing during arm reaching movements correspondence should addressed
cortical has been proposed mechanism enhancing selectivity neurons primary visual cortex less fact same form used de tune selectivity using network model recurrent cortical circuitry propose spatial phase invariance complex cell responses arises through recurrent feedforward input neurons network respond like simple cells low gain complex high gain similar recurrent mechanisms may play role generating invariant representations feedforward input elsewhere visual processing pathway
human animal studies show mammalian brain massive synaptic pruning during removing about half synapses until have previously shown maintaining network memory performance while synapses requires synapses properly modified pruned removing weaker synapses now show neuronal regulation mechanism recently observed maintain average neuronal input field results weight dependent synaptic modification under correct range degradation dimension synaptic upper bound neuronal regulation removes weaker synapses remaining synapses implements near optimal synaptic modification maintains memory performance network undergoing massive synaptic pruning thus paper shows addition known effects hebbian changes neuronal regulation may play important role self organization brain networks during development
determining relationship between activity single nerve cell entire population fundamental question basic neural computation paradigms paper apply information theoretic approach quantify level cooperative activity among cells behavioral context possible discriminate between activity cells vs redundant activity depending difference between information they provide measured jointly information they provide independently define value positive first case negative second show value measured detecting behavioral mode animal simultaneously recorded activity cells observe among cortical cells positive found while cells basal ganglia active during same task do exhibit similar activity tishby address institute computer science center neural computation university tishby
event related potentials portions eeg recordings both phase locked experimental events usually averaged increase their ratio relative non phase locked eeg activity regardless fact response activity single epochs may vary widely time course scalp distribution study applies linear decomposition tool independent component analysis ica single trial eeg records derive spatial filters decompose single trial eeg epochs into sum temporally independent spatially fixed components arising distinct overlapping brain extra brain networks our results normal subjects show ica separate stimulus locked response locked non event related background eeg activities into separate components allowing removal artifacts types single trial eeg records identification both eeg components second study proposes new visualization tool erp image investigating variability amplitudes event evoked responses spontaneous eeg meg records show sorting single trial erp epochs order reaction time potentials clearly reveals underlying patterns response variability linked performance these analysis visualization tools appear broadly applicable research both normal clinical populations analyzing visualizing single trial event related potentials
correlation based learning rule spike level formulated mathematically analyzed compared learning firing rate description differential equation learning dynamics derived under assumption time scales learning spiking separated linear neuron model receives time dependent stochastic input show spike correlations time scale play indeed role correlations between input output spikes tend stabilize structure formation provided form learning window principle conditions intrinsic normalization average synaptic weight discussed
here derive measures quantifying information loss synaptic signal due presence noise sources along weakly active dendrite model dendrite infinite linear noise sources distributed along its length noise sources consider thermal noise channel noise arising stochastic nature voltage dependent channels synaptic noise due spontaneous background activity assess efficacy information transfer using signal detection paradigm objective detect presynaptic spike post synaptic membrane voltage allows us analytically assess role each these noise sources information transfer our choice parameters find synaptic noise dominant noise source limits maximum length over information reliably transmitted
lateral competition within layer neurons response input stimulus here investigate model activity dependent development ocular dominance maps allows vary degree lateral competition weak competition resembles correlation based learning model strong competition becomes self organizing map thus regime weak competition receptive fields shaped second order statistics input patterns whereas regime strong competition higher moments features individual patterns become important correlated localized stimuli two eyes drive cortical development find topographic map binocular localized receptive fields emerge degree competition exceeds critical value ii receptive fields exhibit eye dominance beyond second critical value anti correlated activity between eyes second order statistics drive system develop ocular dominance even weak competition but topography emerges topography established beyond critical degree competition
new paradigm proposed sorting spikes multi electrode data using ratios transfer functions between cells assumed every cell electrode stable linear relation these properties tissue their relative main advantage method insensitive variations shape amplitude spike spike sorting carried out two separate steps first templates describing statistics each spike type generated clustering transfer function ratios spikes detected data using spike statistics these techniques applied data generated escape response system
study effect correlated noise accuracy population coding using model population neurons broadly tuned angle two dimension fluctuations he activity modeled gaussian noise pairwise correlations exponentially difference between preferred orientations pair calculating fisher information system show biologically relevant regime parameters positive correlations decrease estimation capability network relative uncorrelated population moreover strong positive correlations result information capacity saturates finite value number cells population grows contrast negative correlations substantially increase information capacity neuronal population
graphical models provide broad probabilistic applications speech recognition hidden markov models medical diagnosis belief networks artificial intelligence boltzmann machines computing time typically exponential number nodes graph within variational approximating these models present two classes distributions boltzmann machines tractable belief networks go beyond standard approach give mean field equations both these directed undirected approximations simulation results small benchmark problem suggest using these richer approximations compares favorably against others previously reported literature
study dynamics supervised learning layered neural networks regime size training set proportional number inputs here local fields longer described gaussian distributions use dynamical replica theory predict evolution macroscopic observables including relevant error measures incorporating old formalism limit
kernel parameter few parameters support vector machines controlling complexity resulting hypothesis its choice amounts model selection its value usually found means validation set present algorithm automatically perform model selection little additional computational cost need validation set procedure model selection learning separate but kernels dynamically adjusted during learning process find kernel parameter provides best possible upper bound generalisation error theoretical results approach experimental results its validity presented
solve dynamics hopfield type neural networks store sequences patterns close saturation interaction matrix such models leads violation detailed balance out equilibrium statistical mechanical analysis using generating functional methods derive exact closed equations dynamical order parameters sequence overlap correlation response functions limit infinite system size calculate time translation invariant solutions these equations describing stationary limit cycles leads phase effective self interaction usually symmetric models here found vanish causes significantly storage capacity compared hopfield networks static patterns our results tested against extensive computer simulations excellent agreement found
gaussian process gp prediction suffers scaling data set size using finite dimensional basis approximate gp predictor computational complexity reduced derive optimal finite dimensional predictors under number assumptions show superiority these predictors over projected bayes regression method asymptotically optimal show calculate minimal model size given calculations up numerical experiments
describe unifying method proving relative loss bounds online linear threshold classification algorithms such perceptron winnow algorithms classification problems discrete loss used ie total number prediction mistakes introduce continuous loss function called linear hinge loss employed derive updates algorithms first prove bounds wrt linear hinge loss convert them discrete loss introduce notion average margin set examples show relative loss bounds based linear hinge loss converted relative loss bounds discrete loss using average margin
recent works parameter estimation neural coding have demonstrated optimal performance related mutual information between parameters data consider mutual information case dependency parameter vector conditional pdf each observation vector through scalar product derive bounds asymptotic behaviour mutual information compare results obtained same model replica technique
wake sleep algorithm simple learning rule models hidden variables shown algorithm applied factor analysis model linear version helmholtz machine but even factor analysis model general convergence proved theoretically article describe geometrical understanding algorithm contrast em algorithm algorithm result prove convergence algorithm factor analysis model show condition convergence general models
show similarity between belief propagation tap decoding corrupted messages encoded method latter special case gallager error correcting code code word comprises products ix bits selected randomly original message examine efficacy solutions obtained two methods various values ix show solutions may sensitive choice initial conditions case unbiased patterns good approximations obtained generally biased patterns case ix especially temperature being used
following recent results showing importance dimension explaining effect large margin generalization performance current paper investigates implications these results case datasets develops two approaches setting threshold approaches incorporated into boosting algorithm dealing unequal loss functions performance two approaches tested experimentally keywords computational learning theory generalization large margin pac estimates unequal loss datasets
study probabilistic inference large layered bayesian networks represented directed acyclic graphs show intractability exact inference such networks does their effective use give algorithms approximate probabilistic inference exploit averaging phenomena occurring nodes large numbers parents show these algorithms compute rigorous lower upper bounds marginal probabilities interest prove these bounds become exact limit large networks provide rates convergence
analyze asymptotic behavior autoregressive neural network ar nn processes using techniques markov chains non linear time series analysis shown standard ar nns without connections asymptotically stationary linear connections allowed weights determine whether overall system stationary hence standard conditions linear ar processes used
connected recurrent networks have recently been used models neural computations because separation between excitation inhibition biological neural networks study characteristic differences between networks their counterparts showing they have dramatically different dynamical behavior differences exploited computational ends illustrate our results case network selective amplifier
consider recurrent analog neural nets each gate subject gaussian noise other common noise distribution whose probability density function nonzero large set show many regular languages cannot recognized networks type example language begins give precise characterization those languages recognized result implies severe constraints constructing recurrent analog neural nets robust against realistic types analog noise other hand present method analog neural nets robust regard analog noise type
oo margin cumulative training margin distributions adaboost versus our direct optimization margins doom algorithm dark curve adaboost light curve doom doom significant training error improved test error horizontal margin line
study approximation functions two layer feedforward neural networks focusing incremental algorithms add units estimating single unit parameters each stage opposed standard algorithms fixed architectures optimization each stage performed over small number parameters many difficult numerical problems inherent high dimensional non linear optimization establish upper bounds error algorithm approximating functions class thereby extending previous results provided rates convergence functions certain convex hulls functional spaces comparing our results recently derived lower bounds show greedy algorithms nearly optimal combined estimation error results greedy algorithms strong case made type approach
based simple convexity lemma develop bounds different types bayesian prediction errors regression gaussian processes basic bounds formulated fixed training set simpler expressions obtained sampling input distribution equals weight function covariance kernel yielding asymptotically tight results results compared numerical experiments
solve dynamics line hebbian learning perceptrons exactly regime size training set scales linearly number inputs consider both noiseless noisy our calculation cannot extended rules but solution provides nice benchmark test more general advanced theories solving dynamics learning restricted training sets
log log upper bounds vc dimension set neural networks units piecewise polynomial activation functions depth network number hidden units number adjustable parameters maximum number polynomial segments activation function maximum degree polynomials lower bound vc dimension such network set tight cases constant special case vc dimension log
new algorithm support vector regression described priori chosen automatically adjusts flexible minimal radius data such most fraction data points lie outside moreover shown use parametric shapes non constant radius algorithm theoretically experimentally
present exact analytical equilibrium solutions class recurrent neural network models both sequential parallel neuronal dynamics competition between long range synaptic interactions competition found induce novel phenomena well discontinuous transitions between pattern recall states cycles non recall states
consider problem calculating learning curves ie average generalization performance gaussian processes used regression simple expression generalization error terms eigenvalue decomposition covariance function derived used starting point several approximation schemes identify these become exact compare existing bounds learning curves new approximations used input space dimension generally get substantially closer truth
present theory mean field approximation based information geometry theory includes consistent way naive mean field approximation well tap approach linear response theorem statistical physics giving clear information theoretic interpretations them
many belief networks have been proposed composed binary units tasks such object speech recognition produce real valued data binary network models usually independent component analysis ica learns model real data but power model limited begin describing independent factor analysis ifa technique overcomes limitations ica create multilayer network ifa models each level ifa network extracts latent variables non linear functions input data highly adaptive functional form resulting hierarchical distributed representation these data whereas exact maximum likelihood learning network intractable derive algorithm maximizes lower bound likelihood based variational approach
introduce semi supervised support vector machine svm method given training set labeled data working set unlabeled data sgvm constructs support vector machine using both training working sets use sgvm solve transduction problem using overall risk minimization posed vapnik transduction problem estimate value classification function given points working set standard inductive learning problem estimating classification function possible values using fixed function classes working set data propose general sgvm model minimizes both misclassification error function capacity based available data show svm model norm linear support vector machines converted mixed integer program solved exactly using integer programming sgvm standard norm support vector machine approach compared ten data sets our computational results support statistical learning theory results showing incorporating working data improves generalization insufficient training information available every case sgvm either improved showed significant difference generalization compared traditional approach semi supervised support vector machines
learning memory based technique once query received extracts prediction interpolating locally neighboring examples query considered relevant according distance measure paper propose data driven method select query query basis optimal number neighbors considered each prediction efficient way identify validate local models recursive least squares algorithm introduced context local approximation learning furthermore winner takes strategy model selection local combination most promising models explored method proposed tested six different datasets compared state art approach
technique principal component analysis pca has recently been expressed maximum likelihood solution generative latent variable model paper use probabilistic basis bayesian treatment pca our key result effective dimensionality latent space equivalent number principal components determined automatically part bayesian inference procedure important application framework mixtures probabilistic pca models each component determine its own effective complexity
standard techniques eg available learning auto process models simple directly observable dynamical processes sensor noise means dynamics observed approximately learning still been achieved via expectation maximisation em together kalman filtering does handle more complex dynamics involving multiple classes motion problem show here em combined algorithm based propagation random sample sets experiments have been performed visually observed plausible dynamical models found emerge learning process
inference key component learning probabilistic models partially observable data learning temporal models each many inference phases requires over entire long data sequence furthermore data structures manipulated exponentially large making process computationally expensive describe approximate inference algorithm monitoring stochastic processes prove bounds its approximation error paper apply algorithm approximate forward propagation step em algorithm learning temporal bayesian networks provide related approximation backward step prove error bounds combined algorithm show empirically real life domain em using our inference algorithm much faster than em using exact inference almost degradation quality learned model extend our analysis online learning task showing bound error resulting attention small window observations present online em learning algorithm dynamic systems show learns much faster than standard em
present monte carlo generalized em equations learning nonlinear state space models difficulties lie monte carlo step consists sampling posterior distribution hidden variables given observations new idea presented paper generate samples gaussian approximation true posterior easy obtain independent samples parameters gaussian approximation either derived extended kalman filter fisher scoring algorithm case posterior density multimodal propose approximate posterior sum gaussians mixture modes approach show sampling approximate posterior densities obtained above algorithms leads better models than using point estimates hidden states our experiment fisher scoring algorithm obtained better approximation posterior mode than ekf multimodal distribution mixture modes approach gave superior results
propose novel strategy training neural networks using sequential sampling importance resampling algorithms global optimisation strategy allows us learn probability distribution network weights sequential framework well suited applications involving line nonlinear non gaussian non stationary signal processing
examine problem estimating parameters multinomial distribution over large number discrete outcomes most do appear training data analyze problem bayesian perspective develop hierarchical prior incorporates assumption observed outcomes constitute small subset possible outcomes show efficiently perform exact inference form hierarchical prior compare standard approaches
present stochastic clustering algorithm based pairwise similarity datapoints our method extends existing deterministic methods including algorithms min cut graph algorithms connected components thus provides common framework these methods our graph based method differs existing stochastic methods based analogy physical systems stochastic nature our method makes more robust against noise including edges small spurious clusters demonstrate superiority our algorithm using example bands lot noise
expectation maximization em algorithm iterative procedure maximum likelihood parameter estimation data sets missing hidden variables has been applied system identification linear stochastic state space models state variables hidden observer both state parameters model have estimated simultaneously present generalization em algorithm parameter estimation nonlinear dynamical systems expectation step makes use extended kalman smoothing estimate state while maximization step re estimates parameters using these uncertain state estimates general nonlinear maximization step because requires integrating out uncertainty states gaussian radial basis function rbf approximators used model nonlinearities integrals become tractable maximization step solved via systems linear equations stochastic nonlinear dynamical systems examine inference learning discrete time dynamical systems hidden state xt inputs ut outputs yt state evolves according stationary nonlinear dynamics driven inputs additive noise xt ut characters except indices denote vectors matrices represented characters ghahramani zero mean gaussian noise covariance outputs related states inputs yt xt zero mean gaussian noise covariance vector valued assumed differentiable but otherwise arbitrary models kind have been examined decades various most notably nonlinear state space models form modern systems control engineering paper examine these models within framework probabilistic graphical models derive novel learning algorithm them based em best our knowledge first paper addressing learning stochastic nonlinear dynamical systems kind have described within framework em algorithm classical approach system identification treats parameters hidden variables applies extended kalman filtering algorithm described section nonlinear system state vector augmented parameters approach inherently line may important certain applications furthermore provides estimate covariance parameters each time step contrast em algorithm present batch algorithm does attempt estimate covariance parameters three important advantages em algorithm has over classical approach first em algorithm provides straightforward principled method missing inputs outputs second em generalizes readily more complex models combinations discrete real valued hidden variables example formulate em mixture nonlinear dynamical systems third whereas often difficult prove analyze stability within classical line approach em algorithm always attempting maximize likelihood acts lyapunov function stable learning next sections describe basic components learning algorithm expectation step algorithm infer conditional distribution hidden states using extended kalman smoothing section maximization step first discuss general case section describe particular case nonlinearities represented using gaussian radial basis function rbf networks section extended kalman smoothing given system described equations need infer hidden states history observed inputs outputs quantity heart inference problem conditional density ut yi yt captures fact system stochastic therefore our inferences about uncertain gaussian noise assumption less restrictive nonlinear systems than linear systems nonlinearity used generate non gaussian state noise authors have become aware volume have applied em essentially same model method uses multilayer perceptrons mlp approximate nonlinearities requires sampling hidden states fit mlp use gaussian radial basis functions rbfs model nonlinearities fit analytically without sampling see section important use extended kalman algorithm simultaneously estimate parameters hidden states our use eks estimate hidden state part step em learning nonlinear dynamics using em linear dynamical systems gaussian state evolution observation conditional density gaussian recursive algorithm computing its mean covariance known kalman kalman smoothing directly analogous forward backward algorithm computing conditional hidden state distribution hidden markov model special case belief propagation algorithm nonlinear systems conditional density general non gaussian fact quite complex multiple approaches exist inferring hidden state distribution such nonlinear systems including sampling methods variational approximations focus instead paper classic approach engineering extended kalman smoothing eks extended kalman smoothing simply applies kalman smoothing local nonlinear system every point space derivatives vector valued functions define matrices ag respectively dynamics about st mean kalman filter state estimate time xt ft ut da xt output equation similarly prior distribution hidden state gaussian system conditional distribution hidden state time given history inputs outputs gaussian thus kalman smoothing used system infer conditional distribution see figure left panel learning step em algorithm re estimates parameters given observed inputs outputs conditional distributions over hidden states model have described parameters define nonlinearities noise covariances two arise step first may computationally feasible fully re estimate example they represented neural network single full step would training procedure using backpropagation conjugate gradients other optimization method alternatively could use partial steps example each consisting few gradient steps second have trained using uncertain state estimates output eks algorithm consider fitting takes inputs xt ut outputs xt each conditional density estimated eks full covariance gaussian xt xt space so has fit set data points but instead mixture full covariance gaussians input output space gaussian data integrating over type noise non trivial almost form simple but approach problem draw large sample these gaussian uncertain data fit these samples usual way similar situation occurs next section show choosing gaussian radial basis functions model both these vanish forward part kalman smoother kalman filter ghahramani fitting radial basis functions gaussian present general formulation rbf network should clear fit special forms consider following nonlinear mapping input vectors output vector pix bu zero mean gaussian noise variable covariance example form represented using xt ut xt another xt parameters coefficients rbfs hi matrices multiplying inputs respectively output bias vector each rbf assumed gaussian space center ci width given covariance matrix si pix exp goal fit model data data set comes form mixture gaussian distributions here show analytically integrate over mixture distribution fit rbf model assume data set uj min ox uj ox uj dx rewrite slightly different notation using denote expectation over aj defining px objective written min observe samples variables each paired gaussian data over gaussian aj has mean uj covariance matrix let ox hi pix set parameters hi log likelihood single data point under model ox ox maximum likelihood rbf fit mixture gaussian data obtained minimizing following integrated quadratic form learning nonlinear dynamics using em taking derivatives respect setting zero gives linear equations solve tj other words given expectations optimal parameters solved via set linear equations show these expectations computed analytically derivation somewhat but intuition simple gaussian rbfs gaussian densities form new unnormalized gaussians space expectations under these new gaussians easy compute fitting algorithm illustrated right panel figure xt gaussian evidence xt gaussian evidence xt gaussian evidence om tl inputs outputs input dimension figure steps algorithm left panel shows information used extended kalman smoothing eks infers hidden state distribution during step right panel illustrates regression technique employed during step fit mixture gaussian densities required gaussian rbf networks used fit solved analytically dashed line shows regular rbf fit four gaussian densities while solid line shows analytic rbf fit using covariance information dotted lines below show support rbf kernels results tested well our algorithm could learn dynamics nonlinear system observing its inputs outputs system consisted single input state output variable each time relation state time step next given tanh nonlinearity sample outputs system response white noise shown figure left panel initialized nonlinear model linear dynamical model trained em turn initialized variant factor analysis model given rbfs xt space uniformly within range automatically determined density points xt space after initialization over algorithm discovered sigmoid nonlinearity dynamics within less than iterations em figure middle right further experiments need done determine practical method real domains ghahramani figure left data set used training first half testing rest consists time series inputs outputs middle representative log likelihood vs iterations em linear dynamical systems dashed line nonlinear dynamical systems trained described paper solid line note actual likelihood nonlinear dynamical systems cannot generally computed analytically what shown here approximate likelihood computed eks solid curve comes initialization linear dynamics ends nonlinearity starts learned right means xx gaussian posteriors computed eks along sigmoid nonlinearity dashed line rbf nonlinearity learned algorithm point does algorithm actually observe pairs these inferred inputs outputs current model parameters discussion paper together two classic algorithms statistics another systems engineering address learning stochastic nonlinear dynamical systems have shown extended kalman smoothing algorithm state estimation step radial basis function learning model permits analytic solution step em algorithm capable learning nonlinear dynamical model data side effect have derived algorithm training radial basis function network fit data form mixture gaussians our initial approach has three potential limitations first step presented does modify widths rbf kernels possible compute expectations required change widths but requires partial step low dimensional state spaces filling space pre fixed kernels feasible but strategy needs exponentially many rbfs high dimensions second em training slow especially initialized poorly understanding different hidden variable models related help devise initialization heuristics example model used nested initialization first learned simple linear dynamical system turn initialized variant factor analysis third method presented here learns data assumes stationary dynamics have recently extended handle online learning nonstationary dynamics belief network literature has recently been two methods approximate inference markov chain monte carlo variational approximations our knowledge paper first instance extended kalman smoothing has been used perform approximate inference step em while eks does have theoretical guarantees variational methods its simplicity has gained wide estimation control method doing inference nonlinear dynamical systems now exploring generalizations method learning nonlinear multilayer belief networks learning nonlinear dynamics using em acknowledgements would like support supported part nsf center neuromorphic systems engineering expectations required fit rbfs expectations need compute starting easier ones equation xj xx do depend rbf kernel observe gaussian rbf kernel pix equation get gaussian density over mean covariance ci extra constant due lack normalization al exp using ij evaluate other expectations pix finally pix exp ci references fisher scoring mixture modes approach approximate inference learning nonlinear state space models volume mit press ap nm maximum likelihood incomplete data via em algorithm statistical society series jordan ghahramani jaakkola saul
investigate problem learning classification task data represented terms their pairwise representation does refer explicit feature representation data items thus more general than standard approach using euclidean feature vectors pairwise always calculated our first approach based combined linear embedding classification procedure resulting extension optimal hyperplane algorithm pseudo euclidean data alternative present another approach based linear threshold model proximity values themselves optimized using structural risk minimization show prior knowledge about problem incorporated choice distance measures examine different metrics wrt their generalization finally algorithms successfully applied protein structure data data cats cerebral cortex they show better performance than nearest neighbor classification
adaptive ridge special form ridge regression balancing quadratic each parameter model shown equivalent lasso least absolute selection operator sense both procedures produce same estimate lasso thus viewed particular quadratic observation derive fixed point algorithm compute lasso solution analogy provides new hyper parameter tuning effectively model complexity finally present series possible extensions lasso performing sparse regression kernel smoothing additive modeling neural net training
cluster analysis fundamental principle exploratory data analysis providing user description group structure given data key problem context interpretation visualization clustering solutions high dimensional abstract data spaces particular probabilistic descriptions group structure essential capture inter cluster relationships hardly simple probabilistic assignment variables present novel approach visualization group structure based statistical model object assignments have been observed estimated probabilistic clustering procedure objects data points embedded low dimensional euclidean space approximating observed data statistics gaussian mixture model algorithm provides new approach visualization inherent structure broad variety data types eg histogram data proximity data co occurrence data demonstrate power approach histograms images example large scale data mining application
paper reveals previously ignored connection between two important fields regularization independent component analysis ica show least representative broad class algorithms regularizers reduce network complexity extracts independent features product algorithm flat minimum search recent general method finding low complexity networks high generalization capability works minimizing both training error required weight precision according our theoretical analysis hidden layer trained attempts coding each input sparse code few simple features possible experiments method extracts optimal codes difficult versions noisy bars benchmark problem separating underlying sources whereas ica pca fail real world images coded fewer bits per pixel than ica pca
data refers domain two finite sets objects observations made ie pairs element either set type data arises naturally many application ranging computational information retrieval preference analysis computer vision paper present systematic domain independent framework learning dyadic data statistical mixture models our approach covers different models fiat hierarchical latent class structures propose annealed version standard em algorithm model fitting empirically evaluated variety data sets different domains
sparse coding method finding representation data each components representation rarely significantly active such representation closely related redundancy reduction independent component analysis has neurophysiological plausibility paper show sparse coding used denoising using maximum likelihood estimation nongaussian variables corrupted gaussian noise show apply nonlinearity components sparse coding so reduce noise furthermore show choose optimal sparse coding basis denoising our method closely related method wavelet but has important benefit over wavelet methods both features parameters estimated directly data
task text retrieval find subset collection documents relevant users information usually expressed set words documents queries represented vectors word counts its simplest form relevance defined dot product between document query vector measure number common terms central difficulty text retrieval presence absence word sufficient determine relevance query linear dimensionality reduction has been proposed technique extracting underlying structure document collection domains such vision dimensionality reduction reduces computational complexity text retrieval more often used improve retrieval performance propose alternative novel technique produces sparse representations constructed sets highly related words documents queries represented their distance these sets relevance measured number common clusters technique significantly improves retrieval performance efficient compute shares properties optimal linear projection operator independent components documents
generative probability models such hidden markov models provide principled way treating missing information dealing variable length sequences other hand discriminative methods such support vector machines enable us construct flexible decision boundaries often result classification performance superior model based approaches ideal classifier should combine these two complementary approaches paper develop natural achieving combination deriving kernel functions use discriminative methods such support vector machines generative probability models provide theoretical justification combination well demonstrate substantial improvement classification performance context dna protein sequence analysis
present conditional maximization algorithm extension em maximization algorithm conditional density estimation under missing data bounding maximization process given specifically optimize conditional likelihood instead usual joint likelihood apply method conditioned mixture models use bounding techniques derive models update rules monotonic convergence computational efficiency regression results superior em demonstrated
principal curves have been defined self consistent smooth curves pass through middle dimensional probability distribution data recently have offered new approach defining principal curves continuous curves given length minimize expected squared distance between curve points space randomly chosen according given distribution new definition made possible carry out theoretical analysis learning principal curves training data paper propose practical construction based new definition simulation results demonstrate new algorithm compares favorably previous methods both terms performance computational complexity
present unsupervised classification algorithm based ica mixture model ica mixture model assumes observed data into several mutually exclusive data classes components each class generated linear mixture independent sources algorithm finds independent sources mixing matrix each class computes class membership probability each data point approach extends gaussian mixture model so classes have non gaussian structure demonstrate method learn efficient codes represent images natural scenes text learned classes basis functions yield better approximation underlying distributions data thus provide greater coding efficiency believe method well suited modeling structure high dimensional data has many potential applications
directed generative model binary data using small number hidden continuous units investigated clipping nonlinearity distinguishes model conventional principal components analysis relationships between correlations underlying continuous gaussian variables binary output variables utilized learn appropriate weights network advantages approach illustrated invariant binary distribution handwritten digit images
introduce two new techniques density estimation our approach poses problem supervised learning task performed using neural networks introduce stochastic method learning cumulative distribution analogous deterministic technique demonstrate convergence our methods both theoretically experimentally provide comparisons parzen estimate our theoretical results demonstrate better convergence properties than parzen estimate
two nonlinear latent variable models based radial basis functions discussed first use priors constraints models considered means preserving data structure low dimensional representations purposes resampling approach introduced makes more effective use latent samples evaluating likelihood
present new energy minimization framework graph problem based equivalent maximum clique formulation approach centered around fundamental result proved mid recently expanded various ways allows us formulate maximum clique problem terms standard quadratic program solve program use equations class simple discrete time dynamical systems developed various branches theoretical biology show despite their inability escape local solutions they nevertheless provide experimental results competitive those obtained using more mean field annealing heuristics
training support vector machine svm requires solution large quadratic programming qp problem paper proposes algorithm training svms sequential minimal optimization smo smo breaks large qp problem into series smallest possible qp problems analytically solvable thus smo does require numerical qp computation time evaluation kernel hence kernel substantially smo mnist database smo times fast chunking while uci adult database linear svms smo times faster than chunking algorithm
boosting methods maximize hard classification margin known powerful techniques do exhibit overfitting low noise cases noisy data boosting try hard margin thereby give much weight outliers leads non smooth fits overfitting therefore propose three algorithms allow soft margin classification introducing regularization variables into boosting concept regularized versions linear quadratic programming adaboost experiments show usefulness proposed algorithms comparison another soft margin classifier support vector machine
signal processing pattern recognition algorithms make extensive use convolution many cases computational accuracy important computational speed feature extraction instance features interest signal usually quite distorted form noise level quantization order achieve faster feature extraction our approach consists approximating regions signal low degree polynomials resulting signals order obtain functions derivatives functions representation convolution becomes extremely simple implemented quite effectively true convolution recovered integrating result convolution method yields substantial speed up feature extraction applicable convolutional neural networks
describe new iterative method parameter estimation gaussian mixtures new method based framework developed kivinen warmuth supervised line learning contrast gradient descent em estimate mixtures covariance matrices proposed method estimates covariance matrices furthermore new parameter estimation procedure applied both line batch settings show experimentally typically faster than em usually requires about half many iterations em
models useful tools case domain knowledge exists about function estimated emphasis put onto model extend two learning algorithms support vector machines linear programming machines case give experimental results sv machines
present probabilistic latent variable framework data key feature its applicability binary data types few established methods exist variational approximation likelihood exploited derive fast algorithm determining model parameters application real synthetic binary data sets given
present split merge em algorithm overcome local maximum problem parameter estimation finite mixture models case mixture models non global maxima often involve having many components mixture model part space few another widely separated part space escape such configurations repeatedly perform simultaneous split merge operations using new criterion efficiently selecting split merge candidates apply proposed algorithm training gaussian mixtures mixtures factor using synthetic real data show effectiveness using split merge operations improve likelihood both training data held out test data
hierarchical representation data has various applications domains such data mining machine vision information retrieval paper extension expectation maximization em algorithm learns mixture hierarchies computationally efficient manner efficiency achieved bottom up fashion ie clustering mixture components given level hierarchy obtain those level above clustering requires knowledge mixture parameters being need intermediate samples addition practical applications algorithm allows new interpretation em makes clear relationship non parametric kernel based estimation methods provides explicit over trade off between bias variance em estimates offers new insights about behavior deterministic annealing methods commonly used em escape local minima likelihood
gaussian process regression covariance between outputs input locations usually assumed depend distance positive definite matrix often taken diagonal but allow general positive definite matrix tuned basis training data analysis shows effectively creating hidden features dimensionality hidden feature space determined data demonstrate superiority predictions using general matrix over those based diagonal matrix two test problems
propose new sample cross validation based method randomized choosing smoothing bandwidth parameters bias variance fit complexity tradeoff soft classification soft classification refers learning procedure estimates probability example given attribute vector class vs class target optimizing tradeoff kullback distance between estimated probability distribution true probability distribution representing knowledge infinite population method uses randomized estimate trace hessian cross validation cost single outcome data
wavelet basis selection procedure presented wavelet regression both basis threshold selected using crossvalidation method includes capability incorporating prior knowledge smoothness shape basis functions into basis selection procedure results method demonstrated using widely published sampled functions results method other basis function based methods
paper introduce new class image models call dynamic trees dynamic tree model specifies prior over large number trees each tree structured belief net experiments show capable generating images less models have better translation invariance properties than fixed balanced show simulated annealing effective finding trees have high posterior probability
paper problem visual search bayesian inference defines bayesian ensemble problem instances particular address problem detection visual contours optimizing global criterion combines local intensity geometry information analyze convergence rates search algorithms using results information theory bound probability rare events within bayesian ensemble analysis determines characteristics domain call order parameters determine convergence rates particular present specific admissible algorithm pruning converges high probability expected time size problem addition briefly summarize extensions work address fundamental limits target contour ie algorithm independent results use non admissible heuristics
paper present novel approach blind deconvolution assuming both mixing demixing models described stable linear state space systems decompose blind separation problem into two process separation state estimation based minimization kullback leibler divergence develop novel learning algorithm train matrices output equation estimate state demixing model introduce new concept called hidden numerically implement kalman filter computer simulations given show validity high effectiveness state space approach
present analog vlsi cellular architecture implementing version boundary contour system real time image processing inspired neuromorphic models across several layers visual cortex design integrates each pixel functions simple cells complex cells hyper complex cells cells three orientations interconnected grid analog current mode cmos circuits used throughout perform edge detection local inhibition selective long range kernels global gain control experimental results fabricated pixel prototype cmos technology demonstrate robustness architecture selecting image contours cluttered noisy background
modular analogue neuro chip set chip learning capability developed active noise analogue neuro chip set incorporates error backpropagation learning rule practical applications allows interconnections multi chip developed neuro board demonstrated active noise without digital signal processor multi path acoustic channels random noise nonlinear distortion speaker adaptive learning circuits neuro chips experimental results reported car noise real time
paper describe architecture implementation experimental results classification compression chip chip processes vector dimensional analogue vectors while consuming maximum power heart rate per minute vector per second supply represents significant advance previous work achieved low power supervised morphology classification template matching scheme used chip enables unsupervised blind classification rhythms computational support low bit rate data compression adaptive template matching scheme used tolerant amplitude variations intra sample time shifts
performance vlsi neural processing hardware depends critically design implemented algorithms have previously proposed algorithm acoustic transient classification having implemented demonstrated algorithm mixed mode architecture now investigate variants algorithm using time frequency channel input output normalization schemes train template values goal achieving optimal classification performance chosen hardware
circuit fast compact low power focal plane motion centroid localization presented chip uses mixed signal cmos components implement edge detection set detection centroid localization models retina superior colliculus centroid localization circuit uses time triggered row column address events two linear resistive grids provide analog coordinates motion centroid vlsi chip used realize fast obstacle avoiding line following algorithm discussed
describe first single microphone sound localization system its theories human sound localization caused external ear allow humans estimate sound source using ear our single microphone localization model relies specially shaped reflecting structure serves role specially designed analog vlsi circuitry uses time processing localize sound cmos integrated circuit has been designed fabricated successfully demonstrated actual sounds
robust algorithm presented computing position focus expansion axis rotation singular point optical flow fields such those generated self motion measurements shown fully parallel cmos analog vlsi motion sensor array computes direction local motion sign optical flow each pixel directly implement algorithm flow field singular point computed real time power consumption less than row computation singular point more general flow fields requires measures field expansion rotation shown computed real time hardware again using sign optical flow field these measures along location singular point provide robust real time self motion information visual guidance moving platform such robot
paper presents novel fast nn classifier based binary correlation matrix memory neural network robust encoding method developed meet input requirements hardware implementation described gives over times speed current mid range large problems tested several benchmarks compared simple nn method classifier gave less than lower over times speed up software hardware respectively
common way represent time series into blocks each represented set basis functions limitation approach temporal alignment basis functions underlying structure time series arbitrary present algorithm encoding time series does require blocking data algorithm finds efficient representation inferring best temporal positions functions kernel basis these have arbitrary temporal extent constrained orthogonal allows model capture structure signal may occur arbitrary temporal positions preserves relative temporal structure underlying events model shown equivalent sparse highly overcomplete basis under model mapping data representation nonlinear but computed efficiently form allows use existing methods adapting basis itself data approach applied speech data results shift invariant spike like representation resembles coding cochlear nerve
paper introduces method regularization hmm systems avoids parameter overfitting caused insufficient training data regularization done augmenting em training method penalty term favors simple smooth hmm systems penalty term constructed mixture model negative exponential distributions assumed generate state dependent emission probabilities hmms new method successful transfer well known regularization approach neural networks hmm domain interpreted generalization traditional state hmm systems effect regularization demonstrated continuous speech recognition tasks improving models speaker adaptation limited training data
describe maximum likelihood continuity mapping malcom alternative hidden markov models hmms processing sequence data such speech while hmms have discrete hidden space constrained fixed finite automaton architecture malcom has continuous hidden space continuity map constrained smoothness requirement paths through space malcom fits into same probabilistic framework speech recognition hmms but represents more realistic model speech production process evaluate extent malcom captures speech production information generated continuous speech continuity maps three speakers used paths through them predict measured speech articulator data median correlation between malcom paths obtained speech articulator measurements independent test set used train malcom predictor unsupervised model achieved correlations over speakers lower than those obtained using analogous supervised method used measurements well
investigate probabilistic framework automatic speech recognition based intrinsic geometric properties curves particular analyze setting two variables continuous discrete evolve jointly time suppose vector traces out smooth multidimensional curve variable evolves stochastically function arc length along curve arc length does depend rate curve gives rise family markov processes whose predictions invariant nonlinear time describe use such models known markov processes curves automatic speech recognition acoustic feature trajectories phonetic two tasks recognizing new names connected digits find yield lower word error rates than comparably trained hidden markov models
has been much recent work measuring image statistics learning probability distributions images observe mapping images statistics many show quantified phase space factor phase space approach light minimax entropy technique learning gibbs distributions images potentials derived image statistics ambiguities inherent determining potentials addition shows phase factor approximated analytic distribution approximation yields algorithm reduces computation time minimax entropy learning illustration concept using gaussian approximate phase factor gives good approximation results seconds cpu time phase space approach gives insight into multi scale potentials found suggests forms potentials influenced greatly phase space considerations finally prove probability distributions learned feature space alone equivalent minimax entropy learning multinomial approximation phase factor
present method learning complex appearance mappings such occur images articulated objects traditional interpolation networks fail case appearance necessarily smooth function nor linear manifold articulated objects define appearance mapping examples constructing set independently smooth interpolation networks these networks cover overlapping regions parameter space set growing procedure used find example clusters well approximated within their convex interpolation proceeds within these sets examples method physically valid images produced even regions parameter space nearby examples have different show results generating both simulated real arm images
seek scene interpretation best explains image data example may want infer projected velocities scene best explain two consecutive image frames image synthetic data model relationship between image scene patches between scene patch neighboring scene patches given new image propagate likelihoods markov network ignoring effect loops infer underlying scene yields efficient method form low level scene interpretations demonstrate technique motion analysis estimating high resolution images low resolution ones
finding articulated objects like people pictures presents particularly difficult object recognition show find people finding body segments constructing assemblies those segments consistent constraints appearance person result kinematic properties reasonable model person requires least segments possible present every group classifier instead search pruned using projected versions classifier accepts groups corresponding people describe efficient projection algorithm popular classifier demonstrate our approach used whether images real scenes contain people
previously proposed quantitative model early visual processing primates based non linearly interacting visual filters statistically efficient decision now use model interpret observed modulation range human psychophysical thresholds without focal visual attention our model automatic fitting procedure simultaneously thresholds four classical pattern discrimination tasks performed while attention another concurrent task our model predicts seemingly complex improvements certain thresholds observed attention fully available discrimination tasks best explained competition among early visual filters
visual search task finding target image against background unique features targets enable them pop out against background while targets defined features conjunctions features more difficult spot known ease target detection change roles figure ground mechanisms underlying ease pop out visual search have been paper shows model segmentation based interactions explain many qualitative aspects visual search
face recognition class problem number known individuals support vector machines svms binary classification method face recognition problem output svm classifier developed svm based face recognition algorithm face recognition problem formulated problem difference space models dissimilarities between two facial images difference space formulate face recognition two class problem classes dissimilarities between faces same person dissimilarities between faces different people modifying interpretation decision surface generated svm generated similarity metric between faces learned examples differences between faces svm based algorithm compared principal component analysis pca based algorithm difficult set images database performance measured both verification identification scenarios identification performance svm versus pca verification equal error rate svm pca
most important problems visual perception visual invariance objects perceived same despite undergoing transformations such translations rotations scaling paper describe bayesian method learning invariances based lie group theory show previous approaches based first order taylor series expansions inputs regarded special cases lie group approach latter being capable handling principle arbitrarily large transformations using based generative model images derive unsupervised algorithm learning lie group operators input data containing transformations line unsupervised learning algorithm maximizes posterior probability generating training data provide experimental results suggesting proposed method learn lie group operators handling reasonably large translations rotations
present probabilistic method fusion images produced multiple sensors approach based image formation model sensor images noisy locally linear functions underlying true scene bayesian framework provides maximum likelihood maximum posteriori estimates true scene sensor images maximum likelihood estimates parameters image formation model involve local second order image statistics thus related local principal component analysis demonstrate efficacy method images visible band sensors
recent neural model illusory contour formation based distribution natural shapes particles moving constant speed directions given motions input model consists pairs position direction constraints output consists distribution contours joining such pairs general these contours closed their distribution scale invariant paper show compute scale invariant distribution closed contours given position constraints alone use result explain well known illusory contour effect
paper describes simple efficient method make template based object classification invariant plane rotations task divided into two parts orientation discrimination classification key idea orientation discrimination before classification accomplished turn input image belongs each class interest image rotated maximize its similarity training images each class these contain prototype object upright orientation process yields set images least have object upright position resulting images classified models have been trained upright examples approach has been successfully applied two real world vision based tasks rotated handwritten digit recognition rotated face detection cluttered scenes
paper presents probabilistic modeling methods solve problem discriminating between five facial orientations little labeled data three models explored first model maintains inter pixel dependencies second model capable modeling set arbitrary pair wise dependencies last model allows dependencies between neighboring pixels show three these models accuracy learned models greatly improved augmenting small number labeled training images large set unlabeled images using expectation maximization important because often difficult obtain image labels while many unlabeled images readily available through large set empirical tests examine benefits unlabeled data each models using two randomly selected labeled examples per class discriminate between five facial orientations accuracy six labeled examples achieve accuracy
gaussian processes provide good prior models spatial data but smooth many physical situations discontinuities along bounding surfaces example near surface wind fields describe modelling method such constrained demonstrate infer model parameters wind fields mcmc sampling
high energy physics experiments has sort through high events rate tens mhz select few interest key factors making decision location interaction led event took place here present novel solution problem finding location based two feedforward neural networks fixed architectures whose parameters chosen so obtain high accuracy system tested simulated data sets shown perform better than conventional algorithms
neural network performs both identification placing test patterns classes encountered during training discrimination whether test pattern belongs classes encountered during training performance tested radar pulse data obtained field compared nearest neighbor based algorithm extension
computer through numerical simulation physics based models offers but computationally demanding paper demonstrates possibility replacing numerical simulation nontrivial dynamic models dramatically more efficient exploits neural networks automatically trained off line physical dynamics through observation physics based models action depending model its neural network yield physically realistic two orders magnitude faster than conventional numerical simulation demonstrate variety physics based models
causes substantial detection systems automatically detect use network used problem previous approaches features derived call patterns individual users paper present call based detection system based hierarchical regime switching model detection problem formulated inference problem regime probabilities inference implemented applying junction tree algorithm underlying graphical model dynamics learned data using em algorithm subsequent discriminative training methods assessed using data real mobile communication network
paper describes bayesian graph matching algorithm data mining large structural data bases matching algorithm uses edge consistency node attribute similarity determine probability query graph each candidate matches data base node feature vectors constructed computing histograms pairwise geometric attributes attribute similarity assessed computing distance between histograms recognition selecting candidate data base has largest probability illustrate recognition technique data base containing line patterns extracted real world imagery here recognition technique shown significantly outperform number algorithm alternatives
execution order block computer instructions make difference its running time factor two more order achieve best possible speed use heuristic appropriate each specific architecture implementation these heuristic time consuming expensive build paper present results using both reinforcement learning construct heuristics scheduling basic blocks outperformed commercial reinforcement learning performed almost well commercial
previous work advanced new technique direct visual matching images purposes face recognition image retrieval using probabilistic measure similarity based primarily bayesian map analysis image differences leading dual basis similar performance advantage probabilistic matching technique over standard euclidean nearest neighbor matching recently demonstrated using results face recognition competition probabilistic matching algorithm found top have further developed simple method replacing costly nonlinear online bayesian similarity measures relatively inexpensive computation linear subspace projections simple online euclidean thus resulting significant computational speed up implementation large image databases typically encountered real world applications
propose train trading systems optimizing financial objective functions via reinforcement learning performance functions consider ratio our recently proposed differential ratio online learning moody presented empirical results demonstrate advantages reinforcement learning relative supervised learning here extend our previous work compare learning our recurrent reinforcement learning algorithm provide new simulation results demonstrate presence stock index year period through well sensitivity analysis provides economic insight into structure
describe real time computer vision machine learning system modeling recognizing human actions interactions two different domains explored recognition two motions art multiple person interactions visual task our system combines top down bottom up information using feedback loop formulated bayesian framework two different graphical models hmms coupled hmms used modeling both individual actions multiple agent interactions shown work more efficiently accurately given amount training finally overcome limited amounts training data demonstrate synthetic agents style agents used develop flexible prior models person person interactions
calcium ubiquitous intracellular cellular processes such cell number different cell types respond stimuli periodic oscillations intracellular free calcium concentration cai these ca signals often organized complex temporal spatial patterns even under conditions sustained stimulation here study spario temporal aspects intracellular calcium cai oscillations cells cells under stimulation et al use novel fast fixed point algorithm independent component analysis ica blind source separation spario temporal dynamics cai cell using approach find two significant independent components out five differently mixed input signals cai signal mean oscillatory period high frequency signal broadband power spectrum considerable spectral density results good agreement study high frequency cai oscillations et al further theoretical experimental studies have performed resolve question functional impact intracellular these independent cai signals et al
have previously presented coarse fine hierarchical network hpnn architecture combines multiscale image processing techniques neural networks paper present applications general architecture two problems computer diagnosis first application detection coarse fine hpnn designed learn large scale context detecting small objects like receiver operating characteristic roc analysis suggests hierarchical architecture improves detection performance well established system roughly second application detect directly large extended objects coarse fine hpnn architecture suitable problem instead construct fine coarse hpnn architecture designed learn small scale detail structure associated extended objects our initial results applying fine coarse hpnn mass detection encouraging detection improvements about conclude ability hpnn architecture integrate information across scales both coarse fine fine coarse makes well suited detecting objects may have contextual detail structure occurring scales other than natural scale object
paper applies mixture gaussians probabilistic model combined expectation maximization optimization task three dimensional range data mobile robot provides flexible way dealing uncertainties sensor information allows
collective intelligence set interacting reinforcement learning rl algorithms designed automated fashion so their collective behavior optimizes global utility function summarize theory present experiments using theory design control traffic routing these experiments indicate outperform previously investigated rl based shortest path routing algorithms
game moore moore moore atkeson reinforcement learning rl algorithm has lot promise overcoming curse dimensionality rl algorithms applied high dimensional problems paper introduce modifications algorithm further improve its performance robustness addition while game solutions improved locally standard local path improvement techniques introduce add algorithm same spirit game instead tries improve solutions non local manner
simple learning rule derived vaps algorithm instantiated generate wide range new algorithms these algorithms solve number open problems define several new approaches reinforcement learning unify different approaches reinforcement learning under single theory these algorithms have guaranteed convergence include modifications several existing algorithms known fail converge simple mdps these include sarsa advantage learning addition these value based algorithms generates pure policy search reinforcement learning algorithms learn optimal policies without learning value function addition allows value based algorithms combined thus unifying two different approaches reinforcement learning into single value policy search vaps algorithm these algorithms converge pomdps without requiring proper belief state simulations results given several areas future research discussed convergence greedy exploration many reinforcement learning algorithms known use parameterized function approximator represent value function adjust weights incrementally during learning examples include learning sarsa advantage learning simple mdps original form these algorithms fails converge summarized table cases algorithms guaranteed converge under reasonable assumptions such gradient descent general reinforcement learning table current convergence results incremental value based rl algorithms residual algorithms changed every first two columns new algorithms proposed change fixed greedy policy distribution lookup table markov chain linear nonlinear lookup table mdp linear nonlinear lookup table pomdp nonlinear convergence guaranteed known either oscillates between best worst possible policies learning rates cases known either oscillate between best worst possible policies have different values happen even infinite training time slowly decreasing learning rates baird gordon each first two columns changed made converge using modified form algorithm residual form baird but possible learning fixed training distribution rarely practical most large problems useful explore policy usually greedy respect current value function changes value function changes case column current convergence guarantees good way guarantee convergence three columns modify algorithm so performing stochastic gradient descent average error function average weighted state visitation frequencies current usually greedy policy weighting changes policy changes might appear gradient difficult compute consider exploring distribution usually greedy respect learned function seems difficult calculate gradients changing single weight change many values changing single value change many action choice probabilities state changing single action choice probability may affect frequency every state mdp visited although might seem difficult surprisingly unbiased estimates gradients visitation distributions respect weights calculated quickly resulting algorithms put every case table derivation vaps equation consider sequence transitions observed while following particular stochastic policy mdp let st xt sequence states actions up time performing action state yields reinforcement transition state baird moore stochastic policy may function vector weights assume mdp has single start state named mdp has terminal states terminal state let st set possible sequences time let given error function calculates error each time step such squared bellman residual time other error occurring time function weights smooth function weights consider period time starting time probability after sequence st occurs probabilities such expected squared period length finite let expected total error during period expectation weighted according state visitation frequencies generated given policy ends time after trajectory es es ps pst put sl note first line particular st error added once every sequence starts st each these terms weighted probability complete trajectory starts st sum probabilities trajectories start st simply probability st being observed period assumed end eventually probability so second line equals first third line probability sequence factor might function so probability smooth function weights nonzero everywhere partial derivative respect particular element weight vector es es es jl space here limited may clear short sketch derivation but summing over entire period does give unbiased estimate expected total error during period incremental algorithm perform stochastic gradient descent weight update given left side table summation over previous time steps replaced trace each weight algorithm more general than previously published algorithms form function previous states actions rather than current reinforcement what allows vaps do both value policy search every algorithm proposed paper special case vaps equation left side table note model needed algorithm probability needed algorithm policy transition probability mdp stochastic gradient descent update rule correct observed transitions sampled trajectories found following gradient descent general reinforcement learning table general vaps algorithm left several right single algorithm includes both value based policy search approaches their combination guaranteed convergence case er qx qx max current stochastic policy both should smooth functions given vector should bounded algorithm simple but actually generates large class different algorithms depending choice trace reset zero single sequence sampled following current policy sum along sequence give unbiased estimate true gradient finite variance therefore during learning weight updates made end each trial weights stay within bounded region learning rate approaches zero converge probability adding weight decay term constant times norm weight vector onto prevent weight divergence small initial learning rates guarantee global minimum found using general function approximators but least converge true backprop well vaps algorithm many reinforcement learning algorithms value based they try learn value function satisfies bellman equation examples learning learns value function actor critic algorithms learn value function policy greedy respect td learns value function based future rewards other algorithms pure policy search algorithms they directly learn policy returns high rewards these include reinforce williams backprop through time learning automata genetic algorithms algorithms proposed here combine two approaches they perform value policy search vaps general vaps equation instantiated choosing expression bellman residual yielding value based reinforcement yielding policy search linear combination two yielding value policy search single vaps update rule left side table generates variety different types algorithms described following sections reducing mean squared residual per trial mdp has terminal states trial time start until terminal state reached possible minimize expected total error per trial trace zero start each trial convergent form sarsa learning incremental value iteration advantage learning generated choosing squared bellman residual shown right side table each case expected value taken over possible xt baird moore given policy smooth nonzero function weights so could greedy policy chooses greedy action probability chooses uniformly otherwise would cause gradient two values state equal but policy could something approaches greedy positive temperature approaches zero number possible actions each state each instance table other than value iteration gradient estimated using two independent unbiased estimates expected value example qx qx estimate true gradient residual algorithm described baird retains guaranteed convergence but may learn more quickly than pure gradient descent values note gradient time uses primed variables means new state action time generated independently state action time course mdp deterministic primed variables same mdp but model known model evaluated additional time get other state model known three choices first model could learned past data evaluated give independent sample second issue could ignored simply variables place primed variables may affect quality learned function depending random mdp but doesnt stop convergence acceptable approximation practice third past transitions could recorded primed variables could found searching times has been seen before randomly choosing those transitions using its state action primed variables equivalent learning certainty equivalence model sampling so special case first choice extremely large state action spaces many starting states likely give same result practice simply variables primed variables note weights do effect policy these algorithms reduce standard residual algorithms baird possible reduce mean squared residual per step rather than per trial done making period lengths independent policy so minimizing error per period minimize error per step example period might defined first steps after traces reset state start state note every state action pair has positive chance being seen first steps solving finite horizon problem actually solving discounted infinite horizon problem reducing bellman residual every state but weighting residuals determined what happens during first steps many different problems solved vaps algorithm definition period different ways policy search value based learning possible add term tries maximize reinforcement directly example could defined rather than table gradient descent general reinforcement learning oo beta figure pomdp number trials needed learn vs combination policy search value based rl outperforms either alone trace reset zero after each terminal state reached constant does affect expected gradient but does affect noise distribution discussed williams algorithm try learn function satisfies bellman equation before directly learns policy minimize expected total discounted reinforcement resulting function may even close containing true values satisfying bellman equation give good policy between algorithm tries both satisfy bellman equation give good greedy policies similar modification made algorithms table special case algorithm reduces reinforce algorithm williams reinforce has been special case gaussian action distributions extensions appear marbach case pure policy search particularly interesting because need kind model generating two independent other algorithms have been proposed finding policies directly such those given various algorithms learning automata theory summarized vaps algorithms proposed here appears first unifying these two approaches reinforcement learning finding value function both approximates bellman equation solution directly optimizes greedy policy figure shows simulation results combined algorithm run said have learned greedy policy optimal consecutive trials graph shows average plot runs different initial random weights between learning rate optimized separately each value ri leaving state leaving state end otherwise algorithm used modified learning table exploration equation states share same parameters so ordinary sarsa greedy learning could never converge shown gordon pure value based new algorithm converges but course cannot learn optimal policy start state those two values learn equal pure policy search learning converges optimality but slowly value function results long sequence states near end combining two approaches new algorithm learns much more quickly than either alone interesting vaps algorithms described last three sections applied directly partially observable markov decision process pomdp true state hidden available each time step baird moore ambiguous observation function true state normally algorithm such sarsa has guaranteed applied mdp vaps algorithms converge such cases conclusion new algorithm has been presented special cases give new algorithms similar learning sarsa advantage learning but guaranteed convergence wider range problems than previously possible including pomdps first time these guaranteed converge even exploration policy changes during learning other special cases allow new approaches reinforcement learning tradeoff between satisfying bellman equation improving greedy policy mdp simulation showed combined algorithm learned more quickly than either approach alone unified theory unifying first time both value based reinforcement learning theoretical interest practical value simulations performed future research unified framework may able empirically analytically address old question better learn value functions better learn policy directly may light new question best do both once acknowledgments research part us air force references baird residual algorithms reinforcement learning function approximation eds machine learning proceedings international conference morgan san ca gordon stable fitted reinforcement learning tesauro mozer eds advances neural information processing systems pp mit press cambridge ma reinforcement learning its application control technical report university massachusetts ma littman planning acting partially observable stochastic domains artificial intelligence appear available now marbach simulation based optimization markov decision processes th massachusetts institute technology reinforcement learning selective perception hidden state department computer science university ny learning automata
paper examines application reinforcement learning problem problem requires maximized while simultaneously quality service constraint into certain states present general solution multi criteria problem able significantly higher than alternatives
classifier systems now viewed because their problems such rule strength vs rule set performance problem credit assignment problem order solve problems have developed hybrid classifier system generalization learning system designing view model free learning pomdps take hybrid approach finding best generalization given total number rules uses policy improvement procedure jaakkola et al locally optimal stochastic policy set rule conditions given uses ga search best set rule conditions
paper address two issues long standing interest reinforcement learning literature first what kinds performance guarantees made learning after finite number actions second what quantitative comparisons made between learning model based indirect approaches use experience estimate next state distributions off line value iteration first show both learning indirect approach rather rapid convergence optimal policy function number state transitions observed particular order transitions sufficient both algorithms come within optimal policy idealized model assumes observed transitions well mixed throughout state mdp thus two approaches have roughly same sample complexity perhaps surprisingly sample complexity far less than what required model based approach actually construct good approximation next state distribution result shows amount memory required model based approach closer than either approach remove assumption observed transitions well mixed consider model transitions determined fixed arbitrary exploration policy bounds number transitions required order achieve desired level performance related stationary distribution mixing time policy
learning real time popular control method planning plan execution has been shown solve search problems known environments efficiently paper apply problem getting given goal location initially unknown environment maximal always moves shortest path closest state closest potential goal state good exploration heuristic but show does minimize worst case plan execution time compared other exploration methods result interest reinforcement learning researchers many reinforcement learning methods use asynchronous dynamic programming planning plan execution exhibit face uncertainty like
agents acting real world problem making good decisions limited knowledge environment partially observable markov decision processes pomdps model decision problems agent tries maximize its reward face limited sensor feedback recent work has shown empirically reinforcement learning rl algorithm called sarsa efficiently find optimal memoryless policies map current observations actions pomdp problems singh sarsa algorithm uses form short term memory called eligibility trace temporally delayed rewards observation action pairs lead up reward paper explores effect eligibility traces ability sarsa algorithm find optimal memoryless policies variant sarsa called step truncated sarsa applied four test problems taken recent work littman littman empirical results show eligibility traces significantly truncated without affecting ability sarsa find optimal memoryless policies pomdps
reinforcement learning methods used improve performance local search algorithms combinatorial optimization learning evaluation function predicts outcome search evaluation function therefore able guide search low cost solutions better than original cost function describe reinforcement learning method enhancing local search combines aspects previous work boyan moore boyan off line learning phase value function learned useful guiding search multiple problem sizes instances illustrate our technique developing several such functions problem our learning enhanced local search algorithm exhibits improvement more over standard local search algorithm
order find optimal control continuous state space time reinforcement learning rl problems approximate value function particular class functions called establish sufficient conditions under rl algorithm converges optimal even use approximate models state dynamics reinforcement functions
order object need solve inverse kinematics problem ie coordinate transformation visual coordinates joint angle vector coordinates arm although several models coordinate transformation learning have been proposed they suffer number human motion control learning hand position error feedback controller inverse kinematics important paper proposes novel model coordinate transformation learning human visual feedback controller uses change joint angle vector corresponding change square hand position error norm feasibility proposed model illustrated using numerical simulations
present method automatically constructing macro actions scratch primitive actions during reinforcement learning process overall idea reinforce tendency perform action after action such pattern actions has been test method task car hill task track task grid world tasks track tasks use macro actions approximately learning time while grid world tasks learning time reduced factor method did work car hill task reasons discuss conclusion
article propose new reinforcement learning rl method based actor critic architecture actor critic approximated normalized gaussian networks networks local linear regression units trained line em algorithm proposed our previous paper apply our rl method task up single pendulum task balancing double pendulum near upright position experimental results show our rl method applied optimal control problems having continuous spaces method achieves good control small number trial errors
describe reinforcement learning algorithm partially observable environments using short term memory call learns stochastic model based bayesian learning overfitting problem reasonably solved moreover has efficient implementation paper shows model learned converges provides most accurate predictions rewards given short term memory
partially observable markov decision processes pomdps constitute important class reinforcement learning problems present unique theoretical computational difficulties absence markov property popular reinforcement learning algorithms such learning may longer effective memory based methods remove partial via state estimation notoriously expensive alternative approach seek stochastic memoryless policy each observation environment probability distribution over available actions maximizes average reward per reinforcement learning algorithm learns locally optimal stochastic memoryless policy has been proposed jaakkola singh jordan but empirically verified present variation algorithm discuss its implementation demonstrate its using four test problems
virtual vr provides experimental environments bounds possible evoked potential ep experiments providing complex dynamic environments order study cognition without environmental control vr serves dynamic brain computer interface bci research has been about detecting ep signals complex vr environment paper shows exist red green yellow stop lights virtual driving environment experimental results show existence ep go stop lights negative variation ep slow down lights order test feasibility line recognition vr recognizing ep red stop lights absence signal yellow slow down lights recognition results show may successfully used control vr car stop lights
psychophysical evidence selective attention mainly visual search experiments work formulate hierarchical system interconnected modules consisting populations neurons modeling underlying mechanisms involved selective visual attention demonstrate our neural system visual search works across visual field parallel but due different intrinsic dynamics show two experimentally observed modes visual attention namely serial parallel search mode other words neither explicit model focus attention nor maps used focus attention appears property dynamic behavior system neural population dynamics handled framework mean field approximation consequently whole process expressed system coupled differential equations
spatial information comes two forms direct spatial information example retinal position indirect temporal information objects encountered sequentially general spatially close acquisition spatial information neural network investigated here given spatial layout several objects networks trained prediction task networks using temporal sequences direct spatial information found develop internal representations show distances correlated distances external layout influence spatial information analyzed providing direct spatial information system during training either consistent layout approach allows relative contributions spatial temporal
quantitative data speed animals acquire behavioral responses during classical conditioning experiments should provide strong constraints models learning most models have simply ignored these data few have attempted address them have failed least order magnitude discuss key data speed acquisition show account them using statistically sound model learning differential stimuli play crucial role
many classification tasks recognition accuracy low because input corrupted noise spatially temporally overlapping propose approach overcoming these limitations based model human selective attention model early selection filter guided top down attentional control each candidate output class sequence adjusts attentional gain coefficients order produce strong response class chosen class obtains response least modulation attention present simulation results classification corrupted handwritten digit showing significant improvement recognition rates algorithm has been applied domain speech recognition comparable results
figure ground segregation network proposed based novel boundary pair representation nodes network boundary segments obtained through local grouping each node coupled neighboring nodes belong same region coupled corresponding paired node grouping rules incorporated modulating connections node represents its probability being updated according differential equation system solves figure ground segregation problem through temporal evolution different perceptual phenomena such modal completion virtual contours grouping shape decomposition explained through local diffusion system eliminates combinatorial optimization accounts many psychophysical results fixed set parameters
examine psychophysical law describes influence stimulus context perception according law choice probability ratios into components independently controlled stimulus context has been argued pattern results feedback models perception paper examine claim using neural network models defined via stochastic differential equations show law related condition named channel has little do existence feedback connections channels separable they converge into response units without direct lateral connections other channels their sensors directly contaminated external inputs other channels implications analysis cognitive computational discussed
introduce novel method constructing language models avoids problems associated recurrent neural networks method creating prediction fractal machine briefly described experiments presented demonstrate suitability language modeling distinguish reliably between minimal pairs their behavior consistent hypothesis graded absolute discussion their potential offer insights into language acquisition processing follows
paper two apparently distinct modes generalizing concepts rules computing similarity exemplars should both seen special cases more general bayesian learning framework bayes explains specific these two modes rules similarity measured well why generalization should appear similarity based different situations analysis suggests why distinction even computationally fundamental may still useful algorithmic level part principled approximation fully bayesian learning
recent theories suggest language acquisition evolution languages towards forms easily learnable paper evolve combinatorial languages learned recurrent neural network quickly relatively few examples additionally evolve languages generalization different generalization specific examples find languages evolved facilitate different forms impressive generalization biased general purpose learner results provide empirical support theory language itself well language environment learner plays substantial role learning far more language acquisition than language acquisition device
paper question levels expert guided abstraction learning hard statistically classification tasks focus two tasks date calculation parity require intermediate levels abstraction defined human expert challenge claim demonstrating empirically single hidden layer bp som network learn both tasks without guidance moreover analyze networks solution parity task show its solution makes use elegant computation
investigate short term dynamics recurrent competition neural activity primary visual cortex terms information processing context orientation selectivity propose after stimulus onset strength recurrent excitation decreases due fast synaptic depression consequence network shifts initially highly nonlinear more linear operating regime sharp orientation tuning established first highly competitive phase second less competitive phase precise multiple orientations long range modulation eg inter connections becomes possible surround effects thus network first extracts salient features stimulus starts process details show signal processing strategy optimal neurons have limited bandwidth their objective transmit maximum amount information time interval stimulus onset
paper classical neuroscience paradigm hebbian learning find necessary requirement effective associative memory learning incoming synapses should uncorrelated requirement difficult achieve robust manner hebbian synaptic learning depends network level information effective learning yet obtained neuronal process maintains zero sum incoming synaptic normalization drastically improves memory capacity associative networks essentially bounded capacity linearly scales networks size enables effective storage patterns heterogeneous coding levels single network such neuronal normalization successfully carried out activity dependent neurons synaptic recently observed cortical tissue thus our findings strongly suggest effective associative learning hebbian synapses alone biologically implausible hebbian synapses continuously driven regulatory processes brain
consider topographic projection between two neuronal layers different densities neurons given number output neurons connected each input neuron divergence fan out number input neurons each output neuron convergence fan determine widths axonal dendritic minimize total volume axons dendrites analytical results summarized qualitatively following rule neurons layer should have wider than those layer agrees anatomical data retinal cerebellar neurons whose morphology connectivity known rule may used infer connectivity neurons their morphology
encoding accuracy population stochastically spiking neurons studied different distributions their tuning widths situation identical symmetric receptive fields neurons usually considered literature turns out information theoretic point view both variability tuning widths neural population into specialized improve encoding accuracy
investigate behavior hebbian cell assembly spiking neurons formed via temporal synaptic learning curve learning function based recent experimental findings includes potentiation short time delays between preand post synaptic neuronal spiking depression spiking events reverse order coupling between dynamics synaptic learning neuronal activation leads interesting results find cell assembly fire but may function complete synchrony distributed synchrony latter implies spontaneous division hebbian cell assembly into groups cells fire cyclic manner behavior distributed synchrony both simulations analytic calculations resulting synaptic distributions
visual image consists figure against background cells physiologically observed give higher responses image regions corresponding figure relative their responses background medial axis figure induces relatively higher responses compared responses other locations figure except boundary between figure background receptive fields cells small compared global scale figure ground medial axis effects has been suggested these effects may caused feedback higher visual areas show these effects accounted mechanisms size figure small certain scale they processes pre segmentation detect highlight boundaries between homogeneous image regions
stochastic fluctuations voltage gated ion channels generate current voltage noise neuronal noise may critical determinant efficacy information processing within neural systems using monte carlo simulations carry out systematic investigation relationship between channel resulting membrane voltage noise using stochastic markov version sejnowski model dendritic excitability cortical neurons our simulations show parameters lead increase membrane excitability increasing channel densities decreasing temperature lead increase magnitude sub threshold voltage noise noise increases membrane rest towards threshold suggests channel fluctuations may neurons ability function its synaptic inputs may limit reliability precision neural information processing
long term potentiation ltp has long been held biological associative learning recently evidence has emerged long term depression ltd results presynaptic cell fires after postsynaptic cell computational utility ltd explored here synaptic modification kernels both ltp ltd have been proposed other laboratories based studies postsynaptic unit here interaction between time dependent ltp ltd studied small networks
previous biophysical modeling work showed nonlinear interactions among nearby synapses located active dendritic trees provide large boost memory capacity cell mel aim our present work quantify boost estimating capacity neuron model passive dendritic integration inputs combined linearly across entire cell followed single global threshold active dendrite model threshold applied separately output each branch branch combined linearly focus here limiting case binary valued synaptic weights derive expressions measure model capacity estimating number distinct input output functions available both neuron types show application fixed nonlinearity each dendritic compartment substantially increases models flexibility neuron realistic size capacity nonlinear cell exceed same sized linear cell more than order magnitude largest capacity boost occurs cells relatively large number dendritic relatively small size validated analysis empirically measuring memory capacity randomized two class classification problems stochastic delta rule used train both linear nonlinear models found large capacity predicted nonlinear dendritic model readily achieved practice mel
neocortical circuits massive excitatory feedback more than percent synapses made excitatory cortical neurons onto other excitatory cortical neurons why such massive recurrent excitation what its role cortical computation recent neurophysiological experiments have shown plasticity recurrent neocortical synapses governed temporally asymmetric hebbian learning rule describe such rule may allow cortex modify recurrent synapses prediction input sequences goal predict next cortical input recent past based previous experience similar input sequences show temporal difference learning rule prediction used conjunction dendritic back propagating action potentials temporally asymmetric hebbian plasticity observed physiologically biophysical simulations demonstrate network cortical neurons learn predict moving stimuli develop direction selective responses consequence learning space time response properties model neurons shown similar those direction selective cells monkey
simple model two connected attractor neural networks studied analytically situations similar those encountered delay match sample tasks stimuli tasks memory guided attention model qualitatively many experimental data these types tasks provides framework understanding experimental observations context attractor neural network scenario
reliability accuracy spike trains have been shown depend nature stimulus neuron encodes adding ion channel neuronal models results macroscopic behavior replicates input dependent reliability precision real neurons calculate amount information ion channel based stochastic hodgkin huxley neuron model encode about wide set stimuli show both information rate information per spike stochastic model similar values reported experimentally moreover amount information neuron encodes correlated amplitude fluctuations input less so average firing rate neuron show ion channel density information capacity robust changes density ion channels membrane whereas changing ratio between ion channels has considerable effect information neuron encode finally suggest neurons may maximize their information capacity appropriately balancing density different ion channels neuronal excitability
human reaction times during sensory motor tasks vary considerably begin understand variability arises examined neuronal response time variability early versus late visual processing stages conventional view precise temporal information gradually lost information passed through layered network mean rate units tested humans whether neuronal populations different processing stages behave like mean rate units blind source separation algorithm applied meg signals sensory motor integration tasks response time latency variability multiple visual sources estimated detecting single trial stimulus locked events each source two subjects tested four visual reaction time tasks reliably identified sources belonging early late visual processing stages standard deviation response latency smaller early rather than late processing stages supports hypothesis human response time variability increases early late visual processing stages
study population decoding paradigm maximum likelihood inference based decoding model usually case neural population decoding because encoding process brain exactly known because simplified decoding model preferred computational cost consider decoding model pair wise correlation between neuronal activities prove asymptotically efficient neuronal correlation uniform limited range performance compared maximum likelihood inference based model center mass decoding method turns out has advantages decreasing computational complexity maintaining high level decoding accuracy same time effect correlation decoding accuracy discussed
analyze conditions under synaptic learning rules based action potential timing approximated learning rules based firing rates particular consider form plasticity synapses presynaptic spike followed postsynaptic spike opposite temporal ordering such differential anti approximated under certain conditions learning rule depends time derivative postsynaptic firing rate such learning rule acts stabilize persistent neural activity patterns recurrent neural networks
paper presents novel practical framework bayesian model averaging model selection probabilistic graphical models our approach approximates full posterior distributions over model parameters structures well latent variables analytical manner these posteriors fall out free form optimization procedure naturally incorporates conjugate priors unlike large sample approximations posteriors generally nongaussian hessian needs computed predictive quantities obtained analytically resulting algorithm generalizes standard expectation maximization algorithm its convergence guaranteed demonstrate approach applied large class models several domains including mixture models source separation
unsupervised learning algorithms designed extract structure data samples reliable robust inference requires guarantee extracted structures typical data source ie similar structures have inferred second sample set same data source overfitting phenomenon maximum entropy based annealing algorithms studied class histogram clustering models inequality large deviations used determine maximally achievable approximation quality parameterized minimal temperature monte carlo simulations support proposed model selection criterion finite temperature annealing
give necessary sufficient conditions uniqueness support vector solution problems pattern recognition regression estimation general class cost functions show solution unique support vectors necessarily bound give simple examples non unique solutions note uniqueness dual solution does necessarily imply uniqueness dual solution show compute threshold solution unique but support vectors bound case usual method determining does work
new functionals parameter model selection support vector machines introduced based concepts span support vectors feature space shown using these functionals both predict best choice parameters model relative quality performance value parameter
generalize recent formalism describe dynamics supervised learning layered neural networks regime data case noisy our theory generates reliable predictions evolution time generalization errors extends class mathematically solvable learning processes large neural networks those situations overfitting occur
show recently proposed variant support vector machine svm algorithm known svm interpreted maximal separation between subsets convex hulls data call soft convex hulls soft convex hulls controlled choice parameter intersection convex hulls empty hyperplane between them such distance between convex hulls measured along normal maximized hyperplanes normal similarly determined soft convex hulls but its position distance adjusted minimize error sum proposed geometric interpretation svm leads necessary sufficient conditions existence choice svm solution nontrivial
present three simple approximations calculation posterior mean gaussian process classification first two methods related mean field ideas known statistical physics third approach based bayesian online approach motivated recent results statistical mechanics neural networks present simulation results showing mean field bayesian evidence may used tuning online approach may achieve low training error fast
recent interpretations adaboost algorithm view performing gradient descent potential function simply changing potential function allows create new algorithms related adaboost these new algorithms generally known have formal boosting property paper question potential functions lead new algorithms two main results general sets conditions potential set implies resulting algorithm while other implies algorithm these conditions applied previously studied potential functions such those used doom ii
bayesian predictions stochastic like predictions other inference scheme generalize finite sample while simple variational argument shows bayes averaging generalization optimal given prior matches teacher parameter distribution situation less clear teacher distribution unknown define class averaging procedures likelihoods including both bayes averaging uniform prior maximum likelihood estimation special cases show bayes generalization optimal family teacher distribution two learning problems analytically tractable learning mean gaussian smooth learners
performance regular irregular gallager type code investigated via methods statistical physics transmitted comprises products original message bits selected two randomly constructed sparse matrices number non zero elements these matrices family codes show channel capacity may equilibrium many regular codes while slightly lower performance obtained others may higher practical relevance decoding aspects considered employing tap approach identical commonly used belief propagation based decoding show irregular codes may capacity but improved dynamical properties
gaussian mixtures so called radial basis function networks density estimation provide natural counterpart sigmoidal neural networks function fitting approximation both cases possible give simple expressions iterative improvement performance components network introduced time particular mixture density estimation show component mixture estimated maximum likelihood iterative likelihood improvement introduce achieves log likelihood within order log likelihood achievable convex combination consequences approximation estimation using kullback leibler risk given minimum description length principle selects optimal number components minimizes risk bound
important issue neural computing concerns description learning dynamics macroscopic dynamical variables recent progress line learning addresses often case infinite training set introduce new framework model batch learning restricted sets examples widely applicable learning cost function fully taking into account temporal correlations introduced examples illustration analyze effects weight decay early stopping during learning teacher generated examples
neural networks need more than single layer nonlinear units compute interesting functions show false employs winner take nonlinear unit boolean function computed single winner unit applied weighted sums input variables continuous function approximated arbitrarily well single soft winner take unit applied weighted sums input variables positive weights needed these linear weighted sums may interest point view neurophysiology synapses cortex inhibitory addition widely special cortex compute winner take our results support view winner take useful basic computational unit neural vlsi wellknown winner take input variables computed efficiently transistors total wire length area linear analog vlsi lazzaro et al show winner take useful special purpose computations but may serve nonlinear unit neural circuits universal computational power show multi layer perceptron needs many gates compute winner take input variables hence winner take provides substantially more powerful computational unit than perceptron about same cost implementation analog vlsi complete proofs further details these results found
known decision tree learning viewed form boosting existing boosting theorems decision tree learning allow binary branching trees generalization multi branching trees immediate practical decision tree algorithms such cart implement trade off between number branches improvement tree quality measured index function here give boosting justification particular quantitative trade off curve our main theorem states require improvement proportional log number branches top down greedy construction decision trees remains effective boosting algorithm
order compare learning algorithms experimental results reported machine learning often use statistical tests significance unfortunately most these tests do take into account variability due choice training set perform theoretical investigation variance cross validation estimate generalization error takes into account variability due choice training sets allows us propose two new ways estimate variance show via simulations these new statistics perform well relative statistics considered
study here simple stochastic single neuron model delayed self feedback capable generating spike trains simulations show its spike trains exhibit resonant behavior between noise delay order gain insight into resonance simplify model study stochastic binary element whose transition probability depends its state fixed interval past simplified model analytically compute interspike interval histograms show resonance between noise delay arises resonance observed such elements coupled through delayed interaction
article study effects introducing structure input distribution data learnt simple perceptron determine learning curves within framework statistical mechanics generalization occurs function number examples distribution patterns highly anisotropic although extremely simple model seems capture relevant features class support vector machines recently shown present behavior
calculate lower bounds size sigmoidal neural networks approximate continuous functions particular show approximation polynomials network size has grow log degree polynomials bound valid input dimension ie independently number variables result obtained introducing new method employing upper bounds vapnik chervonenkis dimension proving lower bounds size networks approximate continuous functions
paper define probabilistic computational model generalizes many noisy neural network models including recent work identify weak mechanism responsible restriction computational power probabilistic models definite languages independent characteristics noise whether discrete analog depends input independent whether variables discrete continuous give examples weakly models including noisy computational systems noise depending current state inputs aggregate models computational systems update continuous time
effective methods capacity control via uniform convergence bounds function expansions have been largely limited support vector machines good bounds entropy number approach extend these methods systems expansions terms arbitrary parametrized basis functions wide range regularization methods covering whole range general linear additive models achieved data dependent analysis eigenvalues corresponding design matrix
hierarchical learning machines non regular non statistical models whose true parameter sets analytic sets singularities using algebraic analysis prove stochastic complexity non learning machine asymptotically equal number training samples moreover show rational number integer ml calculated using resolution singularities algebraic geometry obtain inequalities ml number parameters
paper discuss statistical model blind deconvolution first introduce lie group manifold filters blind deconvolution problem formulated framework model family estimating functions derived blind deconvolution natural gradient learning algorithm developed training filters stability natural gradient algorithm analyzed framework
recently sample complexity bounds have been derived problems involving linear functions such neural networks support vector machines paper extend theoretical results area deriving dimensional independent covering number bounds regularized linear functions under certain regularization conditions show such bounds lead class new methods training linear classifiers similar theoretical advantages support vector machine furthermore present theoretical analysis these new methods asymptotic statistical point view technique provides better description large sample behaviors these algorithms
paper propose full bayesian model neural networks model treats model dimension number neurons model parameters regularisation parameters noise parameters random variables need estimated propose reversible jump markov chain monte carlo mcmc method perform necessary computations find results better than previously reported ones but appear robust respect prior specification moreover present geometric convergence theorem algorithm
present new technique time series analysis based dynamic probabilistic networks approach observed data modeled terms unobserved mutually independent factors recently introduced technique independent factor analysis ifa unlike ifa factors iid each factor has its own temporal statistical characteristics derive family em algorithms learn structure underlying factors their relation data these algorithms perform source separation noise reduction integrated manner demonstrate superior performance compared ifa
layered sigmoid belief networks directed graphical models local conditional probabilities weighted sums states learning inference such networks generally intractable approximations need considered progress learning these networks has been made using variational procedures demonstrate variational procedures inappropriate equally important issue inference calculating network introduce alternative procedure based assuming weighted input node approximately gaussian distributed our approach goes beyond previous gaussian field assumptions take into account correlations between parents nodes procedure specialized calculating marginals significantly faster simpler than variational procedure
cp curse dimensionality severe modeling high dimensional discrete data number possible combinations variables exponentially paper propose new architecture modeling high dimensional data requires resources parameters computations grow most square number variables using multi layer neural network represent joint distribution variables product conditional distributions neural network interpreted graphical model without hidden random variables but conditional distributions tied through hidden units connectivity neural network pruned using dependency tests between variables experiments modeling distribution several discrete data sets show statistically significant improvements over other methods such naive bayes comparable bayesian networks show significant improvements obtained pruning network
replace commonly used gaussian noise model nonlinear regression more flexible noise model based student degrees freedom distribution chosen such special cases either gaussian distribution distribution realized latter commonly used robust regression distribution interpreted being infinite mixture gaussians parameters hyperparameters such degrees freedom distribution learned data based em leaming algorithm show modeling using distribution leads improved predictors real world data sets particular outliers present distribution superior gaussian noise model effect adapting degrees freedom system learn distinguish between outliers non outliers especially online learning tasks interested avoiding inappropriate weight changes due measurement outliers maintain stable online learning capability show experimentally using distribution noise model leads stable online learning algorithms outperforms state art online learning methods like extended kalman filter algorithm
introduce algorithm estimating values function set test points given set training points xt yt without estimating intermediate step regression function demonstrate direct way estimating values regression classification pattern recognition more accurate than traditional based two steps first estimating function calculating values function points interest
nonnegative boltzmann machine recurrent neural network model describe multimodal nonnegative data application maximum likelihood estimation model gives learning rule analogous binary boltzmann machine examine utility mean field approximation describe monte carlo sampling techniques used learn its parameters slice sampling particularly well suited distribution efficiently implemented sample distribution illustrate learning invariant distribution well generative model images human faces
many problems correct behavior model depends its input output mapping but properties its matrix matrix partial derivatives models outputs respect its inputs introduce algorithm efficient general method computing exact partial derivatives variety simple functions model respect its free parameters algorithm applies parametrized feedforward model including nonlinear regression multilayer perceptrons radial basis function networks
present algorithm infers model structure mixture factor using efficient deterministic variational approximation full bayesian integration over model parameters procedure automatically determine optimal number components local dimensionality each component ie number factors each factor alternatively used infer posterior distributions over number components parameters integrated out method overfitting using stochastic procedure adding components possible perform variational optimisation incrementally avoid local maxima results show method works well practice correctly infers number dimensionality nontrivial synthetic examples importance sampling variational approximation show obtain unbiased estimates true evidence exact predictive density kl divergence between variational posterior true posterior model but variational approximations general
transduction inference principle takes training sample aims estimating values function given points contained so called working sample opposed whole input space induction transduction provides confidence measure single predictions rather than classifiers feature particularly important risk sensitive applications possibly infinite number functions reduced finite number equivalence classes working sample rigorous bayesian analysis reveals standard classification loss cannot benefit considering more than test point time probability label given test point determined posterior measure corresponding subset hypothesis space consider pac setting binary classification linear discriminant functions perceptrons kernel space such probability labels determined volume ratio version space suggest sample region experimental results real world data indicate bayesian transduction compares well known support vector machine particular posterior probability used confidence measure exclude test points low confidence
describe class probabilistic models call networks using parse trees internal representations images networks able perform segmentation recognition simultaneously removing need ad hoc segmentation heuristics promising results problem segmenting handwritten digits obtained
present general framework discriminative estimation based maximum entropy principle its extensions calculations involve distributions over structures andor parameters rather than specific settings reduce relative entropy projections holds even data separable within chosen parametric class context anomaly detection rather than classification labels training set uncertain incomplete support vector machines naturally under class provide several extensions able estimate exactly efficiently discriminative distributions over tree structures class conditional models within preliminary experimental results potential these techniques
invariance topographic transformations such translation image has been successfully incorporated into feedforward mechanisms eg convolutional neural networks tangent propagation describe way add transformation invariance generafive density model approximating nonlinear transformation manifold discrete set transformations em algorithm original model extended new model computing expectations over set transformations show add discrete transformation variable gaussian mixture modeling factor analysis mixtures factor analysis give results filtering images face facial pose clustering handwritten digit modeling recognition
new decomposition algorithm training regression support vector machines svm presented algorithm builds basic principles decomposition proposed et al addresses issue optimal working set selection new criteria testing optimality working set derived based these criteria principle maximal proposed form approximately optimal working sets experimental results show superior performance new algorithm comparison traditional training regression svm without decomposition similar results have been previously reported decomposition algorithms pattern recognition svm new algorithm applicable advanced svm formulations based regression such density estimation integral equation svm
latent variable generative model finite noise used describe several different algorithms independent components analysis ica particular fixed point ica algorithm shown equivalent expectation maximization algorithm maximum likelihood under certain constraints allowing conditions global convergence algorithms explained their generic behavior near singular point size optimal generarive bases expansion likelihood about singular point indicates role higher order correlations determining features discovered ica application convergence these algorithms demonstrated simple illustrative example
describe new incremental algorithm training linear threshold functions online maximum margin algorithm romma romma viewed approximation algorithm repeatedly chooses hyperplane classifies previously seen examples correctly maximum margin known such maximum margin hypothesis computed minimizing length weight vector subject number linear constraints romma works maintaining relatively simple relaxation these constraints efficiently updated prove mistake bound romma same proved perceptron algorithm our analysis implies more computationally intensive maximum margin algorithm satisfies mistake bound first worst case performance guarantee algorithm describe experiments using romma variant updates its hypothesis more batch algorithms recognize handwritten digits computational complexity simplicity these algorithms similar perceptron algorithm but their generalization much better describe sense performance romma converges svm limit bias considered
recent years bayesian networks have become highly successful tool diagnosis analysis decision making real world domains present efficient algorithm learning bayes networks data our approach constructs bayesian networks first identifying each nodes markov connecting nodes maximally consistent way contrast majority work typically uses hill approaches may produce dense incorrect nets our approach yields much more compact causal networks data compact causal networks facilitate fast inference easier understand prove under mild assumptions our approach requires time polynomial size data number nodes randomized variant presented here yields comparable results much higher speeds
provide abstract characterization boosting algorithms gradient cost functionals inner product function space prove convergence these functional gradient descent algorithms under quite weak conditions following previous theoretical results bounding generalization performance convex combinations classifiers terms general cost functions margin present new algorithm doom ii performing gradient descent optimization such cost functions experiments several data sets uc repository demonstrate doom ii generally outperforms adaboost especially high noise situations overfitting behaviour adaboost predicted our cost functions
paper present committee new multi class learning algorithm related winnow family algorithms committee algorithm combining predictions set sub experts online mistake bounded model learning sub expert special type attribute predicts distribution over finite number classes committee learns linear function sub experts uses function make class predictions provide bounds committee show performs well target represented few relevant sub experts show committee used solve more traditional problems composed attributes leads natural extension learns multi class problems contain both traditional attributes sub experts
incorporate prior knowledge construct nonlinear algorithms invariant feature extraction discrimination employing unified framework terms nonlinear variant coefficient propose non linear generalizations discriminant oriented pca using support vector kernel functions extensive simulations show utility our approach
present class approximate inference algorithms graphical models qmr dt type give convergence rates these algorithms jaakkola jordan algorithm verify these theoretical predictions empirically present empirical results difficult qmr dt network problem obtaining performance new algorithms roughly comparable jaakkola jordan algorithm
local linear regression performs well many low dimensional forecasting problems high dimensional spaces its performance typically due well known curse dimensionality possible way approach problem varying shape weighting kernel work suggest new data driven method estimating optimal kernel shape experiments using artificially generated data set data uc repository show benefits kernel
present new learning architecture decision directed acyclic graph used combine many two class classifiers into multiclass classifier class problem contains nn classifiers each pair classes present vc analysis case node classifiers hyperplanes resulting bound test error depends margin achieved nodes but dimension space algorithm operates kernel induced feature space uses two class maximal margin hyperplanes each decision node substantially faster train evaluate than either standard algorithm max while maintaining comparable accuracy both these algorithms
bayesian mixture model necessary priori limit number components finite paper infinite gaussian mixture model presented difficult problem finding right number mixture components inference model done using efficient parameter free markov chain relies entirely gibbs sampling
adaboost other ensemble methods have successfully been applied number classification tasks seemingly problems overfitting adaboost performs gradient descent error function respect margin asymptotically patterns learn noisy problems indeed theoretical analysis has shown margin distribution opposed minimal margin plays crucial role understanding phenomenon outliers should has benefit substantially increasing margin remaining points propose new boosting algorithm allows possibility pre specified fraction points lie margin area even wrong side decision boundary
linear discriminant analysis lda classical multivariate technique both dimension reduction classification data vectors transformed into low dimensional subspace such class spread out much possible subspace lda works simple prototype classifier linear decision boundaries many applications linear boundaries do adequately separate classes present nonlinear generalization discriminant analysis uses kernel trick representing dot products kernel functions presented algorithm allows simple formulation em algorithm terms kernel functions leads unique concept unsupervised mixture analysis supervised discriminant analysis semi supervised discriminant analysis partially unlabelled observations feature spaces
provide analysis turbo decoding algorithm tda setting involving gaussian densities context able show algorithm converges somewhat surprisingly though density generated tda may differ significantly desired posterior density means these two densities
suppose you given dataset drawn underlying probability distribution you want estimate simple subset input space such probability test point drawn lies outside equals priori specified between propose method approach problem trying estimate function positive negative complement functional form given kernel expansion terms potentially small subset training data regularized controlling length weight vector associated feature space provide theoretical analysis statistical performance our algorithm algorithm natural extension support vector algorithm case unlabelled data
paper describes recurrent mixture density networks model multi modal distributions type without explicit assumptions about use context these expressions occur frequently pattern recognition problems sequential data example speech recognition experiments show proposed generative models give higher likelihood test data compared traditional modeling approach indicating they summarize statistical properties data better
present simple variation importance sampling explicitly searches important regions target distribution prove technique yields unbiased estimates show empirically reduce variance standard monte carlo estimators achieved samples more significant regions sample space
present variational bayesian method model selection over families kernels classifiers like support vector machines gaussian processes algorithm needs user interaction able adapt large number kernel parameters given data without having training cases validation possibility use sophisticated families kernels situations small standard kernel classes clearly inappropriate relate method other work done gaussian processes clarify relation between support vector machines certain gaussian process models
describe iterative algorithm building vector machines used classification tasks algorithm builds ideas support vector machines boosting generalized additive models algorithm used various continuously differential functions bound discrete classification loss simple implement test proposed algorithm two different loss functions synthetic natural data describe norm penalized version algorithm exponential loss function used adaboost performance algorithm natural data comparable support vector machines while typically its running time shorter than svm
introduce novel clustering algorithm maximizes mutual information per cluster between data given categories algorithm considered bottom up hard version recently introduced information bottleneck method algorithm compared top down soft version information bottleneck method relationship between hard soft results established demonstrate algorithm data set subset two achieve compression orders magnitudes original mutual information
paper consider problem active learning polynomial networks give necessary sufficient condition sample points provide optimal generalization capability analyzing condition functional analytic point view clarify mechanism achieving optimal generalization capability show set training examples satisfying condition does provide optimal generalization but reduces computational complexity memory required calculation learning results finally examples sample points satisfying condition given computer simulations performed demonstrate effectiveness proposed active learning method
gaussian processes powerful regression models specified parametrized mean covariance functions standard approaches estimate these parameters known name hyperparameters maximum likelihood ml maximum map approaches paper propose investigate predictive approaches namely maximization surrogate predictive probability minimization mean square error respect referred predictive mean square error estimate hyperparameters derive results standard cross validation error make comparison these approaches tested number problems experimental results show these approaches strongly competitive existing approaches
paper treat input selection radial basis function rbf like classifier within bayesian framework approximate posteriori distribution over both model coefficients input subsets samples drawn gibbs updates reversible jump moves using public datasets compare classification accuracy method conventional scheme these datasets used infer posteriori probabilities different input subsets
propose novel approach building finite memory predictive models similar spirit variable memory length markov models models constructed first transforming block structure training sequence into spatial structure points unit hypercube such longer common suffix shared two blocks closer lie their point representations such transformation markov assumption blocks long common likely produce similar finding set prediction contexts formulated resource allocation problem solved vector spatial block representation compare our model both classical variable memory length markov models three data sets different memory stochastic components our models have superior performance yet their construction fully automatic shown problematic case
support vector machine svm state art technique regression classification combining excellent generalisation properties sparse kernel representation does suffer number notably absence probabilistic outputs requirement estimate trade off parameter need mercer kernel functions paper introduce relevance vector machine rvm bayesian treatment linear model identical functional form svm rvm suffers none above examples demonstrate comparable generalisation performance rvm requires dramatically fewer kernel functions
new method multivariate density estimation developed based support vector method svm solution inverse ill posed problems solution has form mixture densities method gaussian kernels compared favorably both method gaussian mixture model method synthetic data achieve more accurate estimates densities dimensions
dual estimation refers problem simultaneously estimating state dynamic system model gives rise dynamics algorithms include expectation maximization em dual kalman filtering joint kalman methods these methods have recently been explored context nonlinear modeling neural network used functional form unknown model typically extended kalman filter ekf smoother used part algorithm estimates clean state given current estimated model ekf may used estimate weights network paper points out using ekf proposes improvement based new approach called transformation ut substantial performance gain achieved same order computational complexity standard ekf approach illustrated several dual estimation methods
local belief propagation rules sort proposed pearl guaranteed converge correct posterior probabilities singly connected graphical models recently number researchers have empirically demonstrated good performance oopy belief these same rules graphs loops perhaps most dramatic instance near shannon limit performance turbo codes whose decoding algorithm equivalent loopy belief propagation except case graphs single loop has been little theoretical understanding performance oopy propagation here analyze belief propagation networks arbitrary topologies nodes graph describe jointly gaussian random variables give analytical formula relating true posterior probabilities those calculated using loopy propagation give sufficient conditions convergence show belief propagation converges gives correct posterior means graph topologies networks single loop related max product belief propagation algorithm finds maximum posterior probability estimate singly connected networks show even non gaussian probability distributions convergence points max product algorithm loopy networks maxima over particular large local neighborhood posterior probability these results help clarify empirical performance results motivate using powerful belief propagation algorithm broader class networks problems involving probabilistic belief propagation arise wide variety applications including error correcting codes speech recognition medical diagnosis graph singly connected exist local message passing schemes calculate posterior probability unobserved variable given observed variables pearl derived such scheme singly connected bayesian networks showed belief propagation algorithm guaranteed converge correct posterior probabilities beliefs several groups have recently reported excellent experimental results running algorithms weiss freeman equivalent algorithm networks loops perhaps most dramatic instance performance turbo code error correcting codes these codes have been described most potentially important development coding theory many years have recently been shown utilize algorithm equivalent belief propagation network loops progress analysis loopy belief propagation has been made case networks single loop these networks shown unless deterministic oopy belief propagation converge difference between loopy beliefs true beliefs related convergence rate messages faster convergence more exact approximation hidden nodes binary loopy beliefs true beliefs both maximized same assignments although confidence assignment wrong loopy beliefs paper analyze belief propagation graphs arbitrary topology nodes describing jointly gaussian random variables give exact formula relating correct marginal posterior probabilities ones calculated using oopy belief propagation show belief propagation converges give correct posterior means graph topologies networks single loop show covariance estimates generally incorrect but present relationship between error covariance estimates convergence speed gaussian non gaussian variables show max product algorithm calculates map estimate singly connected networks converges points maxima over particular large neighborhood posterior probability loopy networks analysis simplify notation assume graphical model has been into undirected graphical model pairwise potentials graphical model converted into form running belief propagation pairwise graph equivalent running belief propagation original graph assume each node zi has local observation each iteration belief propagation each node zi message each neighboring based messages received other neighbors its local observation pairwise potentials ij xi ii xi yi assume message passing occurs parallel idea behind analysis build unwrapped tree unwrapped tree graphical model belief propagation solving exactly applies belief propagation rules loopy network constructed maintaining same local neighborhood structure oopy network but nodes replicated so loops potentials observations replicated loopy graph figure shows unwrapped tree shaped graph construction belief root node identical node zx loopy graph after four iterations belief propagation each node has observed node attached omitted here because original network represents jointly gaussian variables so unwrapped tree tree belief propagation guaranteed give correct answer unwrapped graph thus use gaussian formulae calculate true mean variances both original unwrapped networks way calculate accuracy belief propagation gaussian networks arbitrary topology assume joint mean zero means added later joint belief propagation figure left markov network multiple loops right unwrapped network corresponding structure given straightforward construct inverse covariance matrix joint gaussian describes given gaussian graphical model writing out exponent joint square shows mean given observations given covariance matrix ci given ci denote ith row so marginal posterior variance zi given data iv use unwrapped quantities scan tree first order denote vector values hidden nodes tree so scanned denote observed nodes scanned same order pv inverse covariance matrices first order last nodes leaf nodes denote number leaf nodes nature mean belief node zx after iterations belief propagation number similarly variance belief node after iterations because data replicated write oi replica otherwise potentials yi replicated write over zi replicated non leaf have same connectivity corresponding zi write zero but last rows these relationships between loopy unwrapped inverse covariance matrices into loopy unwrapped versions equation obtains following expression true iteration vector zero everywhere but last components corresponding leaf nodes our choice node root tree arbitrary so applies nodes oopy network formula relates node network loops means calculated each iteration belief propagation true posterior means similarly relationship between loopy unwrapped inverse covariance matrices into loopy unwrapped definitions ci relate weiss freeman node oo figure conditional correlation between root node other nodes unwrapped tree fig after eight iterations potentials chosen randomly nodes presented first order so last elements correlations between root node leaf nodes show correlation goes zero belief propagation converges loopy means exact symbols star denote correlations nodes correspond node loopy graph sum these correlations gives correct variance node ca while loopy propagation uses first correlation covariances calculated belief propagation true ones el vector zero everywhere but last components while equal nodes unwrapped tree replicas ca except other components zero figure shows iv network fig generated random potential functions observations calculated conditional correlations unwrapped tree note conditional correlation decreases distance tree first order so last components correspond leaf nodes number iterations loopy propagation increased size unwrapped tree increases conditional correlation between leaf nodes root node decreases equations clear conditional correlation between leaf nodes root nodes zero sufficiently large belief propagation converges means exact variances may incorrect practice conditional correlations actually equal zero finite give more precise statement conditional correlation root node leaf nodes decreases rapidly enough belief propagation converges means exact variances may incorrect show sufficient conditions potentials xi xj correlation decrease rapidly enough rate correlation decreases determined ratio off diagonal diagonal components quadratic form defining potentials wrong variances term ox equation simply sum many components shows these components correct variance sum components while belief propagation variance approximates sum first dominant term whenever positive correlation between root node other replicas loopy variance strictly less than true variance oopy estimate correctness belief propagation iterations figure graphical model simulation unobserved nodes connected their four nearest neighbors observation node error estimates loopy propagation successive over relaxation sor function iteration note belief propagation converges much faster than sor note conditional correlation decreases rapidly zero two happen first convergence faster because approaches zero faster second approximation error variances smaller because smaller thus have shown single loop case quick convergence correlated good approximation simulations ran belief propagation grid fig joint probability px xj yi nodes zi neighbors otherwise randomly selected probability set observations yi chosen randomly problem corresponds approximation problem sparse data points visible found exact posterior solving equation ran belief propagation found converged calculated means identical true means up machine precision predicted theory calculated variances small belief propagation estimate many applications solution equation matrix inversion intractable iterative methods used figure compares error means function iterations oopy propagation successive over relaxation sor considered best relaxation methods note after essentially five iterations loopy propagation gives right answer while sor requires many more expected fast convergence approximation error variances quite small median error comparison true variances ranged mean nodes approximation error worse indeed nodes converged slower weiss freeman discussion independently two other groups have recently analyzed special cases gaussian graphical models frey analyzed graphical model corresponding factor analysis gave conditions existence stable fixed point van analyzed graphical model topology turbo decoding but gaussian joint density specific graph they gave sufficient conditions convergence showed means exact our main interest gaussian case understand performance belief propagation general networks multiple loops similarity our results gaussians arbitrary networks results single loops arbitrary distributions first single loop networks binary nodes loopy belief node true belief node maximized same assignment while confidence assignment incorrect gaussian networks multiple loops mean each node correct but confidence around mean may incorrect second both gaussian networks fast belief propagation convergence accurate beliefs third both gaussians discrete valued single loop networks statistical dependence between root leaf nodes convergence rate accuracy two models quite different mean field approximations exact gaussian mrfs while they work poorly connected discrete networks single loop results gaussian single loop cases lead us believe similar results may hold larger class networks our analysis extended non gaussian distributions basic idea applies arbitrary graphs arbitrary potentials belief propagation performing exact inference tree has same local neighbor structure oopy graph linear algebra used calculate exact expressions error belief propagation iteration holds gaussian variables have used similar approach analyze related max product belief propagation algorithm arbitrary graphs arbitrary distributions both discrete continuous valued nodes show max product algorithm converges max product assignment has greater posterior probability assignment particular large region around assignment while weaker condition than global maximum much stronger than simple local maximum posterior probability sum product max product belief propagation algorithms fast due well known probabilistic inference graphical models belief propagation work arbitrary networks distributions nevertheless growing body empirical evidence shows its success many networks loops our results applying belief propagation certain networks multiple loops may enable fast approximate probabilistic inference range new applications references sm convergence iterative decoding graphs single cycle proc near shannon limit error correcting coding decoding turbo codes proc ieee international communications conference advanced inference bayesian networks mi jordan learning graphical models mit press er iterative decoding tail presented information theory san correctness belief propagation freeman weiss fixed points max product algorithm technical report cambridge ma wt freeman ec learning estimate scenes images ms sa solla da editors neural information processing systems mit press frey turbo factor analysis neural information processing systems appear frey bayesian networks pattern classification data compression channel coding mit press gallager low density parity check codes mit press frey iterative decoding codes probability propagation graphical models ieee journal selected areas communication turbo decoding instance belief propagation algorithm ieee journal selected areas communication turbo decision algorithm proc rd conference communications control computing pages il kp murphy weiss mi jordan loopy belief propagation approximate inference empirical study proceedings uncertainty ai van analysis turbo decoding gaussian densities neural information processing systems appear pearl probabilistic reasoning intelligent systems networks inference morgan kaufmann
many hierarchical clustering algorithms available but these lack statistical basis here set up hierarchical probabilistic mixture model data generated hierarchical tree structured manner markov chain monte carlo mcmc methods demonstrated used sample posterior distribution over trees containing variable numbers hidden units
data visualization feature selection methods proposed based joint mutual information ica visualization methods find many good projections high dimensional data interpretation cannot easily found other existing methods new variable selection method found better eliminating redundancy inputs than other methods based simple mutual information efficacy methods illustrated radar signal analysis problem find viewing coordinates data visualization select inputs neural network classifier keywords feature selection joint mutual information ica classification
propose new markov chain monte carlo algorithm generalization stochastic dynamics method algorithm performs exploration state space using its intrinsic geometric structure facilitating efficient sampling complex distributions applied bayesian learning neural networks our algorithm found perform least well best state art method while consuming considerably less time
imagine you wish classify data consisting tens thousands examples thousand dimensional space apply standard machine learning algorithms describe parallel problems they allow users computers work large data sets within work motivated bring many benefits scientific computing algorithms computational power machine learning researchers demonstrate usefulness system number tasks example perform independent components analysis large text corpora consisting tens thousands documents making minimal changes original bell sejnowski source bell sejnowski applying ml techniques data previously beyond their reach leads interesting analyses both data algorithms
system functionality moving eye hence name motor system has been built successfully tested made optical device field view image sensor up direction four neuromorphic analog vlsi circuits implementing motor control loop off custom integrated circuits communicate each other primarily non address event system implements behaviors saliency based saccadic exploration smooth pursuit light duration saccades ranges ms ms comparable human eye performance smooth pursuit operates light sources moving up visual field
describe silicon network consisting group excitatory neurons global inhibitory neuron output inhibitory neuron normalized respect input strengths output models normalization property wide field cells fly visual system normalizing property useful system wish output signal code strength inputs dependent number inputs circuitry each neuron equivalent winner take wta circuit additional transistor voltage reference circuit outputs excitatory neurons code neuron largest input difference here multiple winners chosen varying voltage reference neuron network transition between soft max behavior hard wta behavior show results fabricated chip neurons pm cmos technology
have developed tested vlsi system models coordination biological segmental oscillators underlying locomotion animals such its current form system consists chain pattern generating circuits capable arbitrary contralateral inhibitory synaptic coupling each pattern generating circuit implemented two independent silicon neurons total programmable floating gate based inhibitory synapses asynchronous address event interconnection element provides synaptic connectivity implements axonal delay describe analyze data set experiments exploring system behavior terms synaptic coupling
have developed vlsi silicon neuron corresponding mathematical model two state variable system describe circuit implementation compare behaviors observed silicon neuron mathematical model perform bifurcation analysis mathematical model varying externally applied current show behaviors exhibited silicon neuron under corresponding conditions good agreement those predicted bifurcation analysis
paper presents electronic system extracts sound uses three analogue vlsi building blocks silicon cochlea two inner hair cell circuits two spiking neuron chips silicon cochlea consists cascade filters because delay between two outputs silicon cochlea spike trains created these outputs synchronous narrow range contrast traditional bandpass filters increase selectivity has off against decrease response time proposed system responds quickly independent selectivity
neural model described uses oscillatory correlation speech interfering sound sources core model two layer neural oscillator network sound stream represented synchronized population oscillators different streams represented oscillator populations model has been evaluated using corpus speech mixed interfering sounds produces improvement signal noise ratio every mixture
present hidden markov model hmm inferring hidden psychological state neural activity during single trial fmri activation experiments task paradigms inference based bayesian methodology using combination analytical variety markov chain monte carlo mcmc sampling techniques advantage method detection short time learning effects between repeated trials possible inference based single trial experiments
paper examines role biological constraints human auditory localization process psychophysical neural system modeling approach performance comparisons between competing models human subject explore relevant biologically plausible constraints directional cues upon sound localization based derived human subjects head related transfer functions sound stimuli generated bandpass noise presented both subject model input stimuli model processed using auditory image model cochlear processing cochlear data analyzed time delay neural network integrated temporal spectral information determine spatial location sound source combined cochlear model neural network provided system model sound localization process human like localization performance qualitatively achieved broadband bandpass stimuli model architecture incorporated frequency division trained using variable bandwidth center frequency sounds
differential contribution interaural spectral cues human sound localization examined using combined psychophysical analytical approach cues sounds location correlated individual basis human localization responses variety manipulated sounds spectral cues derive filtering individuals auditory periphery characterized measured head related transfer functions auditory localization performance determined virtual auditory space experiments conducted amplitude spectra sound stimulus varied independently each ear while preserving normal timing cues free field environment virtual auditory noise stimuli generated over specified target direction such false flat spectrum left using subjects sound spectrum right adjusted so either true right spectral cue true interaural spectral cue preserved subjects showed systematic both true right true interaural spectral conditions absent their control localization performance analysis different cues along subjects localization responses suggests significant differences use interaural spectral cues auditory systems spectral cues varies sound condition
wideband sources recorded using closely receivers separated based second order statistics using physical model mixing process case show parameter estimation problem essentially reduced considering directions arrival each signal paper presents two demixing methods operating time frequency domain experimentally shows always possible signals arriving different angles moreover use spatial cues solve channel selection problem post processing wiener filter artifacts caused demixing
stochastic meta descent new technique online adaptation local learning rates arbitrary twice differentiable systems like matrix momentum uses full second order information while retaining computational complexity exploiting efficient computation hessian vector products here apply independent component analysis employ resulting algorithm blind separation time varying mixtures matching individual learning rates rate change each source signals mixture coefficients our technique capable simultaneously tracking sources move different priori unknown speeds
speech waveform modelled piecewise stationary linear stochastic state space system its parameters estimated using expectation maximisation em algorithm problem initialisation em algorithm standard initialisation schemes lead poor formant trajectories but these trajectories important vowel intelligibility aim paper investigate suitability subspace identification methods em paper compares subspace state space system identification sid method em algorithm sid em methods similar they both estimate state sequence but using kalman filters kalman respectively estimate parameters but using least squares maximum likelihood respectively similarity sid em use sid em sid non iterative requires initialisation whereas em iterative requires initialisation sid sub optimal compared em probabilistic sense during experiments real speech sid methods compare conventional initialisation techniques they produce smoother formant trajectories have greater frequency resolution produce higher likelihoods work done while cambridge engineering dept speech modelling using subspace em techniques
paper use mutual information characterize distributions phonetic information space mutual information mi between phonetic label feature joint mutual information between phonetic label two three features estimated bias formulas entropy mutual information estimates extended include higher order terms mi recognition estimated results complementary those phonetic classification our results show phonetic information locally spread information globally spread time frequency
psychophysical physiological evidence shows sound localization acoustic signals strongly influenced their synchrony visual signals effect known work sound coming side set coming mouth effect suggests important information about sound location encoded synchrony between audio video signals evidence synchrony rarely used source information computer vision tasks paper explore use audio visual synchrony locate sound sources developed system searches regions visual landscape highly acoustic signals them likely contain acoustic source discuss our experience implementing system present results speaker localization task discuss potential applications approach
three dimensional motion humans observation limited single camera due inherent ambiguity video present system motion human subjects single camera video prior knowledge about human motion learned training data resolve those ambiguities after initialization tracking reconstruction automatic show results several video sequences results show power treating body tracking inference problem
independent component analysis natural images leads emergence simple cell properties ie linear filters resemble wavelets gabor functions paper extend ica explain further properties cells first decompose natural images into independent subspaces instead scalar components model leads emergence phase shift invariant features similar those complex cells second define topography between linear components obtained ica topographic distance between two components defined their higher order correlations so two components close each other topography they strongly dependent each other leads simultaneous emergence both topography invariances similar complex cell properties
paper propose information maximization provide unified framework understanding saccadic eye movements framework mutual information among cortical representations retinal image priors constructed our long term visual experience dynamic short term internal representation constructed recent saccades provides map guiding eye navigation eyes locations maximum complexity neuronal ensemble responses each step automatic saccadic eye movement system information about external world while modifying neural representations process framework attempts several psychological phenomena such pop out inhibition return long term visual experience short term working memory provides interesting perspective contextual computation formation neural representation visual system
describe method learning overcomplete set basis functions purpose modeling sparse structure images sparsity basis function coefficients modeled mixture gaussians distribution gaussian captures coefficients small variance distribution centered zero while more other gaussians capture active coefficients large variance distribution show prior such form exist efficient methods learning basis functions well parameters prior performance algorithm demonstrated number test cases natural images basis functions learned natural images similar those obtained other methods but sparse form coefficient distribution much better described parameters prior adapted data assumption about sparse structure images need made priori rather learned data
formulate model probability distributions image spaces show distribution images factored exactly into conditional distributions feature vectors resolution pyramid level conditioned image information lower resolutions would like factor over positions pyramid levels make tractable but such may long range dependencies fix introduce hidden class labels each pixel pyramid result hierarchical mixture conditional probabilities similar hidden markov model tree model parameters found maximum likelihood estimation using em algorithm have obtained encouraging preliminary results problems detecting various objects images target recognition optical images
statistics images represented using multiscale wavelet bases exhibit two striking types nongaussian behavior first marginal densities coefficients have extended heavy second joint densities exhibit variance dependencies captured second order models examine properties class gaussian scale mixtures show these densities accurately characterize both marginal joint distributions natural image wavelet coefficients class model suggests markov structure wavelet coefficients linked hidden scaling variables corresponding local image structure derive estimator these hidden variables show nonlinear normalization procedure used coefficients recent years have interest modeling statistics natural images such models important applications image processing computer vision many techniques rely either implicitly explicitly prior density number empirical studies have demonstrated power spectra natural images follow law radial frequency exponent typically close two eg such second order characterization because images usually exhibit highly non gaussian behavior instance marginals wavelet coefficients typically have much than gaussian furthermore despite being approximately suggested theoretical analysis processes orthonormal wavelet coefficients exhibit striking forms statistical dependency particular standard deviation wavelet coefficient typically scales absolute values its neighbors number researchers have modeled marginal distributions wavelet coefficients generalized exp eg special cases include gaussian laplacian but appropriate supported nsf grant mixing density positive stable explicit form gsm density gsm function gamma student stable generalized laplacian exp pe explicit form exp explicit form table example densities class gaussian scale mixtures denotes positive gamma variable density exp characteristic function random variable defined px exp dx natural images typically less than has modeled variance dependencies pairs wavelet coefficients et al have modeled wavelet densities using two component mixtures gaussians have modeled marginal densities cross sections joint densities multi dimensional generalized following sections explore semi parametric class gaussian scale mixtures show members class satisfy dual requirements being heavy exhibiting multiplicative scaling between coefficients show particular member class multiplier variables distributed according gamma density captures range joint statistical behaviors seen wavelet coefficients natural images derive estimator multipliers show nonlinear normalization procedure used wavelet coefficients form random linking multipliers multiresolution tree scale mixtures gaussians random vector gaussian scale mixture gsm denotes equality distribution scalar random variable iv gaussian random vector independent consequence gsm variable has density given integral oo exp probability density mixing variable multiplier special case gsm finite mixture gaussians discrete random variable more generally straightforward provide conditions either density characteristic function ensure gsm but these conditions do necessarily provide explicit form nevertheless number well known distributions may written gaussian scale mixtures scalar case few these densities along their associated characteristic functions table each variable characterized scale parameter tail parameter gsm models table produce heavy marginal variance scaling joint densities scale mixtures gaussians statistics natural images figure dashed lines fitted empirical histograms solid lines below each plot parameter values relative entropy between histogram bins model fraction histogram entropy modeling natural images mentioned
novel learning approach human face detection using network linear units presented learning architecture sparse network linear functions over pre defined incrementally learned feature space specifically learning presence large number features wide range face images different poses different expressions under different lighting conditions used training set capture variations human faces experimental results commonly used benchmark data sets wide range face images show based approach outperforms methods use neural networks bayesian methods support vector machines others furthermore learning evaluation using based method significantly more efficient than other methods
fundamental problem modeling chaotic time series data minimizing short term prediction errors does guarantee match between reconstructed attractors model experiments introduce modeling paradigm simultaneously learns short term predict locate outlines attractor new way nonlinear principal component analysis closed loop predictions constrained stay within these outlines prevent divergence attractor learning fast parameter estimation sample laser data time series competition took less than minute mhz
facial action coding system objective method quantifying facial movement terms component actions system widely used behavioral investigations cognitive processes social interaction coding performed highly trained human experts paper explores compares techniques automatically recognizing facial actions sequences images these methods include unsupervised learning techniques finding basis images such principal component analysis independent component analysis local feature analysis supervised learning techniques such linear discriminants these data driven bases compared gabor wavelets basis images best performances obtained using gabor wavelet representation independent component representation both achieved accuracy classifying facial actions ica representation employs orders magnitude fewer basis images than gabor representation takes less cpu time compute new images results provide converging support using local basis images high spatial frequencies statistical independence classifying facial actions
paper examines application reinforcement learning wireless communication problem problem requires channel utility maximized while simultaneously minimizing usage present solution multi criteria problem able significantly reduce power consumption solution uses variable factor capture effects usage
discuss information theoretic approach modeling dynamic processes approach learn compact informative statistic summarizes past states predict future observations furthermore uncertainty prediction characterized joint density over learned statistic present observation discuss application technique both noise driven dynamical systems random processes sampled density conditioned past first case show results both dynamics random walk statistics driving noise captured second case present results statistic learned noisy random waves dependencies past states both cases algorithm yields principled approach discriminating processes dynamics andor dependencies method grounded ideas information theory nonparametric statistics
three contributions developing algorithm designing analog circuits provided paper first method representing highly nonlinear non continuous analog circuits using current law potential functions within context markov field described second relatively efficient algorithm optimizing markov field objective function briefly described convergence proof briefly third empirical results illustrating strengths limitations approach provided within context transistor design problem proposed algorithm generated set circuit components circuit model accurately generated desired characteristic curves analog circuit design using markov random fields markov random field models markov random field mrf generalization concept markov chain markov field begins set random variables neighborhood relation represented graph each random variable assumed paper discrete random variable takes finite number possible values each node graph specific random variable link jth node ith node indicates conditional probability distribution ith random variable field functionally dependent upon jth random variable random variable neighbor random variable restriction upon definition markov field ie condition probability every realization field strictly positive essential idea behind markov field design specifies potential energy function every clique neighborhood graph such subset random variables associated clique obtain their optimal values potential function obtains its minimal value see associate university markov random field models provide convenient mechanism probabilistically representing optimally combining combinations local constraints analog circuit design using mixed signal application specific integrated circuit design problems most circuit design well known but
project paper develop first information geometric principles general method learning similarity between text documents each individual document modeled memoryless information source based latent class decomposition term document matrix curved multinomial learned model canonical similarity function known fisher kernel derived our approach applied unsupervised supervised learning problems particular covers interesting cases both labeled unlabeled data available experiments automated text categorization verify advantages proposed method
committee approach has been proposed reducing model uncertainty improving generalization performance advantage committees depends performance individual members correlational structure errors between members paper presents input grouping technique designing heterogeneous committee technique input variables first based their mutual information statistically similar variables assigned same group each members input set formed input variables extracted different groups our designed committees have less error correlation between its members each member different input variable combinations individual members feature sets contain less redundant information because highly correlated variables combined together member feature sets contain almost complete information each set contains feature each information group empirical study noisy nonstationary economic forecasting problem shows committees constructed our proposed technique outperform committees formed using several existing techniques
provide preliminary evidence existing algorithms inferring small scale gene regulation networks gene expression data adapted large scale gene expression data coming hybridization microarrays essential steps clustering many genes their expression time course data into minimal set clusters co expressed genes theoretically modeling various conditions under time courses measured using time analog recurrent neural network cluster mean time courses fitting such regulatory model cluster mean time courses simulated annealing weight decay several such fits circuit parameter sets including connection matrices procedure used assess existing future gene expression time course data sets determining regulatory relationships such
imagery pixel typically consists mixture reflectance spectra several materials mixture coefficients correspond abundances materials assume linear combinations reflectance spectra additive normal sensor noise derive probabilistic map framework analyzing data reflectance characteristics know priori face problem unsupervised linear incorporation different prior information eg normalization abundances naturally leads family interesting algorithms example noise free case yielding algorithm understood constrained independent component analysis ica simulations usefulness our theory
analysis data recorded optical imaging intrinsic signals measurement changes light reflectance cortical tissue removal noise artifacts such patterns serious problem often bandpass filtering used but underlying assumption spatial frequency exists separates mapping component other components especially global signal here propose alternative ways processing optical imaging data using blind source separation techniques based spatial decorrelation data first perform benchmarks artificial data order select way processing most robust respect sensor noise apply recordings optical imaging experiments macaque primary visual cortex show our bss technique able extract ocular dominance orientation preference maps single condition data standard post processing procedures fail artifacts especially patterns often completely removed maps summary our method blind source separation using extended spatial decorrelation superior technique analysis optical recording data
recently number authors have proposed treating dialogue systems markov decision processes mdps practical application mdp algorithms dialogue systems faces number severe technical challenges have built general software tool reinforcement learning dialogue systems based mdp framework have applied dialogue corpora gathered two dialogue systems built our experiments demonstrate holds promise tool understanding correlations complex temporally dependent dialogue corpora
propose new efficient technique incorporating contextual information into object classification most current techniques face problem exponential computation cost paper propose new general framework incorporates partial context linear cost technique applied microscopic urinalysis image recognition resulting significant improvement recognition rate over context free approach gain would have been impossible using conventional context incorporation techniques background recognition context number pattern recognition problem domains classification object should based more than simply appearance object itself remote sensing image classification each pixel part ground cover pixel more likely area than pixels areas text analysis expect find certain letters occurring particular arrangement other tion etc information accompanying entities referred contextual information human experts apply contextual information their decision making makes sense design techniques algorithms make computers aggregate utilize more complete set information their decision making way human experts do pattern recognition systems author correspondence song primary often source information used identify object set measurements features associated object itself augmenting information incorporating context into classification process yield significant benefits consider set objects ti each object associate class label ci member label set ft characterized set measurements xi many techniques incorporate context each object ti call feature conditioning posterior probability objects identities joint features accompanying objects cn ix xn maximizing respect cn shown pci cr given certain reasonable assumptions once context free posterior probabilities known eg through use standard machine learning model such neural network computing possible would finding maximum has complexity iv intractable large another problem formulation estimation high dimensional joint distribution cn ill posed data way dealing these problems limit context local regions approach pixels close neighborhood letters immediately adjacent considered such techniques may ignoring useful information apply situations context doesnt have such locality case microscopic urinalysis image recognition another way simplify problem using specific domain knowledge but possible certain domains these difficulties motivate efficient incorporation partial context general framework formulated section section discuss microscopic urinalysis image recognition address importance using context application section techniques proposed identify relevant context empirical results shown section followed discussions section formulation incorporation partial context avoid exponential computational cost using identities accompanying objects directly context use partial context denoted called partial because derived class labels opposed consisting explicit objects physical definition depends problem hand our application represents presence absence certain classes posterior probability object ti having class label ci conditioned its feature vector relevant context assume feature distribution object depends its own class ie assumption roughly true most real world problems image recognition context application microscopic urinalysis oc pci pci pci called context ratio through context plays its role context sensitive posterior probability pci obtained through context free posterior probability pci xi modified context ratio pci partial context maximum likelihood decision rule chooses class label element such ai systematic approach identify relevant context addressed section partial context approach treats each element set individually but additional information context factor once known context obtained maximize possible values ci take total number complexity finding maximum nd both linear density estimation part trivial easy estimate pca microscopic urinalysis
describe bayesian approach model selection unsupervised learning determines both feature set number clusters evaluate scheme based marginal likelihood based cross validated likelihood bayesian scheme derive closed form solution marginal likelihood assuming appropriate forms likelihood function prior extensive experiments compare these approaches results verified comparison against ground truth these experiments bayesian scheme using our objective function gave better results than cross validation
formulate problem retrieving images visual databases problem bayesian inference leads natural effective solutions two most challenging issues design retrieval system providing support region based queries without requiring prior image segmentation accounting user feedback during retrieval session present new learning algorithm relies belief propagation account both positive negative examples users
reinforcement learning nonstationary environments generally regarded important yet difficult problem paper partially addresses problem subclass nonstationary environments environment model called hidden mode markov decision process hm mdp assumes environmental changes always confined small number hidden modes mode indexes markov decision process mdp evolves time according markov chain while hm mdp special case partially observable markov decision processes pomdp modeling hm mdp environment via more general pomdp model increases problem complexity variant baum welch algorithm developed model learning requiring less data time
many researchers have explored methods hierarchical reinforcement learning rl temporal abstract actions defined perform many primitive actions before little known about learning state aspects state space ignored previous work developed method hierarchical rl paper define five conditions under state abstraction combined value function decomposition prove learning algorithm converges under these conditions show experimentally state abstraction important successful application learning
consider problem reliably choosing near best strategy restricted class strategies ii partially observable markov decision process pomdp assume given ability simulate pomdp study what might called sample complexity amount data generate pomdp order choose good strategy prove upper bounds sample complexity showing even large arbitrarily complex pomdps amount data needed finite depends linearly complexity restricted strategy class ii exponentially horizon time latter dependence variety ways including application gradient local search algorithms our measure complexity generalizes classical supervised learning notion vc dimension settings reinforcement learning planning
propose analyze class actor critic algorithms simulation based optimization markov decision process over parameterized family randomized stationary policies these two time scale algorithms critic uses td learning linear approximation architecture actor updated approximate gradient direction based information provided critic show features critic should span subspace prescribed choice parameterization actor conclude convergence properties open problems
consider problem learning grid based map using robot noisy sensors compare two approaches online em map treated fixed parameter bayesian inference map matrix valued random variable show even simple example online em get local minima causes robot get lost resulting map contrast bayesian approach maintaining multiple hypotheses much more robust introduce method approximating bayesian solution called rao particle filtering show approximation coupled active learning strategy fast but accurate
propose new approach problem searching space stochastic controllers markov decision process mdp partially observable markov decision process pomdp following several other authors our approach based searching parameterized families policies example via gradient descent optimize solution quality rather than trying estimate values derivatives policy directly do so indirectly using estimates probability densities policy induces states different points time enables our algorithms exploit many techniques efficient robust approximate density propagation stochastic systems show our techniques applied both deterministic propagation schemes mdps dynamics given explicitly compact form stochastic propagation schemes have access generative model simulator mdp present empirical results both these variants complex problems
model predictive control control algorithm uses solve optimal control moves over future time horizon based upon model process has become standard control technique process over past two decades most industrial applications linear dynamic model developed using empirical data used even though process itself often nonlinear linear models have been used because difficulty developing generic nonlinear model empirical data computational expense often involved using nonlinear models paper present generic neural network based technique developing nonlinear dynamic models empirical data show these models efficiently used model predictive control framework nonlinear based approach has been successfully implemented number industrial applications paper performance controller nonlinear industrial process presented
problem developing good policies partially observable markov decision problems pomdps remains most challenging areas research stochastic planning line research area involves use reinforcement learning belief states probability distributions over underlying model states promising method small problems but its application limited intractability computing representing full belief state large problems recent work shows many settings maintain approximate belief state fairly close true belief state particular great success has been shown approximate belief states out correlations between state variables paper investigate two methods full belief state reinforcement learning novel method reinforcement learning using factored approximate belief states compare performance these algorithms several well known problem literature our results demonstrate importance approximate belief state representations large problems
problem address paper mobile robot plan order arrive its goal minimum uncertainty traditional motion planning algorithms often assume mobile robot track its position reliably real world situations reliable localization may always feasible partially observable markov decision processes pomdps provide way maximize certainty reaching goal state but cost computational intractability large state spaces method propose explicitly models uncertainty robots position state variable generates trajectories through augmented pose uncertainty space minimizing uncertainty goal robot reduces likelihood becomes lost demonstrate experimentally navigation reduces uncertainty goal especially degraded localization
problem reinforcement learning non markov environment explored using dynamic bayesian network conditional independence assumptions between random variables represented network parameters parameters learned line approximations used perform inference compute optimal value function relative effects inference value function approximations quality final policy investigated learning solve difficult driving task two value function approximations linear quadratic found perform similarly but quadratic model more sensitive initialization both performed below level human performance task dynamic bayesian network performed comparably model using hidden state representation while requiring exponentially fewer parameters
function approximation essential reinforcement learning but standard approach approximating value function determining policy has so far proven theoretically intractable paper explore alternative approach policy explicitly represented its own function approximator independent value function updated according gradient expected reward respect policy parameters reinforce method actor critic methods examples approach our main new result show gradient written form suitable estimation experience approximate action value advantage function using result prove first time version policy iteration arbitrary differentiable function approximation convergent locally optimal policy large applications reinforcement learning rl require use generalizing function approximators such neural networks decision trees instance based methods dominant approach last has been value function approach function approximation effort goes into estimating value function action selection policy represented implicitly greedy policy respect estimated values eg policy selects each state action highest estimated value value function approach has well many applications but has several limitations first oriented toward finding deterministic policies whereas optimal policy often stochastic selecting different actions specific probabilities eg see singh jaakkola jordan second arbitrarily small change estimated value action cause selected such discontinuous changes have been identified key obstacle establishing convergence algorithms following value function approach bertsekas tsitsiklis example learning sarsa dynamic programming methods have been shown unable converge policy simple mdps simple function approximators gordon baird tsitsiklis van bertsekas tsitsiklis occur even best approximation found each step before changing policy whether notion best mean squared error sense slightly different senses residual gradient temporal difference dynamic programming methods paper explore alternative approach function approximation rl sutton singh rather than approximating value function using compute deterministic policy approximate stochastic policy directly using independent function approximator its own parameters example policy might represented neural network whose input representation state whose output action selection probabilities whose weights policy parameters let denote vector policy parameters performance corresponding policy eg average reward per step policy gradient approach policy parameters updated approximately proportional gradient op positive definite step size above achieved usually converge locally optimal policy performance measure unlike value function approach here small changes cause small changes policy state visitation distribution paper prove unbiased estimate gradient obtained experience using approximate value function satisfying certain properties reinforce algorithm finds unbiased estimate gradient but without learned value function reinforce learns much more slowly than rl methods using value functions has received relatively little attention learning value function using variance gradient estimate appears essential rapid learning jaakkola singh jordan proved result similar ours special case function approximation corresponding pomdps our result generalizes arbitrary differentiable function approximators tsitsiklis independently developed result ours see bartlett marbach tsitsiklis our result suggests way proving convergence wide variety algorithms based actor critic policy iteration architectures eg barto sutton anderson sutton paper take first step direction proving first time version policy iteration general differentiable function approximation convergent locally optimal policy baird moore obtained weaker but similar result their vaps family methods like policy gradient methods vaps includes separately parameterized policy value functions updated gradient methods vaps methods do gradient performance expected long term reward but measure combining performance accuracy result vaps does converge locally optimal policy except case weight put upon value function accuracy case vaps reinforce similarly fitted value iteration convergent value based but does find locally optimal policy policy gradient theorem consider standard reinforcement learning framework see eg sutton barto learning agent markov decision process mdp state action reward each time denoted st rt respectively environments dynamics characterized state transition probabilities pa pr st expected rewards st vs agents decision making procedure each time characterized policy rs pr vs ji parameter vector assume respect its parameter ie exists usually write rsa policy gradient methods rl function approximation function approximation two ways formulating agents objective useful average reward formulation policies ranked according their long term expected reward per step pr pr rn rsa pr st stationary distribution states under assume exists independent so policies average reward formulation value state action pair given policy defined second formulation cover start state so about long term reward obtained give our results once but they apply formulation well under definitions pr sor st ar tl rate allowed tasks formulation define drs discounted weighting states encountered starting so following drs our first result concerns gradient performance metric respect policy parameter theorem policy gradient mdp either average reward start state formulations op oo oo proof see way gradient first discussed average reward formulation marbach tsitsiklis based related expression terms state value function due jaakkola singh jordan extend their results start state formulation provide simpler more direct proofs theory reinforce algorithms viewed event key aspect both expressions gradient their terms form effect policy changes distribution states does appear convenient approximating gradient sampling example sampled distribution would unbiased estimate obtained following course normally known estimated ap use actual returns rt oo oo re start state formulation approximation each leads reinforce algorithm oc actions preferred known follow expected value williams policy gradient approximation now consider case approximated learned function approximator approximation sufficiently good might use place sutton singh still point roughly direction gradient example jaakkola singh jordan proved special case function approximation arising pomdp could positive inner product gradient sufficient ensure improvement moving direction here extend their result general function approximation prove equality gradient let fw our approximation qx parameter natural learn fw following updating rule such unbiased tj estimator perhaps such process has converged local optimum drs sa ow theorem policy gradient function approximation fw satisfies compatible policy parameterization sense ow rsa op proof combining gives rsa oo us error orthogonal gradient policy parameterization because expression above zero policy gradient theorem yield op drs drs rsa fsa oo sa application deriving algorithms advantages given policy parameterization theorem used derive appropriate form value function parameterization example consider policy gibbs distribution linear combination features vs tsitsiklis communication points out being linear features given side may way satisfy condition policy gradient methods rl function approximation each dimensional feature vector characterizing state action pair condition requires so natural parameterization fw other words fw linear same features policy except normalized mean zero each state other algorithms easily derived variety nonlinear policy such multi layer backpropagation networks careful have form given above requires have zero mean each state sense better think fw approximation advantage function much baird rather than qx our convergence requirement really fw get relative value actions correct each state absolute value nor variation state state our results viewed justification special advantages target value function approximation rl fact our generalized include arbitrary function state added value function its approximation example generalized vs arbitrary function follows immediately because choice does affect our theorems but substantially affect variance gradient estimators issues here entirely analogous those use reinforcement earlier work eg williams dayan sutton practice should presumably set best available approximation our results establish approximation process without affecting expected evolution convergence policy iteration function approximation given theorem prove first time form policy iteration function approximation convergent locally optimal policy theorem policy iteration function approximation let fw differentiable function approximators policy value function respectively satisfy condition oo let step size sequence such oo mdp bounded rewards sequence defined ok such ok oo converges such proof our theorem update direction gradient mdps rewards together us bounds oo sutton singh bounded these together step size requirements necessary conditions apply page bertsekas convergence local optimum acknowledgements authors wish thank precup comments insights into notion optimal policy under function approximation references baird advantage updating technical report baird residual algorithms reinforcement learning function approximation proc machine learning pp morgan kaufmann baird moore gradient descent general reinforcement learning nips mit press barto sutton anderson elements solve difficult learning control problems ieee trans systems bartlett direct gradient based reinforcement learning gradient estimation algorithms bertsekas tsitsiklis neuro dynamic programming scientific perturbation realization potentials sensitivity analysis markov processes ieee trans automatic control dayan reinforcement comparison touretzky sejnowski hinton eds connectionist models proceedings school pp morgan kaufmann gordon stable function approximation dynamic programming proceedings machine learning pp morgan kaufmann gordon cmu learning technical report jaakkola singh jordan reinforcement learning algorithms partially observable markov decision problems nips pp morgan analysis algorithms using eligibility traces reinforcement learning imperfect value functions proc icml pp tsitsiklis actor critic algorithms marbach tsitsiklis simulation based optimization markov reward processes technical report massachusetts institute technology singh jaakkola jordan learning without state estimation partially observable markovian decision problems proc icml pp sutton temporal credit assignment reinforcement learning university massachusetts sutton barto reinforcement learning
present monte carlo algorithm learning act partially observable markov decision processes pomdps real valued state action spaces our approach uses importance sampling representing beliefs monte carlo approximation belief propagation reinforcement learning algorithm value iteration employed learn value functions over belief states finally version nearest neighbor used generalize across states initial empirical results suggest our approach works well practical applications
introduce novel algorithm termed performance prediction algorithm quantitatively measures contributions elements neural system tasks performs algorithm identifies neurons areas cognitive behavioral task given data about performance decrease small set lesions allows accurate prediction performances due lesions effectiveness new algorithm demonstrated two models recurrent neural networks complex interactions among algorithm scalable applicable analysis large neural networks given recent advances reversible techniques has potential significantly contribute under standing organization biological nervous systems light about local versus distributed tion brain
present expressive agent design language reinforcement learn ing allows user constrain policies considered learn ing language includes standard features such parameter memory variables but allows choices agent program learning specified present provably convergent learning demonstrate example agent programs written language concise well modular facilitates state abstraction learned skills
establish principled framework adaptive transform coding transform often constructed ad hoc choice transform suboptimal bit allocation quantizer design instead start probabilistic latent variable model form mixture constrained gaussian mixtures model derive transform coding algorithm constrained version generalized algorithm vector quantizer design our derivation
model hippocampal place cells cells ing visual stimuli visual put provided video camera robot set gabor filters nodes retinotopic graph hebbian learning employed incrementally build tion localized overlapping place fields place cells serve basis func tions reinforcement learning experimental results navigation mobile robot presented
paper presents unified probabilistic framework denoising dereverberation speech signals framework transforms ing dereverberation problems into signal estimation key idea use strong speech model large data set clean speech computational efficiency achieved using variational em working frequency domain employing conjugate priors framework covers both single multiple phones apply approach noisy speech signals get results substantially better than standard methods
present algorithm between characteristics real world problems assumptions independent component analysis algorithm provide additional information ica network incorporate top down selective attention mlp classi er added separated signal channel error classi er backpropagated ica network backpropagation process results estimation expected ica output signal top down attention matrix according new cost function representing backpropagated error well independence es density recovered signals density appropriate classi cation noisy speech signal recorded real environments algorithm improved recognition performance showed robustness against parametric changes
present computational model neural mechanisms temporal support spatial navigation recall scenes imagery products recall long term representations stored hippocampus associated local spatial features region representations dynamically generated long term memory parietal part model model thereby recall im locations objects complex environments after parietal damage model exhibits mental imagery perspective observer square experiment our model makes novel predictions neural representations parietal regions behavior neuropsychological patients
develop approach object recognition based matching shapes using resulting measure similarity nearest neighbor classi er key algorithmic problem here nding correspondences between image shape stored prototype shape introduce new shape descriptor shape context makes possible using simple robust algorithm shape context point captures distribution over relative positions other shape points thus summarizes global shape rich local descriptor demonstrate shape contexts greatly simplify recovery correspondences between points two given shapes once shapes aligned shape contexts used de robust score measuring shape similarity have used score nearest neighbor classi er recognition hand written digits well objects using exactly same distance function benchmark mnist dataset handwritten digits yields error rate outperforming other published techniques
consider existence efficient algorithms learning class learning model ie ing prior assumptions distribution resulting combinatorial problem finding best agreement over input sample np hard approximate within constant factor suggest way theoretical bound introducing new measure success such algorithms algorithm margin successful agreement ratio outputs good once training points inside margins its separating hyperplane prove computational com results respect success measure hand every positive exist efficient margin learning algorithms other hand prove unless algorithm runs time polynomial sample size margin successful
present novel method clustering using support vector ma approach data points mapped high dimensional feature space support vectors used define sphere them boundary sphere forms data space set closed contours containing data data points each contour defined cluster width parameter gaussian kernel decreased these contours fit data more tightly splitting contours occurs algorithm works separating clusters according un probability distribution thus clusters take arbitrary geometrical shapes other sv algorithms outliers introducing soft margin constant leading smoother cluster bound structure data explored varying two investigate dependence our method these parameters apply several data sets
goal statistical language modeling learn joint probability function sequences words intrinsically difficult because curse dimensionality propose its own proposed approach learns simultaneously distributed each word ie similarity between words along probability function word sequences expressed these generalization obtained because sequence words has never been seen before gets high probability made words similar words forming already seen sentence report experiments using neural networks probability function showing two text corpora proposed approach significantly im model
variational derivation mean theory presented theory applied sigmoidal belief networks aid further approximations empirical evaluation small scale networks show proposed approximations quite competitive
many processes biology regulation gene expression memory brain involve switches constructed networks biochemical crucial molecules present small numbers questions about noise stability analysis noise simple reaction schemes indicates switches stable years milliseconds built fewer than hundred molecules direct tests prediction well implications discussed
olshausen field demonstrated learning algorithm attempts generate sparse code natural scenes develops complete family localised oriented bandpass receptive fields similar those simple cells paper describes algorithm finds sparse code sequences images preserves information about input algorithm trained natural video sequences develops bases representing movement particular directions particular speeds similar receptive fields cells observed cortical visual areas furthermore contrast previous approaches learning direction selectivity timing neuronal activity encodes phase movement so precise timing spikes crucially important information encoding suggested goal early sensory processing reduce redundancy sensory information activity sensory neurons encodes independent features neural modelling give insight into these neural nets may learn operate atick showed training neural network patches natural images remove pairwise correlation between neuronal responses results neurons having receptive fields those retinal ganglion neurons olshausen field demonstrated learning algorithm attempts generate sparse code natural scenes while preserving information about visual input develops complete family localised oriented bandpass receptive fields similar those activities neurons implementing coding signal presence edges basic components natural images olshausen field their algorithm create sparse representation because higher degree statistical independence among its outputs similar receptive fields obtained training neural net so make responses neurons independent possible other authors have shown direction selectivity may emerge unsupervised learning way receptive fields neurons encode movements created paper describes algorithm finds sparse code sequences images preserves critical information about input algorithm trained natural video images develops bases representing movements particular directions particular speeds similar receptive fields cells observed early visual areas activities neurons implementing encoding signal presence edges moving certain speeds certain directions each neuron having its preferred speed direction furthermore contrast previous approaches timing neural activity encodes movements phase so precise timing spikes crucially important information coding proposed algorithm extension proposed olshausen field hence high level algorithm cannot directly implemented biologically plausible neural network plausible neural network performing similar task developed proposed algorithm described section sections show methods results simulations finally section discusses algorithm differs previous approaches implications presented results proposed algorithm extension described olshausen field section starts brief
present novel way obtaining pac style bounds generalization error learning algorithms explicitly using their stability properties stable learner learned solution does change much small changes training set bounds obtain do depend measure complexity hypothesis space eg vc dimension but rather depend learning algorithm searches space thus applied even vc dimension nite demonstrate regularization networks possess required stability property apply our method obtain new bounds their generalization performance
describe extension markov decision process model continuous time dimension included state space allows representation exact solution wide range problems transitions rewards vary over time examine problems based route planning public observation scheduling
novelty detection involves modeling normal behaviour hence enabling detection divergence has potential applications many areas such detection ma damage features medical data approach build hypothesis estimating support normal data ie constructing function positive region data located negative elsewhere recently kernel methods have been proposed estimating support distribution they have performed well practice training involves solution quadratic programming problem pa per propose simpler kernel method estimating support based linear programming method easy implement learn large datasets rapidly demonstrate method medical fault detection datasets
paper presents predictive gain scheduling technique simplify ing reinforcement learning problems decomposition link admission control selfsimilar call traffic used demonstrate technique control problem decomposed into online prediction call arrival rates policies poisson call ar processes decision time predictions used select among policies simulations show technique results faster learning without performance loss compared reinforcement learning controller does decompose problem communications networks such asynchronous transfer mode networks resource control crucial importance network operator well users objective maintain service quality while maximizing operators call level service quality service measured terms call blocking probabilities key resource controlled bandwidth network routing call admission control two such resource control problems markov decision processes offer framework optimal routing model dynamics network traffic computing control policies using dynamic programming resource control optimized standard assumption such models calls arrive according poisson processes makes models dynamics relatively simple although poisson assumption valid most communications networks number studies indicate many types al processes networks well local area networks statistically self similar makes difficult find models dynamics models become large complex number system states large straightforward application ic programming nevertheless fractal burst structure selfsimilar traffic should possible exploit design efficient resource control methods have previously presented method based td learning selfsimilar call traffic yields higher than controller assuming poisson call arrival processes drawback method slow convergence control policy paper presents alternative solution above problem called predictive gain scheduling control problem into two parts timeseries prediction nearfuture call arrival rates set control policies poisson call arrival processes decision time policy selected based these predictions thus selfsimilar arrival process approximated poisson process rate predictions made artificial neural networks nns trained online policies computed using dynamic programming other reinforcement learning techniques paper concentrates link admission control problem controllers describe used building block optimal routing shown other recent work reinforcement learning routing includes marbach et al show extend use td learning network routing et al apply reinforcement learning routing subject quality service constraints limitations traditional poisson model network arrival processes have been demonstrated number studies eg indicate existence heavy time distributions longterm correlations arrival processes selfsimilar models have been shown correspond better traffic selfsimilar arrival process has natural burst length contrary its arrival varies considerably over many time scales makes variance its sample mean decay slowly sample size its function decay slowly time compared poisson traffic complexity control prediction poisson traffic reduced memoryless property poisson process its expected future depends arrival intensity but process history other hand dependence selfsimilar traffic makes possible improve predictions process future observing history compact statistical measure degree stochastic process parameter selfsimilar traffic parameter takes values interval whereas poisson processes have parameter link admission control problem link capacity unitss offered calls different service classes calls belonging such class have same bandwidth requirements unitss perclass call holding times assumed exponentially distributed mean access link controlled policy maps states actions aa set contains feasible link states action set rejecting call set link states given set feasible call number cartesian product representations history perclass call arrival processes needed because memory selfsimilar arrival processes given jj number calls accepted link assume uniform call means reward rate time equal carried bandwidth xt time evolves continuously discrete call arrival events denote immediate reward obtained state time until next state time expectation reward expected time state under policy taking optimal actions policy controls probabilities state transitions so increase probability reaching states yield high longterm rewards objective link admission control find policy maximizes average reward per stage note average reward does depend initial state contribution state average reward tends zero assuming example probability reaching other state every state positive certain states special interest optimal policy these states intelligent blocking set such states ib given ib ib ib set call number available bandwidth multiple bandwidth wideband call states ib longterm reward may rejecting calls bandwidth future expected wideband calls gain scheduling control theory technique parameters controller changed function operating conditions approach taken here look up policies table predictions nearfuture perclass call arrival rates poisson call arrival processes optimal policy link admission control prob does depend history arrival processes due memoryless property constant perclass arrival rates matter our gain scheduled control selfsimilar call arrival processes nearfuture predicted self similar call arrival processes approximated poisson processes selecting poisson arrival processes based predicted function rbf nn per class trained predict its nearfuture arrival rate solving link admission control problem poisson traffic poisson call arrival processes dynamic programming offers solving problem paper policy iteration used involves two steps value determination policy improvement value determination step makes use objective function concept relative values difference between two relative values under policy expected difference reward over infinite time interval starting state instead state paper relative values computed solving system linear equations method chosen its fast convergence dynamics system characterized state transition probabilities given policy per class call arrival intensities mean holding times policy improvement step consists finding action maximizes relative each state after improving policy value determination policy improve ment steps iterated until policy does change determining prediction horizon over what future time horizon should predict rates used select policies work prediction horizon set average estimated mean first times states back themselves following referred mean return time arrival process approximated poisson process within time interval motivation choice prediction horizon effects decision action state influence future probabilities reaching other states receiving rewards until state reached next time happens new made previous decision does longer influence future expected reward assumption mean return time estimated call instead full state descriptor case poisson call arrival processes mean first times other states state unique solution linear system equations mn ln ln limiting probability state determined states intelligent blocking solving linear system equations matrix containing state transition intensities given mean return time link defined average individual mean return times states ib weighted their limiting probabilities normalized nn ib nn nn ib ease implementation time window expressed number call arrivals window length class computed multiplying mean return time arrival rate off integer although window size varies variation partly decreasing increasing prediction future call arrival rates prediction future arrival call rates naturally based measures recent arrival rates work following representation history arrival process used classes exponentially weighted running averages times computed different time scales these history vectors computed using factors taking values interval ji ji arrival time kth call class studies timeseries prediction nonlinear feedforward nns outperform linear time series long memory employ rbf nns symmetric gaussian basis functions activations rbf units normalized division sum activations produce smooth output function locations widths rbf units determined data sets cover region history vectors nn trained average time target after every new call arrival prediction error computed learning performed online using least mean squares rule means up delayed call arrivals predicted perclass arrival rates used select control policy arrival call given prediction horizon arrival rate predictor tuned linear search minimize prediction error sample traffic traces performance gain scheduled admission controller evaluated simulated link capacity unitss offered calls selfsimilar call arrival pro comparison simulations repeated three other link admission con two controllers nn based controller us ing complete sharing ie accept call free capacity link sufficient nn based td controller uses rbf nns per receiving input each nn has hidden units units per call class plus default activation unit its weights initialized feasible calls states td controller assumes poisson call arrival processes follows call number constitute markovian states consequently value function table stores value per controller used evaluation performance loss modelling selfsimilar call traffic poisson traffic synthesis call traffic synthetic traffic traces generated gaussian autoregressive moving average model results statistically selfsimilar arrival process parameter easily tuned generated traces containing pairs two call classes characterized bandwidth requirements wideband unitss call holding times mean parameter used call arrival rates scaled make expected longterm arrival rates two classes fulfill ratio varied gain scheduling simplicity constant prediction horizon used throughout simulations computed according section averaging resulting prediction windows window size obtained table policies used gain scheduling computed predicted ranging step size total policies two nns both had hidden units nns weights initialized numerical results both td learning controllers gain scheduling controller allowed adapt first simulated call arrivals traffic traces throughput obtained four methods measured subsequent call arrivals call arrivals call arrivals cs throughput unitss throughput versus arrival rate ratio initial weight evolution neural predictor longterm weight evolution neural predictor weight evolution nn based td controller call arrivals figure weight evolution nn predictor nn based performance figure shows evolution weights call arrival rate predictor class figure displays weights rbf nn corresponding call number candidate intelligent blocking these weights eight different class center vectors plus default activation majority weights gain scheduling rbf nn seems converge few thousand call arrivals whereas td learning controller needs about call arrivals converge surprising rbf nns td learning controllers split up set training data so single nn updated much less frequently than rate predicting nn gain scheduling controller secondly td learning nns trained moving targets due learning rule stochastic action selection changing policy few weights gain scheduling nn change considerably even after long train ing these weights correspond rbf units activated rare large inputs figure evaluates performance terms throughput versus arrival rate ratio each data point averaged throughput traffic traces gain scheduling achieves same throughput td learning rbf nns up compared td learning up better than complete sharing cs difference throughput between td learning complete sharing low arrival rate ratios throughput increase bandwidth high rate wideband calls considerably higher than loss throughput low rate traffic have presented predictive gain scheduling technique reinforcement learning problems link admission control network routing used demonstrate technique predicting nearfuture call arrival rates part full state descriptor policies poisson call arrival processes computed rest state descriptor selected increased online convergence rate approximately times compared admission controller getting full state descriptor input decomposition did result performance loss computational complexity controller using predictive gain scheduling may reach computational bottleneck size state space increased tion optimal policies poisson traffic policy iteration overcome state aggregation relative value function combined temporal difference learning possible significantly reduce number relative value functions showed linear interpolation relative value functions dis algorithm enables use less than relative value functions without performance loss further have successfully employed gain scheduled link ad control building block network routing performance improve ment compared conventional methods larger than link admission control prob use gain scheduling reduce complexity reinforcement learning problems limited link admission control general technique should applicable problems parts state descriptor used directly after preprocessing select among policies instances simplified version original problem references network resource management dp bertsekas dynamic programming optimal control scientific mass traffic failure poisson modeling trans actions vol pp ms selfsimilar nature traffic extended version transactions vol pp ac changing nature network traffic scaling phenomena computer communication review vol pp rs sutton ag barto reinforcement learning
conventional backprop nets excess hidden units generalize poorly show nets excess capacity generalize well trained backprop early stopping experiments two reasons overfitting vary significantly different regions model excess capacity allows better fit regions high nonlinearity backprop often avoids overfitting regions low nonlinearity regardless size nets learn task similar sequence big nets pass through stages similar those learned smaller nets early stopping stop training large net generalizes comparably smaller net show conjugate gradient yield worse generalization because regions low nonlinearity learning fit regions high nonlinearity
online recursive algorithm training support vector machines vector time presented kuhn conditions previously seen training data number steps each computed analytically incremental procedure re offers efficient method ex evaluate generalization performance interpretation feature space light relationship between generalization geometry data
risk minimization principle establishes between generative models methods derived structural risk principle such support vector machines statistical explain provides framework number existing algorithms such parzen windows support vector machines ridge regression constrained logistic classifiers show approach implies new algorithm solving problems usually associated generative models new algorithms described dealing pattern recognition problems different pattern distributions dealing unlabeled data preliminary empirical results presented
paradigm hebbian learning has recently received novel interpretation discovery synaptic plasticity depends relative timing pre post synaptic spikes paper temporally dependent learning rule basic principle mutual information maximization studies its relation experimentally observed plasticity nd supervised spike dependent learning rule sharing similar structure experimentally observed plasticity increases mutual information stable near optimal level moreover analysis reveals temporal structure time dependent learning rules determined temporal applied neurons over their inputs these results suggest experimental prediction dependency learning rule neuronal biophysical parameters
high dimensional data modeling difficult mainly because curse dimensionality propose technique called tion high dimensional density estimation curse dimensionality exploiting independence structures data gaussianization motivated recent statistics literature projection pursuit independent component analysis mixture models covariances propose gaussianization procedure converges weakly each data first transformed least dependent coordinates each coordinate gaussianization offers density estimation than traditional kernel methods radial basis function methods gaussianization viewed efficient solution nonlinear independent component high dimensional projection pursuit
preliminary work authors made use so called world assumption about scene statistics city scenes assumption stated such scenes built cartesian grid led regularities image edge gradient statistics paper explore general applicability assumption show surprisingly holds large variety less structured environments including scenes enables us single image determine orientation relative scene structure detect target objects aligned grid these inferences performed using bayesian model probability distributions eg image gradient statistics learnt real data
output coding general method solving multiclass problems reducing them multiple binary classification problems previous re search output coding has employed almost solely discrete codes describe algorithm improves performance output codes them continuous codes relaxation procedure cast optimization problem reminiscent quadratic program support vector machines describe experiments proposed algorithm comparing standard discrete output codes experimental results indicate continuous relaxations output codes often improve generalization performance especially short codes
develop approach sparse representation gaussian process gp models order overcome limitations gps caused large data sets method based combination bayesian online al together sequential construction relevant data fully specifies prediction model mental results toy examples large realworld datasets indicate efficiency approach
hebbian competitive hebbian algorithms almost ubiquitous modeling pattern formation cortical development analyse detail particular model adapted development patterns places competitive cortical influences free restricted onto common
explaining away has mostly been considered terms inference states belief networks show arise bayesian context inference about weights governing relationships such those between stimuli conditioning experiments such backward blocking show explaining away weight space accounted using extension kalman filter model pro new approximate way looking kalman gain matrix correlation matrix observation process suggest network implementation using architecture due show resulting model exhibits backward blocking
trying recover structure set images most difficult problem establishing correspondence between measurements most existing approaches assume features across frames whereas methods exploit constraints facilitate matching do so under restricted motion paper propose bayesian approach avoids associated out best instead consider distribution over possible correspondences treat both fully bayesian approach yields posterior distribution map approach makes use em maximize posterior show markov chain monte carlo methods used implement these techniques practice present experimental results real data
nearest neighbor classification assumes locally constant class con probabilities assumption becomes high dimensions finite samples due curse dimensionality severe bias introduced under these conditions using nearest neighbor rule propose locally adaptive nearest neighbor classification method try minimize bias use distance analysis compute flexible metric pro neighborhoods along less relevant feature dimensions along most ones result class conditional probabilities tend smoother neighborhoods whereby better classification performance achieved efficacy our method validated compared against other techniques using variety real world data
recent work has exploited data unsupervised learning new types generative model nonnegative data recently shown generative model non negative boltzmann distribution gaussian distribution model constrained match first second order statistics data learning practical sized problems made difficult need compute expectations under model distribution cost markov chain monte carlo methods low fidelity naive mean field techniques has led increasing interest advanced mean field theories variational methods here present second order meanfield approximation nonnegative boltzmann machine model obtained using expansion theory tested learning dimensional model highdimensional invariant distribution generative model hand written digits
incorporating prior knowledge particular task into architecture learning algorithm greatly improve generalization performance study here case know function learned two its arguments convex them purpose propose class functions similar multilayer neural networks but has those properties universal approximator continuous functions these other properties apply new class functions task modeling price call options experiments show improvements price call options using new types function classes incorporate priori con
serious problem learning probabilistic models presence variables these variables observed yet interact several observed variables such they induce seemingly complex de among latter recent years much attention has been development algorithms learning parameters cases structure presence hidden variables pa per address related problem detecting hidden variables interact observed variables problem interest both improving our understanding domain preliminary step learning procedure towards promising models natural approach search structural signatures hidden variables substructures learned network tend suggest presence hidden variable make basic idea concrete show integrate algorithms evaluate method several synthetic datasets show performs well
many neural systems extend their dynamic range adaptation ex adaptation context dynamically stimuli demonstrate fly visual system adaptation statistical ensemble stimulus dynamically maximizes information transmission about stimulus further while rate response has long transients adaptation takes place consistent optimal variance estimation
people understand complex auditory visual information often using other automated analysis even low level faces severe challenges including lack accurate statistical models signals their varied rates previous approaches assumed simple parametric models joint distribution while tractable cannot capture com signal relationships learn joint distribution visual auditory signals using nonparametric approach first project data into maximally informative subspace suitable density estimation model complicated stochastic between signals using nonparametric density estimator these learned densities allow processing across signal modalities demonstrate synthetic real signals localization video face audio conversely audio enhancement particular speaker selected video
way approximate inference richly connected graphical models apply sum product algorithm probability propagation algorithm while ignoring fact graph has cycles sum product algorithm directly applied gaussian networks graphs coding but many conditional probability functions including sigmoid function direct application sum product algorithm possible introduce networks have low local complexity but exponential global complexity so sum product algorithm directly applied network probability child given its parents computed accumulating inputs parents markov chain more generally tree after giving expressions inference learning networks give results bars problem problem extracting translated overlapping faces image
important class problems cast inference bayesian networks binary state each variable logical noisy versions states variables parents example medical diagnosis presence symptom expressed noisy diseases may cause symptom disease may fail activate symptom inference richly connected noisy networks intractable but approximate methods eg variational techniques showing increasing promise practical solutions problem most approximations they tend concentrate relatively small number modes true posterior ignoring other plausible con gurations hidden variables introduce new sequential variational method bipartite networks favors including modes true posterior models posterior distribution tree compare method other approximations using ensemble networks network statistics comparable qmr dt medical network variational approximations approximate algorithms probabilistic inference now even being incorporated into vlsi hardware communication approximate methods include variational techniques ghahramani jordan saul et al frey hinton jordan et al local probability propagation gallager pearl frey freeman weiss markov chain monte carlo many algorithms have been proposed each these classes problem most above algorithms er tendency concentrate relatively small number modes target distribution distribution being approximated case medical diagnosis di erent modes correspond di erent explanations symptoms markov chain monte carlo methods usually guaranteed eventually sample modes but may take extremely long time even transitions px px qx qx figure approximate adjusting mean variance gaussian qx result minimizing dqjjp done most variational methods result minimizing dp jjq used preliminary results local probability propagation richly connected networks show sometimes able oscillate between plausible modes murphy et al frey but other results show sometimes oscillates between implausible con gurations et al most variational techniques minimize cost function favors nding single most massive mode less probable modes target distribution eg saul et al ghahramani jordan jaakkola jordan frey hinton attias more sophisticated variational techniques capture multiple modes using substructures saul jordan leaving part original network approximating jaakkola jordan although these methods increase number modes captured they still exclude modes variational techniques approximate target distribution using simpler parameterized distribution qx parameterized bound example disease disease disease may approximated distribution disease disease disease current set observed symptoms parameters distributions adjusted make close possible common approach variational inference minimize relative entropy dqjjp qx log qx dqjjp dp jjq often dqjjp minimized respect parameters using iterative optimization even exact optimization see minimizing dqjjp may exclude modes target distribution suppose gaussian region vanishing density between two modes shown fig minimize dqjjp respect mean variance cover two modes illustrated fig assume symmetry broken because dqjjp tend nonzero region has vanishing density contrast minimize dp jjq respect mean variance cover modes dp jjq tend region nonzero see fig many problems including medical diagnosis easy argue more important our approximation include modes than exclude con gurations cost other modes former leads low number false whereas latter may lead large number false disease present figure bipartite bayesian network observed dn hidden bipartite noisy networks fig shows bipartite noisy bayesian network binary hidden variables dn binary observed variables later present results medical diagnosis dn indicates disease active dn indicates disease inactive indicates symptom active indicates symptom inactive joint distribution jd case medical diagnosis form assumes diseases independent although diseases probably do depend other diseases form considered representation problem et al likelihood takes noisy form pearl probability symptom fails activated product probabilities each active disease fails activate jd dn kn kn probability active dn fails activate accounts probability probability symptom active none diseases active exact inference computes distribution over given subset observed values observed corresponding likelihood node plus edges may give new network describes marginal distribution over remaining variables so assume considering variables observed variables so rst variables active remaining variables inactive posterior distribution written djs dn kn dn kn taken together two terms right take simple product form over variables so rst step inference inactive diseases dependent given symptoms present variables modifying priors follows np kn dn constant assuming inactive symptoms have been have djs dn kn term left does have product form entire expression multiplied out give sum product forms exact inference performed combining results exact inference each product forms exponential time complexity makes large problems such qmr dt intractable sequential inference using variational trees described above many variational methods minimize dqjjp nd approximations exclude modes posterior distribution present method minimizes dp jjq sequentially absorbing observation time so exclude modes posterior approximate posterior distribution tree directed undirected trees equivalent use directed representation each variable has most parent algorithm active symptom time producing new tree searching tree closest dp jjq sense product previous tree likelihood next symptom search performed time using probability propagation two versions previous tree compute weights edges new tree applying minimum weight spanning tree algorithm let tree approximation obtained after absorbing kth symptom initially take tree variables has marginals equal marginals obtained absorbing inactive symptoms described above interpreting tree previous step current prior over diseases use likelihood jd next symptom obtain new estimate posterior djs dp jd dn kn dn kn ed tree let new tree jd index parent dn new tree parent function conditional probability tables found minimizing djs log djs ignoring constants have djs log log jd log jd dn log jd given structure parent function optimal conditional probability tables jd constant ensures dn jd table easily computed using probability propagation two trees compute two marginals needed di optimal conditional probability table variable independent parentchild relationships network so current symptom compute optimal conditional probability tables nn possible parent child relationships time using probability propagation use minimum weight directed spanning tree algorithm search best tree once symptoms have been use tree distribution make inferences about given order symptoms generally quality resulting tree jaakkola jordan but used random ordering experiments reported below results qmr dt type networks using structural parameter statistics qmr dt network given et al simulated qmr dt type networks roughly diseases each networks each groups instantiated active symptoms number active symptoms small enough compare our approximate method exact method two other approximate inference methods local probability propagation murphy et al variational upper bound jaakkola jordan medical diagnosis important question many most probable diseases under approximate posterior examined before most probable diseases under exact posterior found clearly exact inference algorithm give whereas approximate algorithm ranks most probable disease last give each group networks each inference method averaged values each value left column fig shows average versus active symptoms sequential tree tting method closest optimal cases right column shows extra work caused excess number diseases examined approximate methods positive findings exact ub tree pp positive findings ub tree pp positive findings exact ub tree pp positive findings ub tree pp positive findings exact ub tree pp positive findings ub tree pp figure comparisons number most probable diseases under approximate posterior examined before most probable diseases under exact posterior found approximate methods include sequential tree tting method presented paper tree local probability propagation pp variational upper bound ub summary noisy networks used model variety problems including medical diagnosis exact inference large richly connected noisy networks intractable most approximate inference algorithms tend concentrate small number most probable con gurations hidden variables under posterior presented variational method bipartite noisy networks favors including probable con gurations cost including con gurations method tree posterior distribution sequentially ie observation time results ensemble qmr dt type networks show method performs better than local probability propagation variational upper bound ranking most probable diseases acknowledgements thank discussions about work references attias independent factor analysis neural computation algorithm construct minimum directed spanning tree directed network operations research gordon new york freeman weiss xed points max product algorithm appear ieee transactions information theory special issue codes graphs iterative algorithms frey graphical models machine learning digital communication mit press cambridge ma frey filling scenes propagating probabilities through layers into appearance models proceedings ieee conference computer vision pattern recognition ieee computer society press ca frey hinton variational learning non linear gaussian belief networks neural computation gallager low density parity check codes mit press cambridge ma ghahramani jordan factorial hidden markov models machine learning tractable inference algorithm multiple diseases proceedings conference uncertainty intelligence jaakkola jordan variational probabilistic inference qmr dt network journal intelligence research jordan ghahramani jaakkola saul
new form covariance modelling gaussian mixture models hidden markov models presented extension efficient form covariance modelling used speech recognition co variance matrices standard form covariance matrices covariance matrix decomposed into highly shared transform diagonal covariance matrix use factored transform presented paper effectively increases number possible transforms without number free parameters maximum likelihood estimation schemes model parameters presented including assignment transform component parameters new model form evaluated large vocabulary speech recognition task shown using factored form covariance modelling reduces word error rate
new incremental learning algorithm described approximates maximal margin hyperplane wrt norm set linearly separable data our algorithm called alma approximate large algorithm wrt norm takes corrections rate data margin larger than margin data bound alma avoids quadratic higherorder programming meth ods easy implement fast online algorithms such perceptron report experiments comparing alma two incremental algorithms perceptron li romma our algorithm seems perform quite better than both accuracy levels achieved alma slightly inferior those obtained support vector machines svms other hand alma quite faster easier implement than standard svms training algorithms
variational approximations widespread tool bayesian learning graphical models provide theoretical results variational updates general family conjugate exponential graphical models show belief propagation junction tree algorithms used inference step variational bayesian learning applying these results bayesian analysis linear gaussian state space models obtain learning procedure exploits kalman smoothing propagation while integrating over model parameters demonstrate used infer hidden state dimensionality state space model variety synthetic problems real high dimensional data set
many algorithms approximate reinforcement learning known converge fact showing adjustable weights algorithms may oscillate within region rather than converging point paper shows two popular algorithms such oscillation worst happen weights cannot but instead converge bounded region algorithms sarsa latter algorithm used well known td program
present algorithm samples hypothesis space kernel classiers given uniform prior over weight vectors likelihood based model label noise leads piecewise constant posterior sampled kernel gibbs markov chain monte carlo method chooses random direction parameter space samples resulting piecewise constant density along line chosen used analytical tool exploration bayesian transduction bayes point machines active learning evidence based model selection small data sets contaminated label noise simple toy example demonstrate experimentally bayes point machine based outperforms svm taking into account label noise
present improvement perceptron convergence theorem mistake bound margin dependent sparsity guarantee allows us give generalisation error bound learned perceptron learning algorithm bound value crucially depends margin support vector machine would achieve same data set using same kernel bound yields better guarantees than currently available support vector solution itself
present efficient algorithms problems problems ubiquitous statistical learning focus six examples including classification kernel density estimation outlier detection correlation these include problem requires comparison each points dataset each other point would solved using distance computations practice often large enough make present new geometric techniques applicable principle computation including largescale mixtures gaussians rbf neural networks hmms our algorithms exhibit favorable asymptotic scaling empirically several orders magnitude faster than naive computation even small datasets aware exact algorithms these problems more either empirically theoretically addition our framework yields simple elegant algorithms permits two important generalizations beyond standard problems more difficult these represented our final examples multiple correlation correlation
examine eight di erent techniques developing visual representations machine vision tasks particular compare di erent versions principal component independent component analysis combination regression methods variable selection found local methods based statistics image patches consistently outperformed global methods based statistics entire images result consistent previous work facial expression recognition addition use regression technique selecting variables regions interest substantially boosted performance
computational principles neural feedback circuits important problem theoretical neuroscience study symmetric threshold linear networks derive stability results go beyond insights gained lyapunov theory energy functions applying linear analysis subnetworks composed neurons determine stability potential steady states nd stability depends two types type determines global stability other type determines whether multistability possible prove equivalence our stability criteria criteria taken quadratic programming show permitted sets neurons steady state forbidden sets cannot permitted sets clustered sense subsets permitted sets permitted forbidden sets forbidden viewing permitted sets memories stored synaptic connections provide formulation longterm memory more general than traditional perspective xed point attractor networks lyapunov function used prove given set di equations convergent example neural network lyapunov function almost initial condition outputs neurons converge stable steady state past stability property used construct attractor networks recall patterns lyapunov theory applies mainly symmetric networks neurons have monotonic activation functions here show restriction activation functions threshold linear ones limitation but yield new insights into computational behavior recurrent networks completeness see present three main theorems about neural responses constant inputs rst theorem provides necessary sucient conditions synaptic weight matrix existence globally asymptotically stable set xed points these conditions expressed terms concept quadratic programming linear theory alternatively they expressed terms certain eigenvalues eigenvectors synaptic weight matrix making connection linear systems theory theorem guarantees network produce steady state response constant input regard response computational output network its characterization topic second third theorems second theorem introduce idea permitted forbidden sets under certain conditions synaptic weight matrix show exist sets neurons forbidden recurrent synaptic connections being stable steady state matter what input applied other sets permitted sense they input same conditions synaptic weight matrix lead conditional multistability meaning exists input more than stable steady state other words forbidden sets conditional multistability concepts existence permitted forbidden sets suggests new way about memory neural networks input applied network select set active neurons selection constrained permitted sets therefore permitted sets regarded memories stored synaptic connections our third theorem states constraints groups permitted forbidden sets stored network matter learning algorithm used store memories active neurons cannot arbitrarily divided into permitted forbidden sets because subsets permitted sets have permitted forbidden sets have forbidden basic de our theory applicable network dynamics dx dt ij cation nonlinearity synaptic weight matrix symmetric ij ji dynamics written more compact matrix vector form state network input network arbitrary vector output network steady state response existence outputs their relationship input determined synaptic weight matrix vector said nonnegative its components nonnegative nonnegative orthant set nonnegative vectors shown trajectory starting nonnegative orthant remains nonnegative orthant therefore simplicity consider initial conditions con ned nonnegative orthant global asymptotic stability de nition steady state stable initial conditions suciently close state trajectory remains close later times steady state asymptotically stable initial conditions suciently close state trajectory converges set steady states globally asymptotically stable almost initial conditions state trajectories converge steady states measure zero de nition principal submatrix square matrix square matrix constructed certain set rows corresponding columns following theorem establishes necessary sucient conditions global asymptotic stability theorem symmetric following conditions equivalent nonnegative eigenvectors principal have positive eigenvalues matrix copositive nonnegative except network has set steady states globally asymptotically stable proof sketch let minimum over nonnegative unit sphere false minimum value less than equal zero follows lagrange multiplier methods nonzero elements nonnegative eigenvector corresponding principal submatrix eigenvalue greater than equal unity function lower bounded unbounded under network dynamics nonnegative orthant constant steady states lyapunov stability theorem stable steady states globally asymptotically stable language optimization theory network dynamics converges local minimum subject constraint suppose false exists nonnegative eigenvector principal submatrix eigenvalue greater than equal unity used construct unbounded trajectory dynamics meaning these stability conditions best comparing analogous conditions purely linear network obtained cation linear network eigenvalues would have smaller than unity ensure asymptotic stability here nonnegative eigenvectors able grow without bound due cation so their eigenvalues less than unity principal considered because di erent sets feedback connections active depending set neurons above threshold linear network would have positive de nite ensure asymptotic stability but because cation here condition replaced weaker condition conditions theorem global asymptotic stability depend but other hand steady states do depend next lemma says mapping input output lemma nonnegative vector exists input such steady state equation input proof de choose lemma states nonnegative vector realized xed point sometimes xed point stable such networks subject theorem single neuron active indeed principal submatrix corresponding single active neuron corresponds diagonal elements according positive hence always possible activate single neuron asymptotically stable xed point become clear following theorem nonnegative vectors realized asymptotically stable xed point forbidden permitted sets following stable steady states based theorem theorem says principal submatrix symmetric matrix eigenvalues fall between eigenvalues particular largest eigenvalue always smaller than largest eigenvalue de nition set neurons permitted neurons asymptotically stable steady state input other hand set neurons forbidden they cannot asymptotically stable steady state matter what input alternatively might have de ned permitted set set corresponding square sub matrix has positive eigenvalues similarly forbidden set could de ned set least non positive eigenvalue follows theorem matrix copositive eigenvectors corresponding non positive eigenvalues forbidden sets have have both positive non positive components theorem matrix copositive following statements equivalent matrix positive de nite exists forbidden set network conditionally exists input such more than stable steady state proof sketch positive de nite so asymptotically stable steady state neurons active eg set neurons forbidden denote forbidden set active neurons without loss generality assume principal submatrix corresponding has positive eigenvalues non positive eigenvalue virtue theorem fact diagonal elements positive always subset true choosing neurons belonging neurons belonging quadratic lyapunov function de ned theorem forms saddle nonnegative de ned saddle point point restricted hyperplane de ned positive eigenvalues reaches its minimum but because neurons initialized lower values either side hyperplane because non increasing along trajectories way trajectories cross hyperplane conclusion have constructed input network suppose false lyapunov function convex so has single local minimum convex domain local minimum global minimum dynamics converge minimum positive de nite symmetric threshold linear network has unique steady state has been shown previously next theorem expansion result equivalent condition using concept permitted sets theorem symmetric following conditions equivalent matrix positive de nite sets permitted unique steady state stable proof positive de nite copositive hence theorem false so theorem false eg set permitted suppose false so set neurons active forbidden sets permitted see following theorem characterizes forbidden permitted sets theorem subset permitted set permitted forbidden set forbidden proof according theorem smallest eigenvalue symmetric matrix positive so smallest eigenvalues its principal smallest eigenvalue principal submatrix negative so smallest eigenvalue original matrix example ring network symmetric threshold linear network local excitation larger range inhibition has been studied past model simple cells primary visual cortex obtain their orientation tuning visual stimulation inspired these results have recently built electronic circuit containing ring network using analog vlsi technology have argued xed tuning width neurons network arises because active sets consisting more than xed number contiguous neurons forbidden here give more detailed account fact provide surprising result about existence spurious permitted sets let synaptic matrix neuron ring network invariant connection between neurons given ij ij ij ij ij ij es global inhibition self excitation rst neighbor lateral excitation second neighbor lateral excitation figure have numerically computed permitted sets network parameters taken eg permitted sets determined square sub matrices classifying eigenvalues corresponding nonnegative eigenvectors figure shows resulting parent permitted sets those have permitted consistent nding such ring networks explain contrast invariant tuning cells multiplicative response modulation parietal cells found permitted sets consist more than contiguous active neurons seen many non contiguous permitted sets could principle activated neurons white strongly neurons because activation spurious permitted sets requires highly speci input inhibition high spatial frequency argued presence spurious permitted sets relevant normal operation ring network inputs typically tuned excitatory such inputs lgn primary visual cortex permitted set number neuron number neuron number figure left output ring network neurons uniform input random initial condition right parent permitted sets axis neuron number axis set number white means neurons belongs set means does left right translation symmetric parent permitted sets ones shown have been rst parent permitted set rst row bottom corresponds output left discussion have shown pattern threshold linear networks viewed terms permitted sets neurons eg sets neurons steady state according de nition memories stored synaptic weights independently inputs hence concept memory does er input dependence would case de nition memory based xed points dynamics pattern retrieval strongly constrained input typical input allow retrieval arbitrary stored permitted sets comes fact multistability dependent existence forbidden sets but input theorem example ring network positive input always retrieve permitted sets consisting group contiguous neurons but spurious permitted sets figure generally multistability ring network possible more than single neuron threshold linear networks behave traditional attractor networks inputs represented initial conditions dynamics example copositive network input permitted sets determine stable xed points thus case notion permitted sets di erent xed point attractors hierarchical grouping permitted sets theorem becomes irrelevant attractive xed point per hierarchical group de ned parent permitted set fact permitted set have forbidden subset represents constraint possible computations symmetric networks constraint does have viewed limitation contrary being aware constraint may lead deeper understanding learning algorithms representations constraint satisfaction problems history perceptrons insight they solve linearly separable classi cation problems led multilayer perceptrons backpropagation similar way grouping problems do obey natural hierarchy inherent symmetric networks might
system has been developed extract information engine vibration data support vector machines applied detection provide measure shape tion signature learning representation describe novel method support vector machines including information second class novelty detection give results cation engine vibration analysis
concept averaging over classiers fundamental bayesian analysis learning based viewpoint has recently been demonstrated linear classiers centre mass version space set classiers consistent training set known bayes point exhibits excellent generalisation abilities algorithm presented restricted small sample size because requires memory computational steps number training patterns number random posterior distribution paper present method based simple perceptron learning algorithm allows overcome algorithmic drawback method simple easily extended multi class case present experimental results mnist data set handwritten digits show bayes point machines competitive current world support vector machine addition computational complexity tuned varying number samples posterior finally rejecting test points basis their posterior probability leads rapid decrease generalisation error eg generalisation error given rejection rate
present bound generalisation error linear classiers terms margin quantity training set result obtained framework based geometrical arguments space linear classiers new bound exponential improvement so far margin bound taylor et al scales inverse margin even case less training examples than input dimensions suciently large margins lead non trivial bound values maximum margins vanishing complexity term furthermore classical margin coarse measure essential quantity controls generalisation error volume ratio between whole hypothesis space subset consistent hypotheses practical relevance result lies fact well known support vector machine optimal wrt new bound feature vectors same length consequence use svms feature vectors well supported our numerical experiments two benchmark data sets
key challenge reinforcement learning scaling up large partially observable domains paper show hierarchy behaviors used create select among variable length short term memories appropriate task higher levels hierarchy agent over lower level details back over variable number high level decisions time formalize idea framework called hierarchical memory uses memory based learning method rapidly propagate delayed reward across long decision sequences describe detailed experimental study comparing memory vs hierarchy using framework realistic navigation task
goal many unsupervised learning procedures bring two probability distributions into alignment generative models such gaussian mixtures boltzmann machines cast light models such ica projection pursuit propose novel sample based error measure these classes models applies even situations maximum likelihood ml probability density estimation based formulations cannot applied eg models nonlinear have intractable posteriors furthermore our sample based error measure avoids approximating density function prove unconstrained model our approach converges correct solution number samples goes expected solution our approach generative framework ml solution finally evaluate our approach via simulations linear nonlinear models mixture gaussians ica problems experiments show broad applicability generality our approach
propose general bayesian framework performing independent component analysis ica relies ensemble learning ear response theory known statistical physics apply both discrete continuous sources continuous source overcomplete case studied naive meanfield approach fails case whereas linear response theory gives improved estimate covariances efficient examples given sources without temporal correlations derivation extended treat temporal correlations finally framework offers simple way generating new ica algorithms without define prior distribution sources explicitly
inequality powerful mathematical tool statistical learning its applications include em algorithm bayesian estimation bayesian inference com simple lower bounds otherwise intractable quantities such products sums latent per operations like integration maximization quite often ie discriminative learning upper bounds needed well derive prove efficient analytic inequality provides such variational upper bounds inequality holds latent variable mixtures exponential family distributions thus wide range models discuss applications upper bounds including maximum conditional likelihood large margin discriminative models conditional bayesian inference convergence efficiency prediction results shown
learning complex task significantly defining hierarchy agent learn choose between various temporally abstract actions each solving assigned overall task paper study hierarchical learning using framework options argue take full advantage structure should perform state abstraction scale larger tasks state abstraction should adapt algorithm automatically build representations state feature space resulting algorithm using simple hierarchical task results suggest automated state abstraction attractive approach making hierarchical learning systems more effective
substantial data support temporal difference td model dopamine da neuron activity cells provide global error signal reinforcement learning certain da activity seems under td model responding stimuli address these lies suggesting da cells information about re including exploration et interpret additional role da terms attentional effects dopamine having computational role guiding exploration
paper derive second order mean field theory directed graphical probability models using information theoretic ment shown done partition function method direct generalisation wellknown tap approximation boltzmann machines numerical example shown method greatly improves first order mean field ap restricted class graphical models single overlap graphs second order method has comparable complexity first order method sigmoid belief networks method shown particularly fast effective
paper develop method bounding generalization error classifier terms its margin distribution introduced recent papers bartlett schapire bartlett lee theory gaussian empirical processes allow us prove margin type inequalities most general functional classes complexity class being measured via so called gaussian complexity func tions simple application our results obtain bounds schapire bartlett lee generalization error boost ing substantially improve results bartlett bounding generalization error neural networks terms weights neurons furthermore under additional assumptions complexity class hypotheses provide bounds case boosting improve results schapire bartlett lee
result expectation generalisation error hyperplane bounded expectation ratio number support vectors number training examples extended broad class kernel machines class includes support vector ma soft margin classification regression regularization networks variety kernels cost functions show key inequalities result become once classification error replaced margin error latter defined positive cost particular show expectations true margin error empirical margin error equal sparse solutions kernel machines possible cost function partially insensitive
form particle filtering has been successfully used infer shapes highly constrained active con video sequences contours highly flexible eg tracking hand computationally particles needed successfully approximate contour show algorithm used update particle set representing distribution over contours each frame video sequence compare method using video sequence requires highly flexible contours show new algorithm performs dramatically better algorithm discuss incorporation method into active contour framework used constrain shape variation
nonnegative matrix factorization has previously been shown useful decomposition multivariate data two different multi algorithms analyzed they differ slightly multiplicative factor used update rules algorithm shown minimize conventional least squares error while other minimizes generalized divergence monotonic convergence both algorithms proven using auxiliary func tion analogous used proving convergence expectation maximization algorithm algorithms interpreted rescaled gradient descent factor optimally chosen ensure convergence
introduce total wire length salient complexity measure circuit complexity sensory processing biological neural systems neuromorphic engineering new complexity measure applied set basic computational problems apparently need solved circuits translation sensory process ing exhibit new circuit design strategies these new benchmark functions implemented within realistic complexity bounds particular linear almost linear total wire length
present method bound partition function machine neural network odd order polynomial direct extension mean field bound first order show third order bound strictly better than mean field additionally show outline bound applicable sigmoid belief networks numerical experiments error reduction factor two easily reached region expansion based approximations useful
stimulus arrays presented different positions retina visual tasks even those require applies many perceptual learning tasks show per inference discrimination face variance has structurally different quality inference about fixed position stimuli involving particular quadratic nonlinearity rather than purely ear discrimination show advantage taking nonlinearity into account has discrimination suggest role recurrent con area demonstrating superior discrimination recurrent network propose learning feedforward recurrent neural connections these tasks corresponds fast slow components learning observed perceptual learning tasks
introduce novel kernel comparing two text documents kernel inner product feature space consisting subsequences length ordered characters occurring text though necessarily subsequences weighted exponentially factor their full length text hence those close contiguous direct feature vector would involve amount computation even values dimension feature space grows exponentially paper describes despite fact inner product efficiently evaluated dynamic programming technique preliminary experimental comparison performance kernel compared word feature space kernel made showing encouraging results
paper presents novel technique constrained independent component analysis introduce constraints into classical ica solve constrained optimization problem using lagrange multiplier methods paper shows used order resulted independent components speci manner normalize demixing matrix signal separation procedure systematically eliminate experiments demonstrate use ordering independent components while providing normalized demixing processes keywords independent component analysis constrained independent component analysis constrained optimization lagrange multiplier methods
based statistical mechanics approach develop method approximately computing average case learning curves gaussian process regression models approximation works well large sample size limit arbitrary dimensionality input space explain approximation systematically improved argue similar techniques applied general likelihood models
active set strategy applied dual simple tion standard quadratic program linear support vector machine application generates fast new dual algorithm consists solving finite number linear equations typically large dimensionality equal number points classified making novel use formula much smaller matrix order input space each step thus problem dimensional input space million points required positive definite symmetric matrices size total run time minutes mhz ii algorithm requires specialized quadratic linear programming code but merely linear equation publicly available
problem constructing weak classi ers boosting algorithms studied present algorithm produces linear classi er guaranteed achieve error better than random distribution data while weak learner useful learning general show under reasonable conditions distribution yields weak learner dimensional problems preliminary simulations suggest similar behavior expected higher dimensions result recent theoretical bounds additionally provide improved convergence rate bounds generalization error situations empirical error made small exactly situation occurs weak learners guaranteed performance better than random established
present new view image segmentation pairwise similarities interpret similarities edge markov random walk study eigenvalues eigenvectors walks transition matrix interpretation shows spectral methods clustering segmentation have probabilistic foundation particular prove normalized cut method arises naturally our framework finally framework provides principled method learning similarity function combination features
paper propose new particle based sequential importance sampling algorithm uses obtain importance proposal distribution proposal has two nice properties makes ecient use available information secondly have heavy result nd algorithm outperforms standard particle other nonlinear methods substantially experimental nding agreement theoretical convergence proof algorithm algorithm includes resampling possibly markov chain monte carlo mcmc steps
investigate new kernel based classifier kernel fisher kfd mathematical programming formulation based ob kfd maximizes average margin permits interesting modification original kfd algorithm yielding sparse kfd find both kfd proposed sparse kfd understood unifying probabilistic context furthermore show connections support vector machines relevance vector machines understanding able outline interesting kernel regression technique based upon kfd algorithm simulations support use our approach
central issue principal component analysis pca choosing number principal components interpreting pca density estimation show use bayesian model selection es true dimensionality data resulting estimate compute yet guaranteed pick correct dimensionality given enough data estimate involves integral over manifold difficult compute exactly but after choosing appropriate parameterization applying method rate practical estimator obtained simulations better than crossvalidation other proposed algorithms plus runs much faster
paper describes method steps re steps based projection pro onto subspaces neural networks nonlinear least squares problems particular linear conjugate gradient method works inner iterative algorithm solving normal equation whereas outer ear algorithm repeatedly takes steps re multiplication without explicitly form ing matrix model hessian our iterative algorithm reduce both counts memory space factor number pa comparison direct memoryless property useful largescale problems
nonlinear support vector machines svms investigated visual classification low resolution faces pixels processed images face database performance svms shown superior traditional pattern classifiers linear quadratic fisher linear dis well more modern techniques such radial basis function rbf classifiers large ensemble rbf networks furthermore svm performance error currently best result reported open literature
paper proposes new reinforcement learning rl paradigm explicitly takes into account input disturbance well errors use environmental models rl quite pop both offline learning simulations online ac tion planning difference between model real environment lead often results based theory control consider differential game agent tries make worst possible disturbance while control agent actor tries make best control input problem formulated finding min max solution value function takes into account norm output deviation norm disturbance derive online learning algorithms estimating value function calculating worst disturbance best control refer value function tested paradigm call robust reinforcement learning task pendulum linear domain policy value func tion learned online algorithms those derived analytically linear theory fully nonlinear up task control achieved robust performance against changes pendulum weight while standard rl control could deal such environmental changes
paper explores framework recognition image sequences using partially observable stochastic di equation models monte carlo importance sampling techniques used ecient estimation sequence likelihoods sequence likelihood gradients once network dynamics learned apply models sequence recognition tasks manner similar way hidden markov models hmms commonly applied potential advantage over hmms use continuous state dynamics present encouraging results video sequence recognition task models provided excellent performance compared hidden markov models
propose novel probabilistic framework semantic video define probabilistic objects map features semantic labels graphical network such captures scene con text discovering well dependency relations between concepts main contribution novel application factor graph framework model network model relations between semantic concepts terms their well temporal dependencies between these concepts within video using algorithm approximate exact inference these factor graph attempt correct errors made during isolated concept tion forcing constraints results significant improvement overall detection performance
experimental data have shown synapses heterogeneous different synapses respond different sequences amplitudes postsynaptic responses same spike train neither role synaptic dynamics itself nor role heterogeneity synaptic dynamics com neural circuits well understood present article methods make feasible compute given synapse known synaptic parameters spike train optimally fitted synapse example sense produces largest sum responses our find most these optimally fitted spike trains match common firing patterns specific types neurons discussed literature
experimental data show biological synapses behave quite differently symbolic synapses common artificial neural network models biological synapses dynamic ie their weight changes short time scale several hundred percent dependence past input synapse article explore consequences these synaptic dynamics computational power feedforward neural networks show gradient descent approximate given quadratic filter rather small neural system dynamic synapses compare our network model artificial neural net works designed time series processing our numerical results theoretical analysis show even single hidden layer such networks approximate surprisingly large large class nonlinear filters filters characterized volterra series result robust regard various changes model synaptic dynamics
work introduce parts model alternative hidden markov models hmms tested both models database online cursive show im hmms model letters assumed have same average width give comparable results contrast hmms model handle duration modeling without increase computational complexity
show wavelet basis may adapted best represent natural images terms sparse coefficients wavelet basis may either complete overcomplete specified small number spatial functions repeated across space combined recursive fashion so selfsimilar across scale these functions adapted minimize estimated code length under model assumes images composed linear superposition sparse independent components adapted natural images wavelet bases take different orientations they tile orientation domain contrast standard wavelet bases used image compression basis set allowed overcomplete yields higher coding efficiency than standard wavelet bases
many approaches reinforcement learning combine neural net works other parametric function approximators form learning estimate value function markov decision process significant disadvantage those pro resulting learning algorithms frequently un stable work present new approach reinforcement learning overcomes difficulty provably converges unique solution contrast existing algorithms our method shown consistent sense its costs converge optimal costs asymptotically our focus learning framework practical ap optimal portfolio choice problem
present methods learning tracking human motion video estimate statistical model typical activities large set periodic human motion data segmenting these data automatically into cycles mean components cycles computed using new algorithm accounts missing information smooth between cycles learned temporal model provides prior probability distribution over human motions used bayesian framework tracking human subjects complex monocular video sequences recovering their motion
present evidence several higher order statistical properties natural images signals explained stochastic model simply varies scale otherwise stationary gaussian process discuss two interesting consequences rst variety natural signals related through common model invariant random processes have attractive property joint densities constructed dimensional marginal second cases non assumption second order methods explicitly exploited nd linear basis equivalent independent components obtained higher order methods demonstrated temporal components speech
representations key solving problems high level vision such face compression recognition factorial coding strategies reducing redundancy present natural images basis their statistics have been successful account ing both psychophysical neurophysiological properties early vision representations presumably formed later stages cortical processing here show retinotopic factorial codes derived ensembles natural objects such human faces redundancy but dimensionality re show objects built parts nongaussian fashion allows these codes have substantially lower than respective sampling rates
communication present new algorithm solving support vector classiers large training data sets new algorithm based iterative re weighted least squares procedure used optimize moreover novel sample selection strategy working set presented randomly chooses working set among training samples do stopping criteria validity both proposals optimization procedure sample selection strategy shown means computer experiments using well known data sets
study problem combining outcomes several different classifiers way provides coherent inference satisfies constraints particular develop two general approaches im identifying phrase structure first approach extends standard hmms allow use rich ob structure general classifiers model dependencies second extension constraint satisfaction develop efficient combination algorithms under both study them experimentally context parsing
bayesian paradigm apparently sometimes gives rise occams razor other times large models perform well give simple examples both kinds behaviour two views measuring complexity functions rather than used implement them analyze complexity functions linear parameter models equivalent gaussian processes always find occams razor work
problem reinforcement learning large factored markov decision processes explored pair approximated free energy product experts network network parameters learned online using modified sarsa algorithm minimizes consecutive pairs ac tions chosen based current value estimates fixing current state sampling actions network using gibbs sampling algorithm tested cooperative multiagent task product experts model found perform comparably small instances task perform well problem becomes large representation
consider problem designing linear transformation ir rank features classifier ir onto ir such achieve minimum bayes error misclassification two explored first maximize average divergence between class densities second minimize union bound range while both approaches yield similar performance practice they out perform standard lda features show relative improvement word error rate over features large vocabulary speech recognition task
apply oscillatory networks class learning rules synaptic weights change proportional preand post synaptic activity kernel measuring postsynaptic spike time after presynaptic resulting synaptic matrices have outer product form oscillating patterns represented complex vectors simple model even part enhances resonant response learned stimulus reducing while odd part determines frequency oscillation relate our model olfactory cortex hippocampus their roles forming associative memories input representations
problem neural coding understand sequences action potentials spikes related sensory stimuli motor outputs ultimately clear question whether same coding rules used di erent neurons corresponding neurons di erent individuals present quantitative formulation problem using ideas information theory apply approach analysis experiments visual system nd signi cant individual di structure code particularly way temporal patterns spikes used convey information beyond available variations spike rate other hand our ensemble exhibit high coding so every spike carries same amount information individuals thus neural code has able mixture
method described like kernel trick support vector ma svms lets us generalize algorithms operate feature spaces usually related input space done identifying class kernels represented distances hilbert spaces turns out common kernel algorithms such svms kernel pca actually really distance based algorithms run class kernels well providing useful new insight into these algorithms work present work form basis new algorithms
many problems would natural reinforcement learning reward signal single scalar value but has multiple scalar com examples such problems include agents multiple goals agents multiple users creating single reward value com multiple components away information lead incorrect solutions describe multiple reward source problem discuss problems applying traditional reinforce ment learning present new algorithm finding solution results simulated environments
principle maximizing mutual information applied learning overcomplete recurrent representations underlying model con network input units driving larger number output units recurrent interactions limit zero noise network de mutual information related entropy output units maximizing entropy respect both feed forward connections well recurrent interactions results simple learning rules both sets parameters conventional independent components ica learning algorithm recovered special case equal number output units recurrent con application these new learning rules illustrated simple twodimensional input example
present simple sparse greedy technique approximate maximum posteriori estimate gaussian processes much improved scaling behaviour sample size particular computational requirements storage cost prediction cost compute con bounds show compute stopping criterion give bounds approximation error show applications large scale problems
paper give necessary sucient conditions under kernels dot product type kx kx satisfy condition thus may used support vector machines svm regularization networks rn gaussian processes gp particular show kernel analytic ie expanded taylor series expansion have nonnegative give explicit functional form feature map calculating its eigenfunctions eigenvalues
propose method approximate dynamic programming markov decision processes mdps using algebraic decision diagrams adds produce value functions policies much lower time space requirements than exact dynamic programming our method reduces sizes intermediate value functions generated during value iteration replacing values terminals add ranges values our method demonstrated class large mdps up billion states compare results optimal value functions
control walking robot present novel neuromorphic vlsi chip coordinates relative robots similar spinal central pattern generators control vertebrate locomotion chip controls leg move driving time varying voltages out small network coupled oscillators characteristics chips output voltages depend set input parameters between input parameters output voltages computed analytically idealized system practice ideal re approximately true due transistor mismatch off sets fine tuning chips input parameters done automatically robotic system using unsupervised support vector sv learning algorithm introduced recently learning requires description desired output given machine learns un labeled examples set parameters chip order obtain desired motor behavior
modern classification applications few available labeled examples unlabeled examples improve classi performance present new tractable algorithm exploit ing unlabeled examples discriminative classification achieved essentially input vectors into longer feature vectors via both labeled unlabeled examples resulting classification method interpreted discriminative kernel density estimate read trained via em algorithm case both discriminative achieves optimal solution provide addition purely dis formulation estimation problem appealing maximum entropy framework demonstrate proposed ap requires few labeled examples high classification
analyze bit error probability direct sequence binary channel ad gaussian noise problem cast into decoding problem replica analysis ap evaluate performance resulting mpm marginal mode include optimal map special cases approximate tion proposed using hopfield model naive meanfield approximation mpm its performance evaluated replica analysis results per evaluation shows effectiveness optimal meanfield compared conventional cases small information bit rate low noise level
novel noise suppression scheme speech signals proposed based motivated estimation local signal noise ratio snr di erent frequency channels snr estimation input signal transformed into so called amplitude modulation spectrograms ams represent both spectral temporal characteristics respective analysis frame imitate representation modulation frequencies higher stages mammalian auditory system neural network used analyse ams patterns generated noisy speech estimates local snr noise suppression achieved frequency channels according their snr noise suppression algorithm evaluated digit recognition experiments compared noise suppression spectral
describe unsupervised learning algorithm builds nonlinear generative model pairs face images same individual individuals recognized finding highest relative probability pair among pairs consist test image image whose identity known our method compares favorably other methods literature generative model consists single layer nonlinear feature detectors has property given data vector true posterior probability distribution over feature detector activities inferred rapidly without iteration approximation weights feature detectors learned com correlations pixel intensities feature activations two phases network observing real data observing real data generated feature activations
use graphical models explore question people learn causal relationships data two leading psychological both seen estimating parameters fixed graph argue complete account causal induction should consider people learn underlying causal graph structure propose model inductive process bayesian inference our argument supported through discussion three data sets
kernel principal component analysis pca elegant nonlinear generalisation popular linear data analysis method kernel function implicitly de nonlinear transformation into feature space standard pca performed unfortunately technique sparse components thus obtained expressed terms kernels associated every training vector paper shows approximating covariance matrix feature space reduced number example vectors using maximum likelihood approach may obtain highly sparse form kernel pca without loss
introduce new nonparametric principled distance based clustering method method combines pairwise based ap method provide mean interpretation resulting clusters idea based distance matrix into markov process examine decay during relaxation process clusters emerge structures ing relaxation extracted using information bottleneck method these clusters capture information about initial point relaxation most effective way method cluster data geometric other bias makes assumption about underlying distribution
bayesian networks graphical representations probability distributions work learning these networks assumption presented data set consisting randomly generated instances underlying distribution many situations have option active learning have possibility guiding sampling process certain types samples paper addresses problem estimating parameters bayesian networks active learning setting provide theoretical framework problem algorithm chooses active learning queries generate based model learned so far present experimental results showing our active learning algorithm significantly reduce need training data many situations
introduce mixture gaussian processes model useful applications optimal bandwidth map input dependent derived mixture experts model used modeling general conditional probability densities discuss gaussian processes particular form gaussian process classification support vector machine model used quantifying dependencies graphical models
prior knowledge about video structure used both means improve performance content analysis extract features allow semantic classification introduce statistical models two important components structure shot duration activity demonstrate usefulness these models introducing bayesian formulation shot segmentation problem new formulations shown extend standard methods adaptive intuitive way leading improved segmentation accuracy
analyze gallager codes employing simple meanfield model geometry preserves important tions between sites method naturally probability decoding algorithm proper find thermodynamic phase transition tion theoretical explain practical code performance terms landscape
has been shown receptive fields simple cells ex assuming optimal encoding provided extra constraint sparseness added finding suggests reason dependent optimal representation sparseness work used ad hoc model noise here show biologically more plausible noise model describing neurons poisson processes used sparseness does have added constraint thus con sparseness feature evolution has but simply result evolutionary towards optimal
present embedded trees algorithm iterative technique estimation gaussian processes defined arbitrary graphs exactly solving series modified problems embedded span trees computes conditional means efficiency comparable better than other techniques unlike other meth ods embedded trees algorithm computes exact error co variances error covariance computation most efficient graphs removing small number edges reveals em tree context demonstrate sparse loopy graphs provide significant increase modeling power trees minor increase estimation complexity
algebraic geometry essential learning theory hierarchical learning machines such layered neural networks gaussian mixtures asymptotic does hold fisher formation matrices singular paper rigorous form stochastic complexity based tion singularities two problems studied prior positive stochastic complexity far smaller than resulting smaller generalization error than regular statistical models even true distribution contained parametric model prior free equal zero singularities employed stochastic complexity has same form useful model selection but generalization
introduce method feature selection support vector machines method based upon finding those features minimize bounds error search efficiently performed via gradient descent resulting algorithms shown superior standard feature selection algorithms both toy data problems face recognition detection analyzing dna microarray data
paper show kernel pca algorithm et al interpreted form metric multidimensional scaling mds kernel function kx isotropic ie depends leads metric mds algorithm desired con points found via solution rather than through iterative optimization objective function question kernel choice discussed
using statistical mechanics results calculate learning curves average generalization error gaussian processes gps bayesian neural networks nns used regression applying results learning teacher defined network directly compare gp bayesian nn learning find gp general requires training examples learn input features order input dimension whereas nn learn task order number adjustable weights training examples gp considered infinite nn results show even bayesian approach important limit complexity learning machine theoretical findings confirmed simulations analytical gp learning nn mean field algorithm
introduce processing error correcting codes image restoration extracting information former stage using selectively improve performance latter both mean analysis using method simulations show has advantage being robust against uncertainties estimation
belief propagation bp work networks but works surprisingly well many applications involving networks loops including turbo codes has been little understanding algorithm nature solutions finds general graphs show bp converge stationary point approximate free energy known bethe free energy physics result characterizes bp makes connections variational approaches approximate inference more importantly our analysis lets us build progress made statistical physics approximation introduced others have shown construct more ac free energy approximations approximation simplest exploiting insights our analysis de generalized belief propagation versions these approximations these new message passing algorithms significantly more accurate than ordinary bp adjustable complexity illustrate such new algorithm grid markov network show gives much more accurate marginal probabilities than those found using ordinary bp
theory winnow multiplicative update has certain advantages over perceptron additive update many irrelevant attributes recently has been much effort enhancing perceptron using regularization leading class linear classification methods called support vector machines similarly possible apply regularization idea winnow algorithm gives meth ods call regularized show resulting methods compare basic similar way support vector machine compares perceptron investigate algorithmic learning properties derived methods experimental results provided illustrate different methods
large margin linear classification methods have been successfully ap many applications linearly separable problem known under appropriate assumptions expected misclassification error computed optimal hyperplane approaches zero rate inverse training sample size rate usually margin maximum norm input data paper argue another quantity namely robustness put data distribution plays important role characterizing convergence behavior expected misclassification error based concept robustness show large margin separable linear classification problem expected misclassification error may converge exponentially number training sample size
describe computer system provides realtime accompaniment live piece music accompaniment bayesian network represents joint distribution times accompaniment played relating two parts through layer hidden variables network first con using rhythmic information contained musical score network trained capture musical offline phase during live accompaniment learned distribution network combined realtime analysis acoustic performed hidden markov model generate principled accompaniment available sources knowledge live demonstration provided
paper presents system automatically generating based more selected user uses gaussian process regression learn user preference function over function takes music inputs paper further introduces kernel method learning gaussian process kernel distribution functions generates learned function generation learns kernel large set learned kernel shown more effective predicting users than reasonable kernel
paper deals neural network architecture establishes portfolio management system similar approach allocation scheme across various fi markets while simultaneously specific allocation constraints meet requirements portfolio optimization algorithm modeled feedforward neural network underlying expected return based error correction neural networks utilize last model error auxiliary input evaluate their own portfolio optimization implemented such constraints ii risk controlled demonstrate our approach constructing across different financial markets turns out our approach superior benchmark portfolio
increasing number users mobile computing devices eg digital third generation mobile phones wireless communications increasingly important many applications rely device maintaining replica data structure stored example news databases mail paper explore question optimal strategy such replicas probabilistic models represent data structures evolve model user behaviour formulate objective functions respect demonstrate using two real world data sets user obtain more up date information using our approach
introduce new type selforganizing map som semantic space large text propose hyper som based regular plane space characterized constant negative gaussian curvature exponentially increasing size neighborhood around point space provides more freedom map complex information space arising language into spatial relations describe experiments showing successfully applied text categorization tasks yields results comparable other methods
present probabilistic generative model timing deviations expressive music performance structure proposed model equivalent switching state space model formulate two well known music recognition problems namely tracking automatic transcription rhythm quantization maximum posteriori map state estimation tasks inferences carried out using sequential monte carlo integration particle techniques purpose have derived novel viterbi algorithm rao particle subset hidden variables integrated out resulting model suitable realtime tracking transcription hence useful number music applications such adaptive automatic accompaniment score
investigate following data mining problem computational large data set compounds find those target few iterations biological testing possible each iteration small batch compounds binding target apply active learning techniques selecting successive selection strategy unlabeled examples closest maximum margin hyperplane another produces many weight vectors running perceptrons over multiple permutations data each weight vector its prediction pick unlabeled examples prediction most split between third tion strategy note each unlabeled example version space consistent weight vectors estimate volume both sides split through version space select un labeled examples cause most even split version space demonstrate two data sets provided three selection strategies perform comparably well much better than selecting random testing
packet switches switch inputs out policy directly affects switch best policy depends current state switch current traffic patterns problem hard because state space possible transitions set actions grow exponentially size switch present reinforcement learning formulation problem value function into many small value functions enables efficient action selection
estimating parameters sparse multinomial distributions important component many statistical learning tasks recent approaches have used uncertainty over vocabulary symbols multinomial distribution means accounting sparsity present bayesian approach allows weak prior knowledge form small set approximate candidate vocabularies used dramatically improve resulting estimates demonstrate these improvements applications text compression estimating distributions over words data
estimating data regression problem several reasons large number variables many discrete shape noise distribution asymmetric large majority few unreliable large values compare several machine learning methods estimating test them large data base car policies nd function approximation methods do optimize squared loss like support vector machines regression do work well context compared methods include decision trees generalized linear models best results obtained mixture experts better es least most allows reduce median more most
report use reinforcement learning cobot software agent wellknown online community our initial work cobot et al provided ability social statistics report them users here describe application rl allowing cobot take actions complex social environment adapt behavior multiple sources human reward after training reward events different users cobot learned nontrivial preferences number users his behavior based his current state here describe state action spaces cobot report statistical results learning experiment
algorithm used search engine greatly improves results web search taking into account link structure web assigns page score number times random would page page page following page equal probability propose improve page rank using more intelligent guided probabilistic model relevance page query efficient execution our algorithm query time made possible pre computing time thus once queries terms experiments two large subsets web indicate our algorithm significantly outperforms quality pages while remaining efficient enough used large search
present principled efficient planning algorithm cooperative dynamic systems striking feature our method coordination communication between agents imposed but derived directly system dynamics function approximation architecture view en multiagent system single large markov decision process mdp assume represented factored way using dynamic bayesian net work dbn action space resulting mdp joint action space entire set agents our approach based use factored linear value functions approximation joint value function factorization value function allows agents coordinate their actions using natural message passing scheme provide simple efficient method computing such approximate value function solving single linear pro gram whose size determined interaction between value function structure dbn thereby avoid exponential state action space show our approach compares favorably approaches based reward sharing show our algorithm efficient more complicated algorithms even single agent case
consider use two additive control methods reduce variance performance gradient estimates reinforcement learn ing problems first approach consider baseline method function current state added discounted value estimate relate performance these methods use paths variance estimates based iid data derive baseline function minimizes variance show baseline sum optimal variance weighted squared distance optimal baseline show widely used average discounted value baseline reward replaced difference between reward its expectation suboptimal second approach consider method uses approximate value function give bounds expected squared error its estimates show minimizing distance true value function suboptimal general provide example true value function gives estimate positive variance but op value function gives unbiased estimate zero variance our bounds suggest algorithms estimate gradient performance parameterized baseline value functions present preliminary illustrate performance improvements simple control problem
desirable complex decision making problem uncertain world adequately modeled markov decision process mdp whose structural representation adaptively designed parsimonious resources allocation process resources include time cost exploration amount memory computational time allowed policy value function representation concerned about making best use available resources address problem estimating adding extra resources highly needed order improve expected performance resulting policy possible application reinforcement learning rl real world exploration highly costly concerns detection those areas state space need primarily explored order improve policy another application concerns approximation continuous state space stochastic control problems using adaptive discretization techniques highly ecient grid points allocation high dimensionality surprisingly these two problems formulated under common framework given resource allocation belief state over possible mdps nd adding new resources thus decreasing uncertainty parameters transition probabilities rewards most likely increase expected performance new policy do so use sampling techniques estimating contribution each parameters probability distribution function pdf expected loss using approximate policy such optimal policy most probable mdp instead true but unknown policy
show convergence two deterministic variants rst widely used learning values large initial values follows greedy policy respect values show setting initial value suciently large guarantees converges optimal policy second new novel algorithm incremental learning gradually values actions taken show incremental learning converges limit optimal policy our incremental learning algorithm viewed greedy learning
paper presents reinforcement learning long short term memory recurrent neural network using advantage learning directed exploration solve tasks longterm dependencies relevant events demonstrated task well difficult variation pole balancing task
consider problem learning multiple goals dynamic initially unknown addition environment may contain arbitrarily varying elements related actions other agents nonstationary moves nature problem modelled stochastic markov game between learning agent arbitrary player reward function objective learning agent have its longterm average reward vector belong given target set devise algorithm achieving task based theory stochastic games algorithm com appropriate way finite set standard learning sucient conditions given convergence learning algorithm general target set these results markov decision problem discussed well
address problem non convergence online reinforcement learning algorithms eg learning sarsa incremental batch approach separates exploration process function tting process our batch fit best paths algorithm between exploration phase during trajectories generated try nd optimal policy function tting phase during function approximator best known paths start states terminal states advantage approach batch value function tting global process allows address function approximation cannot handled local online algorithms approach boyan moore their algorithms show improve upon their work applying better exploration process function tting procedure incorporate bellman error advantage error measures into objective function results show improved performance several benchmark problems
address two open theoretical questions policy gradient reinforce ment learning first concerns efficacy using function represent state action value function theory pre showing linear function approximation representations rate convergence performance gradient estimates factor relative function approximation used number possible actions number basis functions function approximation representation sec concerns use bias term estimating state action value function theory presented showing nonzero bias term improve rate convergence performance gradient estimates number possible actions evidence presented showing these theoretical results lead significant improvement convergence properties policy reinforcement learning algorithms
present three ways combining linear programming kernel trick nd value function approximations reinforcement learning formulation based svm regression second based bellman equation third seeks ensure good moves have advantage over bad moves formulations attempt minimize number support vectors while tting data experiments synthetic problem show three formulations give excellent performance but advantage formulation much easier train unlike policy gradient methods kernel methods described here easily adjust complexity function approximator complexity value function
provide natural gradient method represents steepest descent direction based underlying structure parameter space although gradient methods cannot make large changes values parameters show natural gradient moving toward choosing greedy optimal action rather than better action these greedy optimal actions those would chosen under improvement step policy iteration approximate compatible value functions sutton et al show performance improvements simple mdps more challenging mdp
propose new approach reinforcement learning combines least squares function approximation policy iteration our method completely off policy motivated least squares temporal difference learning algorithm known its efficient use sample experiences compared pure temporal difference algorithms ideal prediction problems has had straightforward application control problems moreover approximations learned strongly influenced visitation distribution over states our new algorithm least squares policy iteration addresses these issues result method use data collected source have tested several problems including simulator learns guide goal efficiently merely observing relatively small number completely random trials
present simple approach computing reasonable policies factored markov decision processes mdps optimal value function approximated compact linear form our method based solving single linear program approximates best linear optimal value function applying ecient constraint generation procedure obtain iterative solution method concise linear programs direct linear programming approach experimentally yields significant reduction computation time over approximate policy iteration methods sometimes reducing several few seconds quality solutions produced linear programming about twice approximation error same approximating class nevertheless speed advantage allows use larger approximation classes achieve similar error reasonable time
propose new classification multiagent learning algorithms each characterized both their possible strategies possible beliefs using classification review optimality ex algorithms including case play propose incremental improvement existing algorithms seems achieve average payoffs least nash equilibrium payoffs long run against opponents
standard reinforcement learning view neuromodulatory systems conditioning includes rather straightforward motivation prediction sum future reward competition between actions based characteristics their states sense substantial careful experiments reviewed into neurobiology psychology motivation shows view incomplete many cases animals faced choice between many different actions given state but rather whether single response evidence suggests motivational process underlying choice has psychological neural properties underlying action choice describe model these motivational systems consider way they interact
paper explore two quantitative approaches modelling reasoning linear model based formation contained conceptual dependency networks empirical data acquired study fit models compared con considering appropriateness nonparametric approaches reasoning examining other metric approaches future
present neural network model shows prefrontal cortex interacting basal ganglia maintain sequence phonological information activation based working memory ie phonological loop primary function phonological loop may encode arbitrary information necessary tasks combinatorial expressive power language enables binding essentially arbitrary pieces information our model takes advantage closed class nature phonemes allows di erent neural representations possible phonemes each sequential position encoded make work suggest basal ganglia provide region speci update signal phonemes appropriate sequential coding demonstrate arbitrary binding novel sequences supported mechanism show model generalize novel sequences after moderate amounts training
promise computational modeling fully realized higher level cognitive domains such language processing principled methods developed construct semantic representations used such models paper propose use established formalism mathematical psychology additive clustering means auto constructing binary representations objects using pair wise similarity data existing methods unsupervised learning additive clustering models do scale well large prob present new algorithm additive clustering based novel heuristic technique combinatorial optimization algorithm simpler than previous formulations makes fewer independence extensive empirical tests both human synthetic data suggest more effective than previous methods scales better larger problems making additive clustering practical take significant step toward scaling connectionist models beyond examples
unsupervised learning algorithms have been derived several statistical models english grammar but their computational complexity makes applying them large data sets intractable paper presents probabilistic model english grammar much simpler than conventional models but ecient em training algorithm model based upon grammatical ie syntactic relationships between pairs words present results experiments quantify representational grammatical model its ability generalize labelled data its ability induce syntactic structure large amounts raw text
temporal coding hypothesis miller colleagues suggests animals integrate related temporal patterns stimuli into single memory representations formalize concept using quasi bayes estimation update parameters constrained hidden markov model approach allows us account surprising temporal ects second order conditioning experiments miller et al other models unable explain
theory categorization presented knowledge causal relationships between category features represented bayesian network referred theory theory predicts objects classified category members extent they likely have been produced category causal model view people have models world lead them expect certain distribution features category members eg correlations between feature pairs directly connected causal relationships consider exemplars good category members they those expectations these expectations include sensitivity higherorder feature interactions emerge asymmetries inherent causal relationships research topic categorization has traditionally focused problem learning new categories given observations category members contrast view categories influence prior theoretical knowledge learners often contribute their representations categories contrast models accounting effects empirical observations have been few models developed account effects prior knowledge purpose article present model categorization referred theory cmt according cmt knowledge many categories includes features but explicit representation causal mechanisms people believe link features many categories article apply cmt problem establishing objects category membership psychological literature standard view categorization objects placed category extent they have features have often been observed members category example object has most features eg fly build trees etc few features other categories thought view categorization prototype models classification function similarity ie number shared features between mental representation category prototype object wellknown difficulty prototype models feature contribution category membership independent presence absence other features contrast consideration category theoretical knowledge likely influence combinations features make acceptable category members example people believe have trees because they fly light knowledge animal fly yet still builds trees might considered less plausible than animal builds ground fly eg even though latter animal has fewer features typical assess whether knowledge fact influences feature combinations make good category members following experiment novel categories whose four binary features exhibited either commoncause commoneffect schema figure commoncause schema category feature described three other features commoneffect schema feature described being caused three others cmt assumes people represent causal knowledge such figure kind bayesian network nodes variables representing binary category features directed edges causal relationships representing presence probabilistic causal mechanisms between features specifically cmt assumes cause feature present enables operation causal mechanism probability bring about presence effect feature cmt allow possibility effect features have potential background causes explicitly represented network represented parameter probability effect present even its network causes absent finally each cause node has parameter represents probability cause feature present commoncause schema commoneffect schema commoncause correlations commoneffect correlations figure figure central prediction cmt object considered category member extent its features likely have been generated causal mechanisms example table presents likelihoods causal models figure generate possible combinations each likelihood equation derived application simple boolean algebra operations example probability exemplar present absent being generated commoncause model probability present times probability brought about its background cause times probability brought about neither nor its background cause times probability brought about its background cause probability exemplar present absent being generated commoneffect model probability present times probability absent times probability brought about its background cause note these likelihoods assume causal mechanisms each model operate independently same probability restrictions other applications categorization offered cmt implies people theoretical knowledge leads them expect certain distribution features category members they use information assigning category membership thus gain insight into categorization performance predicted cmt examine statistical properties category features expect generated causal model example dotted lines figure represent features correlations generated causal schemas figure would expect pairs features directly linked causal relationships correlated commoncause schema correlated its effects commoneffect schema correlated its causes thus cmt predicts combinations features serve evidence category membership extent they preserve these expected correlations ie both cause effect present both absent against category membership extent they break those correlations present other absent table likelihoods equations observed predicted values common cause schema common effect schema control exemplar likelihood observed predicted likelihood observed predicted observed cb cb cb cc cb cc cm cc cb cc cb cc cb cm cm cc cm cm cm cm cb cm cm cm note causal networks predict pairwise correlations between directly connected features figure indicates result asymmetries inherent causal relationships important between commoncause commoneffect schemas although commoncause schema implies three effects correlated more weakly than directly connected features commoneffect schema does imply three causes correlated between commoncause commoneffect schemas has been focus considerable investigation psychological use these schemas following experiment enables test whether sensitive pattern correlations between features causal laws those arise due asymmetries inherent causal relationships shown figure moreover show cmt predicts humans exhibit sensitivity interactions among features higherorder than pairwise interactions shown figure method six novel categories used description causal relationships between features consisted sentence indicating cause effect feature two sentences describing mechanism responsible causal relationship example novel categories described having four binary features eg high quantity neurotransmitter flight response accelerated sleep cycle etc causal relationships among those features eg high quantity neurotransmitter causes flight response duration electrical signal muscles longer because excess amount neurotransmitter participants first studied several computer information about their assigned category their own participants first presented category four features participants commoncause condition additionally commoncause causal relationships participants commoneffect condition commoneffect relationships participants took test tested them knowledge they had studied participants required test until they errors participants performed classification task they scale category membership exemplars consisting possible objects formed four binary features example those participants assigned learn category asked classify high amounts neurotransmitter normal flight response accelerated sleep cycle normal body weight order test exemplars randomized each hundred eight university received course credit experiment they randomly assigned equal numbers three conditions six experimental categories categorization ratings test exemplars averaged over participants commoncause commoneffect control conditions presented table presence causal knowledge had large effect ratings instance exemplars given lower ratings commoncause commoneffect conditions respectively than control condition presumably because these exemplars correlations broken effect features present even though their causes absent contrast exemplar received significantly higher rating commoncause commoneffect conditions than control condition vs presumably because both conditions correlations preserved confirm causal schemas induced sensitivity interactions between features categorization ratings analyzed performing multiple regression each four predictor variables coded feature absent present additional six predictor variables formed multiplicative interaction between pairs features those feature pairs connected causal relationship interaction terms represent whether causal relationship confirmed cause effect both present both absent violated present absent finally four interactions single interaction included predictors regression weights averaged over participants presented figure function causal schema condition figure indicates interaction terms corresponding those feature pairs assigned causal relationships had significantly positive weights both commoncause condition commoneffect condition predicted figure exemplar better category member preserved expected correlations cause effect feature either both present both absent worse member those correlations absent other present feature weight regression term common cause vs control cc predict cc predict control cc predicted control observed cc observed feature weight regression term common effect vs control cc predict cc predict control cc control cc ce predicted control observed ce observed figure addition shown earlier figure because their commoncause three effect features commoncause schema correlated more weakly than features consistent prediction condition three interaction terms between effect features greater than those interactions control condition contrast commoneffect schema does imply three cause features correlated fact condition interactions between cause attributes did differ those control condition figure figure reveals higherorder interactions among features commoneffect condition weights interaction terms significantly different those control condition these higherorder interactions because commoneffect schema requires cause feature explain presence common effect figures presents ratings commoneffect condition those test exemplars common effect present function number cause features present ratings increased more
present model binding relationship information spatial domain eg square above triangle uses representations instead more popular temporal synchrony mechanisms temporal synchrony argue representations lack both efficiency ie combinatorial numbers units required ie resulting representations specific thus do support generalization novel these claims show our model uses far fewer hidden units than number conjunctions represented us ing distributed representations each unit has broad tuning curve through highdimensional conjunction space ca considerable generalization novel inputs
hand velocity profiles rapid human arm movements ten appear sequences several phases called movement units suggests nervous system might efficiently control motor plant presence noise feedback delay another critical observation stochastic motor control problem makes optimal control policy different optimal control policy deterministic case use simplified dynamic model arm address rapid aimed arm movements use reinforcement learning tool approximate optimal policy presence noise feedback delay using simplified model show multiple emerge optimal policy presence noise feedback delay optimal policy situation drive arms end point close target fast apply few slow drive arms end point into target region our simulations controller sometimes generates before initial fast much like predictive tions observed number psychophysical experiments
has been known people after being exposed sentences generated grammar acquire implicit grammatical knowledge able transfer knowledge inputs generated ed grammar show second order recurrent neural network able transfer grammatical knowledge language generated finite state machine another language di er both vocabularies representation grammatical knowledge network analyzed using linear discriminant analysis
animal data delayed reward conditioning experiments shows striking property data di erent time intervals into single curve data scaled time interval called scalar property interval timing here simple model neural presented shown give rise scalar property model consisting noisy linear spiking neurons analytically tractable contains three parameters coupled reinforcement learning peak procedure experiments producing both scalar property pattern single trial covariances
proposed human language modeled treating human bayesian modeling process bayesian de trees paper extend model make further predictions about reading time given probability difference interpretations test model against reading time data experiment
partial information trigger complete memory same time human memory perfect cue contain enough information specify item memory but fail trigger item context word memory present experiments demonstrate basic patterns human memory errors use cues consist word show short long cues more accurately than medium length ones study factors lead behavior present novel computational model shows flexibility patterns errors occur human memory model between computations these tied together using markov model words allows memory simple feature set enables process compute probability distribution possible word manner similar models visual perceptual completion
describe programmable multi chip vlsi neuronal system used exploring spike based information processing models system consists silicon retina chip whose integrate fire neurons connected soft winner take architecture circuit multi neuron chip approximates cortical microcircuit neurons configured different computational properties virtual connections selected set pixels silicon retina virtual between different chips event driven communication protocol uses asynchronous digital pulses similar spikes neuronal system used multi chip spike based system synthesize orientation tuned neurons using both feedforward model feedback model performance our analog hardware spiking model matched experimental observations digital simulations continuous valued neurons multi chip vlsi system has advantages over computer neuronal models real time computational time does scale size neuronal network
paradigm presented parallel inner product computation high dimensions suitable efficient im kernels image processing core externally digital architecture analog array performing partial multiplication full digital resolution even conversion ow ing random statistics analog summation binary products random modulation scheme produces statistics even highly correlated inputs approach validated real image data experimental results analog array prototype cmos
paper describes clustering algorithm vector using stochastic association model offers new simple powerful soft max adaptation rule adaptation process same online clustering method except adding random distortion error evaluation process simulation results demonstrate new algorithm achieve efficient adaptation high neural gas algorithm reported most efficient clustering methods key add uncorrelated random evaluation process each reference vector hardware process propose whose operation described circuit positively uses quantum mechanical processes
experimental data has shown synaptic strength modification types biological neurons depends upon precise spike ing differences between presynaptic postsynaptic spikes hebbian learning rules motivated data have been proposed argue such learning rules suitable analog vlsi implementation describe circuit modify weight silicon spiking neuron according those learning rules test results fabrication circuit using cmos process given
learning curves gaussian process regression well understood student model happens match teacher true data generation process derive approximations learning curves more generic case models find rich behaviour large input space dimensionality results become exact universal learning curve transitions between exhibit arbitrarily many overfitting maxima overfitting occur even student estimates teacher noise level correctly lower dimensions appear learning curve remains dependent mismatch between student teacher even asymptotic limit large number training examples learn ing strong smoothness assumptions example student standard radial basis function covariance function learn teacher func tion slowly predictions confirmed simulations
study online learning boolean domains using kernels feature expansions equivalent using conjunctions over basic demonstrate tradeoff between computational efficiency these kernels computed generalization resulting classifier first describe several kernel functions capture either limited forms conjunctions conjunctions show these kernels used efficiently run algorithm over exponential number conjunctions prove using such kernels perceptron algorithm make exponential number mistakes even learning simple func tions consider analogous use kernel functions run winnow algorithm over expanded feature space exponentially many conjunctions while known upper bounds imply winnow learn formulae polynomial mistake bound setting prove computationally hard simulate behavior learning over such feature set thus such kernel functions winnow efficiently computable
singularities ubiquitous parameter space hierarchical models such multilayer perceptrons singularities fisher information matrix paradigm does more hold classical model selection such mdl cannot applied important study relation between generalization error training error singularities present paper demonstrates method analyzing these errors both maximum likelihood bayesian predictive distribution terms gaussian random fields using simple models
investigate generalization performance learning prob hilbert functional spaces introduce notion convergence estimated functional predictor best underlying predictor obtain estimate rate convergence estimate allows us derive generalization bounds learning formulations
partition function boltzmann machine bounded above below use bound means correlations networks small weights values these statistics restricted nontrivial regions ie subset gamma experimental results show reasonable bounding occurs weight sizes mean field expansions generally give good results
report result perturbation analysis decoding error belief propagation decoder gallager codes analysis based geometry shows principal term decoding error equilibrium comes curvature spanned estimated full marginal partial posteriors each takes single check into account number gallager code shown principal error term matrix code so sparse two columns overlap greater than
paper show online algorithms classification re naturally used obtain hypotheses good data dependent tail bounds their risk our results proven without re complicated arguments they hold arbitrary online learning algorithms furthermore applied concrete online algorithms our results yield tail bounds many cases comparable better than best known bounds
propose method fast estimation hyperparameters large networks based linear response relation method empirical measurement function simulation results show ecient precise compared cross validation other techniques require matrix inversion
derive equivalence between adaboost dual convex optimization problem showing difference between exponential loss used adaboost maximum likelihood exponential models latter requires model normal form conditional probability distribution over labels tion establishing simple easily understood connection between two methods framework enables us derive new regularization procedures boosting directly correspond penalized maximum likelihood experiments uci datasets support our theoretical give additional insight into relationship between boosting logistic regression
recurrent neural networks analog units computers real valued functions study time complexity real tion general recurrent neural networks these have sigmoidal linear product units order nodes re weights networks operating discrete time exhibit family functions arbitrarily high complexity derive almost tight bounds time required compute these functions thus evidence given computational analog recurrent neural networks subject
consider noisy euclidean traveling salesman problems plane random combinatorial problems underlying structure gibbs sampling used compute average trajectories estimate underlying structure common instances procedure requires identifying exact relationship between permutations learning setting average trajectory used model construct solutions new instances sampled same source experimental results show average trajectory fact estimate underlying structure over tting ects occur trajectory adapts closely single instance
consider problem measuring eigenvalues randomly drawn sample points show these values reliably estimated sum tail eigenvalues furthermore residuals data projected into subspace shown reliably estimated random sample experiments presented con rm theoretical results
give results about learnability required complexity logical formulae solve classification problems these results obtained linking logic kernel machines particular show decision trees normal forms help special kernel linking regularized risk tion margin subsequently derive number lower bounds required complexity logic formulae using properties algorithms generation linear estimators such perceptron maximal learning
bootstrapping introduced its co training variant mitchell have met considerable em success earlier work theory has been related empirically useful algorithms here give new bound generalization error both use partial rules partial labeling unlabeled data use objective function singer our bounds apply multiclass case ie instances assigned labels
give unified convergence analysis ensemble learning meth ods including eg adaboost logistic regression boost algorithm regression these methods have common they iteratively call base learning algorithm returns hypotheses linearly combined show these methods related method known numerical optimization state convergence results these methods our analysis includes norm regularized cost functions leading clean general way ensemble learning
propose randomized techniques up kernel principal component analysis three levels sampling quantization gram matrix training randomized evaluating kernel expansions random projections evaluating kernel itself three cases give sharp bounds accuracy obtained ap rather three techniques viewed following idea replace kernel function randomized kernel behaves like expectation
introduce notion kernel alignment measure similarity between two kernel functions between kernel target function quantity captures degree agreement between kernel given learning task has natural interpretations machine learning leading simple algorithms model selection learning analyse its theoretical properties proving around its expected value discuss its relation other standard measures performance finally describe algorithms obtained within framework giving experimental results showing adapting kernel improve alignment labelled data signi increases alignment test set giving improved classi cation accuracy hence approach provides principled method performing transduction keywords kernels alignment eigenvectors eigenvalues transduction
contrast standard statistical learning theory studies uniform bounds expected error present framework exploits learning algorithm used motivated framework able exploit training sample main previous approaches lies complexity measure rather than covering hypotheses given hypothesis space necessary cover functions could have been learned using xed learning algorithm show resulting framework relates vc compression finally present application framework maximum margin algorithm linear classiers results bound exploits both margin distribution data feature space
study dynamics hebbian ica algorithm extracting nongaussian component highdimensional gaussian back ground both online batch learning find surprisingly large number examples required avoid suboptimal state close initial conditions extract signal least examples required data required extract signal nonzero kurtosis
mutual information two random variables joint probabilities ij commonly used learning bayesian nets well many other ij usually estimated empirical sampling frequency ij leading point estimate ij mutual information answer questions like ij consistent zero what probability true mutual information much larger than point estimate has go beyond point estimate bayesian framework answer these questions utilizing second order prior distribution comprising prior information about prior compute posterior distribution pi mutual information calculated derive reliable quickly computable approximations pi concentrate mean variance kurtosis non informative priors mean give exact expression numerical issues range validity discussed
recent
cluster variation method class approximation methods containing bethe approximations special cases derive two novel iteration schemes cluster variation method xed point iteration scheme gives signi cant improvement over loopy bp mean tap methods directed graphical models other gradient based method guaranteed converge shown give useful results random graphs mild conclude methods signi cant practical value large inference problems
using methods statistical physics investigate role model complexity learning support vector machines svms show advantages using svms kernels nite complexity noisy target rules contrast common theoretical beliefs found achieve optimal generalization error although training error does converge generalization error moreover nd universal learning curves depend target rule but svm kernel
combine replica approach statistical physics approach analyze learning curves analytically apply method gaussian process regression main result derive ap relations between empirical error measures tion error posterior variance
belief propagation bp decoder especially turbo decoding studied information geometrical viewpoint loopy belief network turbo codes makes difficult obtain true belief bp characteristics algorithm its clearly understood our study gives intuitive understand ing mechanism new framework analysis based framework reveal basic properties turbo decoding
work introduce based correction term likelihood ratio classification method multiple classes under certain conditions term sufficient optimally correcting between true estimated likelihood ratio analyze gaussian case find new correction term improves classification results tested medium speech recognition tasks moreover addition term makes class comparisons analogous game therefore use several strategies deal issue find further small improvements obtained using find appears good measure classification confidence
important issue applying svms speech recognition ability classify variable length sequences paper presents extensions standard scheme handling variable length data fisher score more useful mapping introduced based likelihood ratio score space de ned mapping avoids limitations fisher score class conditional generative models directly incorporated into de nition score space mapping appropriate schemes evaluated speaker independent isolated letter task new mapping outperforms both fisher score hmms trained maximise likelihood
logistic units rst hidden layer feedforward neural network compute relative probability data point under two gaussians leads us consider other density models present architecture performing discriminative learning hidden markov models using network many small hmms experiments speech data show superior standard method training hmms
novel approach comparing sequences observations using kernel demonstrated kernel derived using assumption independence sequence observations error training criterion use explicit kernel reduces classifier model size computation dramatically resulting model sizes computation times smaller our application explicit expansion preserves computational advantages earlier architecture based error train ing training using standard support vector machine methodology gives accuracy significantly exceeds performance error training speaker recognition task
challenging problem speech recognition community recognizing speech signals corrupted highly nonstationary noise approach noisy speech recognition automatically remove noise sequence before feeding clean speech recognizer previous work published showed probability model trained clean speech separate probability model trained noise could combined purpose estimating speech noisy speech showed iterative nd order vector taylor series approximation could used probabilistic inference model many circumstances possible obtain examples noise without speech noise statistics may change signi during utterance so frames sucient estimating noise model paper show noise model learned even data contains speech particular noise model learned test utterance used test utterance approximate inference technique used approximate step generalized em algorithm learns parameters noise model test utterance both wall street journal data added noise samples benchmark show new noise adaptive technique performs well signi better than non adaptive algorithm without need separate training set noise examples
model auditory grouping described auditory attention plays key role model based upon oscillatory correlation framework neural oscillators representing single perceptual stream oscillators representing other streams model suggests mechanism attention directed high low tones sequence tones alternating frequencies addition perceptual segregation harmonic complex tone
missing data approach improving robustness automatic speech recognition added noise initial process identifies spectral temporal regions speech source remaining regions considered missing paper develop connectionist approach problem adapting speech recognition missing data case using recurrent neural networks contrast methods based hidden markov models allow us make use longterm time constraints make problems classification incomplete data missing values interact report encouraging results isolated digit recognition task
applying unsupervised learning techniques like ica decorrelation key question whether discovered pro reliable other words give error bars assess quality our separation use resampling meth ods tackle these questions show experimentally our proposed variance estimations strongly correlated error demonstrate reliability estimation used choose appropriate enhance separation performance most important mark components have actual physical meaning application meg ex usefulness our approach
well known under noisy conditions speech much more clearly read speakers suggests utility audio visual information task speech enhancement propose method exploit audio visual cues enable speech separation under non stationary noise single microphone extend hmm based speech enhancement techniques signal noise models combined incorporate visual information employ novel signal hmms dynamics narrow band wide band components factorial avoid combinatorial explosion factorial model using simple approximate inference technique quickly estimate clean signals mixture present preliminary evaluation approach using small vocabulary audio visual database showing promising improvements machine intelligibility speech enhanced using audio visual information
present sequential monte carlo method applied additive noise robust speech recognition noise method generates set samples according prior distribution given clean speech models noise prior evolved previous estimation explicit model representing noise speech features used so extended kalman filter constructed each sample generating updated continuous state estimate estimation noise parameter tion likelihood weighting each sample minimum mean square error inference noise parameter car out over these samples fusion estimation samples ac their weights residual resampling selection step smoothing step used improve tion experiments conducted speech recognition simulated nonstationary noise power changed ar highly nonstationary noise experiments carried out observed method have recognition performance improvement over achieved noise stationary noise assumption
cortical model motion depth selectivity complex cells visual cortex proposed model based time extension phase based techniques disparity estimation consider computation total temporal derivative time varying disparity through combination responses disparity energy units take into account physiological plausibility model based combinations binocular cells characterized di erent ocular dominance indices resulting cortical units model show sharp selectivity motion has been compared reported literature real cortical cells
theories cue combination suggest possibility constructing visual stimuli different patterns neural activity sensory areas brain but cannot distinguished behavioral measure perception such stimuli they exist would interesting two reasons first could know none differences between stimuli past computations used build second difficult distinguish stimulus driven components measured neural activity top down components such those due stimuli changing stimulus without changing percept could exploited measure activity here describe stimuli vertical horizontal trade during construction surfaces yielding stimulus equivalence classes equivalence class membership changed after change eye alone without changes retinal images formal correspondence drawn between these perceptual more familiar sensory such color
recent work has shown impressive modeling clustering sets images objects similar appearance seek expand these capabilities sets images object class show considerable variation across individual instances eg images using representation based similarities similarity templates because its invariance colors particular components object representation en detection instances object class enables alignment those instances further model implicitly represents re color regularity image set enabling decomposition object class into component regions
present new simulation results computational model interacting visual neurons simultaneously predicts modulation spatial vision thresholds focal visual attention ve dual task human psychophysics experiments new study our previous attention competition among early visual neurons within cortical ed competition hypothesis assumed attention equally ects neurons yielded two predictions increase gain tuning attention while both ects have been separately observed single unit study has yet shown them simultaneously hence here explore whether our model could still predict our data attention might modulate neuronal gain but do so non uniformly across neurons tasks speci investigate whether modulating gain neurons best tuned most informative about stimulus neurons equally but task dependent manner may account data nd none these hypotheses yields predictions plausible ed competition hypothesis hence providing additional support our original
optimization pattern discrimination goal graph partitioning approaches often lack capability integrate prior knowledge guide grouping paper consider priors generative models partially labeled data spatial attention these priors modelled constraints solution space condition constraints restrict feasible space smooth solutions subspace projection method developed solve constrained demonstrate simple priors greatly improve image segmentation results
nonlinear supervised learning model specialized mappings architecture described applied estimation human body pose monocular images consists several specialized forward mapping functions inverse map function each specialized function maps certain domains input space image features onto output space body pose parameters key algorithmic problems faced those learning specialized domains mapping functions op way well performing inference given inputs edge inverse function solutions these problems employ em algorithm alternating choices conditional assumptions performance approach evaluated synthetic real video sequences human motion
locally linear embedding elegant nonlinear dimensionality reduction technique recently introduced saul fails data divided into separate groups study variant simultaneously group data calculate local embedding each group estimate upper bound intrinsic dimension data set obtained automatically
paper develops new approach extremely fast detection do distribution positive negative examples highly eg face detection database retrieval such domains cascade simple classifiers each trained achieve high detection rates false positive rates yield final detector many able features including high detection rates low false positive rates fast performance achieving extremely high detection rates rather than low error task typically addressed machine learning al propose new variant adaboost mechanism training simple classifiers used cascade experimental results domain face detection show training algorithm yields improvements performance over conventional adaboost final face detection system process frames per second achieves over detection false positive rate
most popular algorithms object detection require use exhaustive spatial scale search procedures such approaches object de ned means local features paper show including contextual information object detection procedures provides ecient way down need exhaustive search present results real images showing proposed scheme able accurately predict likely object classes locations sizes
describe neural network enhances salient closed contours our work different previous work three important ways first like input provided lgn put our computation isotropic input composed edges second our network computes well defined function input based distribution closed contours characterized random process third even though our computation implemented discrete network its output invariant continuous rotations translations input pattern
describe algorithm automatically learning discriminative com objects svm classifiers based growing image parts minimizing theoretical bounds error probability svm face classifiers combined second stage yield hierarchical svm classifier experimental results face classification show considerable robustness against rotations depth suggest performance significantly better level than other face detection systems novel aspects our approach algorithm learn classification experts their combination use models training maximum operation output each component classifier may relevant bio logical models visual recognition
describe factor relates probability distributions image features distributions images themselves factor depends our choice features lattice quantization independent training image data illustrate importance factor analyzing parameters markov random field ie gibbs log linear probability models images learned data maximum likelihood estimation particular study homogeneous mrf models learn image distributions terms clique potentials corresponding feature histogram statistics cf minimax entropy learning mel rst use our analysis factor determine clique potentials di erent features second show clique potentials computed analytically approximating factor third demonstrate connection between approximation generalized iterative scaling algorithm due calculating potentials connection enables us use improve our multinomial approximation using bethe approximations simplify procedure support our analysis computer simulations
key question neuroscience encode sensory stimuli such images sounds motivated studies response neurons early cortical areas propose encoding scheme absolute measures signal intensity contrast uses instead local ordinal measures scheme structure signal represented set inequalities across adjacent regions paper focus characterizing fidelity representation strategy develop regularization approach image reconstruction ordinal measures thereby demonstrate ordinal scheme faithfully encode signal structure present neurally plausible implementation computation uses local update rules results highlight robust generalization ability local ordinal encodings task pattern classification
paper presents unsupervised learning algorithm derive probabilistic dependence structure parts object moving body our examples automatically unlabeled data dis part work based unlabeled data ie training features include both useful parts background clutter correspondence between parts detected features unknown use graphs probabilistic independence parts but unsupervised technique limited type graph new approach labeling data part assignments taken hidden variables em applied greedy algorithm developed select parts search optimal structure based differential entropy these variables success our algorithm demonstrated applying generate models human motion automatically unlabeled real image sequences
investigate bayesian alternatives classical monte carlo methods evaluating integrals bayesian monte carlo allows incorporation prior knowledge such smoothness into estimation simple problem show outperforms classical importance sampling method attempt more challenging multidimensional integrals involved computing marginal likelihoods statistical models partition functions model find bayesian monte carlo outperformed annealed importance sampling although high dimensional problems problems massive may less adequate advantage bayesian approach monte carlo samples drawn distribution allows possibility active design sample points so maximise information gain
study explicit parametric model documents queries assessment information retrieval ir mean field methods applied analyze model derive efficient practical algorithms estimate parameters problem hyperparameters estimated fast approximate leave out cross validation procedure based method algorithm further evaluated several benchmark databases comparing standard algorithms ir
many algorithms rely critically being given good metric over their inputs instance data often clustered many plausible ways clustering algorithm such means initially fails find meaningful user may user manually metric until sufficiently good clusters found these other applications requiring good metrics desirable provide more systematic way users indicate what they consider similar instance may ask them provide examples paper present algorithm given examples similar desired pairs points learns distance metric over learning convex optimization problem allows us give efficient local optima free algorithms demonstrate empirically learned metrics used significantly improve clustering performance
paper consider formulations multi class problems based generalized notion margin using output coding includes but restricted standard multi class svm formulations differently many previous approaches learn code well embedding function illustrate lead formulation allows solving wider range problems instance many classes even missing classes keep our optimization problems tractable propose algorithm capable solving them using classifiers similar spirit boosting
prior knowledge form multiple sets each belonging two categories introduced into linear support vector machine classifier resulting formulation leads linear program solved efficiently real world examples dna sequencing breast cancer demonstrate effectiveness proposed method numerical results show improvement test set accuracy after incorporation prior knowledge into ordinary data based linear support vector machine classifiers experiment shows linear classifier based solely prior knowledge far outperforms direct application prior knowledge rules classify data keywords use prior knowledge support vector machines linear programming
consider problem multi step ahead prediction time series analysis using non parametric gaussian process model step ahead forecasting discrete time non linear dynamic system performed doing repeated step ahead predictions state space model form prediction time per show using analytical gaussian approximation formally incorporate uncertainty about intermediate values thus updating uncertainty current prediction based point estimates previous outputs pa
focus paper problem learning kernel operators empirical data cast kernel design problem construction accurate kernel simple less accurate base kernels use boosting paradigm perform kernel construction process do so modify so accommodate kernel operators devise efficient weak learner simple kernels based generalized vector decomposition demonstrate effectiveness our approach synthetic data dataset dataset performance perceptron algorithm learned kernels systematically better than fixed rbf kernel
introduce family classifiers based physical analogy system family called classifiers includes two best known support vector machines svms svm analogy training example corresponds given location space classification function corresponds potential function training objective function corresponds energy framework provides novel interpretation existing algorithms their but suggests variety new methods svms including kernels gap between polynomial radial basis functions objective functions do require positive definite kernels regularization techniques allow construction optimal classifier space based framework propose novel svms perform simulation studies show they comparable superior standard svms experiments include classification tasks data represented terms their pairwise classifier outperformed standard svms
paper introduces algorithm automatic relevance determination input variables support vector machines relevance measured scale factors defining input space metric feature selection performed assigning zero weights irrelevant variables metric automatically tuned minimization standard svm empirical risk scale factors added usual set parameters defining classifier feature selection achieved constraints encouraging sparsity scale factors resulting algorithm compares favorably state art feature selection procedures demonstrates its effectiveness demanding facial expression recognition problem
paper present new algorithm suitable matching discrete objects such strings trees linear time thus dynamic programming quadratic time complexity furthermore prediction cost many cases reduced linear cost length sequence classified regardless number support vectors improvement currently available algorithms makes string kernels viable alternative
introduce generalized linear model statistical estimator combines features nonlinear regression factor analysis approximately rectangular matrix into simpler representation here low rank matrices while link functions include many useful models special cases including principal components analysis exponential family pca infomax formulation independent components analysis linear regression generalized linear models they include new interesting special cases describe below present iterative procedure optimizes parameters procedure reduces well known algorithms special cases above other special cases new
propose framework incorporate unlabeled data kernel classi er based idea two points same cluster more likely have same label achieved modifying kernel matrix experimental results assess validity approach
state networks novel approach recurrent neural network training consists large xed recurrent network desired output obtained training suitable output connection weights determination optimal output weights becomes linear uniquely solvable task mse minimization article basic ideas describes online adaptation scheme based rls algorithm known adaptive linear systems example th order system adaptively ed known rls algorithms carry over linear systems nonlinear ones speci convergence rate determined design time
introduce general family kernels based weighted rational relations rational kernels used analysis variable length sequences more generally weighted automata applications such computational biology speech recognition show rational kernels computed efficiently using general algorithm composition weighted general single source shortest distance algorithm describe several general families positive definite symmetric rational kernels these general kernels combined support vector machines form efficient powerful techniques spoken classification highly complex kernels become easy design implement lead substantial improvements classification accuracy show string kernels considered applications computational biology specific instances rational kernels
present framework sparse gaussian process gp methods uses forward selection criteria based principles previously suggested active learning our goal learn predictors evaluated rather than number training points but perform training under strong restrictions time memory requirements scaling our method show match prediction performance popular most large real world classification experiments support vector machine svm yet significantly faster training contrast svm our approximation produces estimates predictive probabilities error bars allows bayesian model selection less complex implementation
model selection linked model assessment problem comparing different models model parameters specific learning task supervised learning standard practical technique crossvalidation applicable semi supervised unsupervised settings paper new model assessment scheme introduced based notion stability stability measure yields upper bound cross validation supervised case but extends semi supervised unsupervised problems experimental part performance stability measure studied model order selection comparison standard techniques area
exist many approaches clustering but important issue feature selection ie selecting data attributes relevant clustering rarely addressed feature selection clustering difficult due absence class labels propose two approaches feature selection context gaussian mixture based clustering first instead making hard estimate feature expectation maximization em algorithm derived task second approach extends mutual feature relevance criterion unsupervised case feature selection carried out backward search scheme scheme classified mixture estimation outer layer performs feature selection experimental results synthetic real data show both methods have promising performance
paper show generation documents thought markov process leads fisher string kernels reconstructed fisher kernel view gives more flexible insight into string kernel suggests way re statistics training corpus furthermore prob modelling approach suggests extending markov pro consider subsequences varying length rather than standard approach used string kernel give procedure determining subsequences informative features hence generate finite state machine model again used obtain fisher kernel adjusting influence weighting received features way able obtain logarithmic weighting fisher kernel finally experiments reported comparing different kernels using standard bag words kernel baseline
several authors have suggested viewing boosting gradient descent search good fit function space apply gradient based boosting methodology unsupervised learning problem density estimation show convergence properties algorithm prove strength weak learnability property applies problem well illustrate potential approach through experiments boosting bayesian networks learn density models
present simple direct approach solving ica problem using density estimation maximum likelihood given candidate orthogonal frame model each coordinates using semi parametric density estimate based splines our estimates have two continuous derivatives easily run second order search frame parameters our method performs favorably compared state art techniques
standard representation text documents words ers well known limitations mostly due its inability exploit semantic similarity between terms attempts incorporate notion term similarity include latent semantic use semantic networks probabilistic methods paper propose two methods inferring such similarity corpus rst de word similarity based document similarity giving rise system equations whose equilibrium point use obtain semantic similarity measure second method models semantic relations means di process graph de ned co occurrence information both approaches produce valid kernel functions real number paper shows alignment measure used successfully perform model selection over parameter combined use support vector machines obtain positive results
boosting algorithms successful applications classification regression learning problems but unsupervised learning propose sequential approach adding features random field model training them improve classification performance between data equal sized sample negative examples generated models current estimate data density training each boosting proceeds three stages first sample negative examples models current boltzmann distribution next feature trained improve classification performance between data negative examples finally coefficient learned determines importance feature relative ones already pool negative examples need generated once learn each new feature validity approach demonstrated binary digits continuous synthetic data
propose new algorithm estimate intrinsic dimension data sets method based geometric properties data requires neither parametric assumptions data generating model nor input parameters set method compared similar algorithm same family geometric techniques experiments show our method more robust terms data generating distribution more reliable presence noise
using markov chain perspective spectral clustering present algorithm automatically find number stable clusters dataset markov chains behaviour characterized spectral properties matrix transition probabilities derive along their describes flow probability mass due markov chain characterized its eigenvalue equivalently its decay markov chain iterated ideal stable cluster zero infinite half life key insight paper between weakly coupled clusters identified computing sensitivity variations edge weights propose novel algorithm perform clustering removes these identified iterative fashion
common objective learning model data recover its network structure while model parameters minor interest example may wish recover regulatory networks high throughput data sources paper examine bayesian regularization using product independent dirichlet priors over model parameters ects learned model structure domain discrete variables show small scale parameter often interpreted equivalent sample size prior strength leads strong regularization model structure sparse graph given suciently large data set particular empty graph obtained limit vanishing scale parameter opposite what may expect limit namely complete graph maximum likelihood estimate prior ects parameters expected scale parameter trade between parameters vs structure model demonstrate optimizing trade sense predictive accuracy
recently proposed algorithms nonlinear dimensionality reduction fall broadly into two categories have different advantages global isomap local locally linear embedding laplacian eigenmaps present two variants isomap combine advantages global approach what have previously been exclusive advantages local methods computational sparsity ability maps
application variable dynamic bayesian networks constrained complexity over latent variables reason either small latent dimensions gaussian latent conditional tables linearly dependent past states typically considered order inference tractable suggest alternative approach latent variables modelled using deterministic conditional probability tables has advantage tractable inference even highly complex non gaussian visible conditional probability tables approach enables consideration highly complex latent dynamics retaining benefits tractable probabilistic model
propose probabilistic generative models called parametric mixture models pmms multiclass multi labeled text categorization problem binary classification approach has been employed whether text belongs category binary classifier every category contrast our approach simultaneously detect multiple categories text using pmms derive efficient learning prediction algorithms pmms empirically show our method could significantly outperform conventional binary methods applied multi labeled text categorization using real world wide web pages
recently fisher score fisher kernel increasingly used feature extractor classification problems fisher score vector parameter derivatives loglikelihood probabilistic model paper gives theoretical analysis about class information preserved space fisher score turns out fisher score consists few important dimensions class information many dimensions perform clustering fisher score means type methods inappropriate because they make use dimensions so develop novel but simple clustering algorithm specialized fisher score exploit important dimensions algorithm successfully tested experiments artificial data real data amino acid sequences
propose paper probabilistic approach adaptive inference generalized nonlinear classification combines computational advantage parametric solution flexibility sequential sampling techniques regard parameters classifier latent states first order markov process propose algorithm regarded variational generalization standard kalman filtering variational kalman filter based two novel lower bounds enable us use non degenerate distribution over adaptation rate extensive empirical evaluation demonstrates proposed method capable competitive classifiers both stationary non stationary environments although focus classification algorithm easily extended other generalized nonlinear models
introduce novel learning algorithm binary classification hyperplane discriminants based pairs training points opposite classes dyadic algorithm further extended nonlinear discriminants using kernel functions satisfying conditions ensemble simple dyadic learned incrementally means confidence version adaboost provides sound strategy searching through finite set hypotheses experiments real world datasets uci repository generalization performance classifiers found comparable svms nn classifiers furthermore computational cost classification run time found similar better than svm similarly svms boosted dyadic kernel discriminants tend maximize margin via adaboost contrast svms offer line incremental learning machine building kernel discriminants whose complexity number kernel evaluations directly controlled off accuracy
greedy importance sampling unbiased estimation technique reduces variance standard importance sampling explicitly searching modes estimation objective previous work has demonstrated feasibility implementing method proved technique unbiased both discrete continuous domains paper present greedy importance sampling eliminates free parameters original estimator introduces new regularization strategy further reduces variance without resulting estimator shown effective difficult estimation problems arising markov random field inference particular improvements achieved over standard mcmc estimators distribution has multiple modes
problems novel situations should detected approached describing domain class typical examples these applications come areas machine fault detection identification principle refer problem little knowledge available outside typical class paper explain why natural representations domain descriptors propose simple class classifier dissimilarity representations use linear programming efficient class description found based small number prototype objects classifier made more robust transforming dissimilarities compute using reduced representation set finally comparison comparable class classifier given
formulate regression problem maximizing minimum probability future predicted outputs regression model within bound true regression function our formulation unique obtain direct estimate lower probability bound proposed framework minimax probability machine regression based recently described minimax probability machine classification algorithm et al uses mercer kernels obtain nonlinear regression models tested both toy real world data accuracy bound efficacy regression models
recent years variational methods have become popular tool approximate inference learning wide variety probabilistic models each new application currently necessary first derive variational update equations implement them application specific code each these steps both time consuming error paper describe general purpose inference engine called variational inference bayesian networks allows wide variety probabilistic models implemented solved without coding new models specified either through simple via graphical interface analogous drawing automatically generates solves variational equations illustrate power flexibility using examples bayesian mixture modelling
new approach inference belief networks has been recently proposed based algebraic representation belief networks using multilinear functions according approach key computational question representing multilinear functions inference reduces simple process such functions show here inference algorithms based special case approach precise sense use result new properties algorithms discuss its practical theoretical implications
constraint classification framework captures many multiclass classification including winner take multiclass classification classification ranking present meta algorithm learning framework learns via single linear classifier high dimension discuss distribution independent well margin based generalization bounds present empirical theoretical evidence showing constraint classification benefits over existing methods multiclass classification
introduce iterative local message passing algorithm computing nash equilibria multi player games represented arbitrary undirected graphs provide formal analysis experimental evidence demonstrating performs well large graphical games many loops often converging iterations graphs hundreds nodes generalizes tree algorithm et al viewed similar spirit belief propagation probabilistic inference thus recent work who explored junction tree approach thus probabilistic inference have least two promising general purpose approaches equilibria computation graphs
focus problem efficient learning dependency trees well known given pairwise mutual information coefficients minimum weight spanning tree algorithm solves problem exactly polynomial time large data sets construction correlation matrix running time have developed new spanning tree algorithm capable exploiting partial knowledge about edge weights partial knowledge maintain probabilistic confidence interval coefficients derive examining small sample data algorithm able need interval translates more data particular attribute pair experimental results show running time near constant number records without significant loss accuracy generated trees interestingly our spanning tree algorithm based solely red edge rule generally considered guaranteed bad performance
describe method computing provably exact maximum posteriori map estimates subclass problems graphs cycles basic idea represent original problem graph cycles convex combination tree structured problems convexity argument guarantees optimal value original problem ie log probability map assignment upper bounded combined optimal values tree problems prove upper bound met equality tree problems share optimal configuration common important such shared configuration map configuration original problem next develop tree max product algorithm attempting find convex combinations tree structured problems share common optimum give necessary sufficient conditions fixed point yield exact map estimate attractive feature our analysis generalizes naturally convex combinations structured distributions
pairwise data empirical sciences typically either due noise due estimates therefore hard analyze conventional machine learning technology paper therefore study ways work around problem first present alternative embedding multi dimensional scaling mds allows us apply variety classical machine learning signal processing algorithms class pairwise grouping algorithms share shift invariance property statistically invariant under embedding procedure leading identical assignments objects clusters based new representation denoising methods applied second step both steps provide theoretically well controlled setup pairwise data respective metric representation demonstrate practical usefulness our theoretical reasoning discovering structure protein sequence data bases improving performance upon existing automatic methods
similarity between objects fundamental element many learning algorithms most non parametric methods take similarity fixed but much recent work has shown advantages learning particular exploit local invariances data capture possibly non linear manifold most data lies propose new non parametric kernel density estimation method captures local structure underlying manifold through leading eigenvectors regularized local covariance matrices experiments density estimation show significant improvements respect parzen density estimators density estimators used within bayes classifiers yielding classification rates similar svms much superior parzen classifier
describe probabilistic approach task placing objects described high dimensional vectors pairwise dissimilarities low dimensional space way preserves neighbor identities gaussian centered each object high dimensional space densities under gaussian given dissimilarities used define probability distribution over potential neighbors object aim embedding approximate distribution well possible same operation performed low dimensional images objects natural cost function sum kullback leibler divergences per object leads simple gradient adjusting positions low dimensional images unlike other dimensionality reduction methods probabilistic framework makes easy represent each object mixture widely separated low dimensional images allows ambiguous objects like document count vector word have versions close images both without forcing images outdoor concepts located close those concepts
present automatic alignment procedure maps internal representations learned several local dimensionality reduction experts into single coherent global coordinate system original data space our algorithm applied set experts each produces low dimensional local representation highdimensional input unlike recent efforts coordinate such models modifying their objective functions our algorithm after training applies efficient post process trained models post processing has local optima size system solve scales number local models rather than number original data points making more efficient than model free algorithms such isomap
low rank approximation techniques widespread pattern recognition research they include latent semantic analysis probabilistic principal components pca generative aspect model many forms analysis make use low dimensional manifold onto data projected such techniques generally unsupervised allows them model data absence labels categories many practical problems prior knowledge available form context paper describe principled approach incorporating such information demonstrate its application pca based approximations several data sets
problem extracting relevant aspects data face multiple structures inherent modeling complex data extracting structure random variable relevant another variable has been addressed recently via information bottleneck method such auxiliary variables often contain more information than actually required due structures irrelevant task many other cases fact easier specify what irrelevant than what task hand identifying relevant structures thus considerably improved minimizing information about another irrelevant variable paper give general formulation problem derive its formal well algorithmic solution its operation demonstrated synthetic example two real world problems context text categorization face images while original information bottleneck problem related rate distortion theory distortion measure replaced relevant information extracting relevant features while removing irrelevant ones related rate distortion side information
show existence critical points lines likelihood function mixture type models they given embedding critical point models less components sufficient condition critical line gives local maxima saddle points derived based fact component split method proposed mixture gaussian components its effectiveness verified through experiments
consider learning problem nding dependency between general class objects another possibly di erent general class objects objects example vectors images strings trees graphs such task made possible employing similarity measures both input output spaces using kernel functions thus embedding objects into vector spaces experimentally validate our approach several tasks mapping strings strings pattern recognition reconstruction partial images
missing data common real world datasets problem many estimation techniques have developed variational bayesian method perform independent component analysis ica high dimensional data containing missing entries missing data handled naturally bayesian framework integrating generative density model modeling distributions independent sources mixture gaussians allows sources estimated different kurtosis variational bayesian method automatically determines dimensionality data yields accurate density model observed data without overfitting problems allows direct probability estimation missing values high dimensional space avoids dimension reduction preprocessing feasible missing data
investigate problem learning classification task datasets described matrices rows columns these matrices correspond objects row column objects may belong different sets entries matrix express relationships between them interpret matrix elements being produced unknown kernel operates object pairs show under mild assumptions these kernels correspond dot products unknown feature space minimizing bound generalization error linear classifier has been obtained using covering numbers derive objective function model selection according principle structural risk minimization new objective function has advantage allows analysis matrices positive definite even symmetric square consider case row objects interpreted features suggest additional constraint imposes sparseness row objects show method used feature selection finally apply method data obtained dna microarrays column objects correspond samples row objects correspond genes matrix elements correspond expression levels benchmarks conducted using standard gene classification support vector machines nearest neighbors after standard feature selection our new method extracts sparse set genes provides superior classification results
paper study special kind learning problem each training instance given set distribution over candidate class labels candidate labels correct such problem occur eg information retrieval setting set words associated image classes labels organized propose novel discriminative approach handling ambiguity class labels training examples experiments proposed approach over five different uci datasets show our approach able find correct label among set candidate labels actually achieve performance close case each training instance given single correct label contrast methods rapidly more ambiguity introduced into labels
paper consider problem novelty detection presenting algorithm aims nd minimal region input space containing fraction probability mass underlying data set single class minimax probability machine built distribution free methodology minimizes worst case probability data point outside convex set given mean covariance matrix distribution making further assumptions present robust approach estimating mean covariance matrix within general two class mpm setting show approach single class problem provide empirical results comparing single class mpm single class svm two class svm method
consider problem illusory structure high dimensional data particular examine role distance metric use topographic mappings based statistical field multidimensional scaling show use squared euclidean metric ie measure gives rise structure input data drawn highdimensional isotropic distribution provide theoretical justification observation
introduce new learning algorithm decision allow features constructed data allow tradeoff between accuracy complexity bound its generalization error terms number errors size classifier finds training data compare its performance natural data sets set covering machine support vector machine
consider general problem utilizing both labeled un labeled data improve classification accuracy under tion data lie high dimensional space develop algorithmic framework classify partially labeled data set principled manner central idea our approach classification functions naturally defined sub manifold question rather than total ambient space using laplace operator produces basis hilbert space square functions recover such basis unlabeled examples required once basis ob training performed using labeled data set our algorithm models manifold using graph data approximates laplace operator graph laplacian practical applications image text classification considered
discuss problem ranking instances use large margin principle introduce two main approaches first fixed margin policy margin closest neighboring classes being maximized turns out direct margins sum margins maximized approach tion svm ranking learning second approach allows shown reduce svm number classes approaches optimal size both total number training examples experiments performed visual classification collaborative filtering show both approaches outperform existing ordinal regression algorithms applied ranking multi class svm applied general multi class classification
describe new algorithmic framework learning multiclass categorization problems framework multiclass predictor composed pair map both instances labels into common space space each instance assigned label nearest outline analyze algorithm termed learning pair labeled data key construction analysis algorithm notion probabilistic output codes generalization error correcting output codes furthermore method multiclass categorization using shown instance demonstrate advantage over comparing their performance numerous categorization problems
gaussian process regression allows simple analytical treatment exact bayesian inference has been found provide good performance yet scales number training data paper compare several approaches towards scaling gaussian processes regression large data sets subset method reduced rank approximation online gaussian processes bayesian committee machine furthermore provide theoretical insight into our experimental results found subset methods give good particularly fast predictions data sets high medium noise levels complex low noise data sets bayesian committee machine achieves significantly better accuracy yet higher computational cost
paper introduce methodology determine bifurcation structure optima class similar cost functions rate distortion theory deterministic annealing information distortion information bottleneck method introduce numerical algorithm uses explicit form branches find optima bifurcation point
paper investigates boosting approach discriminative learning label sequences based sequence rank loss function proposed method combines many advantages boosting schemes dynamic programming methods attractive both conceptually computationally addition discuss alternative approaches based hamming loss label sequences sequence boosting algorithm ers interesting alternative methods based hmms more recently proposed conditional random fields applications areas presented technique range natural language processing information extraction computational biology include experiments named entity recognition part speech demonstrate validity our approach
propose framework classifier design based discriminative densities representation differences class conditional distributions way optimal classification densities selected parametrized set constrained maximization objective function measures average bounded difference ie contrast between discriminative densities show maximization contrast equivalent minimization approximation bayes risk therefore using suitable classes probability density functions resulting maximum contrast classifiers approximate bayes rule general multiclass case particular certain density functions obtain have same functional form well known support vector machines svms show training general requires nonlinear optimization but under certain conditions problem concave single linear program indicate close relation between training particular show linear programming machines viewed approximate realization experiments benchmark data sets shows competitive classification performance
adaboost minimizes upper error bound exponential function margin training set goal applications pattern classification always minimum error rate other hand adaboost needs effective procedure learning weak classifiers itself difficult especially high dimensional data paper present novel procedure called learning better boosted classifier uses mechanism after each iteration adaboost remove weak classifiers cause higher error rates resulting boosted classifier consists fewer weak classifiers yet achieves lower error rates than adaboost both training test propose statistical model learning weak classifiers based approximation posterior using overcomplete set scalar features experimental comparisons adaboost provided through difficult classification problem face detection goal learn training examples highly nonlinear classifier between face patterns high dimensional space results clearly demonstrate made over adaboost
paper consider relevance vector machine rvm formalize incremental training strategy variant expectation maximization em algorithm call subspace em working subset active basis functions sparsity rvm solution ensure number basis functions thereby computational complexity kept low introduce mean field approach intractable classification model expected give good approximation exact bayesian inference contains laplace approximation special case test algorithms two large data sets examples results indicate bayesian learning large data sets eg mnist database realistic
present class algorithms learning structure graphical models data algorithms based measure known kernel generalized variance essentially allows us treat variables equal gaussians feature space obtained mercer kernels thus able learn hybrid graphs involving discrete continuous variables arbitrary type explore computational properties our approach showing use kernel trick compute relevant statistics linear time illustrate our framework experiments involving discrete continuous data
propose model learn parts based representations highdimensional data our key assumption dimensions data separated into several disjoint subsets factors take values independently each other assume each factor has small number discrete states model using vector quantizer selected states each factor represent multiple causes input given set training examples our model learns association data dimensions factors well states each vq inference learning carried out efficiently via variational algorithms present applications model problems image decomposition collaborative filtering text classification
classification partially labeled data requires using large number unlabeled examples estimated marginal px further constrain conditional beyond few available labeled examples formulate regularization approach linking marginal conditional general way regularization penalty measures information about labels over covering regions parametric assumptions required approach remains tractable even continuous marginal densities px develop algorithms solving regularization problem finite covers establish limiting differential equation behavior new regularization approach simple cases
gaussian processes provide approach nonparametric modelling allows straightforward combination function derivative observations empirical model particular importance identification nonlinear dynamic systems experimental data allows us combine derivative information associated uncertainty normal function observations into learning inference process derivative information form priors specified expert identified perturbation data close equilibrium allows fusion multiple local linear models consistent manner inferring consistent models constraints met improves dramatically computational efficiency gaussian process models dynamic system identification large quantities near equilibrium data reducing training set size traditionally problem gaussian process models
derive multiplicative updates solving nonnegative quadratic programming problem support vector machines svms updates have simple closed form prove they converge solution maximum margin hyperplane updates optimize traditionally proposed objective function svms they do involve heuristics such choosing learning rate deciding variables update each iteration they used adjust quadratic programming variables parallel guarantee improvement each iteration analyze asymptotic convergence updates show coefficients non support vectors decay zero rate depends their margins practice updates converge rapidly good classifiers
given set hidden variables priori markov structure derive online algorithm approximately updates posterior pairwise measurements between hidden variables become available update performed using assumed density filtering incorporate each pairwise measurement compute optimal markov structure represents true posterior use prior incorporating next measurement demonstrate resulting algorithm calculating globally consistent trajectories robot along trajectory update trajectory length update takes ot conditional distributions linear gaussian algorithm thought kalman filter simplifies state covariance matrix after incorporating each measurement
particle filters estimate state dynamical systems sensor information many real time applications particle filters sensor information significantly higher rate than update rate filter approach dealing such situations update particle filter often possible sensor information cannot processed time paper present real time particle filters make use sensor information even filter update rate below update rate sensors achieved representing posteriors mixtures sample sets each mixture component integrates observation arriving during filter update weights mixture components set so minimize approximation error introduced mixture representation thereby our approach focuses computational resources samples valuable sensor information experiments using data collected mobile robot show our approach yields strong improvements over other approaches
present novel generative model natural language tree structures semantic dependency syntactic structures separate models factorization provides conceptual simplicity straightforward separately improving component models level performance comparable similar non factored models most importantly unlike other modern parsing models factored model extremely effective parsing algorithm enables efficient exact inference
explore consequences viewing semantic association result attempting predict concepts likely arise particular context argue success existing accounts semantic representation comes result indirectly addressing problem show closer correspondence human data obtained taking probabilistic approach explicitly models generative structure language
standard view memory episodes stored hippocampus through various recent experimental challenges idea transfer particularly human memory forcing its re evaluation although independent neurophysiological evidence short transfer few theoretical ideas what might doing suggest demonstrate two important computational roles associated neocortical indices
behavioral goals achieved reliably repeatedly movements rarely their detail here offer explanation show variability goal compatible but indeed allowing variability redundant dimensions optimal control strategy face uncertainty optimal feedback control laws typical motor tasks obey minimal principle deviations average trajectory they task goals resulting behavior exhibits task constrained variability well coupling among another empirical phenomenon
present account human concept learning categories examples principle minimum description length support wide range two dimensional concept both regular highly irregular mdl theory give suggests concept description influences its learnability structure number different principles have been explain manner humans learn has been underlying principle might objects decision boundaries bayesian inference while many these theories mathematically well been successful explaining range experimental findings they have commonly been narrow collection concept types similar simple categories figure ac figure categories similar those previously represent contours equal except moreover research has look beyond simple category types goal has largely been investigate categorization performance isolated irregular distributions rather than present survey across range example has previously examined diagonal category similar concept well theoretically important they way range concept structures indeed view dimensional cartesian space upon category may represented set space may considered potential category therefore natural ask whether such category manifold principle easier more difficult learn than previous investigations have never considered reasonably broad range structures they have never been position answer question paper present theory human categorization based much better answer questions about intrinsic learnability both structurally regular structurally irregular support theory briefly present experiment testing human range types defined over continuous two dimensional feature space including both highly regular highly irregular find our mdl based theory gives good account learning these concepts complexity accurately predicts subjective difficulty ofthe various concept types tested previous investigations category structure role structure determining learnability has been entirely fact intrinsic structure categories has been investigated quite classic work et human performance learning such boolean categories varies greatly depending intrinsic logical structure ofthe recently have shown performance well predicted intrinsic concept given length ofthe shortest boolean formula describes objects category result suggests principle minimization complexity might play important role human category learning details complexity analysis do generalize easily type feature spaces wish investigate required similar general spirit but goals therefore complexity minimization technique such quantify complexity defined over continuous features investigate influence complexity experiment while plan employ applicable concepts dimension reasons experiment limited category structures formed within two dimensional feature feature space into motivation feature space place constraint possible category structure facilitate computation complexity feature values limited machine precision but matter restrict features possible values range figure abstract concepts used experiment particular abstract category structures experiment shown figure these concepts considered individually interesting cross theoretical jointly representative broader range available concepts two categories each concept referred negative positive category represented dark regions corresponding negative category its complement note many cases categories multimodal nevertheless these categories sense probabilistic ill given point feature space always either positive negative during experiment each stimulus drawn randomly feature space labeled region drawn uniform sampling used so categories figure have same base rate positives experiment itself video game required subjects discriminate between two classes quick each subject five minute games figure physical features cases radius shown figure these physical features mapped randomly onto abstract feature space such experimental concepts may rigid rotation abstract concepts figure positive derivation mdl principle optimal bayesian inference while several bayesian algorithms have previously been proposed models human concept learning implications mdl principle human learning have recently come under briefly review relevant theory figure possible concept figure posterior data sides obtain log log log log problem thus problem log log constant its value does enter into hypothesis such minimize quantity log log follow quantity log us select hypothesis minimizes what means hypothesis ofthe bayesian same hypothesis compactness corresponds equivalence between learning theory investigate concepts these compute first language hypotheses about expressed choose use rectangle alphabet table consists different sizes rectangle grid each member ofthe class rectangle particular grid four distinct rectangles ie four symbols suggests log each symbol corresponds so therefore compute shannon rectangle alphabet equivalently particular choice paper might such although have possibility adopt its emphasis ie class contains use value log rather integer log base table rectangle alphabet third fourth columns show probability source generates given member class corresponding rectangle class possible locations probability log log log log log log log log log log computing these requires specify probability mass function source convenient purpose compatible subjects imagine concepts figure produced concept information source whose parameters essentially unknown reasonable assumption source randomly selects rectangle class uniform selects individual member chosen class uniform probability assumption regarding class selection places prior each rectangle class assumption uniform within class sampling means order encode individual need consider cardinality class belongs now recall individual rectangles class differ their positions within grid cardinality class equal probability associated individual rectangle class corresponding shannon shown next these probabilities table description length particular hypothesis lengths rectangles up hypothesis length second part two part description concept respect selected bayes likelihood several possible approaches computing recall hypothesis discuss particularly straightforward composed up four rectangular regions computing therefore involves describing portion positive category falls within each rectangular hypothesis region conceptually same problem faced computing region interest fixed table minimum description lengths abstract concepts concept mdlhypothesis bits bits bits bits bits bits bits bits bits bits bits bits while regions smaller guided follow procedure previous section compute appropriate function capture positive squares hypothesis region maximum four classes needed alphabet those size four minimum description applying above concepts figure requires compute total description length corresponding viable hypotheses each concept hypothesis corresponding shortest total concept concepts shown table along corresponding minimum observed while concepts mdlhypothesis true positive almost concept information carried hypothesis concepts mdlhypothesis true meaning concept information distributed between hypothesis likelihood codes note mdlhypothesis general most compact minimum mdlhypothesis sum minimum results each game played subject ie each concept figure overall measure computed figure shows performance subjects concepts function ofthe concept complexities table linear plot unlikely statistical thus mdl complexity predicts subjective difficulty across broad range shows highly significant meaning complexity figure performance vs complexity subjects performance each concept indicated mean each concept indicated described here further modified make most approach prediction each new stimulus made based mdlhypothesis time stimulus observed correlation between subjects actual decision found highly significant concept types statistics given below concept each ofthe figure illustrates behavior ofthe real time simulations variety sets found figure real time mdlhypothesis evolution actual concept data size ofthe data set grows beyond oscillation between rectangle hypothesis shown step two rectangle hypothesis shown step discriminability gives measure capacity discriminate categories ie independent criterion responding positive conclusions discussed above mdl tight relationship bayesian inference hence serves reasonable basis theory data presented above suggest human learners indeed guided something much like principle classifying conclude humans construct precisely corresponding two part code equation seems likely they employ closely related complexity minimization principle associated minimum principles guiding human inference especially perception eg our findings suggest approach predicting subjective difficulty concepts defined over continuous had previously found boolean concepts subjective difficulty intrinsic mdl approach elegant apparently well human performance acknowledgments research supported nsf references rm exemplar based accounts relations between classification recognition vol pp la categorization probability density estimation vol pp vol pp bayesian modeling human concept processing systems vol mit press cambridge ma rm optimal oxford university press oxford pp vol pp structure perceptual vol pp rn ci hm learning vol pp minimization boolean complexity human concept learning nature vol pp modeling shortest data vol pp li
people make sophisticated causal inferences little data often few observations argue these inferences explained bayesian computations over hypothesis space causal graphical models shaped strong top down prior knowledge form intuitive theories present two case studies our approach including quantitative models human causal brief comparisons traditional bottom up models inference
argue human inductive generalization best explained bayesian framework rather than traditional models based similarity computations go beyond previous work bayesian concept learning introducing unsupervised method constructing flexible hypothesis spaces propose version bayesian occams razor off priors likelihoods prevent over generalization these flexible spaces analyze two published data sets inductive reasoning well results new behavioral study have carried out
causes dense but similar produced hippocampal damage does action hippocampus cause less storage less accurate storage information long term memory used simple variant rem model fit data collected fisher effects study time word frequency both know recognition memory simple strength model fit well contrary expectations et al more important within bayesian based rem modeling framework data consistent view causes less accurate storage rather than less storage information memory
current psychological theories human causal learning focus primarily long run predictions two estimating parameters causal bayes nets though different third through structural learning paper focuses short run behavior examining dynamical versions these three theories comparing their predictions real world dataset
consider hypothesis systems learning aspects visual perception may benefit use suitably designed developmental during training four models trained estimate motion velocities sequences visual images three models developmental models sense nature their input changed during course training they received relatively visual input early training quality input improved training model used coarse multiscale developmental progression ie received coarse scale motion features early training finer scale features added its input training another model used fine multiscale progression third model used random progression final model sense nature its input same throughout training period simulation results show coarse multiscale model performed best hypotheses offered account models superior performance conclude suitably designed developmental sequences useful systems learning estimate motion velocities idea visual development aid visual learning viable hypothesis need further study
according series models dopamine da neurons signal reward prediction error using temporal difference td algorithm address problem solved these accounts maintain representation cues predict delayed consequences our new model uses td rule grounded partially observable semi markov processes formalism captures two largely features da experiments hidden state temporal variability previous models predicted rewards using delay line representation sensory inputs replace more active process inference about underlying state world da system learn map these inferred states reward predictions using td new model explain previously data responses da neurons face temporal variability combining statistical model based learning physiologically grounded td theory into contact physiology insights about behavior had previously been confined more abstract psychological models
consider bayesian mixture approaches predictor constructed forming weighted average hypotheses space functions while such procedures known lead optimal predictors several cases sufficiently accurate prior information available has been clear they perform prior assumptions violated paper establish data dependent bounds such procedures extending previous randomized approaches such gibbs algorithm fully bayesian setting finite sample guarantees established work enable bayesian mixture approaches settings usual assumptions bayesian paradigm fail hold moreover bounds derived directly applied non bayesian mixture approaches such bagging boosting
apply replica method statistical physics combined variational method approximate analytical computation bootstrap averages estimating generalization error demonstrate our approach regression gaussian processes compare our results averages obtained monte carlo sampling
information bottleneck ib method information theoretic formulation clustering problems given joint distribution method constructs about maximum likelihood ml mixture models standard statistical new variable defines partitions over values informative approach clustering problems paper ask two methods related define simple mapping between ib problem ml problem multinomial mixture model show under mapping problems strongly related fact uniform input distribution over large sample size problems mathematically equivalent specifically these cases every fixed point ib functional defines fixed point log likelihood moreover values functionals fixed points equal under simple transformations result these cases every algorithm solves problems induces solution other
extend recent work connection between loopy belief propagation bethe free energy constrained minimization bethe free energy turned into unconstrained saddle point problem both converging double loop algorithms standard loopy belief propagation interpreted attempts solve saddle point problem stability analysis leads us conclude stable fixed points loopy belief propagation local minima bethe free energy perhaps surprisingly need case minima unstable fixed points illustrate example discuss implications
paper gives distribution free concentration inequalities missing mass error rate histogram rules negative association methods used reduce these concentration problems concentration questions about independent sums although sums independent they highly heterogeneous such highly heterogeneous independent sums cannot analyzed using standard concentration inequalities such inequality bound inequality inequality theorem
classification trees most popular types classifiers ease implementation interpretation being among their attractive features despite widespread use classification trees theoretical analysis their performance paper show new family classification trees called dyadic classification trees dcts near optimal minimax sense broad range classification problems demonstrates other schemes eg neural networks support vector machines cannot perform significantly better than dcts many cases show near optimal performance attained linear number training data complexity growing pruning algorithms moreover performance dcts benchmark datasets compares favorably standard cart generally more computationally intensive does possess similar near optimality properties our analysis theoretical results structural risk minimization pruning rule dcts based
paper analyze relationships between eigenvalues mm gram matrix kernel corresponding sample xm drawn density px eigenvalues corresponding continuous bound differences between two spectra provide performance bound kernel pca
new family kernels statistical learning introduced exploits geometric structure statistical models based equation riemannian manifold defined fisher information metric information diffusion kernels generalize gaussian kernel euclidean space provide natural way combining generative statistical modeling non parametric discriminative learning special case kernels give new approach applying kernel based learning algorithms discrete data bounds covering numbers new kernels proved using spectral theory differential geometry experimental results presented text classification
population based incremental learning shown require sensitive scaling its learning rate learning rate scale system size problem dependent way shown two problems learning rate vanish exponentially system size smooth function learning rate vanish like square root system size two methods proposed removing sensitivity learning dynamics obeys detailed balance shown give consistent performance over entire range learning rates analog mutation shown require learning rate scales inverse system size but problem independent
lot learning machines hidden variables used information science have singularities their parameter spaces singularities fisher information matrix becomes degenerate resulting learning theory regular statistical models does hold recently proven true parameter contained singularities coefficient bayes generalization error equal pole function kullback information paper under condition true parameter almost but contained singularities show two results dimension parameter inputs hidden units larger than three region true parameters generalization error larger than those regular models otherwise true parameter generalization error smaller than those regular models symmetry generalization error training error does hold singular models general
investigate data based procedures selecting kernel learning support vector machines provide generalization error bounds estimating complexities corresponding function classes particular obtain complexity bound function classes induced kernels given eigenvectors ie allow vary spectrum keep eigenvectors fix bound logarithmic factor than complexity function class induced single kernel optimizing margin over such classes leads overfitting thus propose suitable way class use efficient algorithm solve resulting optimization problem present preliminary experimental results compare them alignment based approach
applied statistical mechanics inverse problem linear mapping investigate physics optimal used replica symmetry breaking technique toy model demonstrate result rate distortion function widely known theoretical limit compression fidelity criterion derived numerical study shows sparse constructions model provide suboptimal
distance based conditional model ranking presented use classification ranking model extension model generalizes classifier combination methods used several ensemble learning algorithms including error correcting output codes discrete adaboost logistic regression algebraic structure ranking leads simple bayesian interpretation conditional model its special cases addition unifying view framework suggests probabilistic interpretation error correcting output codes extension beyond binary coding scheme
show two related given classi er consists weighted sum features large margin construct stochastic classi er larger training error rate stochastic classi er has future error rate bound depends margin distribution independent size base hypothesis class new true error bound classi ers margin simpler functionally more data dependent than previous bounds
consider loopy belief propagation approximate inference probabilistic graphical models limitation standard algorithm clique marginals computed loops graph overcome limitation introduce belief propagation belief propagation formulated terms family approximate free includes bethe free energy naive mean field free special cases using linear response correction clique marginals scale parameters tuned simulation results illustrate potential merits approach
although study clustering centered around intuitively goal has been difficult develop unified framework reasoning about technical level diverse approaches clustering research community here suggest formal perspective difficulty finding such form theorem set three simple properties show clustering function satisfying three relaxations these properties interesting trade work well studied clustering techniques such single sum pairs means median
investigate generalization performance learning prob hilbert function spaces introduce concept scale sensitive effective data dimension show characterizes con rate underlying learning problem using concept naturally extend results parametric estimation problems finite dimensional spaces nonparametric kernel learning methods de upper bounds generalization performance show resulting convergent rates optimal under various circumstances
prototypes based algorithms commonly used reduce computational complexity nearest nn classifiers paper discuss theoretical aspects such algorithms theory side present margin based generalization bounds suggest these kinds classifiers more accurate nn rule furthermore derived training algorithm selects good set prototypes using large margin principles show years old learning vector quantization lvq algorithm emerges naturally our framework
work study information filtering model relevance labels associated sequence feature vectors unknown probabilistic linear function building analysis restricted version our model derive general filtering rule based margin ridge regression estimator while our rule may observe label vector vector relevant experiments real world document filtering problem show performance our rule close line classifier allowed observe labels these empirical results theoretical analysis consider randomized variant our rule prove its expected number mistakes never much larger than optimal filtering rule hidden linear model
consider problem choosing kernel suitable estimation using gaussian process estimator support vector machine novel solution presented involves defining reproducing kernel hilbert space space kernels itself utilizing analog classical theorem problem choosing kernel parameterized family kernels eg varying width reduced statistical estimation problem akin problem minimizing regularized risk functional various classical settings model kernel selection special cases our framework
comparison other sensory functional properties cells primary auditory cortex yet well understood recent attempts obtain generalized description auditory cortical responses have often upon characterization receptive field amounts model function srf linear spectrogram stimulus well such model account neural responses first stages auditory cortical processing answer question develop novel methodology evaluating fraction stimulus related response power population captured given type srf model use technique show layers primary auditory cortex models account more than stimulus related power neural responses
direct neural control external devices such computer displays requires accurate decoding neural activity representing continuous movement develop real time control system using spiking activity approximately neurons recorded electrode array arm area primary motor cortex contrast previous work develop control theoretic approach explicitly models motion hand probabilistic relationship between motion mean firing rates cells bins focus realistic control task subject move randomly placed targets computer monitor encoding decoding neural data achieved kalman filter has number advantages over previous linear filtering techniques particular kalman filter hand trajectories off line experiments more accurate than previously reported results model provides insights into nature neural coding movement
inner product operators often referred kernels statistical learning define mapping input space into feature space focus paper construction biologically motivated kernels cortical activities kernels derive termed map spike count sequences into abstract vector space perform various prediction tasks discuss detail derivation describe efficient algorithm computing their value two sequences neural population spike counts demonstrate merits our modeling approach using various standard kernels task predicting hand movement velocities cortical recordings our experiments kernels tested outperform standard scalar product used regression consistently achieving best performance
do cortical neurons represent acoustic environment question often addressed simple stimuli such tone such stimuli have advantage yielding easily interpreted answers but have disadvantage they may fail complex higher order neuronal response properties here adopt alternative approach neuronal responses complex acoustic stimuli including animal music have used vivo whole cell methods rat auditory cortex record subthreshold membrane potential fluctuations elicited these stimuli whole cell recording reveals total synaptic input neuron other neurons circuit instead its output sparse binary spike train conventional single unit physiological recordings whole cell recording thus provides much richer source information about neurons response many neurons robustly reliably complex stimuli our ensemble here analyze linear component receptive field transformation sound represented its time varying spectrogram neurons membrane potential find has rich dynamical structure including excitatory regions general prediction simple tuning curve find many cases much neurons response although related stimulus cannot predicted linear component indicating presence yet nonlinear response properties
show two important properties primary visual cortex emerge principle temporal coherence applied natural image sequences properties simple cell like receptive fields complex cell like simple cell outputs emerge apply two different approaches temporal coherence first approach extract receptive fields whose outputs temporally coherent possible approach yields simple cell like receptive fields oriented localized multiscale thus temporal coherence alternative sparse coding modeling emergence simple cell receptive fields second approach based two layer statistical generative model natural image sequences addition modeling temporal coherence individual simple cells model includes inter cell temporal dependencies estimation model natural data yields both simple cell like receptive fields complex cell like simple cell outputs completely unsupervised learning both layers generative model estimated simultaneously scratch significant improvement earlier statistical models early vision layer has been learned others have been fixed priori
consider statistical framework learning class networks spiking neurons our aim show optimal local learning rules readily derived once neural dynamics desired functionality neural assembly have been specified contrast other models assume sub optimal learning rules within framework derive local rules learning temporal sequences model spiking neurons demonstrate its superior performance correlation hebbian based approaches further show include mechanisms such synaptic depression outline framework readily learning networks highly complex spiking neurons stochastic release mechanism considered implications complexity learning discussed
inference adaptation noisy changing rich sensory environments variety specific sorts variability experimental theoretical studies suggest these different forms variability play different behavioral neural computational roles may reported different notably neuromodulatory systems here refine our previous theory role cortical inference terms expected uncertainty theory norepinephrine terms uncertainty suggest norepinephrine reports divergence bottom up inputs top down interpretations influence inference plasticity illustrate proposal using adaptive factor analysis model
single unit activity awake monkeys shows marked dependence expected reward behavior present computational model neurons principal neurons assess hypothesis direct neuromodulatory ects dopamine through activation receptors reward dependency neuron activity dopamine release results cation key ion currents leading emergence peak ring rate but introduces temporal state dependence models response thus improving temporally correlated inputs
analyze convergence properties three spike triggered data analysis techniques our results obtained setting possibly multidimensional linear nonlinear ln cascade model stimulus driven neural activity start giving exact rate convergence results common spike triggered average sta technique next analyze spike triggered covariance method variants have been recently exploited successfully bialek colleagues these first two methods suffer conditions their convergence therefore introduce estimator ln model parameters designed consistent under general conditions provide algorithm computation estimator derive its rate convergence close brief discussion efficiency these estimators application data recorded primary motor cortex awake behaving primates
what determines axonal branches hypothesis axonal has evolved minimize signal propagation delays while keeping arbor volume minimum show general cost function optimal branching law derivation relies fact conduction speed scales axon power axons axons test branching law available experimental data find reasonable agreement
population neurons typically exhibits broad diversity responses sensory inputs intuitive notion functional classification cells clustered so most diversity captured identity clusters rather than individuals within clusters show intuition made precise using information theory without need introduce metric space stimuli responses applied retinal ganglion cells approach classical results but provides clear evidence beyond those identified previously further find each ganglion cells functionally unique even within same subclass few spikes needed reliably distinguish between cells
key challenge neural modeling explain continuous stream multi modal input rapidly changing environment processed recurrent circuits integrate fire neurons real time propose new computational model based principles high dimensional dynamical systems combination statistical learning theory implemented generic evolved found recurrent circuitry
adaptation ubiquitous neural psychological phenomenon implications although basic form plasticity has computational theory main variety paper study adaptation perspective factor analysis technique unsupervised learning use factor analysis re interpret standard view adaptation apply our new model recent data adaptation domain face discrimination
re mapping patterns order their distribution may greatly simplify both structure training classi ers here properties such map obtained running few steps discrete time dynamical system explored system called digital lobe because inspired recent studies lobe structure olfactory system pattern spreading properties well its average behavior function its few design parameters analyzed extending previous results van furthermore technique adapting parameters initial design order obtain noise rejection behavior suggested our results demonstrated number simulations
cortical synaptic plasticity depends relative timing preand postsynaptic spikes temporal pattern presynaptic spikes postsynaptic spikes study hypothesis cortical synaptic plasticity does associate individual spikes but rather whole firing episodes depends these episodes start long they last but little possible timing individual spikes here present mathematical background such study standard methods hidden markov models used define what firing episodes estimating probability being such requires knowledge past spikes but future spikes show construct causal learning rule depends past spikes but preand postsynaptic firing episodes future spikes show learning rule agrees features synaptic plasticity layers rat visual cortex nature
unified biophysically motivated calcium dependent learning model has been shown account various rate based spike time dependent paradigms inducing synaptic plasticity here investigate properties model multi synapse neuron receives inputs different spike train statistics addition present physiological form activity driven regulation mechanism essential robustness model neuron thus implemented develops stable selective receptive fields given various input statistics
maximally informative dimensions analyzing neural responses natural signals bialek center theoretical neurobiology department physiology university california san san california center neural science new york university new york ny department physics princeton university princeton new propose method allows rigorous statistical analysis neural responses natural stimuli non gaussian exhibit strong correlations have mind model neurons selective small number stimulus dimensions out high dimensional stimulus space but within subspace responses arbitrarily nonlinear therefore maximize mutual information between sequence elicited neural responses ensemble stimuli has been projected trial directions stimulus space procedure done iteratively increasing number directions respect information maximized those directions allow recovery information between spikes full stimuli describe relevant subspace dimensionality relevant subspace indeed much smaller than overall stimulus space may become experimentally feasible map out neurons input output function even under fully natural stimulus conditions methods based correlations functions reverse correlation spike triggered covariance require simplified stimulus statistics use them
cortex uses spike timing compute timing spikes robust perturbations based recent framework provides simple criterion determine whether spike sequence produced generic network sensitive initial conditions numerical simulations variety network architectures argue within limits set our model neuron unlikely precise sequences spike used computation under conditions typically found cortex
responses cortical sensory neurons notoriously variable number spikes evoked identical stimuli varying significantly trial trial variability most often interpreted noise purely sensory system paper propose alternative view variability related uncertainty about world parameters inherent sensory stimulus specifically responses population neurons interpreted stochastic samples posterior distribution latent variable model addition giving theoretical arguments supporting such representational scheme provide simulations suggesting aspects response variability might understood framework
psychophysical data suggest temporal stimulus amplitude play role perceptual segregation concurrent sounds particular detection signal significantly improved adding amplitude modulation spectral envelope competing noise perceptual phenomenon known release despite obvious influence temporal structure perception complex auditory scenes physiological mechanisms contribute auditory well known recent physiological study colleagues has demonstrated enhanced cortical representation auditory signals modulated noise our study evaluates these like response patterns perspective auditory edge detection neuron shown simple neural model detection amplitude transients reproduce physiological data et al but light previous results variety physiological phenomena related perceptual segregation concurrent sounds
framework introduced encoding accuracy ability population neurons upon simultaneous presentation multiple stimuli minimal square estimation errors obtained fisher information analysis abstract space comprising features stimuli even simplest case linear superposition responses gaussian tuning symmetries space different those case single stimulus analysis allows quantitative description attentional effects extended include neural nonlinearities such receptive fields
essential step understanding function sensory nervous systems characterize accurately possible stimulus response function srf neurons process sensory information increasingly common experimental approach present rapidly varying complex stimulus animal while recording responses more neurons directly estimate functional transformation input accounts neuronal firing estimation techniques usually employed such wiener filtering other correlation based estimation wiener volterra kernels equivalent maximum likelihood estimation gaussian output noise regression model explore use bayesian evidence optimization techniques condition these estimates show learning hyperparameters control smoothness sparsity transfer function possible improve dramatically quality srf estimates measured their success predicting responses novel input
present method distinguish direct connections between two neurons common input other neurons distinction computed spike times two neurons response white noise stimulus although method based highly idealized linear nonlinear approximation neural response demonstrate via simulation approach work more realistic integrate fire neuron model propose approach analysis may yield viable tools reconstructing stimulus driven neural networks data gathered neurophysiology experiments
certain simple images known trigger percept transparency input image perceived sum two images ix ix ix percept first why do choose more complicated description two images rather than simpler explanation second given infinite number ways express sum two images do compute best decomposition here suggest transparency rational percept system adapted statistics natural scenes present probabilistic model images based qualitative statistics derivative filters corner detectors natural scenes use model find most probable decomposition novel image optimization performed using loopy belief propagation show our model computes correct synthetic images discuss its application real images
goal work accurately detect localize boundaries natural scenes using local image measurements formulate features respond characteristic changes texture associated natural boundaries order combine information these features optimal way classifier trained using human labeled images ground truth present precision recall curves showing resulting detector outperforms existing approaches
dimensionality reduction techniques such principal component analysis factor analysis used discover linear mapping between high dimensional data samples points lower dimensional subspace frey introduced mixture transformation invariant component account global transformations such translations rotations perform clustering learn local appearance deformations dimensionality reduction due computational requirements em algorithm learning model dimensionality data sample practical most applications paper demonstrate fast fourier transforms reduce computation order log speedup show effectiveness various applications tracking video textures clustering video sequences object recognition object detection images
present ongoing work project automatic recognition spontaneous facial actions spontaneous facial expressions differ substantially posed expressions similar continuous spontaneous speech differs isolated words produced command previous methods automatic facial expression recognition assumed images collected controlled environments subjects faced camera people often turn their automatic recognition spontaneous facial behavior requires methods handling out image plane head rotations here explore approach based warping images into canonical views evaluated performance approach front end spontaneous expression recognition system using support vector machines hidden markov models system employed general purpose learning mechanisms applied recognition facial movement system tested recognition set facial actions defined facial action coding system showed tracking warping followed machine learning techniques directly applied images viable promising technology automatic facial expression recognition aspect approach presented here information about movement dynamics emerged out filters derived statistics images
extraction single high quality image set images important problem arises such remote sensing medical imaging extraction still images video typical approaches based use cross correlation images followed inversion transformation unknown high resolution image observed low resolution images using regularization resolve ill posed nature inversion process paper develop bayesian treatment super resolution problem likelihood function image parameters based over unknown high resolution image approach allows us estimate unknown point spread function tractable through
recent algorithms sparse coding independent component analysis ica have demonstrated localized features learned natural images these approaches do take image transformations into account result they produce image codes redundant because same feature learned multiple locations describe algorithm sparse coding based bilinear generative model images explicitly modeling interaction between image features their transformations bilinear approach helps reduce redundancy image code provides basis vision present results demonstrating bilinear sparse coding natural images explore extension model capture spatial relationships between independent features object thereby providing new framework parts based object recognition
problem super resolution involves generating feasible higher resolution images eye realistic given low resolution image might attempted using simple filters smoothing out high resolution blocks through applications substantial prior information used imply textures shapes occur images paper describe approach lies between two generic unsupervised method domains but goes beyond simple smoothing methods what achieves use dynamic tree like architecture model high resolution data approximate conditioning low resolution image achieved through mean field approach
introduced linear statistical model joint color changes images due variation lighting certain non geometric camera parameters did measuring mappings colors image scene colors another image same scene under different lighting conditions here increase flexibility color flow model allowing flow coefficients vary according low order polynomial over image allows us better fit smoothly varying lighting conditions well curved surfaces without our model much capacity show results image matching removal detection
accurate representation articulated motion challenging problem machine perception several successful tracking algorithms have been developed model human body articulated tree propose learning based method creating such articulated models observations multiple rigid motions paper concerned recovering topology articulated model rigid motion constituent segments known our approach based finding maximum likelihood tree shaped factorization joint probability density function pdf rigid segment motions topology graphical model formed factorization corresponds topology underlying articulated body demonstrate performance our algorithm both synthetic real motion capture data
neurons fly brain sensitive typical optic flow patterns generated during self motion study examine whether simplified linear model these neurons used estimate self motion optic flow present theory construction estimator consisting linear combination optic flow vectors incorporates prior knowledge both about distance distribution environment about noise self motion statistics sensor estimator tested vision sensor experiments show proposed approach leads accurate robust estimates rotation rates whereas translation estimates turn out less reliable
describe method learning sparse multiscale image representations using sparse prior distribution over basis function coefficients prior consists mixture gaussian delta function thus coefficients have exact zero values coefficients image computed sampling resulting posterior distribution gibbs learned basis similar pyramid basis yields slightly higher snr same number active coefficients denoising using learned image model demonstrated standard test images results compare favorably other denoising methods
goal low level vision estimate underlying scene given observed image real world scenes eg shapes complex high dimensional representations hard estimate store propose low dimensional representation called scene relies image itself describe complex scene configurations shape example these regression coefficients predict shape image data describe benefits representation show two uses illustrating their properties improve stereo shape estimates learning shape low resolution applying them full resolution shape implicitly contain information about lighting materials use them segmentation
present algorithm uses multiple cues recover shading reflectance intrinsic images single image using both color information classifier trained recognize gray scale patterns each image derivative classified being caused shading change surfaces reflectance generalized belief propagation used propagate information areas correct classification clear areas ambiguous show results real images
address question feature selection context visual recognition shown efficient computational infomax principle nearly optimal minimum bayes error sense concept marginal diversity introduced leading generic principle feature selection principle maximum marginal diversity extreme computational simplicity relationships between infomax maximization marginal diversity identified existence family classification procedures near optimal bayes error sense feature selection does require combinatorial search family light recent studies statistics natural images suggests visual recognition problems subset
propose model natural images probability image proportional product probabilities filter outputs system find sparse features using distribution model each filter output distribution used model combined outputs sets neurally adjacent filters system learns topographic map orientation spatial frequency location filters change smoothly across map even though maximum likelihood learning intractable our model product form allows relatively efficient learning procedure works well even highly overcomplete sets filters once model has been learned used prior derive iterated wiener filter purpose denoising images
present hierarchical bayesian model learning efficient codes higher order structure natural images model non linear generalization independent component analysis replaces standard assumption independence joint distribution coefficients distribution adapted variance structure coefficients efficient image basis offers novel description higherorder image structure provides way learn coarse coded representations abstract image properties such object location scale texture
paper presents kernel method allows combine color shape information appearance based object recognition doesnt require de new common representation but use power kernels combine di erent representations together manner these results achieved using results statistical mechanics spin combined markov random via kernel functions experiments show increase recognition rate up respect conventional strategies
consider data images containing views multiple objects our task learn about each objects present images task approached factorial learning problem each image explained model each objects present correct parameters major problem learning factorial model number objects increases combinatorial explosion number configurations need considered develop method extract object models sequentially data making use robust statistical method thus avoiding combinatorial explosion present results showing successful extraction objects real images
problem approximating product several gaussian mixture distributions arises number contexts including nonparametric belief propagation nbp inference algorithm training product experts models paper develops two multiscale algorithms sampling product gaussian mixtures compares their performance existing methods first multiscale variant previously proposed monte carlo techniques comparable theoretical guarantees but improved empirical convergence rates second makes use approximate kernel density evaluation methods construct fast approximate guaranteed sample points within parameter their true probability compare both multiscale set computational examples motivated nbp demonstrating significant improvements over existing methods
provide compact generative representation sequential activity number individuals within group tradeoff between definition individual specific global models paper proposes linear time distributed model finite state symbolic sequences representing traces individual user activity making assumption heterogeneous user behavior may explained relatively small number common structurally simple behavioral patterns may randomly user specific proportion results empirical study three different sources user traces indicates modelling approach provides efficient representation scheme reflected improved prediction performance well providing intuitively representations
address problem learning topic hierarchies data model selection problem domain large collection possible trees use take bayesian approach generating appropriate prior via distribution partitions refer nested process nonparametric prior allows arbitrarily large branching factors readily growing data build hierarchical topic model combining prior likelihood based hierarchical variant latent dirichlet allocation illustrate our approach simulated data application modeling nips
typical classification tasks seek function assigns label single object kernel based approaches such support vector machines svms maximize margin confidence classifier method choice many such tasks their both ability use high dimensional feature spaces their strong theoretical guarantees many real world tasks involve sequential spatial structured data multiple labels assigned existing kernel based methods ignore structure problem assigning labels independently each object much useful information conversely probabilistic graphical models such markov networks represent correlations between labels exploiting problem structure but cannot handle high dimensional feature spaces lack strong theoretical generalization guarantees paper present new framework combines advantages both approaches maximum margin markov networks incorporate both kernels efficiently deal high dimensional features ability capture correlations structured data present efficient algorithm learning networks based compact quadratic program formulation provide new theoretical bound generalization structured domains experiments task handwritten character recognition collective classification demonstrate significant gains over previous approaches
knowledge about local invariances respect given pattern transformations greatly improve accuracy classification previous approaches either based regularisation generation virtual transformed examples develop new framework learning linear classifiers under known transformations based semidefinite programming present new learning algorithm semidefinite programming machine able find maximum margin hyperplane training examples polynomial trajectories instead single points solution found sparse dual variables allows identify those points trajectory minimal real valued output virtual support vectors extensions segments trajectories more than transformation parameter learning kernels discussed experiments use taylor expansion locally approximate invariance pixel images find improvements over known methods
paper presents method learning distance metric relative comparison such closer than taking support vector machine svm approach develop algorithm provides flexible way describing qualitative training data set constraints show such constraints lead convex quadratic programming problem solved adapting standard methods svm training empirically evaluate performance modelling flexibility algorithm collection text documents
standard norm svm known its good performance paper consider norm svm argue norm svm may have advantage over standard norm svm especially redundant noise features propose algorithm computes whole solution path norm svm hence facilitates adaptive selection tuning parameter norm svm
common way image denoising project noisy image subspace admissible images made instance pca major drawback method pixels updated projection even few pixels corrupted noise occlusion update identified pixels identification updating noisy pixels formulated linear program solved efficiently especially apply trick directly specify fraction pixels reconstructed moreover extend linear program able exploit prior knowledge occlusions often appear contiguous blocks eg faces basic idea penalize boundary points points area differently able show property extended leading method easy use experimental results demonstrate power our approach
learning ambiguous training data highly relevant many applications present new learning algorithm classification problems labels associated sets pattern instead individual patterns multiple instance learning special case our approach based generalization linear programming boosting uses results programming generate successively stronger linear relaxations discrete non convex problem
class transduction problem formulated vapnik involves finding separating hyperplane labelled data set maximally distant given set unlabelled test points form problem has exponential computational complexity size working set so far has been means integer programming techniques do scale reasonable problem sizes local search procedures paper present relaxation task based semidefinite programming resulting convex optimization problem has polynomial complexity size data set results encouraging mid sized data sets cost still high large scale problems due high dimensional search space end restrict feasible region introducing approximation based solving approximation computational cost algorithm such problems more than points treated
propose novel method dimensionality reduction supervised learning given regression classification problem wish predict variable vector treat problem dimensionality reduction finding low dimensional effective subspace retains statistical relationship between show problem formulated terms conditional independence turn formulation into optimization problem characterize notion conditional independence using covariance operators reproducing kernel hilbert spaces allows us derive contrast function estimation effective subspace unlike many conventional methods proposed method requires neither assumptions marginal distribution nor parametric model conditional distribution
clustering aims extracting hidden structure dataset while problem finding compact clusters has been widely studied literature extracting arbitrarily formed structures considered much harder problem paper present novel clustering algorithm problem two step procedure first data transformed such way structures become compact ones second step these new objects clustered optimizing compactness based criterion advantages method over related approaches robustness properties compactness based criteria naturally transfer problem extracting structures leading model highly robust against outlier objects ii transformed distances induce mercer kernel allows us hard clustering problem iii new method does contain free kernel formulate polynomial approximation scheme generally contrast methods like spectral clustering mean shift clustering
new feature extraction criterion maximum margin criterion proposed paper new criterion general sense combined suitable constraint actually give rise most popular feature extractor literature linear discriminate analysis lda derive new feature extractor based using different constraint does depend within class matrix such dependence major drawback lda especially sample size small nonlinear counterpart linear feature extractor established paper our preliminary experimental results face images demonstrate new feature extractors efficient stable
minimax probability machine classification mpmc framework et al builds classifiers minimizing maximum probability misclassification gives direct estimates probabilistic accuracy bound assumptions mpmc makes good estimates means covariance matrices classes exist support vector machines mpmc computationally expensive requires extensive cross validation experiments choose kernels kernel parameters give good performance paper address computational cost mpmc algorithm constructs nonlinear sparse mpmc models incrementally adding basis functions ie kernels time selecting next maximizes accuracy bound automatically chooses both kernel parameters feature weights without using computationally expensive cross validation therefore algorithm simultaneously addresses problem kernel selection feature selection ie feature weighting based solely maximizing accuracy bound experimental results indicate obtain reliable bounds well test set comparable state art classification algorithms
propose method sequential bayesian kernel regression case popular relevance vector machine rvm method automatically identifies number locations kernels our algorithm overcomes computational difficulties related batch methods kernel regression non iterative requires single pass over data thus applicable sequential data sets batch data sets algorithm based generalisation importance sampling allows design intuitively simple efficient proposal distributions model parameters comparative results two standard data sets show our algorithm compare existing batch estimation strategies
new feature selection algorithms linear threshold functions described combine backward elimination adaptive regularization method makes them particularly suitable classification microarray expression data goal obtain accurate rules depending few genes our algorithms fast easy implement they center incremental large margin algorithm allows us avoid linear quadratic higher order programming methods report preliminary experiments five known dna microarray datasets these experiments suggest multiplicative large margin algorithms tend outperform additive algorithms such svm feature selection tasks
consider question predicting nonlinear time series kernel dynamical modeling new method based kernels proposed extension linear dynamical models kernel trick used twice first learn parameters model second compute time series predicted feature space means support vector regression our model shows strong connection classic kalman filter model kernel feature space hidden state space kernel dynamical modeling tested against two benchmark time series achieves high quality predictions
principal components analysis pca most widely used techniques machine learning data mining minor components analysis less well known but play important role presence constraints data distribution paper present probabilistic model extreme components analysis maximum likelihood solution extracts optimal combination principal minor components given number components log likelihood model guaranteed larger equal than probabilistic models pca describe efficient algorithm solve globally optimal solution log convex spectra prove solution consists principal components while log concave spectra solution consists minor components general solution combination both experiments explore properties synthetic real world datasets
formulate linear dimensionality reduction semi parametric estimation problem enabling us study its asymptotic behavior generalize problem beyond additive gaussian noise unknown nongaussian additive noise unbiased non additive models
many problems information processing involve form dimensionality reduction paper introduce locality preserving projections lpp these linear projective maps arise solving variational problem optimally preserves neighborhood structure data set lpp should seen alternative principal component analysis pca classical linear technique data along directions maximal variance high dimensional data lies low dimensional manifold embedded ambient space locality preserving projections obtained finding optimal linear approximations eigenfunctions laplace operator manifold result lpp shares many data representation properties nonlinear techniques such laplacian eigenmaps locally linear embedding yet lpp linear more crucially defined everywhere ambient space rather than training data points out illustrative examples high dimensional data sets
search engine has huge success its web page ranking algorithm exploits global rather than local structure web using random walks here propose simple universal ranking algorithm data euclidean space such text image data core idea our method rank data respect intrinsic manifold structure revealed great amount data encouraging experimental results synthetic image text data illustrate validity our method
several unsupervised learning algorithms based provide either embedding clustering given training points straightforward extension out sample examples short eigenvectors paper provides unified framework extending local linear embedding isomap laplacian eigenmaps multi dimensional scaling dimensionality reduction well spectral clustering framework based seeing these algorithms learning eigenfunctions data dependent kernel numerical experiments show generalizations performed have level error comparable variability embedding algorithms due choice training data
significant progress clustering has been achieved algorithms based pairwise between datapoints particular spectral clustering methods have advantage being able arbitrarily shaped clusters based efficient eigenvector calculations spectral methods lack straightforward probabilistic interpretation makes difficult automatically set parameters using training data paper use previously proposed typical cut framework pairwise clustering show equivalence between calculating typical cut inference undirected graphical model show clustering problems hundreds datapoints exact inference may still possible more complicated datasets show loopy belief propagation bp generalized belief propagation give excellent results challenging clustering problems use graphical models derive learning algorithm affinity matrices based labeled data
approximation structure plays important role inference loopy graphs tractable structure tree approximations have been utilized variational method ghahramani jordan sequential projection method frey et al belief propagation represents each factor graph product single node messages paper belief propagation extended represent factors tree approximations way expectation propagation framework each factor message pairs nodes tree structure result more accurate inferences more convergence than ordinary belief propagation lower cost than variational trees double loop algorithms
maximisation information transmission over noisy channels common generally computationally difficult problem approach difficulty computing mutual information noisy channels using variational approximation resulting im algorithm em algorithm yet mutual information opposed likelihood apply method several practical examples including linear compression population encoding
online incremental gradient backpropagation algorithm widely considered method solving large scale neural network nn learning problems contrast show appropriately implemented iterative batch mode block mode learning method much faster example three times faster uci letter classification problem outputs data items parameters two hidden layer multilayer perceptron times faster nonlinear regression problem arising color prediction outputs data items parameters neuro fuzzy modular network three principal ingredients our algorithm following first use scaled region regularization inner outer iteration solve associated nonlinear least squares problem inner iteration performs truncated method second employ implicit sparse hessian matrix vector algorithm construct subspaces used solve truncated update third exploit sparsity matrices resulting nns having many outputs
consider situations training data computing resources argue suitably designed online learning algorithms asymptotically outperform batch learning algorithm both theoretical experimental presented
online algorithms classification often require vast amounts memory computation time employed conjunction kernel functions paper describe analyze simple approach fly reduction number past examples used prediction experiments performed real datasets show using proposed algorithmic approach single epoch competitive support vector machine svm although latter being batch algorithm each training example multiple times
work presents architecture based perceptrons recognize phrase structures online learning algorithm train perceptrons together recognition strategy applies learning two layers filtering layer reduces search space identifying plausible phrase candidates ranking layer recursively builds optimal phrase structure provide recognition based feedback rule reflects each local function its errors global point view allows train them together online perceptrons syntactic parsing problem recognition clause hierarchies improves state art results advantages our global training method over optimizing each function locally independently
paper sparse representation factorization data matrix first discussed overcomplete basis matrix estimated using means method have proved estimated overcomplete basis matrix sparse solution coefficient matrix minimum norm unique probability obtained using linear programming algorithm comparisons norm solution norm solution presented used analysis blind source separation bss next apply sparse matrix factorization approach bss overcomplete case generally sources sufficiently sparse perform blind separation time frequency domain after preprocessing observed data using wavelet transformation third eeg experimental data analysis example presented illustrate usefulness proposed approach demonstrate its performance two almost independent components obtained sparse representation method selected phase synchronization analysis their periods significant phase synchronization found related tasks finally review approach state areas require further study
recently relevance vector machines rvm have been sparse bayesian learning sbl framework perform supervised learning using weight prior sparsity representation methodology incorporates additional set hyperparameters governing prior each weight specific approximation full over weights hyperparameters despite its empirical success rigorous motivation particular approximation currently available address issue demonstrate sbl application rigorous variational approximation full model prior dual form formulation assuming leads natural intuitive explanations why sparsity achieved practice
describe nonparametric bayesian approach generalizing few labeled examples guided larger set unlabeled objects assumption latent tree structure domain tree distribution over trees may inferred using unlabeled data prior over concepts generated mutation process inferred trees allows efficient computation optimal bayesian classification function labeled examples test our approach eight real world datasets
paper about non approximate acceleration high dimensional nonparametric operations such nearest neighbor classifiers prediction phase support vector machine classifiers attempt exploit fact even want exact answers nonparametric queries usually do need explicitly find datapoints close query but merely need ask questions about properties about set datapoints offers small amount computational investigate much exploited paper concentrates pure nn classification prediction phase svms introduce new tree algorithms real world datasets give fold up fold compared against highly optimized traditional tree based nn these results include datasets up dimensions records show non trivial while giving exact answers
introduce class nonstationary covariance functions gaussian process gp regression nonstationary covariance functions allow model adapt functions whose smoothness varies inputs class includes nonstationary version stationary covariance regression function controlled parameter fixing advance experiments nonstationary gp regression model performs well input space two three dimensions outperforming neural network model bayesian free knot spline models competitive bayesian neural network but outperformed dimension state art bayesian free knot spline model model readily generalizes non gaussian data use computational methods gp fitting may allow implementation method larger datasets
clustering dataset right number clusters use often obvious choosing automatically hard algorithmic problem paper present improved algorithm learning while clustering means algorithm based statistical test hypothesis subset data follows gaussian distribution means runs means increasing hierarchical fashion until test accepts hypothesis data assigned each means center gaussian two key advantages hypothesis test does limit covariance data does compute full covariance matrix additionally means requires intuitive parameter standard statistical significance level present results experiments showing algorithm works well better than recent method based penalty model complexity these experiments show scoring function does penalize strongly enough models complexity
loopy belief propagation bp has been successfully used number difficult graphical models find most probable configuration hidden variables applications ranging protein image analysis would like find best configuration but rather top while problem has been solved using junction tree formalism many real world problems clique size junction tree prohibitively large work address problem finding best configurations exact inference impossible start developing new exact inference algorithm calculating best configurations uses max marginals approximate inference replace max marginals beliefs calculated using max product bp generalized bp show empirically algorithm accurately rapidly approximate best configurations graphs hundreds variables
propose non linear canonical correlation analysis cca method works mixtures linear models same way cca extends idea pca our work extends recent methods non linear dimensionality reduction case multiple same underlying low dimensional coordinates observed each different high dimensional manifold show special case our method applied single manifold reduces laplacian eigenmaps algorithm previous alignment schemes once mixture models have been estimated parameters our model estimated closed form without local optima learning experimental results illustrate approach non linear extension cca
spectral clustering refers class techniques rely similarity matrix partition points into disjoint clusters points same cluster having high similarity points different clusters having low similarity paper derive new cost function spectral clustering based measure error between given partition solution spectral relaxation minimum normalized cut problem minimizing cost function respect partition leads new spectral clustering algorithm minimizing respect similarity matrix leads algorithm learning similarity matrix develop tractable approximation our cost function based power method computing eigenvectors
consider general problem learning labeled unlabeled data often called semi supervised learning transductive inference principled approach semi supervised learning design classifying function sufficiently smooth respect intrinsic structure revealed known labeled unlabeled points present simple algorithm obtain such smooth solution our method yields encouraging experimental results number classification problems demonstrates effective use unlabeled data
paper introduce new underlying probabilistic model principal component analysis pca our formulation pca particular gaussian process prior mapping latent space observed data space show priors covariance function mappings linear model equivalent pca extend model considering less restrictive covariance functions allow non linear mappings more general gaussian process latent variable model evaluated approach high dimensional data three different data sets additionally our non linear algorithm further leading kernel pca mapping between feature spaces occurs
gaussian process gp framework regression learning nonlinear transformation gp outputs allows non gaussian processes non gaussian noise learning algorithm chooses nonlinear transformation such transformed data well modelled gp seen including preprocessing transformation integral part probabilistic modelling problem rather than ad hoc step demonstrate several real regression problems learning transformation lead significantly better performance than using regular gp gp fixed transformation
novel algorithm actively trading stocks presented while traditional universal algorithms technical trading heuristics attempt predict winners trends our approach relies statistical relations between pairs stocks market our empirical results markets provide strong evidence type technical trading market moreover best stock market doing so utilize new idea smoothing critical parameters context expert learning
discuss integration expectation maximization em algorithm maximum likelihood learning bayesian networks belief propagation algorithms approximate inference speci propose combine outer loop step convergent belief propagation algorithms step em algorithm yields approximate em algorithm essentially still double loop important advantage inner loop guaranteed converge simulations illustrate merits such approach
belief propagation cyclic graphs efficient algorithm computing approximate marginal probability distributions over single nodes neighboring nodes graph paper propose two new algorithms approximating joint probabilities arbitrary pairs nodes prove number desirable properties these estimates fulfill first algorithm propagation algorithm shown converge belief propagation converges stable fixed point second algorithm based matrix inversion experiments compare number competing methods
present new method calculating approximate marginals probability distributions defined graphs cycles based gaussian entropy bound combined semidefinite outer bound marginal combination leads log determinant maximization problem solved efficient point methods bethe approximation its generalizations optimizing arguments problem taken approximations exact marginals contrast approaches our variational problem strictly convex so has unique global optimum additional desirable feature value optimal solution guaranteed provide upper bound log partition function experimental trials performance log determinant relaxation comparable better than sum product algorithm substantial margin certain problem classes finally zero temperature limit our log determinant relaxation class well known semidefinite relaxations integer programming eg
consider question well given distribution approximated probabilistic graphical models introduce new parameter effective captures degree tradeoff between accuracy complexity approximation present simple approach analyzing achievable exploits threshold behavior graph properties provide experimental results support approach
paper addresses problem hidden graphs set noisy undirected edges present model generation observed graph includes degree based structure priors hidden graphs exact inference model intractable present efficient approximate inference algorithm compute edge appearance posteriors evaluate our model algorithm biological graph inference problem
present analysis concentration expectation phenomena layered bayesian networks use generalized linear models local conditional probabilities framework wide variety probability distributions including both discrete continuous random variables utilize ideas large deviation analysis delta method devise evaluate class approximate inference algorithms layered bayesian networks have superior asymptotic error bounds fast computation time
describe markov chain method sampling distribution hidden state sequence non linear dynamical system given sequence observations method updates states sequence simultaneously using embedded hidden markov model hmm update begins pools candidate states each time define embedded hmm whose states indexes within these pools using forward backward dynamic programming algorithm efficiently choose state sequence appropriate probabilities exponentially large number state sequences pass through states these pools illustrate method simple dimensional example example showing embedded hmm used effect state space without discretization error compare embedded hmm particle smoother more substantial problem inferring human motion traces markers
applying hidden markov models analysis massive data streams often necessary use artificially reduced set states due large part fact basic hmm estimation algorithms have quadratic dependence size state set present algorithms reduce computational bottleneck linear near linear time states embedded underlying grid parameters type state representation arises many domains particular show application traffic analysis high volume web site
models define probabilities via maximum likelihood learning typically involves using markov chain monte carlo sample models distribution markov chain started data distribution learning often works well even chain run few time steps but data distribution contains modes separated regions low density brief mcmc ensure different modes have correct relative because cannot move particles mode another show improve brief mcmc allowing long range moves suggested data distribution model approximately correct these long range moves have reasonable rate
approximation technique probabilistic inference combines exact inference sampling useful models conditioning variables leaves simpler inference problem solved paper presents sample propagation efficient implementation approximate inference large class models sample propagation tightly integrates sampling message passing junction tree named its simple appealing structure walks clusters junction tree sampling current clusters variables passing message its neighbors discuss application sample propagation conditional gaussian inference problems such switching linear dynamical systems
discrete fourier transforms other related fourier methods have been practically implementable due fast fourier transform fft many situations doing fast fourier transforms without complete data would desirable paper formulating fft algorithm belief network allows suitable priors set fourier coefficients furthermore efficient belief propagation methods between clusters four nodes enable fourier coefficients inferred missing data estimated near log time total given missing data points method compared number common approaches such setting missing data zero interpolation tested generated data fourier analysis damaged audio signal
present novel method approximate inference bayesian models regularized risk functionals based propagation mean variance derived laplace approximation conditional probabilities distributions much akin expectation propagation jointly normal case latter belief propagation whereas general case provides optimization strategy containing support vector chunking bayes committee machine gaussian process chunking special cases
consider problem reconstructing patterns feature map learning algorithms using kernels operate reproducing kernel hilbert space rkhs express their solutions terms input points mapped into rkhs introduce technique based kernel principal component analysis regression reconstruct corresponding patterns input space pre images review its performance several applications requiring construction pre images introduced technique avoids difficult andor unstable numerical optimization easy implement unlike previous methods permits computation pre images discrete input spaces
present modified version perceptron learning algorithm solves semidefinite programs polynomial time algorithm based following three observations semidefinite programs linear programs many linear constraints ii every linear program solved sequence constraint satisfaction problems linear constraints iii general perceptron learning algorithm solves constraint satisfaction problem linear constraints many updates combining probabilistic algorithm average increases size region results probabilistic algorithm solving runs polynomial time present preliminary results demonstrate algorithm works but competitive state art point methods
density estimation gaussian mixture models popular generative technique used clustering develop framework incorporate side information form equivalence constraints into model estimation procedure equivalence constraints defined pairs data points indicating whether points arise same source positive constraints different sources negative constraints such constraints gathered automatically learning problems natural form supervision others estimation model parameters present closed form em procedure handles positive constraints generalized em procedure using markov net handles negative constraints using publicly available data sets demonstrate such side information lead considerable improvement clustering tasks our algorithm two other suggested methods using same type side information
novel approach combining clustering feature selection presented implements strategy feature selection sense features directly selected optimizing discriminative power used partitioning algorithm technical side present efficient optimization algorithm guaranteed local convergence property free parameter method selected resampling based stability analysis experiments real world datasets demonstrate our method able infer both meaningful partitions meaningful subsets features
describe procedure finds hierarchical clustering cost function use hierarchical extension means cost our local moves tree node show these accomplished efficiently exploiting special properties squared euclidean distances using techniques scheduling algorithms
propose information theoretic clustering approach incorporates pre known partition data identify common clusters cut across given partition standard clustering setting formation clusters guided single source feature information newly utilized pre partition factor introduces additional bias impact features whenever they become correlated known partition resulting algorithmic framework applied successfully synthetic data well identifying text based cross correspondences
label ranking task inferring total order over set labels each given instance present general framework batch learning label ranking functions supervised data assume each instance training data associated list preferences over label set do assume list either complete consistent enables us accommodate variety ranking problems contrast general form supervision our goal learn ranking function induces total order over entire set labels special cases our setting categorization hierarchical classification present general boosting based learning algorithm label ranking problem prove lower bound progress each boosting iteration applicability our approach demonstrated set experiments large scale text corpus
most machine learning researchers perform quantitative experiments estimate generalization error compare algorithm performances order draw statistically conclusions important estimate uncertainty such estimates paper studies estimation uncertainty around fold cross validation estimator main theorem shows exists universal unbiased estimator variance fold cross validation analysis based covariance matrix errors helps better understand nature problem shows naive estimators may underestimate variance numerical experiments
bootstrap has become popular method exploring model structure uncertainty our experiments artificial realworld data demonstrate graphs learned bootstrap samples biased towards complex graphical models accounting bias hence essential eg exploring model uncertainty find bias tied well known spurious induced bootstrap leading order bias correction equals half penalty model complexity demonstrate effect simple bias correction our experiments relate bias bias estimator entropy well difference between expected test training errors graphical model asymptotically equals penalty rather than half
pairwise coupling popular multi class classification method combines together pairwise comparisons each pair classes paper presents two approaches obtaining class probabilities both methods reduced linear systems easy implement show conceptually experimentally proposed approaches more stable than two existing popular methods
pattern classification tasks errors introduced because differences between true model obtained via model estimation using likelihood ratio based classification possible correct finding class pair specific terms adjust likelihood ratio directly make class pair preference relationships work introduce new methodology makes necessary corrections likelihood ratio specifically those necessary achieve perfect classification but perfect likelihood ratio correction new corrections while weaker than previously reported such analytically challenging they involve discontinuous functions therefore requiring several approximations test number these new schemes speech recognition task well uci machine learning data sets results show using bias terms calculated new way classification accuracy substantially improve over both baseline over our previous results
although trained classifiers usually more accurate labeled training data previous work has shown training data limited generative classifiers out perform them paper describes hybrid model high dimensional subset parameters trained maximize generative likelihood another small subset parameters trained maximize conditional likelihood give sample complexity bound showing order fit discriminative parameters well number training examples required depends number feature feature set size experimental results show hybrid models provide lower test error produce better curves than either their purely generative purely discriminative counterparts discuss several advantages hybrid models further work area
propose approach learning semantics images allows us automatically image keywords retrieve images based text queries do using formalism models generation annotated images assume every image divided into regions each described continuous valued feature vector given training set images compute joint probabilistic model image features words allow us predict probability generating word given image regions may used automatically retrieve images given word query experiments show our model significantly outperforms best previously reported results tasks automatic image retrieval
paper applies fast sparse multidimensional scaling mds large graph music similarity vertices represent tracks edges represent similarity between those entities once vertices assigned locations euclidean space locations used music generate mds large sparse graphs effectively performed family algorithms called rectangular rd mds algorithms these rd algorithms operate dense rectangular slice distance matrix created calling constant number times two rd algorithms compared mds uses approximation perform mds new algorithm called fast sparse embedding uses these algorithms compare favorably laplacian eigenmaps both terms speed embedding quality
article present novel approach solving localization problem cellular networks goal estimate mobile users position based measurements signal strengths received network base our solution works building gaussian process models distribution signal strengths obtained series measurements localization stage users position estimated maximizing likelihood received signal strengths respect position investigate accuracy proposed approach data obtained within large cellular network
present software architecture robotic system mapping software capable consistent maps large many cycles represented markov random space maps acquired local range scans used identify paths using search our system has been three two people has acquired maps detail accuracy
key issue supervised protein classification representation input sequences amino recent work using string kernels protein data has achieved state art classification performance such representations based labeled data examples known structures organized into structural classes while practice unlabeled data far more work develop simple scalable cluster kernel techniques incorporating unlabeled data into representation protein sequences show our methods greatly improve classification performance string kernels outperform standard approaches using unlabeled data such adding close positive examples training data achieve equal superior performance previously presented cluster kernel methods while achieving far greater computational efficiency
present novel strategy automatically programs given sampled data thousands actual user runs our goal those features most correlated accomplished maximizing appropriately defined utility function has intuitive heuristics demonstrate able deal various types occur real programs
examine use hidden markov hidden semi markov models automatically segmenting waveform into its constituent waveform features wavelet transform used generate overcomplete representation signal more appropriate subsequent modelling show state implicit standard hidden markov model ill suited those real ecg features investigate use hidden semi markov models improved state duration modelling
part environmental observation forecasting system sensors information physical dynamics changes these sensors particularly gradually degrades sensor response critical data automatic fault detectors have capability identify bio fouling early minimize data loss development classifiers bio fouling onset examples variability bio fouling signature solve these problems take novelty detection approach incorporates parameterized bio fouling model these detectors identify occurrence bio fouling its onset time reliably human experts real time detectors during produced false yet detected episodes sensor degradation before field scheduled these sensors initial through our bio fouling detectors have essentially amount useful data coming sensors
paper present generative latent variable model rating based collaborative filtering called user rating profile model generative process designed produce complete user rating profiles assignment rating each item each user our model represents each user mixture user mixing proportions distributed according dirichlet random variable rating each item generated selecting user item selecting rating according preference pattern associated related several models including multinomial mixture model aspect model lda but has clear advantages over each
article addresses issues classification detection they occur robot environment show method class classification support vector machines svms applied solve these tasks using limited hardware capacity prescribed robots experimental evaluation shows improvement over our previous methods fitting classification statistical approach used detection
paper novel kernel function structured natural language data field natural language processing feature extraction consists following two steps analyzing raw data ie character strings representing results discrete structures such parse trees dependency graphs part speech creating possibly high dimensional numerical feature vectors discrete structures new kernels called hierarchical directed acyclic graph kernels directly accept whose nodes contain data structures needed fully reflect syntactic semantic structures natural language data inherently have paper define kernel function show permits efficient calculation experiments demonstrate proposed kernels superior existing kernel functions eg sequence kernels tree kernels bag words kernels
given grid squares each square has count underlying population our goal find square region highest density calculate its significance density measure dependent total count total population region used example each count represents number disease cases occurring square use spatial scan statistic find most significant spatial disease cluster naive approach finding maximum density region requires time generally computationally present novel algorithm partitions grid into overlapping regions bounds maximum score contained each region regions cannot contain maximum density region sufficiently dense regions method finds maximum density region optimal time practice resulting significant
many real world domains relational nature consisting set objects related each other complex ways paper focuses predicting existence type links between entities such domains apply relational markov network framework et al define joint probabilistic model over entire link graph entity attributes links application algorithm task requires definition probabilistic patterns over structures apply method two new relational datasets involving university other social network show collective classification approach
accurate spectral decomposition essential analysis diagnosis tissue sections paper present first automated system performing decomposition compare performance our system ground truth data report favorable results
propose unsupervised methodology using independent component analysis ica cluster genes dna microarray data based ica mixture model expression patterns linear nonlinear ica finds components specific certain biological processes genes exhibit significant up regulation down regulation within each component into clusters test statistical significance gene within each cluster ica based clustering outperformed other leading methods constructing functionally coherent clusters various datasets result supports our model expression data composite effect independent biological processes comparison clustering performance among various ica algorithms including kernel based nonlinear ica algorithm shows nonlinear ica performed best small datasets natural gradient maximization likelihood well datasets
propose functional mixture model simultaneous clustering alignment sets curves measured discrete time grid model specifically gene expression time course data each functional cluster center nonlinear combination solutions simple linear differential equation describes change individual mrna levels synthesis decay rates constant mixture continuous time parametric functional forms allows account heterogeneity observed profiles profiles time estimating real valued time shifts capture synthesis decay mrna course experiment noisy profiles enforcing smoothness mean curves derive em algorithm estimating parameters model apply proposed approach set genes experiments show consistent improvement predictive power within cluster variance compared regular gaussian mixtures
existing source location recovery algorithms used imaging generally assume source activity different brain locations independent correlation structure known electrophysiological recordings local field potentials show strong correlations aggregate activity over significant distances indeed seems likely stimulus evoked activity would follow strongly correlated time courses different brain areas here present validate through simulations new approach source reconstruction correlation between sources modelled estimated explicitly variational bayesian methods facilitating accurate recovery source locations time courses their activation
understand brain mechanisms involved reward prediction different time scales developed markov decision task requires prediction both immediate future rewards analyzed subjects brain activities using functional mri estimated time course reward prediction reward prediction error different time scales subjects performance data used them variables analysis found topographic maps different time scales medial frontal cortex result suggests different cortico basal ganglia loops specialized reward prediction different time scales
consider learning classify cognitive states human subjects based their brain activity observed via functional magnetic resonance imaging fmri problem important because such classifiers constitute virtual sensors hidden cognitive states may useful cognitive science research clinical applications recent work mitchell et al have demonstrated feasibility training such classifiers individual human subjects eg distinguish whether subject reading ambiguous sentence whether they reading noun here extend line research exploring train classifiers applied across multiple human subjects including subjects who involved training classifier describe design several machine learning approaches training multiple subject classifiers report experimental results demonstrating success these methods learning cross subject classifiers two different fmri data sets
nonlinear filtering solve complex problems but typically involve time consuming calculations here show filters constructed rbf network gaussian basis functions decomposition into linear filters exists computed efficiently frequency domain yielding dramatic improvement speed present application idea image processing images photoreceptor terminals fly synaptic containing neurotransmitter should detected labeled automatically use hand labels provided human experts learn rbf filter using support vector regression gaussian kernels show resulting nonlinear filter solves task degree accuracy close what achieved human experts allows time consuming task data evaluation done efficiently
paper presents energy normalization transform method reduce system errors brain computer interface energy normalization transform has two major benefits system performance first increase class separation between active eeg data second system signal amplitude variability four subjects study benefits resulted performance improvement range while subject who had highest non normalized accuracy performance did change notably normalization
brain computer interfaces bci interesting emerging technology driven motivation develop effective communication interface human into control signal devices like computers done usual human output pathways like peripheral muscles ultimately become valuable tool patients most activity bci research finding suitable features algorithms increase information transfer rates present paper studies implications using more classes eg left vs right hand vs operating bci contribute theoretical study showing under mild assumptions practically useful employ more than three four classes two extensions common spatial pattern algorithm interestingly based simultaneous controlled eeg experiments our theoretical findings show excellent improved
describe system single reasonable accuracy noisy meg measurements real time its core multilayer perceptron mlp trained map sensor signals head position location including head position overcomes previous need mlp each subject session training dataset generated mapping randomly chosen head positions through analytic model adding noise real meg recordings after training localization took ms average error cm few iterations using mlps output its initial guess took ms improved accuracy cm slightly above statistical limits accuracy imposed noise applied these methods localize single sources meg components isolated blind source separation compared estimated locations those generated standard manually commercial software
exploit useful properties gaussian process gp regression models reinforcement learning continuous state spaces discrete time demonstrate gp model allows evaluation value function closed form resulting policy iteration algorithm demonstrated simple problem two dimensional state space further intrinsic ability gp models distributions functions would allow method capture entire instead has traditionally been focus much reinforcement learning
recent grid based point based approximation algorithms pomdps have greatly improved pomdp planning these approaches operate sets belief points individually learning value function each point belief points exist highly structured metric simplex but current pomdp algorithms do exploit property paper presents new metric tree algorithm used context pomdp planning sort belief points spatially perform fast value function updates over groups points present results showing approach reduce computation point based pomdp algorithms wide range problems
real world planning problems time often limited well suited these problems they find feasible solution quickly work improving until time runs out paper propose heuristic search its performance bound based available search time starts finding suboptimal solution quickly using bound bound time allows given enough time finds provably optimal solution while improving its bound previous search efforts result significantly more efficient than other search methods addition our theoretical analysis demonstrate practical utility experiments simulated robot kinematic arm dynamic path planning problem outdoor
recent research has demonstrated useful pomdp solutions do require consideration entire belief space extend idea notion temporal abstraction present explore new reinforcement learning algorithm over grid points belief space uses macro actions monte carlo updates values apply algorithm large scale robot navigation task demonstrate temporal abstraction consider even smaller part belief space learn pomdp policies faster do information more efficiently
mobile robot acting world faced large amount sensory data uncertainty its action outcomes indeed almost interesting sequential decision making domains involve large state spaces large stochastic action sets investigate way act quickly possible domains finding complete policy would take long time approach relational planning large noisy problems along two first describing domain relational mdp instead factored mdp allows problem structure dynamics captured small set probabilistic relational rules second envelope based approach planning lets agent begin acting quickly within restricted part full state space expand its envelope resources permit
online mechanism design md considers problem providing implement desired system wide outcomes systems self interested agents arrive dynamically agents choose their arrival times addition information about their value different outcomes consider problem maximizing total longterm value system despite self interest agents online md problem induces markov decision process mdp solved used implement optimal policies truth bayesian nash equilibrium
autonomous helicopter flight represents challenging control problem complex noisy dynamics paper describe successful application reinforcement learning autonomous helicopter flight first fit stochastic nonlinear model helicopter dynamics use model learn place fly number taken rc helicopter competition
large multiagent games partial coordination credit assignment attempts design good learning algorithms provide simple efficient algorithm part uses linear system model world single agents limited perspective takes advantage kalman filtering allow agent construct good training signal learn effective policy
so called experts algorithms constitute methodology choosing actions repeatedly rewards depend both choice action unknown current state environment experts algorithm has access set strategies experts each may action choose algorithm learns combine individual experts so long run fixed sequence states environment does well best expert would have done relative same sequence methodology may suitable situations evolution states environment depends past chosen actions usually case example repeated non zero sum game new experts algorithm presented analyzed context repeated games shown asymptotically under certain conditions performs well best available expert algorithm quite different previously proposed experts algorithms represents shift paradigms regret minimization optimization consideration long term effect actions opponents actions environment importance shift demonstrated fact algorithm capable inducing cooperation repeated game whereas previous experts algorithms converge suboptimal non cooperative play
describe new approximation algorithm solving partially observable mdps our bounded policy iteration approach searches through space bounded size stochastic finite state controllers combining several advantages gradient ascent efficiency search through restricted controller space policy iteration less local optima
consider policy search approach reinforcement learning show baseline distribution given indicating roughly often expect good policy each state derive policy search algorithm finite number steps provide non trivial performance guarantees demonstrate algorithm several grid world pomdps planar biped walking robot double pole balancing problem
optimal solutions markov decision problems mdps sensitive respect state transition probabilities many practical problems estimation those probabilities far accurate hence estimation errors limiting factors applying mdps realworld problems propose algorithm solving finite state finite action mdps solution guaranteed robust respect estimation errors state transition probabilities our algorithm involves statistically accurate yet numerically efficient representation uncertainty via kullback leibler divergence bounds worst case complexity robust algorithm same original bellman hence robustness added practically extra computing cost
explore approximate policy iteration replacing usual learning step learning step policy space give policy language biases enable solution large relational markov decision processes mdps previous technique solve particular induce high quality domain specific classical planning domains both deterministic stochastic variants solving such domains extremely large mdps
predictive state representations psrs use predictions set tests represent state controlled dynamical systems reason why representation alternative partially observable markov decision processes pomdps models dynamical systems may much more compact than pomdp models empirical work psrs date has focused linear psrs have allowed compression relative pomdps introduce new notion tests allows us define new type nonlinear general allows exponential compression deterministic dynamical systems these new tests called tests related tests used schapire their work diversity representation but our avoids their representation particular its potential exponentially larger than equivalent pomdp
study learn play optimal strict nash equilibrium exist multiple equilibria agents may have different preferences among equilibria focus repeated coordination games non identical interest agents do know game structure up front receive noisy payoffs design efficient near optimal algorithms both perfect monitoring imperfect monitoring agents observe their own payoffs joint actions
recent multi agent extensions learning require knowledge other agents payoffs functions assume game theoretic play times other agents paper proposes different approach hyper learning values mixed strategies rather than base actions learned other agents strategies estimated observed actions via bayesian inference hyper may effective against many different types adaptive agents even they dynamic against certain broad categories adaptation argued hyper may converge exact optimal time varying policies tests using paper hyper learns significantly exploit gradient ascent player well policy hill player preliminary analysis hyper against itself presented
design cooperative multi robot systems highly active research area two lines research particular have generated interest solution large weakly coupled mdps design implementation market architectures propose new algorithm together these two lines research class coupled mdps our algorithm automatically designs market architecture causes multi robot system converge consistent policy show policy same would produced particular planning algorithm demonstrate new algorithm three simulation examples multi robot multi robot path planning limited resource behaviors game
develop protocol optimizing dynamic behavior network simple electronic components such sensor network ad hoc network mobile devices network communication switches protocol requires local communication simple computations distributed among devices protocol scalable large networks example discuss problem involving optimization power consumption delay sensor network our approach builds policy gradient methods optimization markov decision processes protocol viewed extension policy gradient methods context involving team agents optimizing aggregate performance through asynchronous distributed communication computation establish dynamics protocol approximate solution ordinary differential equation follows gradient performance objective
approximate linear programming has emerged recently most promising methods solving complex factored mdps finite state spaces work show solutions limited mdps finite state spaces but they applied successfully factored continuous state mdps show build based approximation such model contrast existing solution methods argue approach offers robust alternative solving high dimensional continuous state space problems point supported experiments three problems continuous state factors
attempt understand visual classification humans using both psychophysical machine learning techniques frontal views human faces used gender classification task human subjects classified faces their gender reaction time confidence rating recorded several hyperplane learning algorithms used same classification task using principal components texture shape representation faces classification performance learning algorithms estimated using face database true gender faces labels gender estimated subjects correlated human responses distance stimuli separating hyperplane learning algorithms our results suggest human classification modeled hyperplane algorithms feature space used classification brain needs more processing stimuli close hyperplane than those further away
why sensory modalities way they paper show sensory modalities well designed self supervised cross modal learning using minimizing disagreement algorithm unsupervised speech categorization task visual moving auditory sound signal inputs show informative auditory dimensions actually performance moved visual side network better them away than consider them part visual input explain finding terms statistical structure sensory inputs
show temporal logic combinations temporal modal knowledge effectively represented artificial neural networks present translation algorithm temporal rules neural networks show networks compute fixed point semantics rules apply translation children has been used distributed multi agent systems provide complete solution use simple neural networks capable reasoning about time knowledge acquisition through inductive learning
present connectionist architecture learn model relations between actions use model behavior planning state representations learned growing selforganizing layer directly coupled perception motor layer knowledge about possible state transitions encoded lateral connectivity motor signals modulate lateral connectivity dynamic field layer planning process mechanisms local adaptation based hebbian ideas model continuous action perception time domain
despite connectionist models cognitive science their performance often difficult evaluate inspired geometric approach statistical model selection introduce conceptually similar method examine global behavior connectionist model counting number types response patterns simulate markov chain monte carlo based algorithm constructed these patterns efficiently demonstrate approach using two network models speech perception
way algorithm linked unknown body infer itself information about body world taking case space example way algorithm realize its body three dimensional world possible algorithm discover move straight line more do these questions make sense given algorithm has access high dimensional data consisting its sensory inputs motor outputs demonstrate article these questions given positive answer show possible make algorithm analyzing law links its motor outputs its sensory inputs discovers information about structure world regardless devices body linked present results simulations demonstrating way issue motor orders resulting fundamental movements body structure physical world
explore phenomena subjective randomness case study understanding people discover structure embedded noise present rational account randomness perception based statistical problem model selection given stimulus inferring whether process generated random regular inspired mathematical definition randomness given complexity characterize regularity terms hierarchy automata finite controller different forms memory find regularities detected binary sequences depend upon presentation kinds automata identify these regularities informative about cognitive processes different
describe pattern acquisition algorithm learns unsupervised fashion representation linguistic structures natural language corpus paper addresses issues learning structured knowledge large scale natural language data set generalization unseen text implemented algorithm represents sentences paths graph whose vertices words parts words significant patterns determined recursive context sensitive statistical inference form new vertices linguistic constructions represented trees composed significant patterns their associated equivalence classes input module allows algorithm standard test english second language results encouraging model attains level performance considered intermediate th students despite having been trained corpus containing speech parents directed small children
present novel connectionist model semantics simple language through behavioral experiences real robot focus semantics fundamental characteristic human language ability understand meaning sentence combination words much attention robot means robot should acquire semantics matches its body sensory motor system essential claim compositional semantic representation self organized generalized correspondences between sentences behavioral patterns claim examined confirmed through simple experiments robot generates corresponding behaviors sentences analogy correspondences between learned sentences behaviors
develop framework based bayesian model averaging explain animals cope uncertainty about classical conditioning experiments traditional accounts conditioning fit parameters within fixed generative model delivery uncertainty over model structure considered apply theory explain relationship between second order conditioning conditioned inhibition two similar conditioning regimes result strongly behavioral outcomes according theory second order conditioning results limited experience leads animals simpler world model produces spurious correlations conditioned inhibition results more complex model justified additional experience
have designed tested single chip analog vlsi sensor detects collisions measuring optic flow design chip based model proposed explain leg extension behavior during approaches new elementary motion detector circuit developed measure optic flow circuit models bandpass nature large cells immediately postsynaptic photoreceptors fly visual system array motion detectors fabricated mm mm die standard cmos process chip power supply addition wide angle sensor able detect collisions around ms before impact complex real world scenes
synapses critical element biologically realistic spike based neural computation role communication computation modification many different circuit implementations synapse function exist different computational goals mind paper describe new cmos synapse design separately controls current synaptic gain time constant decay circuit implements part commonly used model synaptic conductance show theoretical analysis experimental data prototypes fabricated available cmos process
paper presents vlsi circuits continuous valued probabilistic behaviour realized noise into each computing noisy neurons forms continuous restricted boltzmann machine has shown promising performance modelling classifying noisy data divergence learning algorithm implemented mixed mode vlsi adapt noisy neurons parameters chip
most proposals quantum neural networks have over problem train networks mechanics quantum computing different enough classical computing issue training should treated detail propose simple quantum neural network training method shown algorithm works quantum systems results several real world data sets show algorithm train proposed quantum neural networks has advantages over classical learning algorithms
present test results spike timing correlation learning experiments carried out silicon neurons stdp spike timing dependent plasticity synapses weight change scheme stdp synapses set either weight independent weight dependent mode present results learning window implemented both modes operation presented spike trains different types neurons develop weight distributions show layered network silicon spiking neurons stdp synapses perform hierarchical synchrony detection
mixed signal image filtering vlsi has been developed real time generation edge based image vectors robust image recognition four stage asynchronous median detection architecture based analog digital mixed signal circuits has been introduced determine threshold value edge detection key processing parameter vector generation result fully processing threshold detection edge feature map generation has been established prototype chip designed double three layer cmos technology concept verified fabricated chip chip generates dimension feature vector pixel gray scale image every sec about times faster than software computation making real time image recognition system feasible
have constructed second generation cpg chip capable generating necessary timing control leg walking machine demonstrate improvements over previous chip moving toward significantly more device includes larger number silicon neurons more sophisticated neurons including voltage dependent relative absolute refractory periods enhanced neural networks chip builds basic results achieved previous chip its get closer self contained locomotion controller walking robots
relative depth objects causes small shifts left right retinal positions these objects called binocular disparity here describe neuromorphic implementation disparity selective complex cell using binocular energy model has been proposed model response disparity selective cells visual cortex our system consists two silicon chips containing spiking neurons monocular gabor type spatial receptive fields rf circuits combine spike outputs compute disparity selective complex cell response disparity selectivity cell adjusted both position phase shifts between monocular rf profiles both used biology our neuromorphic system performs better phase encoding because relative responses neurons tuned different phase shifts better matched than responses neurons tuned position shifts
decision functions constructed support vector machines svms usually depend subset training set so called support vectors derive asymptotically sharp lower upper bounds number support vectors several standard types svms particular show gaussian rbf kernel fraction support vectors tends twice bayes risk svm probability noise svm ls svm
purpose paper investigate infinity sample properties risk minimization based multi category classification methods these methods considered natural extensions binary large margin classification establish conditions guarantee infinity sample consistency classifiers obtained risk minimization framework examples provided two specific forms general formulation extend number known methods using these examples show risk minimization formulations used obtain conditional probability estimates underlying problem such conditional probability information useful statistical inferencing tasks beyond classification motivation consider binary classification problem want predict label based observation most significant binary classification machine learning large margin methods include support vector machines boosting algorithms based set observations xy large margin classification algorithm produces decision function fn empirically minimizing loss function often convex upper bound binary classification error function given fn binary decision rule predict predict otherwise decision rule important literature following form large margin binary classification often encountered minimize empirical risk associated convex function pre chosen function class cn fn arg min originally such scheme regarded compromise avoid computational difficulties associated direct classification error minimization often leads np hard problem current view statistical literature such methods algorithms obtain conditional probability estimates example see related studies point view allows people show consistency various large margin methods large sample limit obtained classifiers achieve optimal bayes error rate example see consistency learning method desirable property may argue good classification method should consistent large sample limit although statistical properties binary classification algorithms based risk minimization formulation quite well understood due many recent works such those mentioned above much fewer studies risk minimization based problems generalizes binary large margin method complexity possible generalizations may reason another reason may always estimate conditional probability multi category problem using binary classification formulation each category pick category highest estimated conditional probability score still useful understand whether more natural alternatives what kind risk minimization formulation generalizes used yield consistent classifiers large sample limit important step toward direction has recently been taken authors proposed multi category extension support vector machine bayes consistent note number earlier proposals consistent purpose paper generalize their investigation so include much wider class risk minimization formulations lead consistent classifiers infinity sample limit see rich structure risk minimization based multi category classification formulations multi category large margin methods have started draw more attention recently example learning bounds multi category convex risk minimization methods obtained although authors did study possible choices bayes consistent formulations multi category classification consider following class classification problem would like predict label input vector paper consider simplest scenario classification loss have loss correct prediction loss incorrect prediction binary classification class label determined using sign decision function generalized class classification problem follows consider decision functions predict label arg max denote fx vector function fx fx note two more components achieve same maximum value may choose them tf framework often regarded scoring function category correlated likely belongs category compared remaining categories classification error given note relative strength compared alternatives important particular decision rule given does change add same numerical quantity each component fx allows us impose constraint vector fx decreases degree freedom component vector fx approach often called versus ranking machine learning another main ap encode multi category classification problem into binary classification sub problems consistency such encoding schemes difficult analyze discuss them example binary classification case hence fx represented fx fx decision rule compares fx fx equivalent fx leads binary classification rule mentioned
paper concerned transductive learning although transduction appears easier task than induction have been many provably useful algorithms bounds transduction present explicit error bounds transduction derive general technique bounds within setting technique applied derive error bounds compression schemes such transductive svms transduction algorithms based clustering
consider online learning scenario learner make predictions basis fixed set experts derive upper lower relative loss bounds class universal learning algorithms involving switching dynamics over choice experts basis performance bounds provide optimal priori discretization learning parameter switching dynamics demonstrate new algorithm context wireless networks
order understand dynamics especially its ability maximize margins derive associated simplified nonlinear iterated map analyze its behavior low dimensional cases find stable cycles these cases explicitly used solve output considering adaboost dynamical system able prove conjecture adaboost may fail converge maximal margin combined classifier given weak learning algorithm adaboost known coordinate descent method but other known algorithms explicitly aim maximize margin such adaboost arc consider differentiable function coordinate ascent yield maximum margin solution make simple approximation derive new boosting algorithm whose updates slightly more than those arc
investigate improvements adaboost exploit fact weak hypotheses ie either its positive negative predictions correct particular set labeled examples consistent case adaboost constructs consistent hypothesis using ok log iterations other hand greedy set covering algorithm finds consistent hypothesis size ok log our primary question whether simple boosting algorithm performs well greedy set covering first show modification adaboost proposed different purpose does perform well greedy set covering algorithm show adaboost requires log iterations learning achieve construction well simple experiments based artificial data further give variant called handle degenerate case given examples have same label conclude showing used produce small conjunctions well
paper reports family computationally practical classifiers converge bayes error near minimax optimal rates variety distributions classifiers based dyadic classification trees dcts involve adaptively pruned partitions feature space key aspect dcts their spatial enables local rather than global fitting decision boundary our risk analysis involves spatial decomposition usual concentration inequalities leading spatially adaptive data dependent pruning criterion distribution whose bayes decision boundary behaves locally like smooth function show error converges bayes error rate within logarithmic factor minimax optimal rate study dcts polynomial classification rules each leaf show smoothness boundary increases their errors converge bayes error rate parametric rate aware other practical classi provide similar rate convergence guarantees fast algorithms tree pruning discussed
exist many different generalization error bounds classification each these bounds contains improvement over others certain situations our goal combine these different improvements into single bound particular combine pac bayes approach introduced interesting averaging classifiers optimal union bound provided generic technique developed combination quite natural generic based notion measures considered priors set classifiers such priors arise pac bayesian setting
problem probability forecasting learners goal output given training set new object suitable probability measure possible values new objects label line algorithm probability forecasting said well probabilities outputs agree observed frequencies give natural notion well study under assumption randomness pairs independent distributed turns out although probability forecasting algorithm automatically well our sense exists wide class algorithms forecasting such algorithms allowed output set narrow probability measures satisfy property call algorithms class probability machines our experimental results demonstrate nearest neighbor probability machine performs reasonably well standard benchmark data set our theoretical results simple probability machine asymptotically approaches true conditional probabilities regardless without knowledge true probability measure generating examples
interpret non negative matrix factorization problem finding cone contains data points contained positive orthant show under certain conditions requiring data spread across faces positive orthant unique such cone give examples synthetic image databases obey these conditions these require separated support factorial sampling such databases generative model terms parts correctly identifies parts show our theoretical results predictive performance published code running published algorithms our synthetic image databases
paper obtain convergence bounds concentration bayesian posterior distributions around true distribution using novel method simplifies enhances previous results based analysis introduce generalized family bayesian posteriors show convergence behavior these generalized posteriors completely determined local prior structure around true distribution important surprising robustness property does hold standard bayesian posterior may concentrate exist bad prior structures even places far away true distribution
general linear response method deriving improved estimates correlations variational bayes framework presented three applications given discussed use linear response general principle improving mean field approximations
argue deterministic annealing algorithms geometric clustering derived more general information bottleneck approach cluster identities data points preserve information about their location set optimal solutions massively degenerate but treat equations define optimal solution iterative algorithm set smooth initial conditions selects solutions desired geometrical properties addition conceptual argue approach more efficient robust than classic algorithms
many classification algorithms including support vector machine boosting logistic regression viewed minimum contrast methods minimize convex surrogate loss function characterize statistical consequences using such surrogate providing general quantitative relationship between risk assessed using loss risk assessed using nonnegative surrogate loss function show relationship gives nontrivial bounds under possible condition loss function satisfy form fisher consistency classification relationship based variational transformation loss function easy compute many applications present refined version result case low noise finally present applications our results estimation convergence rates general setting function classes scaled hulls finite dimensional base class
derive limiting form eigenvalue spectrum sample covariance matrices produced non isotropic data analysis standard pca study case data has increased variance along small number symmetry breaking directions spectrum depends strength symmetry breaking signals parameter ratio sample size data dimension results derived limit large data dimension while keeping fixed increases transitions delta functions emerge upper end spectrum corresponding symmetry breaking directions data calculate bias corresponding eigenvalues kernel pca covariance matrix feature space may contain symmetry breaking structure even data components independently distributed equal variance show examples phase transition behaviour analogous pca results case
compute approximate analytical bootstrap averages support vector classification using combination replica method statistical physics tap approach approximate inference test our method few datasets compare exact averages obtained extensive monte carlo sampling
gradient following learning methods problems implementation many applications stochastic variants frequently used overcome these difficulties derive quantitative learning curves three online training methods used linear perceptron direct gradient descent node perturbation weight perturbation maximum learning rate stochastic methods scales first power dimensionality noise into system sufficiently small learning rate three methods give identical learning curves these results suggest these stochastic methods limited their utility considerations architectures they effective
what happens optimal interpretation noisy data exists more than equally plausible interpretation data bayesian model learning framework answer depends prior expectations dynamics model parameter inferred data local time constraints priors insufficient pick interpretation over another other hand time constraints induced noise spectrum priors shown permit learning specific model parameter even many equally plausible interpretations data transition inferred remarkable mapping model estimation problem physical system allowing use powerful statistical mechanical methods transition model learning
problem extracting relevant aspects data addressed through information bottleneck ib method soft clustering variable while preserving information about another relevance variable interesting question addressed current work extension these ideas obtain continuous representations preserve relevant information rather than discrete clusters give formal definition general continuous ib problem obtain analytic solution optimal representation important case multivariate gaussian variables obtained optimal representation noisy linear projection eigenvectors normalized correlation matrix xy basis obtained canonical correlation analysis gaussian ib compression tradeoff parameter uniquely determines dimension well scale each eigenvector introduces novel interpretation solutions different ranks lie continuum parametrized compression level our analysis provides analytic expression optimal tradeoff information curve terms eigenvalue spectrum
address paper question knowledge marginal distribution incorporated learning algorithm suggest three theoretical methods taking into account distribution regularization provide links existing graph based semi supervised learning algorithms propose practical implementations
present unified view online classification regression problems view leads single algorithmic framework three problems prove worst case loss bounds various algorithms both realizable case non realizable case conversion our main online algorithm setting batch learning discussed end result new algorithms accompanying loss bounds hinge loss
margin maximizing properties play important role analysis models such boosting support vector machines margin maximization theoretically interesting because facilitates generalization error analysis practically interesting because presents clear geometric interpretation models being built formulate prove condition solutions regularized loss functions converge margin maximizing regularization condition covers hinge loss svm exponential loss adaboost logistic regression loss generalize multi class problems present margin maximizing multiclass versions logistic regression support vector machines
balanced network leads constraints memory models previous work chains here show these constraints overcome introducing inhibitory pattern each excitatory pattern model interpreted principle whereby exists both global balance between average excitatory inhibitory currents local balance between currents coherent activity given time frame principle applied networks hebbian cell assemblies leading high capacity associative memory number possible patterns limited combinatorial constraint turns out within specific model employ limit reached hebbian cell assembly network best our knowledge first time such high memory capacities demonstrated asynchronous state models spiking neurons
employ efficient method using bayesian linear classifiers analyzing dynamics information high dimensional states generic cortical microcircuit models shown such recurrent circuits spiking neurons have inherent capability carry out rapid computations complex spike patterns merging information contained order spike arrival previously acquired context information
signal transduction networks biochemical biological information processing systems individual cells neurons respond their chemical environments introduce simplified model single biochemical analyse its capacity communications channel cell received binding receptor protein receiving cell receptor interaction creates nonlinear communications channel non gaussian noise model channel numerically study its response input signals different frequencies order estimate its channel capacity stochastic effects introduced both diffusion process receptor interaction give channel low pass characteristics estimate channel capacity using water filling formula adapted additive white noise gaussian channel
dopamine two classes effect sustained neural activity prefrontal cortex working memory direct release cortex increases contrast prefrontal neurons enhancing robustness storage release dopamine associated salient stimuli makes medium neurons bistable modulation output neurons affects prefrontal cortex so indirectly gate access working memory additionally sensitivity noise existing models have treated dopamine other structure have addressed basal ganglia gating working memory exclusive dopamine effects paper combine these mechanisms explore their joint effect model memory guided task illustrate actions lead working memory selective salient input has increased robustness
connectivity nervous system elegans has been described completely but analysis neuronal basis behavior system here used optimization algorithm search patterns connectivity sufficient compute sensorimotor transformation underlying elegans simple form spatial orientation behavior probability modulated rate change chemical concentration optimization produced networks inhibitory feedback among neurons further analysis showed feedback latency between sensory input behavior common patterns connectivity between model biological networks suggest new functions previously identified connections elegans nervous system
significant plasticity sensory cortical representations driven animals either behavioural tasks pair sensory stimuli reinforcement electrophysiological experiments pair sensory input direct stimulation neuromodulatory nuclei but usually sensory stimuli presented alone biologically motivated theories representational learning have focus unsupervised mechanisms may play significant role evolutionary developmental but essential role reinforcement adult plasticity contrast theoretical reinforcement learning has generally acquisition optimal policies action uncertain world rather than concurrent sensory representations paper develops framework representational learning builds relative success unsupervised accounts cortical encodings incorporate effects reinforcement biologically plausible way
paper proposes neural mechanisms magnetic stimulation tms tms brain non through brief magnetic pulse placed scalp interfering specific cortical functions high temporal resolution due these advantages tms has been popular experimental tool various neuroscience fields neural mechanisms underlying interference still unknown theoretical basis tms has been developed paper provides computational evidence inhibitory interactions neural population isolated single neuron play critical role yielding neural interference induced tms
discuss idea data relatively efficient manner our point view bayesian information theoretic given trial want adaptively choose input such way mutual information between unknown state system stochastic output maximal given prior information including data collected previous trials prove theorem effectiveness strategy give few illustrative examples comparing performance adaptive technique more usual experimental design example able explicitly calculate asymptotic relative efficiency method widely employed psychophysics research demonstrate dependence efficiency form function underlying output responses
learn new motor have both variability inherent our sensors task sensory uncertainty reduced using information about distribution previously experienced tasks here impose distribution novel sensorimotor task variability sensory feedback show subjects internally represent both distribution task well their sensory uncertainty moreover they combine these two sources information way qualitatively predicted optimal bayesian processing further analyze subjects represent multimodal distributions such mixtures gaussians results show cns employs probabilistic models during sensorimotor learning even priors multimodal
model higher order functions such learning memory face difficulty comparing neural activities hidden variables depend history sensory motor signals dynamics network here propose novel method estimating hidden variables learning agent such connection weights sequences observable variables bayesian estimation method estimate posterior probability hidden variables observable data sequence using dynamic model hidden observable variables paper apply particle filter estimating internal parameters reinforcement learning model verified effectiveness method using both artificial data real animal behavioral data
spike timing plasticity stdp special form synaptic plasticity relative timing presynaptic activity determines change synaptic weight postsynaptic side active spikes dendrites seem play crucial role induction spike timing dependent plasticity argue temporal change membrane potential determines weight change coming presynaptic side induction stdp closely related activation nmda channels therefore calculate analytically change synaptic weight derivative membrane potential activity nmda channel thus calculation biophysical variables physiological cell final result shows weight change curve measurements biology positive part weight change curve determined nmda activation negative part weight change curve determined membrane potential change therefore weight change curve should change its shape depending distance postsynaptic cell find temporally asymmetric weight change close temporally symmetric weight change dendrite
barn owl capable capturing using auditory information alone neural basis localization behavior existence auditory neurons spatial receptive fields provide mathematical description operations performed auditory input signals barn owl facilitate representation auditory space develop our model first formulate sound localization problem solved barn owl statistical estimation problem implementation solution constrained known neurobiology
decoding strategy allows us assess amount information neurons provide about certain aspects visual scene study develop method based bayesian sequential updating particle filtering algorithm activity neurons awake monkeys distinction our method use volterra kernels filter particles live high dimensional space parametric bayesian decoding scheme compared optimal linear decoder shown work consistently better than linear optimal decoder interestingly our results suggest decoding real time spike trains few independent but similar neurons would sufficient decoding critical scene variable particular class visual stimuli reconstructed variable predict neural activity about well actual signal respect volterra kernels
report compare performance different learning algorithms based data cortical recordings task predict orientation visual stimuli activity population simultaneously recorded neurons compare several ways improving coding input ie spike data well output ie orientation report results obtained using different kernel algorithms
recent area significant progress speaker recognition use high level features phonetic relations structure etc speaker has distinctive acoustic sound but uses language characteristic manner large corpora speech data available recent years allow long term statistics phone patterns word patterns etc individual propose use support vector machines term frequency analysis phone sequences model given speaker end explore techniques text categorization applied problem derive new kernel based upon likelihood ratio scoring introduce new phone based svm speaker recognition approach error rate conventional phone based approaches
over last years significant efforts have been made develop kernels applied sequence data such dna text speech video images fisher kernel similar variants have been suggested good ways combine underlying generative model feature space discriminant classifiers such svms paper suggest alternative procedure fisher kernel systematically finding kernel functions naturally handle variable length sequence data domains particular domains such speech images explore use kernel functions take full advantage well known probabilistic models such gaussian mixtures single full covariance gaussian models derive kernel distance based kullback leibler kl divergence between generative models effect our approach combines best both generative discriminative methods replaces standard svm kernels perform experiments speaker image classification tasks show these new kernels have best performance speaker verification mostly outperform fisher kernel based svms generative classifiers speaker identification image classification
many techniques complex speech processing such denoising deconvolution warping multiple speaker separation multiple microphone analysis operate sequences short time power spectra spectrograms representation often well suited these tasks significant problem algorithms spectrograms output spectrogram does include phase component needed create time domain signal has good perceptual quality here describe generative model time domain speech signals their spectrograms show efficient used find maximum posteriori speech signal given spectrogram contrast techniques alternate between estimating phase consistent signal our technique directly infers speech signal thus jointly optimizing phase consistent signal compare our technique standard method using signal noise ratios but provide audio web purpose demonstrating improvement perceptual quality our technique offers
eigenvoice speaker adaptation has been shown effective small amount adaptation data available heart method principal component analysis pca employed find most important paper nonlinear pca particular kernel pca may even more effective major challenge map feature space back observation space so state observation likelihoods computed during estimation eigenvoice weights subsequent decoding our solution compute kernel pca using composite kernels call our new method kernel eigenvoice speaker adaptation corpus found compared speaker independent model our kernel eigenvoice adaptation method reduce word error rate while standard eigenvoice approach match performance speaker independent model
major issue evaluating speech enhancement algorithms come up suitable metric predicts intelligibility human previous methods such widely used speech transmission index fail account effects arise highly nonlinear cochlear transfer function therefore propose neural index estimates speech intelligibility instantaneous neural spike rate over time produced signal processed auditory neural model using well developed model auditory periphery detection theory show human perceptual discrimination closely matches modeled distortion instantaneous spike rates auditory nerve highly frequency transfer conditions prediction error versus prediction error
speech dereverberation desirable view achieving example robust speech recognition real world still challenging problem especially using single microphone although blind techniques have been exploited they cannot deal speech signals appropriately because their assumptions satisfied speech signals propose new dereverberation principle based inherent property speech signals namely quasi present methods learn dereverberation filter lot speech data prior knowledge data achieve high quality speech dereverberation especially time long
selectively single filter out other simulate perceptual ability remains great challenge paper describes novel supervised learning approach speech segregation target speech signal separated interfering sounds using spatial location cues interaural time differences itd interaural intensity differences iid motivated auditory effect employ notion ideal time frequency binary selects target stronger than interference local time frequency unit within narrow frequency band modifications relative strength target source respect interference trigger systematic changes estimated itd iid given spatial configuration interaction produces characteristic clustering feature space consequently perform pattern classification order estimate ideal binary systematic evaluation terms signal noise ratio well automatic speech recognition performance shows resulting system produces close ideal binary ones quantitative comparison shows our model yields significant improvement performance over existing approach furthermore under certain conditions model produces large speech intelligibility improvements normal
local phase coherence perception medical institute center neural science institute mathematical sciences new york university new york ny humans able detect visual images but mechanism they do so known traditional view image because reduction energy high frequencies argue local phase more important factor detecting first demonstrate sharp image its high frequency energy reduced but local phase preserved appears much than image its high frequency energy but local phase show precisely localized features such step edges result strong local phase coherence structures across scale space complex wavelet transform domain causes loss such phase coherence propose technique fine phase prediction wavelet coefficients observe such predictions highly effective natural images phase coherence increases strength image features phase coherence relationship images thus new theory perceptual estimation well variety algorithms restoration manipulation images
according widely held view neurons lateral geniculate nucleus lgn operate visual stimuli linear fashion evidence lgn responses entirely linear account nonlinearities propose model more than years research field model neurons have linear receptive field nonlinear field field computes local root contrast test model recorded responses lgn cats estimate model parameters basic set measurements show model accurately predict responses novel stimuli model might serve new standard model lgn responses specifies visual processing lgn involves both linear filtering gain control
psychophysical studies suggest existence specialized detectors component motion patterns radial spiral consistent visual motion properties cells medial superior temporal area mstd non human primates here use biologically constrained model visual motion processing mstd conjunction psychophysical performance two motion pattern tasks computational mechanisms associated processing motion patterns encountered during self motion both tasks discrimination thresholds varied significantly type motion pattern presented suggesting perceptual preferred motion bias reported mstd through model demonstrate while independently responding motion pattern units capable encoding information relevant visual motion tasks equivalent psychophysical performance achieved using interconnected neural populations systematically non units these results suggest cyclic trends psychophysical performance may part recurrent connections within motion pattern areas whose structure function similarity preferred motion patterns receptive field locations between units
paper compares ability human observers detect target image curves ideal observer target curves sampled generative model specifies probabilistically geometry local intensity properties curve ideal observer performs bayesian inference generative model using map estimation varying probability model curve geometry enables us investigate whether human performance best target curves obey specific shape statistics particular those observed natural shapes experiments performed data both rectangular our results show human observers performance approaches ideal observer general closest ideal conditions target curve tends straight similar natural statistics curves suggests bias human observers towards straight curves natural statistics
recent eye tracking studies natural tasks suggest tight link between eye movements goal directed motor actions most existing models human eye movements provide bottom up account relates visual attention attributes visual scene purpose paper introduce new model human eye movements directly eye movements ongoing demands behavior basic idea eye movements serve reduce uncertainty about environmental variables task relevant value assigned eye movement estimating expected cost uncertainty result movement made several candidate eye movements highest expected value chosen model illustrated using figure virtual environment simulations show our protocol superior simple scheduling mechanism
even under perfect human eye under steady motion slow drift dynamic theory vision states eye movements improve according theory eye movements thought create variable spatial excitation patterns photoreceptor grid allow better spatiotemporal summation later stages theory using realistic model vertebrate retina comparing responses moving eye performance simulated ganglion cells task evaluated ideal observer analysis find central retina eye have effect performance here optical limits retinal periphery eye clearly improve performance based roc analysis our predictions quantitatively testable electrophysiological psychophysical experiments
current explanation view independent representation space place cells hippocampus they arise out summation view dependent gaussians proposal assumes visual representations show bounded invariance here investigate whether recently proposed visual encoding scheme called temporal population code provide such representations our analysis based behavior simulated robot virtual environment containing specific visual cues our results show temporal population code provides representational naturally account formation place fields
present empirically test novel approach free form object shapes represented range data contrast traditional surface signature based systems use alignment match specific objects adapted newly introduced symbolic signature representation classify shapes our approach constructs abstract description shape classes using ensemble classifiers learn object class parts their corresponding geometrical relationships set symbolic descriptors used our classification engine series large scale discrimination experiments two well defined classes share many common distinctive features experimental results suggest our method outperforms traditional signature based
standard approaches object detection focus local patches image try classify them background propose use scene context image whole extra source global information help resolve local ambiguities present conditional random field jointly solving tasks object detection scene classification
problem structure motion central problem vision given locations certain points wish recover camera motion coordinates points under simplified camera models problem reduces measurement matrix into product two low rank matrices each element measurement matrix contains position point particular image elements observed problem solved using svd but realistic situation many elements matrix missing ones observed have different directional uncertainty under these conditions most existing factorization algorithms fail while human perception relatively paper use well known em algorithm factor analysis perform factorization allows us easily handle missing data measurement uncertainty more importantly allows us place prior temporal trajectory latent variables camera position show incorporating prior gives significant improvement performance challenging image sequences
mutual boosting method aimed incorporating contextual information object detection multiple detectors objects parts trained parallel using adaboost object detectors might use remaining intermediate detectors weak learner set method generalizes efficient features suggested viola jones thus enabling information inference between parts objects compositional hierarchy our experiments eye nose face detectors trained using mutual boosting framework results show method outperforms applications contextual information suggest achieving contextual integration step toward human like detection capabilities
face detection canonical example rare event detection problem target patterns occur much lower frequency than out face sized windows input image example few typically contain face viola jones recently proposed cascade architecture face detection successfully addresses rare event nature task central part their method feature selection algorithm based adaboost present novel cascade learning algorithm based forward feature selection two orders magnitude faster than viola jones approach yields classifiers equivalent quality faster method could used more demanding classification tasks such line learning
paper present discriminative random fields discriminative framework classification natural image regions incorporating neighborhood spatial dependencies labels well observed data proposed model exploits local discriminative models allows relax assumption conditional independence observed data given labels commonly used markov random field mrf framework parameters model learned using penalized maximum pseudo likelihood method furthermore form model allows map inference binary classification problems using graph min cut algorithms performance model verified synthetic well real world images model outperforms mrf model experiments
detection pose estimation people images video made challenging variability human appearance complexity natural scenes high dimensionality articulated body models cope these problems represent human body graphical model relationships between body parts represented conditional probability distributions formulate pose estimation problem probabilistic inference over graphical model random variables correspond individual parameters position orientation because described dimensional vectors encoding pose space discretization impractical random variables our model approximate belief propagation such graph exploit recently introduced generalization particle filter framework facilitates automatic initialization body model low level cues robust occlusion body parts scene clutter
paper describes system video sequence description appearance each actor actor view representation activity while view system does require fixed background automatic system works tracking people using annotated motion capture dataset annotated motion sequence matching tracks motion capture data manually annotated off line using class structure describes motions allows motion composed may jump while running example descriptions computed video real motions show method accurate
paper presents algorithm learning time varying shape non rigid object tracking data model shape motion rigid component rotation translation combined non rigid deformation reconstruction ill posed arbitrary deformations allowed constrain problem assuming object shape each time drawn gaussian distribution based assumption algorithm simultaneously estimates shape motion each time frame learns parameters gaussian robustly missing data points extend algorithm model temporal smoothness object shape thus allowing handle severe cases missing data
computer agents robots bring social dimension human computer interaction force us think new ways about computers could used life face face communication real time process operating time scale less than second paper present progress perceptual primitive automatically detect frontal faces video stream code them respect dimensions real time face employs cascade feature detectors trained boosting techniques expression recognizer employs novel combination adaboost svms generalization performance new subjects way choice correct two publicly available datasets outputs classifier change smoothly function time providing potentially valuable representation code facial expression dynamics fully automatic manner system evaluated measuring spontaneous facial expressions field application automatic assessment human robot interaction
paper presents novel graph theoretic approach named ratio contour extract salient boundaries set noisy boundary detected real images boundary saliency defined using laws proximity continuity paper first constructs undirected graph two different sets edges solid edges dashed edges weights solid dashed edges measure local saliency between boundary respectively most salient boundary detected searching optimal cycle graph minimum average weight proposed approach guarantees global optimality without introducing biases related region area boundary length variety images testing proposed approach encouraging results
present geometric approach statistical shape analysis closed curves images basic idea specify space closed curves satisfying given constraints exploit differential geometry space solve optimization inference problems demonstrate approach defining computing statistics observed shapes ii defining learning parametric probability model shape space iii designing binary hypothesis test space
super resolution aims produce high resolution image set more low resolution images recovering plausible high frequency image content typical approaches try reconstruct high resolution image using sub pixel several images usually regularized generic smoothness prior over high resolution image space other methods use training data learn low high resolution matches have been highly successful even single input image case here present domain specific image prior form pdf based upon sampled images show certain types super resolution problems sample based prior gives significant improvement over other common multiple image super resolution techniques
present bayesian approach color utilizes nongaussian probabilistic model image formation process parameters model estimated directly image set small number additional algorithmic parameters chosen using cross validation algorithm empirically shown exhibit rms error lower than other color algorithms based surface reflectance model estimating set test images demonstrated via direct performance comparison utilizing publicly available set real world test images code base
consider number moving points each point attached joint human body projected onto image plane showed humans detect recognize presence other humans such displays true even body points missing eg because occlusion clutter points added display interested ability machine end present detection scheme probabilistic framework our method based representing joint probability density positions velocities body points graphical model using loopy belief propagation calculate likely interpretation scene furthermore introduce global variable representing centroid experiments motion captured sequence suggest our scheme improves accuracy previous approach based graphical models especially few parts visible improvement due both more general graph structure use more significantly
learning multiagent system challenging problem due two key factors first other agents simultaneously learning environment longer stationary thus convergence guarantees second learning often other agents may able exploit learners particular dynamics worst case could result performance than agent learning these challenges two most common evaluation criteria multiagent learning algorithms convergence regret algorithms focusing convergence regret numerous paper seek address both criteria single algorithm introducing learning algorithm games prove algorithm guarantees most zero average regret while demonstrating algorithm converges many situations self play prove convergence limited setting give empirical results wider variety situations these results suggest third new learning criterion combining convergence regret call negative non convergence regret
formation among important feature protein structures here develop new methods prediction connectivity first build large data set proteins containing use dimensional recursive neural networks predict probabilities between pairs these probabilities turn lead weighted graph matching problem addressed efficiently show method consistently achieves better results than previous approaches same validation data addition method easily cope chains arbitrary numbers therefore overcomes major limitations previous approaches predictions chains containing more than method applied both situations state each known unknown case state predicted precision recall method yields estimate total number each chain
paper propose efficient algorithm reducing large mixture gaussians into smaller mixture while still preserving component structure original model achieved clustering grouping components method minimizes new easily computed distance measure between two gaussian mixtures motivated suitable stochastic model iterations algorithm use model parameters avoiding need explicit resampling datapoints demonstrate method performing hierarchical clustering images handwritten digits
reactive environment responds actions agent rather than reactive environments experts algorithms balance exploration exploitation experts more than ones addition more subtle definition learnable value expert required general exploration exploitation experts method presented along proper definition value method shown asymptotically perform well best available expert several variants analyzed viewpoint exploration exploitation tradeoff including explore exploit vanishing exploration constant frequency exploration constant size exploration phases complexity performance bounds proven
consider problem deriving class size independent generalization bounds regularized discriminative multi category classification methods particular obtain expected generalization bound standard formulation multi category support vector machines based theoretical result argue formulation over misclassification error theory may lead poor generalization performance based generalization multi category logistic regression conditional maximum entropy proposed its theoretical properties examined
propose new method estimating intrinsic dimension dataset derived applying principle maximum likelihood distances between close neighbors derive estimator poisson process approximation assess its bias variance theoretically simulations apply number simulated real datasets show has best overall performance compared two other intrinsic dimension estimators
belief propagation bp increasingly popular method performing approximate inference arbitrary graphical models times even further approximations required whether quantization other simplified message representations stochastic approximation methods introducing such errors into bp message computations has potential affect solution obtained analyze effect respect particular measure message error show bounds accumulation errors system leads both convergence conditions error bounds traditional approximate bp message passing
consider problem recovering image distorted surface waves large amount video data distorted image acquired problem posed terms finding image patch each spatial location challenging reconstruction task formulated manifold learning problem such center manifold image patch compute center present new technique estimate global distances manifold our technique achieves robustness through convex flow computations solves problem inherent recent manifold embedding techniques
assume uniform multidimensional grid data each cell grid has count ci baseline bi our goal find spatial regions dimensional rectangles ci significantly higher than expected given bi focus two applications detection clusters disease cases data department over drug sales discovery regions increased brain activity corresponding given cognitive tasks fmri data each these problems solved using spatial scan statistic compute maximum likelihood ratio statistic over spatial regions find significance region computing scan statistic spatial regions generally computationally so introduce novel fast spatial scan algorithm generalizing scan algorithm moore arbitrary dimensions our new multidimensional multiresolution algorithm allows us find spatial clusters up faster than naive spatial scan without loss accuracy
paper presents general family algebraic positive definite similarity functions over spaces matrices varying column rank columns represent local regions image whereby images have varying number local parts images image sequence motion trajectories motion so family set kernels derive based group invariant tensor product parameters naturally tuned provide sorts covering possible wish similarity measures over sets varying cardinality highlight strengths our approach demonstrating set kernels visual recognition using local parts representations
paper propose novel method learning distance measure used knn classification algorithm algorithm directly maximizes stochastic variant leave out knn score training set learn low dimensional linear embedding labeled data used data visualization fast classification unlike other methods our classification model non parametric making assumptions about shape class distributions boundaries between them performance method demonstrated several data sets both metric learning linear dimensionality reduction
study problem hierarchical classification labels corresponding partial andor multiple paths underlying allowed introduce new hierarchical loss function loss implementing simple intuition additional mistakes class should based probabilistic data model introduced earlier work derive bayes optimal classifier loss empirically compare two incremental approximations bayes optimal classifier flat svm classifier classifiers obtained using hierarchical versions perceptron svm algorithms experiments show our simplest incremental approximation bayes optimal classifier performs after training epoch nearly well hierarchical svm classifier performs best same incremental algorithm derive loss bound showing data generated our probabilistic data model exponentially fast convergence loss hierarchical classifier based true model parameters
agents learning act partially observable domain may need overcome problem perceptual ie different states appear similar but require different responses problem agents sensors noisy ie sensors may produce different observations same state show many well known reinforcement learning methods designed deal perceptual such suffix memory finite size history windows eligibility traces memory bits do handle noisy sensors well suggest new algorithm noisy suffix memory based uses weighted classification observed trajectories compare above methods show more robust noise
propose soft greedy learning algorithm building small conjunctions simple threshold functions called defined single real valued attributes propose pac bayes risk bound minimized classifiers achieving non trivial tradeoff between sparsity number used magnitude separating margin each finally test soft greedy algorithm four dna array data sets
devise experiment dynamical kernel based system tracking hand movements neural activity state system corresponds hand location velocity acceleration while systems input instantaneous spike rates systems state dynamics defined combination linear mapping previous estimated state kernel based mapping modeling neural activities contrast generative models activity state mapping learned using discriminative methods minimizing noise robust loss function use approach predict hand trajectories basis neural activity motor cortex behaving monkeys find proposed approach more accurate than both static approach based support vector regression kalman filter
present extension frey layered model allows layers undergo affine transformations extension allows affine object pose inferred simultaneously learning object shape appearance learning carried out applying augmented variational inference algorithm includes global search over transform space followed local optimisation aid correct convergence use bottom up cues restrict space possible affine transformations present results number video sequences show model extended track object whose appearance changes throughout sequence
present semi parametric latent variable model based technique density modelling dimensionality reduction visualization unlike previous methods estimate latent distribution non enables us model data generated underlying low dimensional multimodal distribution addition allow components latent variable models drawn exponential family makes method suitable special data types example binary count data simulations real valued binary count data show favorable comparison other related schemes both terms separating different populations generalization unseen samples
show anomaly detection interpreted binary classification problem using interpretation propose support vector machine svm anomaly detection present theoretical results include consistency learning rates finally experimentally compare our svm standard class svm
establish learning rates bayes risk support vector machines svms hinge loss particular svms gaussian rbf kernels propose geometric condition distributions used determine approximation properties these kernels finally compare our methods recent paper et al
paper concerns approximate nearest neighbor searching algorithms have become increasingly important especially high dimensional perception areas such computer vision recent years much due successful new approximate nearest neighbor approach called locality sensitive paper ask question earlier spatial data structure approaches exact nearest neighbor such metric trees provide approximate answers proximity queries so introduce new kind metric tree allows overlap certain datapoints may appear both children parent introduce new approximate nn search algorithms structure show why these structures should able exploit same based approximations but simpler algorithm perhaps greater efficiency provide detailed empirical evaluation five large high dimensional datasets show up fold over result holds true throughout spectrum approximation levels
describe algorithm support vector machines svm efficiently scales large problems hundreds thousands training vectors instead analyzing whole training set optimization step data split into subsets optimized separately multiple svms partial results combined filtered again cascade svms until global optimum reached cascade svm spread over multiple processors minimal communication requires far less memory kernel matrices much smaller than regular svm convergence global optimum guaranteed multiple passes through cascade but already single pass provides good generalization single pass faster than regular svm problems vectors implemented single processor parallel implementations cluster processors tested over million vectors class problems converging day two while regular svm never converged over
nips included feature selection competition organized authors provided participants five datasets different application domains called classification results using minimal number features competition took place over period research groups participants asked make line validation test sets performance validation set being presented immediately performance test set presented participants total entries made validation sets during development period entries test sets final competition winners used combination bayesian neural networks priors dirichlet diffusion trees other top entries used variety methods feature selection combined filters andor embedded methods using random kernel methods neural networks classification engine results benchmark including predictions made participants features they selected scoring software publicly available benchmark available post challenge further research
paper investigates new learning model input data corrupted noise present general statistical framework tackle problem based statistical reasoning propose novel formulation support vector classification allows uncertainty input data derive intuitive geometric interpretation proposed formulation develop algorithms efficiently solve empirical results included show newly formed method superior standard svm problems noisy input
saliency mechanisms play important role visual recognition performed cluttered scenes propose computational definition saliency existing models saliency discrimination particular salient attributes given visual class defined features enable best discrimination between class other classes recognition interest shown definition leads saliency algorithms low complexity scalable large recognition problems compatible existing models early biological vision experimental results demonstrating success context challenging recognition problems presented
graph based prior proposed parametric semi supervised classification prior utilizes both labelled unlabelled data integrates features multiple views given sample eg multiple sensors thus implementing bayesian form co training em algorithm training classifier automatically adjusts tradeoff between contributions labelled data unlabelled data co training information active label query selection performed using mutual information based criterion explicitly uses unlabelled data co training information encouraging results presented public benchmarks measured data single multiple sensors
linear discriminant analysis lda well known scheme feature extraction dimension reduction has been used widely many applications involving high dimensional data such face recognition image retrieval intrinsic limitation classical lda so called singularity problem fails matrices singular well known approach deal singularity problem apply intermediate dimension reduction stage using principal component analysis pca before lda algorithm called used widely face recognition has high costs time space due need decomposition involving matrices paper propose novel lda algorithm namely dlda dimensional linear discriminant analysis dlda overcomes singularity problem implicitly while achieving efficiency key difference between dlda classical lda lies model data representation classical lda works representations data while dlda algorithm works data matrix representation further reduce dimension dlda combination dlda classical lda namely studied lda dlda proposed algorithms applied face recognition compared experiments show dlda achieve competitive recognition accuracy while being much more efficient
linear discriminant analysis lda well known method feature extraction dimension reduction has been used widely many applications such face recognition recently novel lda algorithm based decomposition namely has been proposed competitive terms classification accuracy other lda algorithms but has much lower costs time space based linear projection may suitable data nonlinear structure paper first proposes algorithm called extends algorithm deal nonlinear data using kernel operator efficient approximation called proposed experiments face image data show classification accuracy both competitive generalized discriminant analysis general kernel discriminant analysis algorithm while has much lower time space costs
gaussian processes usually terms their covariance functions makes difficult deal multiple outputs because covariance matrix positive definite problematic alternative formulation treat gaussian processes white noise sources smoothing kernels kernel instead using extend gaussian processes handle multiple coupled outputs
two neural networks trained their mutual output identical time weight vector novel phenomenon used key using public channel several models system have been suggested have been tested their security under different sophisticated strategies most promising models networks involve chaos synchronization synchronization process mutual learning described analytically using statistical physics methods
address problem identifying specific instances class set images belonging class although cannot build model particular instance may provided training example use information extracted observing other members class pose task learning problem learner given image pairs labeled matching discover image features most consistent matching instances discriminative explore patch based representation model distributions similarity measurements defined patches finally describe algorithm selects most salient patches based mutual information criterion algorithm performs identification well our challenging dataset car images after matching few well chosen patches
paper propose probabilistic model online document clustering use non parametric dirichlet process prior model growing number clusters use prior general english language model base distribution handle generation novel clusters furthermore cluster uncertainty modeled bayesian distribution use empirical bayes method estimate hyperparameters based dataset our probabilistic model applied novelty detection task topic detection tracking compared existing approaches literature
experimental studies have observed synaptic potentiation presynaptic neuron fires before postsynaptic neuron synaptic depression presynaptic neuron fires after dependence synaptic modulation precise timing two action potentials known spike timing dependent plasticity stdp derive stdp simple computational principle synapses adapt so minimize postsynaptic neurons variability given presynaptic input neurons output become more reliable face noise using entropy minimization objective function biophysically realistic spike response model simulate neurophysiological experiments obtain characteristic stdp curve along other phenomena including reduction synaptic plasticity synaptic efficacy increases compare our account other efforts derive stdp computational principles argue our account provides most comprehensive phenomena thus reliability neural response face noise may key goal cortical adaptation
paper address problem statistical learning text categorization mtc whose goal choose relevant topics label given set topics proposed algorithm maximal margin labeling mml treats possible labels independent classes learns multi class classifier induced multi class categorization problem cope data sparseness caused huge number possible labels mml combines prior knowledge about label prototypes maximal margin criterion novel way experiments multi topic web pages show mml outperforms existing learning algorithms including support vector machines multi topic text categorization mtc paper addresses problem learning multi topic text categorization mtc whose goal select topics relevant text given set topics mtc multiple topics may relevant single text thus call set topics label say text assigned label topic almost previous text categorization studies eg label predicted each topics relevance text decomposition approach features specific topic label regarded important features approach may result learning explain following example imagine mtc problem scientific papers quantum computing papers assigned multi topic label quantum physics qp computer science cs qp cs topics example words specific quantum computing such say efficient mtc learners should use such words label qp cs decomposition approach likely ignore these words they specific small portion whole qp cs papers many more qp cs papers than quantum computing papers therefore discriminative features either topic qp cs unit quantum information frequently appears real quantum computing but rarely seen other rd tl xi li meaning document vector topics set topics label binary representation tj otherwise set possible labels training samples table notation parametric mixture model pmm another approach mtc assumed pmm multi topic texts generated mixture topic specific word distributions its decision labeling done once separately each topic pmm has problem multi topic specific features such impossible texts have such features given pmms mixture process these problems multi topic specific features caused dependency assumptions between labels explicitly implicitly made existing methods solve these problems propose maximal margin labeling treats labels independent classes learns multi class classifier induced multi class problem paper first discuss why multi class classifiers cannot directly applied mtc section propose mml section address implementation issues section section mml experimentally compared existing methods using collection multi topic web pages summarize paper section solving mtc multi class categorization discuss why existing multi class classifiers do work mtc start multi class classifier proposed use notation given table multi class classifier object into class whose prototype vector closest objects feature vector label class classifier written follows arg max inner product rd rd prototype vector label following similar argument prototype vectors learned solving following maximal margin problem min im li st xi prototype matrix whose columns prototype vectors matrix norm note eq eq cover training samples labels but possible labels because labels unseen training samples may relevant test samples eq penalize violation margin constraints other hand singer penalize largest violation margin constraint each training sample penalize approach leads optimization problem without equality constraints see eq much easier solve than xi li multi class problems such unseen labels exist mtc number labels generally large eg our datasets has labels table unseen labels often exist thus necessary consider possible labels eq eq impossible know unseen labels present test samples two problems eq eq first problem they involve prototype vectors never seen labels without help prior knowledge about prototype vectors should impossible obtain appropriate prototype vectors such labels second problem these equations computationally demanding they involve combinatorial maximization summation over possible labels whose number quite large example number around datasets used our experiments address first problem section second problem section maximal margin labeling section incorporate prior knowledge about location prototype vectors into eq eq propose novel mtc learning algorithm maximal margin labeling mml prior knowledge simply assume prototype vectors similar labels should placed close each other based assumption first rewrite eq yield arg max inner product orthonormal basis classifier eq interpreted two step process first step map vector into second step find closest image replace generally non orthogonal vectors whose geometrical configuration reflects label similarity more formally use vectors satisfy condition inner product vector space spanned mercer kernel similarity measure between labels call vector space spanned vs replacement mmls classifier written follows arg max linear map rd vs solution following problem min li li li st xi li note replaced eq becomes identical eq except scale factor thus eq eq natural extensions multi class classifier call mtc classifier eq eq maximal margin labeling mml figure explains margin inner product eq mml margin represents distance image training sample xi boundary between correct label li wrong label mml optimizes linear map so smallest margin between training samples possible labels becomes maximal along penalty case samples into margin fifigure maximal margin labeling dual form numerical computation following dual form eq more convenient omit its derivation due space limits sli li sli li max xi xi sli sli st li denote li dual variables corresponding first inequality constraints eq note eq does contain computations involving done through label similarity additionally xi appears inner products therefore replaced kernel using solution eq mmls classifier eq written follows sli arg max sli label similarity examples label similarity use two similarity measures dice measure cosine measure dice measure sd sc cosine measure efficient implementation approximation learning following discussion easily extended include case both empty although do discuss case due space limits eq contains sum over possible labels number topics increases summation rapidly becomes intractable grows exponentially problem approximate sum over possible labels eq partial sum over ac set other zero approximation reduces summation quite lot number reduced huge reduction especially many topics exist understand behind approximation first note dual variable corresponding first inequality constraint margin constraint eq thus non zero xi falls margin between li assume margin violation mainly occurs close li ie ac assumption holds well proposed approximation sum lead good approximation exact solution polynomial time algorithms classification classification mml eq involves combinatorial maximization over possible labels so computationally demanding process efficient classification algorithms available either cosine measure dice measure used label similarity eq divided into number topics label arg max gx ln gx arg max gx gx cn cn sd li sc li li li sd used sc used li li here computational cost eq number non zero eq log thus total cost classification eq log other hand under approximation described above therefore classification done within computational steps significant reduction case force search used eq experiments section report experiments compared mml pmm svm using collection web pages used normalized linear kernel kx mml svm real used weak learner experimental setup datasets used our experiment represent web page collection used table web pages collected through top each topic svm classifier trained predict whether topic relevant positive irrelevant negative input name ar bu computers co ed en he rc reference rf science si social science society sc text label size frequency table summary web page datasets text number texts dataset number vocabularies ie features number topics number labels label size frequency relative frequency each label size label size number topics label method mml pmm svm boost feature type tf tf tf binary parameter model model table candidate feature types learning parameters number weak hypotheses parameters selected evaluation test data divided into datasets top category each page labeled second level sub categories page thus sub categories topics our term see more details about collection web pages converted into three types feature vectors binary vectors each feature indicates presence absence term tf vectors each feature number term term frequency vectors each feature product term frequency inverse document frequency select best combinations feature types learning parameters such penalty mml learners trained web pages combinations feature parameter table evaluated labeling measure independently drawn development data combinations achieve best labeling measures table used following experiments evaluation measures used three measures evaluate labeling performance labeling measure exact match ratio retrieval measure following definitions lpred ltrue mean predicted labels true labels respectively labeling measure labeling measure evaluates average labeling performance while taking partial match into account lpred ltrue lpred ltrue li lpred ltrue ar bu co ed en he rc rf si sc md labeling measure mc pm sv bo md exact match ratio mc pm sv bo md retrieval measure mc pm sv bo table performance comparison labeling measure left exact match ratio middle retrieval measure right figures best ones among five methods figures second best ones md mc pm sv bo represent mml sd mml sc pmm svm respectively exact match ratio exact match ratio ex counts exact matches between predicted label true label ex true li statement true otherwise retrieval measure real tasks important evaluate retrieval performance ie accurately classifiers find relevant texts given topic retrieval measure measures average retrieval performance over topics li lpred ltrue results first trained classifiers randomly chosen samples calculated three evaluation measures other randomly chosen samples process repeated five times resulting averaged values shown table table shows mmls dice measure outperform other methods labeling measure exact match ratio mmls show best performance regard retrieval although margins other methods large observed labeling measure exact match ratio note classifier except mml dice measure achieves good labeling three measures example pmm shows high labeling measures but its performance rather poor evaluated retrieval measure second experiment evaluated classifiers trained training samples same test samples figure shows each measure averaged over datasets observed mmls show high generalization even training data small interesting point mml cosine measure achieves rather high labeling measures retrieval measure training data smaller size such high does continue trained larger data called macro average measures text categorization community fifigure learning curve labeling measure left exact match ratio middle retrieval measure right md mc pm sv bo mean same table conclusion paper proposed novel learning algorithm multi topic text categorization algorithm maximal margin labeling labels sets topics into vector space learns large margin classifier space overcome demanding computational cost mml provide approximation method learning efficient classification algorithms experiments collection web pages mml outperformed other methods including svm showed better generalization authors would like thank providing pmms codes datasets references text categorization support vector machines learning many relevant features line editors proc th conference machine learning number pages schapire singer boosting based system text categorization machine learning mixture models multi topic text advances neural information processing systems pages singer algorithmic implementation multiclass kernel based vector machines journal machine learning research
spike sorting involves clustering spike trains recorded according source neuron complicated problem requires lot human partly due non stationary nature data propose automated technique clustering non stationary gaussian sources bayesian framework first search stage data divided into short time frames candidate descriptions data mixture gaussians computed each frame second stage transition probabilities between candidate mixtures computed globally optimal clustering found map solution resulting probabilistic model transition probabilities computed using local assumptions based gaussian version shannon divergence method applied several recordings performance almost humans wide range scenarios including movement splits clusters
structural similarity neural networks genetic regulatory networks digital circuits hence each other noted their study work propose simple biochemical system whose architecture genetic regulation whose components allow implementation arbitrary circuits use two addition dna rna molecules rna develop rate equation networks derive correspondence general neural network rate equations proof principle associative memory task feedforward network computation shown simulation difference between neural network biochemical models global coupling rate equations through saturation lead global feedback regulation thus allowing simple network without explicit mutual inhibition perform winner take computation thus full complexity cell necessary biochemical computation wide range functional behaviors achieved small set biochemical components
standard approach classification objects consider examples independent distributed iid many real world settings assumption valid because relationship exists between objects contribution consider special case image segmentation objects pixels underlying topography regular rectangular grid introduce classification method uses measured feature information but label configuration within topographic neighborhood due resulting dependence between labels neighboring pixels collective classification set pixels becomes necessary propose new method called topographic support vector machine based topographic kernel self consistent solution label assignment shown equivalent recurrent neural network performance algorithm compared conventional svm cell image segmentation task
present probabilistic approach learning gaussian process classifier presence unlabeled data our approach involves null category noise model inspired ordered noise models noise model reflects assumption data density lower between class conditional densities illustrate our approach toy problem present comparative results semi supervised classification handwritten digits
paper generalization classic rescorla wagner learning algorithm studies their relationship maximum likelihood estimation causal parameters prove parameters two popular causal models learnt same generalized linear rescorla wagner algorithm provided conditions apply characterize fixed points these algorithms calculate fluctuations about them assuming input set iid samples fixed unknown distribution describe determine convergence conditions calculate convergence rates algorithms under these conditions
paper analyses divergence algorithm learning statistical parameters relate algorithm stochastic approximation literature enables us specify conditions under algorithm guaranteed converge optimal solution probability includes necessary sufficient conditions solution unbiased
propose convex optimization based strategy deal uncertainty observations classification problem assume instead sample xi yi distribution over xi yi specified particular derive robust formulation distribution given normal distribution leads second order cone programming formulation our method applied problem missing data outperforms direct
describe used data set composed train hidden markov models using probabilistic framework allows us create system learns examples new make quantitative comparison our systems performance against simpler models provide example
paper linear multilayer ica lmica proposed extracting independent components quite high dimensional observed signals such large size natural scenes two phases each layer lmica mapping phase dimensional mapping formed stochastic gradient algorithm makes more non independent signals incrementally another local ica phase each neighbor namely highly correlated pair signals mapping separated algorithm because lmica separates highly correlated pairs instead ones extract independent components quite efficiently appropriate observed signals addition proved lmica always converges numerical experiments verify lmica quite efficient effective large size natural image processing
prediction suffix trees pst provide popular effective tool tasks such compression classification language modeling paper take decision theoretic view task sequence prediction generalizing notion margin present online pst learning algorithm derive loss bound depth pst generated algorithm scales linearly length input describe self bounded enhancement our learning algorithm automatically grows bounded depth pst prove analogous mistake bound self bounded algorithm result efficient algorithm neither relies priori assumptions shape maximal depth target pst nor does require parameters our knowledge first provably correct pst learning algorithm generates bounded depth pst while being competitive fixed pst determined
important aspect clustering algorithms whether partitions constructed finite samples converge useful clustering whole data space sample size increases paper investigates question normalized unnormalized versions popular spectral clustering algorithm surprisingly convergence unnormalized spectral clustering more difficult handle than normalized case even though recently first results convergence normalized spectral clustering have been obtained unnormalized case have develop completely new approach combining tools numerical integration spectral perturbation theory probability turns out while normalized case spectral clustering usually converges nice partition data space unnormalized case same holds under strong additional assumptions always satisfied conclude our analysis gives strong evidence superiority normalized spectral clustering provides basis future exploration other laplacian based methods
computation without stable states computing paradigm different has been demonstrated various types simulated neural networks hardware implemented neural network results software implementation showing performance peaks network exhibits dynamics edge chaos computing approach seems well suited operating analog computing devices such used vlsi neural network
consider problem geometrical surface reconstruction several images using learned shape models while humans retrieve shape information inverse problem has turned out difficult perform automatically introduce framework based level set surface reconstruction shape models achieving goal through merging obtain efficient robust method reconstructing surfaces object category interest shape model includes surface cues such point curve features based ideas active shape models show both geometry appearance these features modelled consistently multi view context complete surface obtained level set driven tries fit surface inferred features addition priori surface model used solution particular surface features sparse experiments demonstrated database real face images
paper show possible model sensory about straightforward task reason induce function maps object descriptions into ratings consider ratings way express their preferences about products presented same testing session therefore had use special purpose svm polynomial kernel training data set used ratings experts provided different weights periods additionally gain insight into preferences used feature subset selection tools result most important improving
problem detecting objects outliers classical topics robust statistics recently has been proposed address problem means class svm classifiers main conceptual most class approaches strict sense they unable detect outliers expected fraction outliers has specified advance method presented paper overcomes problem relating class classification gaussian density estimation induced feature space having established relation possible identify objects quantifying their deviations gaussian model rbf kernels shown gaussian model rich enough sense asymptotically provides unbiased estimator true density order overcome inherent model selection problem cross validated likelihood criterion selecting free model parameters applied
establish mistake bound ensemble method classification based maximizing entropy weights subject margin constraints bound same general bound proved weighted majority algorithm similar bounds other variants winnow prove more refined bound leads nearly optimal algorithm learning again based maximum entropy principle describe line maximum entropy method after each iteration margin constraints replaced single linear inequality simplified algorithm takes similar form winnow achieves same mistake bounds
provide worst case analysis selective sampling algorithms learning linear threshold functions algorithms considered paper perceptron like algorithms ie algorithms efficiently run reproducing kernel hilbert space our algorithms exploit simple margin based randomized rule decide whether query current label obtain selective sampling algorithms achieving average same bounds those proven their deterministic counterparts but using much fewer labels complement our theoretical findings empirical comparison two text categorization tasks outcome these experiments largely predicted our theoretical results our selective sampling algorithms tend perform good algorithms receiving true label after each classification while observing practice substantially fewer labels
describe framework learning object classifier single example goal achieved relevant dimensions classification using available examples related classes learning accurately classify objects single training example often due overfitting effects instance representation provides distance between each two instances same class smaller than distance between two instances different classes nearest neighbor classifier could achieve perfect performance single training example therefore suggest two stage strategy first learn metric over instances achieves distance criterion mentioned above available examples other related classes using single examples define nearest neighbor classifier distance evaluated learned class relevance metric finding metric relevant dimensions classification might possible restricted linear projections therefore make use kernel based metric learning algorithm our setting encodes object instances sets locality based descriptors appropriate image kernel class relevance metric learning proposed framework learning single example demonstrated synthetic setting character classification task
what makes neural microcircuit computationally powerful more precisely quantities could explain why microcircuit better suited particular family computational tasks than another microcircuit propose article quantitative measures evaluating computational power generalization capability neural microcircuit apply them generic neural microcircuit models drawn different distributions validate proposed measures comparing their prediction direct evaluations computational performance these microcircuit models procedure applied first microcircuit models differ regard spatial range synaptic connections regard scale synaptic circuit microcircuit models differ regard level background input currents level noise membrane potential neurons case proposed method allows us quantify differences computational power generalization capability circuits different dynamic regimes down states have been demonstrated through intracellular recordings vivo
paper present framework using multi layer perceptron mlp networks nonlinear generative models trained variational bayesian learning nonlinearity handled using hidden neurons yields accurate approximation cases large posterior variance method used derive nonlinear counterparts linear algorithms such factor analysis independent analysis state space models demonstrated nonlinear factor analysis experiment even sources estimated real world speech data set
auditory scene composed overlapping acoustic sources viewed complex object whose constituent parts individual sources pitch known important cue auditory scene analysis paper goal building agents operate human environments describe real time system identify presence more compute their pitch signal processing front end based instantaneous frequency estimation method tracking speech while pattern matching back end based nonnegative matrix factorization unsupervised algorithm learning parts complex objects while supporting framework analyze complicated auditory scenes our system maintains real time state art performance clean speech
log important property context optimization laplace approximation sampling bayesian methods based gaussian process priors have become quite popular recently classification regression density estimation point process intensity estimation here prove predictive densities corresponding each these applications log concave given observed data prove likelihood log concave hyperparameters controlling mean function gaussian prior density point process intensity estimation cases mean covariance observation noise parameters classification regression cases result leads useful parameterization these hyperparameters indicating suitably large class priors corresponding maximum posteriori problem log concave
present algorithm based convex optimization constructing kernels semi supervised learning kernel matrices derived spectral decomposition graph combine labeled unlabeled data systematic fashion unlike previous work using diffusion kernels gaussian random field kernels nonparametric kernel approach presented incorporates order constraints during optimization results flexible kernels avoids need choose among different parametric forms our approach relies constrained quadratic program computationally feasible large datasets evaluate kernels real datasets using support vector machines encouraging results
has been substantial progress past development object classifiers images example faces humans here address problem eg occlusion test images have explicitly been encountered training data variational classifier algorithm models contamination field binary variables strong spatial coherence prior variational inference used over contamination obtain robust classification way approach turn kernel classifier clean data into contamination without specific training contaminated positives
model paired comparison has been popular many areas propose generalized version paired individual comparisons extended paired team comparisons introduce simple algorithm convergence proofs solve model obtain individual useful application multi class probability estimates using error correcting codes demonstrated
regularization plays central role analysis modern data non regularized fitting likely lead over fitted models both prediction interpretation consider design incremental algorithms follow paths regularized solutions regularization varies these approaches often result methods both efficient highly flexible suggest general path following algorithm based second order approximations prove under mild conditions remains close path optimal solutions illustrate examples
present algorithm overcome local maxima problem estimating parameters mixture models combines existing approaches both em robust fitting algorithm give data driven stochastic learning scheme minimal subsets data points sufficient constrain parameters model drawn proposal densities discover new regions high likelihood proposal densities learnt using em bias sampling toward promising solutions algorithm computationally efficient well effective local maxima compare alternative methods including em both challenging synthetic data computer vision problem
computation classical higher order statistics such higher order moments spectra difficult images due huge number terms estimated interpreted propose alternative approach multiplicative pixel interactions described series wiener functionals functionals estimated implicitly via polynomial kernels combinatorial explosion associated classical higher order statistics avoided first results show image structures such lines corners predicted correctly pixel interactions up order five play important role natural images most interesting structure natural image characterized its higher order statistics arbitrarily oriented lines edges instance cannot described usual pairwise statistics such power spectrum function intensity point line alone cannot predict its intensities would require knowledge second point line ie have consider third order statistics describe interactions between points prediction corner needs least fourth order statistics so terms fourier analysis higher order image structures such edges corners described phase ie phase correlations between several fourier components image harmonic phase interactions measured higher order spectra unfortunately estimation these spectra high dimensional signals such images involves estimation interpretation huge number terms instance order spectrum sized image contains roughly coefficients about would have estimated independently symmetries spectrum considered first attempts estimating higher order structure natural images therefore restricted global measures such kurtosis fourth order spectra here propose alternative approach models interactions image points series wiener functionals wiener functional order captures those image components predicted multiplicative interaction image points contrast higher order spectra moments estimation wiener model does require estimation number terms computed implicitly polynomial kernels allows us decompose image into components characterized interactions given order next section introduce wiener expansion discuss its capability modeling higher order pixel interactions implicit estimation method described followed examples use conclude briefly results possible improvements modeling pixel interactions wiener functionals our analysis adopt prediction framework given image pixel want predict its gray value gray values particularly interested extent interactions different orders contribute overall prediction our basic assumption dependency central pixel value its xi modeled series hn discrete volterra functionals hn hi xi here have into vector xm rm discrete nth order volterra functional accordingly linear combination ordered nth order monomials elements mn coefficients hi volterra functionals provide controlled way introducing multiplicative interactions image points functional order contains products input order terms higher order statistics means control order statistics used nth order volterra series leads dependencies between maximally pixels unfortunately volterra functionals orthogonal each other ie depending input distribution functional order generally leads additional lower order interactions result output functional contain components proportional lower order monomials instance output second order volterra functional gaussian input generally has mean different zero estimate order component image ie constant component created without pixel interactions constant component created second order interactions needs general volterra series correction achieved into new series gn functionals gn uncorrelated ie orthogonal respect input resulting wiener functionals gn linear combinations volterra functionals up order they computed original volterra series procedure akin gram shown wiener expansion finite degree minimizes mean squared error between true system output its volterra series model orthogonality condition ensures wiener functional order captures component image created multiplicative interaction pixels contrast general volterra functionals wiener functional orthogonal monomials lower order so far have gained compared classical estimation higher order moments spectra nth order volterra functional contains same number terms strictly term wiener functional orthogonal volterra functionals respect gaussian input here term used volterra functionals arbitrary input distributions corresponding order spectrum wiener functional same order has even higher number coefficients consists lower order volterra functionals next section introduce implicit representation wiener series using polynomial kernels allows efficient computation wiener functionals estimating wiener series regression rkhs volterra series linear functionals rkhs nth order volterra functional weighted sum nth order monomials input vector interpret evaluation functional given input map defined xn xn xn such maps input rm into vector fn rm containing mn ordered monomials degree using write nth order volterra functional eq scalar product fn hn coefficients into vector fn same idea applied entire pth order volterra series maps into single map obtains mapping rm into rm rm rm rm dimensionality entire pth order volterra series written scalar product hn below show express expansion terms training points dramatically reduce number parameters have estimate procedure works because space fn nth order monomials has special property has structure reproducing kernel hilbert space rkhs consequence dot product fn computed evaluating positive definite kernel function kn monomials easily show eg kn generated direct sum single spaces fn associated scalar product simply sum scalar products fn thus have shown volterra series expressed linear functional rkhs linear regression rkhs our prediction problem rkhs property volterra series leads efficient solution part due so called theorem eg states following suppose given observations similar approach has been taken using polynomial kernel kernel implies map into same space monomials but weights degrees monomials differently seen applying theorem fix xn function arbitrary cost function function norm rkhs associated kernel minimize objective function xn xn over functions rkhs optimal solution expressed aj kx xj aj other words although optimized over entire rkhs including functions defined arbitrary input points turns out always express solution terms observations xj hence optimization problem over extremely large number coefficients eq transformed into over variables aj let us consider special case cost function mean squared error xn xn xj regularizer zero solution readily computed setting derivative respect vector equal zero takes form gram matrix defined xj hence zx zx zx kx kx kx xn implicit wiener series estimation stated above pth degree wiener expansion pth order volterra series minimizes squared error put into regression framework finite volterra series represented linear functional corresponding rkhs find pth order volterra series minimizes squared error linear regression definition pth degree wiener series other volterra series has property obtain following expressions implicit wiener series gn hn kp gram matrix kp coefficient vector computed using kernel eq rn note wiener series represented implicitly using rkhs representation sum scalar products training points thus avoid curse dimensionality ie need compute possibly large number coefficients explicitly explicit volterra wiener expansions recovered eq terms containing monomials desired order summing them up individual nth order volterra functionals wiener series degree given implicitly hn kp xn term constant zero order volterra functional coefficient vector explicit volterra functional obtained kp conditions uniqueness solution see note different regularized approach used zero resulting volterra series different wiener series they orthogonal respect input denotes pseudo inverse assuming volterra kernels obtained volterra design matrix individual wiener functionals recovered applying regression procedure twice interested nth degree wiener functional have compute solution kernels wiener functional obtained difference two results gn kn kn corresponding ith order volterra functionals nth degree wiener functional computed orthogonality resulting wiener functionals fulfill orthogonality condition its form states pth degree wiener functional orthogonal monomials input lower order formally prove following theorem functionals obtained eq fulfill orthogonality condition denotes expectation over input distribution arbitrary show consequence least squares fit linear expansion set basis functions form case wiener volterra expansions basis functions monomials components denote error expansion ex xi minimum expected quadratic loss respect expansion coefficient given ex means expansion set basis functions minimizing squared error error orthogonal basis functions used expansion now let us assume know wiener series expansion minimizes mean squared error system up degree approximation error given sum higher order wiener functionals ex np gn so gp part error consequence linearity expectation eq implies np np order less than difference both equations yields so gp orthogonal lower order basis functions namely monomials order smaller than experiments toy examples our first experiment check whether our about higher order statistics described
new distance measure between probability density functions introduced refer laplacian pdf distance laplacian pdf distance exhibits remarkable connection mercer kernel based learning theory via parzen window technique density estimation kernel feature space defined laplacian data matrix pdf distance shown measure cosine angle between cluster mean vectors laplacian data matrix hence its obtained automatically based data hand optimal parzen window selection show laplacian pdf distance has interesting interpretation risk function connected probability error
many interesting multiclass problems cast general framework label ranking defined given set classes evaluation such ranking generally given terms number violated order constraints between classes paper propose preference learning model unifying framework model solve large class multiclass problems large margin perspective addition original kernel based method proposed evaluated ranking dataset state art results
paper presents application boosting classifying labeled graphs general structures modeling number real world data such chemical compounds natural language texts bio sequences proposal consists decision use features ii boosting algorithm based decision used weak learners discuss relation between our algorithm svms convolution kernels two experiments using natural language data chemical compounds show our method achieves comparable even better performance than svms convolution kernels well improves testing efficiency
dominant sets new graph theoretic concept has proven relevant pairwise data clustering problems such image segmentation they generalize notion maximal clique graphs have non trivial connections continuous quadratic optimization spectral based grouping address problem grouping out sample examples after clustering process has taken place may serve either drastically reduce computational associated processing large data sets efficiently deal dynamic situations whereby data sets need updated show notion dominant set offers simple efficient way doing numerical experiments various grouping problems show effectiveness approach
de sequencing challenging task research while exist reliable dna sequencing methods de sequencing proteins mass still open problem current approaches suffer lack precision detect mass peaks spectrograms paper present novel method de sequencing based hidden markov model experiments effectively demonstrate new method significantly outperforms standard approaches matching quality
paper analyze relationship between computational capabilities randomly connected networks threshold gates timeseries domain their dynamical properties particular propose complexity measure find assume its highest values near edge chaos ie transition ordered chaotic dynamics furthermore show proposed complexity measure predicts computational capabilities well near edge chaos such networks able perform complex computations time series additionally simple synaptic scaling rule self organized presented analyzed
propose selectively remove examples training set using probabilistic estimates related algorithms heuristic procedure aims creating separable distribution training examples minimal impact position decision boundary breaks linear dependency between number number training examples reduces complexity svms during both training prediction stages
prove generalization error bounds predicting entries partially observed matrix fitting observed entries low rank matrix analysis approach take obtain bounds present example class functions finite such sums functions class have unbounded
co training method combining labeled unlabeled data examples thought containing two distinct sets features has had number practical yet previous theoretical analyses have needed strong assumptions data unlikely satisfied practice paper propose much weaker expansion assumption underlying data distribution prove sufficient iterative given appropriately strong pac learning algorithms each feature set extent necessary well expansion assumption fact iterative nature original co training algorithm unlike stronger assumptions such independence given label allow simpler shot co training analyze effect performance noise data predicted behavior qualitatively matched synthetic experiments graphs
learning algorithms have numerous robotic control tasks problems time varying dynamics online learning methods have proved powerful tool automatically tracking andor adapting changing circumstances critical applications such flight these algorithms has been significantly their lack such stability guarantees rather than trying show difficult priori stability guarantees specific learning methods paper propose method monitoring controllers suggested learning algorithm online rejecting controllers leading instability prove even arbitrary online learning method used our algorithm control linear dynamical system resulting system stable
propose new set criteria learning algorithms multi agent systems more argue better justified than previous proposed criteria our criteria apply most repeated games average rewards consist three requirements against specified class opponents class parameter criterion algorithm yield payoff approaches payoff best response against other opponents algorithms payoff least approach possibly exceed security level payoff value subject these requirements algorithm achieve close optimal payoff self play furthermore require these average payoffs achieved quickly present novel algorithm show these new criteria particular parameter class class stationary opponents finally show algorithm effective theory but empirically using recently introduced comprehensive game theoretic test show algorithm almost outperforms previous learning algorithms
describe semi markov conditional random fields semi crfs conditionally trained version semi markov chains intuitively input sequence outputs segmentation labels assigned segments ie subsequences rather than individual elements xi importantly features semi crfs measure properties segments transitions within segment non markovian additional power exact learning inference algorithms semi crfs polynomial time often small constant factor slower than conventional crfs experiments five named entity recognition problems semi crfs generally outperform conventional crfs
give fast rejection scheme based image segments demonstrate canonical example face detection instead focusing detection step focus rejection step show our method simple fast learned thus making excellent pre processing step standard machine learning classifiers such neural networks bayes classifiers svm decompose collection face images into regions pixels similar behavior over image set relationships between mean variance image segments used form cascade reject over image patches thus small fraction image patches passed full scale classifier moreover training time our method much less than standard shape features ie image segments use data driven they cheap compute they form low dimensional feature space exhaustive search best features tractable
paper proposes method computing fast approximations support vector decision functions field object detection present approach building existing algorithm set support vectors replaced smaller so called reduced set synthesized input space points contrast existing method finds reduced set via unconstrained optimization impose structural constraint synthetic points such resulting approximations evaluated via separable filters applications require large images decreases computational complexity significant amount experimental results show face detection rank approximations times faster than unconstrained reduced set systems
introduce novel active learning scenario user work learning algorithm identify useful anomalies these distinguished traditional statistical definition anomalies outliers merely ill modeled points our distinction usefulness anomalies user make two additional assumptions first exist extremely few useful anomalies down within massive dataset second both useful anomalies may sometimes exist within classes similar anomalies challenge thus identify rare category records unlabeled noisy set help form class labels human expert who has small datapoints they propose technique meet challenge assumes mixture model fit data but otherwise makes assumptions particular form mixture components property wide applicability real life scenarios various statistical models give several alternative methods their strengths conclude detailed empirical analysis show our method quickly anomaly set containing few tens points dataset hundreds thousands
computation memory required kernel machines training samples least such complexity significant even moderate size problems large datasets present approximation technique based improved fast transform reduce computation give error bound approximation provide experimental results uci datasets
novel linear feature selection algorithm presented based global minimization data dependent generalization error bound feature selection scaling algorithms often lead non convex optimization problems many previous approaches addressed through gradient descent procedures guarantee convergence local minimum propose alternative approach whereby global solution non convex optimization problem derived via equivalent optimization problem moreover convex optimization task reduced conic quadratic programming problem efficient available highly competitive numerical results both artificial real world data sets reported
during last ten years has been growing interest development brain computer interfaces field has mainly been driven needs completely patients communicate few most human based eeg reported bit rates still low reason low signal noise ratio eeg currently investigating based viable alternative paper present method examples eeg recordings three patients electrode grids placed motor cortex patients asked repeatedly imagine movements two kinds eg movements analyze data using support vector machines svms recursive channel elimination rce
describe methods computing implicit model given finite sampling methods work mapping sample points into reproducing kernel hilbert space determining regions terms hyperplanes
develop family upper lower bounds worst case expected kl loss estimating discrete distribution finite number points given iid samples our upper bounds similar recent bounds estimating discrete entropy lower bounds bayesian based averages kl loss under dirichlet distributions upper bounds convex their parameters thus minimized descent methods provide estimators low worst case error lower bounds dimensional parameter thus easily maximized asymptotic analysis bounds demonstrates uniform kl consistency wide class estimators nm matter slowly shows estimator consistent bounded contrast entropy estimation moreover bounds asymptotically tight shown numerically tight within factor two finally sparse data limit find dirichlet bayes add constant estimator parameter scaling like optimizes both upper lower bounds suggesting optimal choice add constant parameter regime
paper propose combine two powerful ideas boosting manifold learning hand improve adaboost incorporating knowledge structure data into base classifier design selection other hand use efficient learning mechanism significantly improve supervised semi supervised algorithms proposed context manifold learning specific manifold based resulting algorithm boosting large family regularized learning algorithms
propose based distributed algorithm gaussian mixture learning em algorithm operates network topologies each node local quantity communicate other nodes arbitrary point point fashion main difference between em standard em algorithm step our case implemented manner random pairs nodes repeatedly exchange their local parameter estimates combine them weighted averaging provide theoretical evidence demonstrate experimentally under protocol nodes converge exponentially fast correct estimates each step em algorithm
amino acid profiles capture position specific mutation probabilities richer encoding biological sequences than individual sequences themselves profile comparisons much more computationally expensive than discrete symbol comparisons making profiles impractical many large datasets furthermore because they such rich representation profiles difficult overcome these problems propose discretization profiles using expanded alphabet representing individual amino but common profiles using extension information bottleneck ib incorporating constraints priors class distributions find optimal alphabet discretization yields concise informative representation profile sequences between these sequences while nearly accurate full computed almost quickly those between individual consensus sequences full pairwise alignment would take years using profiles but less than using discrete ib encoding illustrating discrete encoding expand range sequence problems profile information applied
formulate problem graph inference part graph known supervised learning problem propose algorithm solve method involves learning mapping vertices euclidean space graph easy infer formulated optimization problem reproducing kernel hilbert space report encouraging results problem network reconstruction data
paper explore use random rfs structured language model uses rich syntactic information predicting next word based words already seen goal work construct rfs randomly growing decision trees using syntactic information investigate performance modeled rfs automatic speech recognition rfs originally developed classifiers combination decision tree classifiers each tree based random training data sampled independently same distribution trees random selection possible questions each node decision tree our approach extends original idea rfs deal data sparseness problem encountered language modeling rfs have been studied context gram language modeling have been shown generalize well unseen data show paper rfs using syntactic information achieve better performance both word error rate large vocabulary speech recognition system compared baseline uses smoothing
visual action recognition important problem computer vision paper propose new method probabilistically model recognize actions articulated objects such hand body gestures image sequences our method consists three levels representation low level first extract feature vector invariant scale plane rotation using fourier transform spatial histogram spectral partitioning utilized obtain initial clustering clustering refined using temporal smoothness constraint gaussian mixture model based clustering density estimation subspace linear discriminant analysis lda applied thousands image feature vectors obtain intermediate level representation finally high level build temporal multiresolution histogram model each action aggregating clustering weights sampled images belonging action discuss high level representation extended achieve temporal scaling invariance include bi gram multi gram transition information both image clustering action results given show validity our three representation
high retrieval precision content based image retrieval attained relevance feedback mechanisms these mechanisms require user quality results query retrieved images being either relevant search engine exploits information adapt search better meet users needs present vast majority proposed relevance feedback mechanisms formulated terms search model has optimized such optimization involves modification search parameters so nearest neighbor query vector contains largest number relevant images paper different approach relevance feedback proposed after user provides first feedback following based knn search but computation relevance score each image database score computed function two distances namely distance nearest non relevant image distance nearest relevant images ranked according score top images displayed reported results three image data sets show proposed mechanism outperforms other state art relevance feedback mechanisms rod ct large number content based image retrieval cbir systems rely vector representation images multidimensional feature space representing low level image characteristics eg color texture shape etc content based queries often expressed visual examples order retrieve database images similar examples kind retrieval often referred nearest neighbor retrieval easy see effectiveness content based image retrieval systems cbir strongly depends choice set visual features choice metric used model users perception image similarity choice image used query database typically allow different users mark images retrieved given query non relevant different subsets images marked relevant accordingly need mechanisms adapt cbir system response based feedback user widely recognized interesting note while relevance feedback mechanisms have been first introduced information retrieval field they receiving more attention cbir field vast majority relevance feedback techniques proposed literature based modifying values search parameters better represent concept user mind end search parameters computed function relevance values assigned user images retrieved so far example relevance feedback often formulated terms modification query vector andor terms adaptive similarity metrics recently pattern classification paradigms such svms have been proposed feedback thus used model concept relevant images adjust search consequently concept modeling may difficult account distribution relevant images selected feature space narrow domain image databases allows extracting good features so images similar concepts belong compact clusters other hand broad domain databases such image collection used those made up images more difficult cluster because high variability concepts these cases extracting low level non specialized features image retrieval better formulated terms search problem rather concept modeling present paper aims original contribution direction rather modeling concept relevance user mind feedback used each image database relevance score such score depends two dissimilarities distances computed against images already marked user dissimilarity set relevant images dissimilarity set non relevant images despite its computational simplicity mechanism allows outperforming state art relevance feedback mechanisms both narrow domain databases broad domain databases paper organized follows section illustrates idea behind proposed mechanism provides basic assumptions section details proposed relevance feedback mechanism results three image data sets presented section performances other relevance feedback mechanisms compared conclusions drawn section st rel ce proposed mechanism has been inspired classification techniques based nearest case nearest case theory provided mechanism compute dissimilarity each image sets relevant non relevant images ratio between nearest relevant image nearest non relevant image has been used compute degree relevance each image database present section illustrates behind use nearest case paradigm let us assume each image database has been represented number low level features dissimilarity measure has been defined so proximity between pairs images represents kind conceptual similarity other words chosen feature space similarity metric meaningful least restricted number users search image databases usually performed retrieving most similar images respect given query dimension usually small avoid large number images time typical values between relevant images user retrieve may fit perfectly similarity metric designed search engine user may interested exploring other regions feature space end user subset relevant images out retrieved usually such relevance feedback used perform new nn search modifying search parameters ie position query point similarity metric other tuning parameters recent works proposed use support vector machine learn distribution relevant images these techniques require assumption about general form distribution relevant images feature space difficult make assumption about such distribution broad domain databases propose exploit information about relevance images retrieved so far nearest neighbor fashion nearest neighbor techniques used statistical pattern recognition case based reasoning instance based learning effective applications difficult produce high level generalization class objects relevance learning content base image retrieval may well fit into definition difficult provide general model adapted represent different concepts similarity addition number available cases may small estimate optimal set parameters such general model other hand more effective use each relevant image well each non relevant image cases instances against images database should compared consequently assume image much relevant much its dissimilarity nearest relevant image small image much non relevant much its dissimilarity nearest non relevant image small rel ce core com ti according previous section each image database thus characterized degree relevance degree non relevance according dissimilarities nearest relevant image nearest non relevant image respectively should noted these degrees should treated differently because relevant images represent concept users mind while non relevant images may represent number other concepts different users interest other words while meaningful treat degree relevance degree membership class relevant images same does apply degree non relevance reason propose use degree non relevance weight degree relevance let us denote subset indexes related set relevant images retrieved so far original query relevant default subset indexes related set non relevant images retrieved so far each image database according nearest neighbor rule let us compute dissimilarity nearest image dissimilarity nearest image let us denote these dissimilarities respectively value clearly used measure degree relevance image assuming small values related relevant images other hand hypothesis image relevant users query supported high value accordingly defined relevance score fi dr relevance dn formulation score easily explained terms nn estimation posterior probability image relevant nearest neighbors made up nearest relevant image nearest image while weights computed inverse distance nearest neighbors relevance score computed according equation used rank images first presented user exp en order test proposed method compare other methods described literature three image databases have been used mit database database contained uci repository subset corel database these databases currently used comparing relevance feedback techniques mit database collected mit database contains texture images have been manually classified into classes each these images has been into non overlapping images obtaining data set images gabor filters used these images so each image represented dimensional feature vector database extracted uci repository consists outdoor images images into seven data classes window path spatial features each image details reported uci web site database extracted corel collection available uci repository used subset made up images manually into classes each image four sets features available web site paper report results related color moments features co occurrence texture features feature sets each dataset euclidean distance metric has been used linear procedure has been performed so each feature takes values range between first two databases each image used query while corel database images have been randomly extracted used query so classes represented each retrieval iteration images relevance feedback performed images belonging same class query relevant other images non relevant users query itself included set relevant images experimental set up objective comparison among different methods currently used many researchers results evaluated term retrieval precision averaged over considered queries precision measured fraction relevant images contained top retrieved images first two databases narrow domain type while third broad domain type experimental set up allowed testing proposed technique comparison retrieval performances obtained two methods recently described literature reported mindreader query vector similarity metric account features relevance bayes qs bayesian query based query these two methods have been selected because they easily implemented their performances compared those provided large number relevance feedback techniques proposed cbir literature see example results presented results presented different papers cannot directly compared each other because they related common experimental set up they related same data sets similar experimental set up qualitative comparisons let us conclude performance two above techniques quite close other results literature experiments ith th mi database database considered narrow domain type contains images textures different types addition selected feature space suited measure texture similarity figure show performances proposed relevance feedback mechanism those two techniques used comparison precision relevance score bayes qs mindreader rf rf rf rf rf rf rel feedback rf rf rf figure retrieval performances mit database terms average percentage retrieval precision after first feedback iteration rf graph each relevance feedback mechanism able improve average precision attained first retrieval more than proposed mechanism performing slightly better than mindreader desired behaviour user typically allows few iterations user aims better refine search additional feedback iteration mindreader bayes qs able exploit additional information they provide improvements after second feedback iteration other hand proposed mechanism provides further improvement precision increasing number iteration these improvements small first feedback already provides high precision value near experiments ith th uc database database considered narrow domain type images clearly belong seven data classes features have been extracted accordingly precision relevance score bayes qs mindreader rf rf rf rf rf rel feedback rf rf rf rf figure retrieval performances uci data set terms average percentage retrieval precision figure show performances attained uci database retrieval precision high after first extraction feedback each considered mechanism able exploit relevance feedback mindreader bayes qs providing improvement while proposed mechanism attains improvement example clearly shows superiority proposed technique attains precision after second iteration further iterations allow attaining precision other hand bayes qs exploits further feedback iteration attaining precision after iterations while mindreader does improve precision attained after first iteration user typically allows few feedback iterations proposed mechanism proved suited narrow domain databases allows attaining precision close experiments ith th co rel figures show performances attained two feature sets extracted corel database database broad domain type images represent large number concepts selected feature sets represent conceptual similarity between pairs images partly reported results clearly show superiority proposed mechanism let us note retrieval precision after first nn search rf graphs quite small consequence difficulty selecting good feature space represent conceptual similarity between pairs images broad domain database difficulty partially overcome using mindreader bayes qs they allow improving retrieval precision according number iteration allowed according selected feature space let us recall both mindreader bayes qs perform query movement order perform nn more promising region feature space other hand proposed mechanism based ranking images database according relevance score provided higher precision after first feedback but allow improve significantly retrieval precision number iteration increased initial precision quite small user may have more perform further iterations proposed mechanism allows retrieving new relevant images figure retrieval performances corel data set color moments feature set terms average percentage retrieval precision figure retrieval performances corel data set co occurrence texture feature set terms average percentage retrieval precision con si paper proposed novel relevance feedback technique content based image retrieval while vast majority relevance feedback mechanisms aims modeling users concept relevance based available labeled samples proposed mechanism based ranking images according relevance score depending dissimilarity nearest relevant non relevant images behind our choice same case based reasoning instance based learning nearest neighbor pattern classification these techniques provide good performances number available training samples small use statistical techniques case relevance feedback cbir use classification models should require suitable formulation order avoid small sample problems reported results clearly showed superiority proposed mechanism especially large databases made up images related many different concepts addition while many relevance feedback techniques require tuning parameters exhibit high computational complexity proposed mechanism does require parameter tuning exhibit low computational complexity number techniques available speed up distance computations references content based image retrieval end early years ieee trans pattern analysis machine intelligence
motivated particular problems involved locked patients aim develop interface uses auditory stimuli describe paradigm allows user make binary decision focusing attention two concurrent auditory stimulus sequences using support vector machine classification recursive channel elimination independent components averaged potentials show users eeg data classified high level accuracy suggests possible users modulate eeg signals single trial direction attention well enough useful bci
statistical language models estimate probability word occurring given context most common language models rely discrete predictive contexts eg consequently fail capture exploit statistical regularities across these contexts paper show learn hierarchical distributed representations word contexts maximize predictive value statistical language model representations initialized unsupervised algorithms linear nonlinear dimensionality reduction fed input into hierarchical mixture experts each expert multinomial distribution over predicted words while distributed representations our model inspired neural probabilistic language model et al our particular architecture enables us work significantly larger vocabularies training corpora example large scale modeling task involving thousand word vocabulary training corpus three million sentences demonstrate consistent improvement over class based models discuss extensions our approach longer contexts
discuss identification framework noisy speech mixtures block based generative model formulated explicitly incorporates time varying harmonic plus noise hn model number latent sources observed through noisy mixtures parameters including source signals amplitudes phases sources mixing filters noise statistics estimated maximum likelihood using em algorithm exact averaging over hidden sources obtained using kalman smoother show pitch estimation source separation performed simultaneously pitch estimates compared measurements artificial real room mixtures used demonstrate approach speech signals re synthesized estimated hn models
provide method mass meta analysis database containing coordinates experiments database labels used group individual experiments eg according cognitive function consistent pattern experiments within groups determined method each group experiments via kernel density estimation forming probability density values probability density compared null hypothesis distributions generated entire unlabeled set experiments distances used sort voxels across groups experiments allows mass meta analysis construction list most associations between brain areas group labels furthermore method used functional labeling voxels
paper propose new method parametric embedding pe visualizing posteriors estimated over mixture model pe simultaneously both objects their classes low dimensional space pe takes input set class posterior vectors given data points tries preserve posterior structure embedding space minimizing sum kullback leibler divergences under assumption samples generated gaussian mixture equal covariances embedding space pe has many potential uses depending source input data providing insight into classifiers behavior supervised semi supervised unsupervised settings pe algorithm has computational advantage over conventional embedding methods based pairwise object relations its complexity scales product number objects number classes demonstrate pe visualizing supervised categorization web pages semi supervised categorization digits relations words latent topics found unsupervised algorithm latent dirichlet allocation
abstract out core search problem active learning schemes better understand extent adaptive labeling improve sample complexity give various upper lower bounds number labels need prove popular greedy active learning rule approximately good other strategy minimizing number labels
typical neuron visual cortex receives most inputs other cortical neurons roughly similar stimulus preference does arrangement inputs allow efficient sensory information target cortical neuron address issue using simple modelling neuronal population activity information theoretic tools find efficient synaptic information transmission requires tuning curve afferent neurons approximately wide spread stimulus preferences afferent neurons reaching target neuron meta analysis neurophysiological data found case cortico cortical inputs neurons visual cortex suggest organization cortico cortical synaptic inputs allows optimal information transmission
consider semi supervised learning problem decision rule learned labeled unlabeled data framework motivate minimum entropy regularization enables incorporate unlabeled data standard supervised learning our approach includes other approaches semi supervised problem particular limiting cases series experiments illustrates proposed solution benefits unlabeled data method challenges mixture models data sampled distribution class spanned generative model performances minimum entropy regularization generative models weighting unlabeled data provides robustness violation cluster assumption finally illustrate method far superior manifold learning high dimension spaces
alternative splicing important step mammalian gene expression allows single gene specify multiple products crucial regulation fundamental biological processes extent regulation mechanisms involved well understood have developed custom dna microarray platform levels large scale present here generative model array platform genasap demonstrate its utility quantifying levels different tissues learning performed using variational expectation maximization algorithm parameters shown correctly capture expected trends comparison results obtained well established but low through put experimental method demonstrate levels obtained genasap highly predictive levels mammalian tissues biological diversity through alternative splicing current estimates place number genes human approximately surprisingly small number considers has genes number genes alone cannot account complexity cell exhibited higher ie mammals etc added complexity achieved through use alternative splicing whereby single gene used code products genes segments double dna contain information required cell protein synthesis information coded using alphabet corresponding four make up dna what known central molecular biology dna rna turn translated into proteins rna mrna synthesized nucleus cell carries information genes generally both exons contain information needed cell synthesize proteins introns sometimes referred dna out pre mrna create mrna estimated human genes figure four types boxes represent exons lines represent introns possible splicing alternatives indicated single exon exons exons included isoforms single alternative exon alternative exon included isoform other alternative alternative splicing sites both exons but may contain alternative andor splicing sites mutually exclusive exons two alternative exons may included isoform but both inclusion may included mrna yield different combinations exons called isoforms phenomenon referred alternative splicing four major types shown figure many multi exon genes may undergo more than alternative splicing event resulting many possible isoforms single gene addition adding genetic enabling single gene code more than protein has been shown critical gene regulation tissue facilitating evolutionary processes despite importance its regulation impact specific genes remains poorly understood work presented here concerned inference single exon levels figure based data obtained rna expression arrays known microarrays exon microarray data set probes alternative splicing events although possible directly analyze proteins synthesized cell easier often more informative instead measure abundance mrna present traditionally gene expression abundance mrna has been studied using low throughput techniques such rt pcr limited studying few sequences time making large scale analysis nearly impossible early microarray technology emerged method capable measuring expression thousands dna sequences simultaneously sequences interest size small form probes mrna extracted cell reverse back into dna labelled red green molecules respectively sample dna over complementary dna sample probes array forming scanned intensity measured each probe generally assumed intensity measure probe linearly related abundance mrna cell over wide dynamic range despite significant improvements microarray recent years microarray data still presents difficulties analysis low measurements tend have extremely low signal noise ratio snr probes often sequences similar but identical they designed process referred body probes inclusion junction probes exclusion junction probe ca ac cc figure each alternative splicing event studied using six probes probes chosen measure expression levels each three exons involved event additionally probes used target junctions formed each two isoforms inclusion isoform would express junctions formed while exclusion isoform would express junction formed hybridization additionally probes exhibit somewhat varying hybridization efficiency sequences exhibit varying efficiency design our data sets public sequence databases identified exons strong candidates exhibiting details analysis provided elsewhere candidates potential events unique genes selected design custom microarray arrays mrna samples extracted type tissues brain heart muscle each event has six target probes arrays chosen regions exon exon exon splice junction ac splice junction splice junction shown figure unsupervised discovery alternative splicing probe measuring alternative exon figure probes measure sequences occur both isoforms example while sequence probe measuring junction ac designed measure inclusion isoform half corresponds sequence found exclusion isoform therefore assume measured intensity each probe result certain amount both isoforms binding probe due generally assumed linear relationship between abundance mrna probe intensity measured model measured intensity weighted sum overall abundance two isoforms stronger assumption single consistent hybridization profile both isoforms across probes would estimate individual hybridization profile each event studied across our current setup number tissues small resulting two difficulties first number parameters large compared number data point using model second portion events do exhibit tissue specific alternative splicing within our small set tissues while first could accounted using parameter estimation second cannot genasap generative model alternative splicing array platform using setup described above expression vector containing six microarray measurements real numbers decomposed linear combination abundance two splice isoforms represented real vector added noise noise weight matrix containing hybridization profiles xc xc xc xc xc xc oc oc figure graphical model alternative splicing each measurement observed expression profile generated either using scale factor linear combination isoforms drawing randomly outlier model detailed description model see text two isoforms across six probes note may have negative amount given isoform nor presence isoform measured expression so both constrained positive expression levels measured microarrays have previously been modelled having expression dependent noise address rewrite above formulation rs scale factor zero mean normally distributed random variable diagonal covariance matrix denoted prior distribution abundance splice isoforms given truncated normal distribution denoted ps function such si otherwise need account observations eg due probes etc outlier model complete genasap model shown figure accounts observations outcome either applying equation outlier model avoid degenerate cases ensure meaningful results number probes considered each event may exceed two indicated square constraint node figure distribution conditional latent variables xi ri oi xi oi oi random variable indicating measurement probe xi result model outlier model parameterized parameters outlier model optimized set mean variance data fi variational learning genasap model infer posterior distribution over splice isoform abundances while same time learning model parameters use variational expectation maximization algorithm em em maximizes log likelihood data iteratively estimating posterior distribution model given data expectation step maximizing log likelihood respect parameters while keeping posterior fixed maximization step variational em used case genasap exact posterior intractable variational em minimizes free energy model defined kl divergence between joint distribution latent observed variables approximation posterior under model parameters approximate true posterior using distribution given ot rt rt oi rt st st ro ro normalization constant indicates constrained diagonal iid events computational efficiency selected finite set rc uniform probability variational free energy given ot rt st ot rt xt variational em minimizes free energy iteratively updating distributions td parameters ro ro step model parameters rc step resulting updates long shown context paper discussed detail elsewhere few particular points regarding step covering detail here ot rt log prior full normal distribution would need variational approach exact em possible truncated normal distribution mixing proportions cannot calculated analytically except case scalar constraint note allowed full covariance matrix equation would true posterior could find sufficient statistics ot rt ot ot ot xt rt ro ro ot ot diagonal matrix elements oi furthermore easily shown optimal settings approximating normal distribution full covariance mean optimal optimal truncated case equation still true equation does hold though optimal cannot found analytically our experiments found using equation still decreases free energy every step significantly more efficient than using example gradient method compute optimal matrix inclusion isoform exclusion isoform optimal weight matrix inclusion isoform exclusion isoform figure intuitive set weights based biological background would expect see inclusion isoform probes measuring ac while exclusion isoform learned set weights closely agrees intuition captures cross hybridization between probes contribution exclusion isoform contribution inclusion isoform model original data rt pcr model measurement prediction exclusion exclusion outliers figure three examples data cases their predictions data does follow our notion single exon but level predicted accurately probe marked outlier allowing model predict other probes accurately two probes marked outliers model still successful predicting levels making biological predictions about alternative splicing results presented paper obtained using two stages learning first step weight matrix learned subset data selected quality two selection criteria used sequencing data used select those cases high confidence other event present figure probe sets selected high expression determined set negative controls second selection criterion motivated common assumption low intensity measurements quality see section second step kept fixed introduce additional constraint noise isotropic learn entire data set constraint noise introduced prevent model using subset six probes making final set predictions show typical learned set weights figure weights fit well our intuition what they should capture presence two isoforms moreover learned weights account specific trends data examples model prediction based microarray data shown figure due nature microarray data do expect inferred abundances equally good devised scoring criterion ranks each event based its fit model intuitively given two input vectors equivalent up scale factor inferred map estimations equal up same scale factor would like their scores identical scoring criterion used therefore correlation coefficient false positive rate table model performance evaluated various ranks using rt pcr measurements able predict models performance various ranks two evaluation criteria used correlation coefficient between models predictions rt pcr measurements false positive rate prediction considered false positive more than away rt pcr measurement map estimations used scoring criterion viewed proportional sum noise signal ratios estimated using two values given observation models best prediction observation relative amount isoforms most interest need use inferred distribution isoform abundances obtain estimate relative levels immediately clear should done do have measurements events guide us see figure details using top ranked rt pcr measurement fit three parameters such proportion isoform present given map estimation abundance inclusion isoform map estimation abundance exclusion isoform rt pcr measurement used target parameters fitted using gradient descent least squared error evaluation criterion used two criteria evaluate quality model predictions correlation coefficient used evaluate overall ability model correctly estimate trends data invariant affine transformation so independent transformation parameters discussed above while parameter found effect little above top two ranked predictions second evaluation criterion used false positive rate prediction considered false positive more than away rt pcr measurement allows us say example prediction within top within actual levels summary designed novel model inference relative abundance two alternatively isoforms six measurements unsupervised learning model performed using structured variational em algorithm correctly captures underlying structure data suggested its biological nature model though presented here exon events used learn type simple adjustment multiple types predictions obtained model currently being used verify various claims about role evolution functional help identify sequences affect regulation fi exclusion isoform rt pcr measurement vs model predictions es sa le en er sc le model prediction rt pcr measurement rt pcr measurements model prediction figure sample rt pcr rna extracted cell reverse dna labelled molecules sample through electric field dna being acid positively shorter travel further through than longer ones resulting two distinct bands corresponding two isoforms exposed plot showing rt pcr measurements compared model predictions plot shows available rt pcr measurements rank better model presented assumes single weight matrix data cases view data current work being carried out identifying probe specific expression profiles due low dimensionality problem tissues six probes per event taken avoid overfitting ensure meaningful interpretations acknowledgments would like thank their contributions generating data set work part operating grants operating grants research references et al wide survey human alternative pre mrna splicing exon junction microarrays science et al understanding affect splicing nature pan et al global regulatory features mammalian alternative splicing using quantitative microarray platform molecular cell pan et al alternative splicing exons frequently species specific human trends press jordan ghahramani jaakkola saul
device mismatch vlsi degrades accuracy analog arithmetic circuits learning performance large scale neural networks implemented technology show compact low power chip techniques compensate device mismatch our techniques enable large scale analog vlsi neural networks learning performance order bits demonstrate our techniques synapse linear perceptron learning least mean squares lms algorithm fabricated cmos process
coreference analysis known record identity uncertainty difficult important problem natural language processing databases matching many other tasks paper introduces several discriminative conditional probability models coreference analysis examples undirected graphical models unlike many approaches coreference models presented here relational they do assume pairwise coreference decisions should made independently each other unlike other relational models coreference generative conditional model here incorporate great variety features input without having concerned about their dependencies advantages conditional random fields over hidden markov models present positive results noun phrase coreference two standard text data sets
examine recent probabilistic generative models social networks classical mathematical particularly interested statistical structure such networks influences global economic quantities such price variation our findings mixture formal analysis simulation experiments international trade data set
decision trees surprisingly adaptive three important they automatically adapt favorable conditions near bayes decision boundary focus data distributed lower dimensional manifolds reject irrelevant features paper examine decision tree based dyadic splits adapts each these conditions achieve minimax optimal rates convergence proposed classifier first known achieve these optimal rates while being practical implementable
given directed graph nodes labeled investigate question exploit link structure graph infer labels remaining unlabeled nodes extent propose regularization framework functions defined over nodes directed graph forces classification function change slowly densely linked powerful yet computationally simple classification algorithm derived within proposed framework experimental evaluation real world web classification problems demonstrates encouraging results validate our approach
generative probabilistic model objects images presented object consists features feature appearance pose modeled probabilistically scene images generated drawing set objects given database random clutter remaining image surface occlusion allowed study case features same object share common reference frame moreover parameters shape appearance densities shared across features previous work probabilistic models features depend each other each feature model have different pose appearance statistics these two differences allow us build models containing hundreds features well train each model single example our model may thought probabilistic model propose efficient entropy minimization inference algorithm constructs best interpretation scene collection objects clutter test our ideas experiments two image databases compare algorithm demonstrate better performance particular presence large amounts background clutter
address problem learning symmetric positive definite matrix central issue design parameter updates preserve positive our updates motivated divergence rather than treating most general case focus two key applications our methods line learning simple square loss finding symmetric positive definite matrix subject symmetric linear constraints updates generalize exponentiated gradient eg update adaboost respectively parameter now symmetric positive definite matrix trace instead probability vector context diagonal positive definite matrix trace generalized updates use matrix preserve positive most importantly show analysis each algorithm generalizes non diagonal case apply both new algorithms called matrix exponentiated gradient meg update learn kernel matrix distance measurements
most existing tracking algorithms construct representation target object prior tracking task starts utilize invariant features handle appearance variation target caused lighting pose view angle change paper present efficient effective online algorithm incrementally learns adapts low dimensional representation reflect appearance changes target thereby facilitating tracking task furthermore our incremental method correctly updates sample mean whereas existing incremental subspace update methods ignore fact sample mean varies over time tracking problem formulated state inference problem within markov chain monte carlo framework particle filter incorporated propagating sample distributions over time numerous experiments demonstrate effectiveness proposed tracking algorithm outdoor environments target objects undergo large pose lighting changes
study number open issues spectral clustering selecting appropriate scale analysis ii handling multi scale data iii clustering irregular background clutter iv finding automatically number groups first propose local scale should used compute affinity between each pair points local scaling leads better clustering especially data includes multiple scales clusters placed within cluttered background further suggest exploiting structure eigenvectors infer automatically number groups leads new algorithm final randomly initialized means stage
paper presents adaptive discriminative generative model generalizes conventional fisher linear discriminant algorithm proper probabilistic interpretation within context object tracking aim find discriminative generative model best separates target background present computationally efficient algorithm update discriminative model time progresses while most tracking algorithms operate object appearance ambient lighting condition does significantly change time progresses our method adapts discriminative generative model reflect appearance variation target background thereby facilitating tracking task changing environments numerous experiments show our method able learn discriminative generative model tracking target objects undergoing large pose lighting changes
present unsupervised algorithm surface scans object undergoing significant deformations our algorithm does need markers nor does assume prior knowledge about object shape dynamics its deformation scan alignment algorithm two optimizing joint probabilistic model over point correspondences between them model preservation local geometry well more global constraints capture preservation distance between corresponding point pairs algorithm applies even incomplete range scan thus used automatically remaining surfaces partial scan even those surfaces previously seen different configuration evaluate algorithm several real world datasets demonstrate good results presence significant movement articulated parts non rigid surface deformation finally show output algorithm used computer tasks such interpolation between two scans non rigid object automatic recovery articulated object models
introduce computationally efficient method estimate validity bp method function graph topology connectivity strength network size present numerical results demonstrate correctness our estimates uniform random model real world network elegans although method restricted pair wise interactions local evidence zero biases binary variables believe its predictions correctly capture limitations bp inference map estimation arbitrary graphical models using approach find bp always performs better than especially large networks broad degree distributions such scale free networks bp turns out significantly outperform
paper argue choice svm cost parameter critical derive algorithm fit entire path svm solutions every value cost parameter essentially same computational cost fitting svm model
complex objects often represented finite sets simpler components such images sets patches texts words study class positive definite kernels two such objects expressed function their respective sets components prove general integral representation such kernels present two particular examples them leads kernel sets points space itself positive definite kernel provide experimental results benchmark experiment handwritten digits image classification illustrate validity approach
present novel approach collaborative prediction using low norm instead low rank approach inspired has strong connections large margin linear discrimination show learn low norm solving semi definite program discuss generalization error bounds them
paper explores computational consequences simultaneous intrinsic synaptic plasticity individual model neurons proposes new intrinsic plasticity mechanism continuous activation model neuron based low order moments neurons firing rate distribution goal intrinsic plasticity mechanism sparse distribution neurons activity level conjunction hebbian learning neurons synapses neuron shown discover sparse directions input
motor control depends sensory feedback multiple modalities different paper consider within framework reinforcement learning different sensory modalities combined selected real time optimal movement control propose actor critic architecture multiple modules whose output combined using softmax function tested our architecture simulation sequential reaching task reaching initially guided visual feedback long latency our learning scheme allowed agent utilize somatosensory feedback shorter latency hand near experienced trajectory simulations different visual somatosensory feedback found agent more feedback shorter latency
consider situation semi supervised learning label sampling mechanism stochastically depends true response well potentially features suggest method moments estimating stochastic dependence using unlabeled data potentially useful two distinct purposes input supervised learning procedure used de bias its results using labeled data potentially interesting learning task itself present several examples illustrate practical usefulness our method
context name appears provides powerful cues who depicted associated image obtain face images using face detector approximately half million news images automatically link names obtained using named entity recognizer these faces simple clustering method produce results improve these results significantly combining clustering process model probability individual depicted given its context once labeling procedure over have accurately labeled set faces appearance model each individual depicted natural language model produce accurate results
describe novel method real time simultaneous multi view face detection facial pose estimation method employs convolutional network map face images points manifold parametrized pose non face images points far manifold network trained optimizing loss function three variables image pose face label test resulting system single configuration three standard data sets frontal pose rotated faces profiles find its performance each set comparable previous multi view face detectors handle form pose variation show experimentally systems accuracy both face detection pose estimation improved training two tasks together
recently have been several advances machine learning pattern recognition developing manifold learning algorithms construct nonlinear low dimensional manifolds sample data points embedded high dimensional spaces paper develop algorithms address two key issues manifold learning adaptive selection neighborhood sizes better fitting local geometric structure account variations curvature manifold its sampling density data set illustrate effectiveness our methods synthetic data sets
propose probabilistic generative account learning phenomena classical conditioning learning experiments probe animals discriminate generalize between patterns simultaneously presented stimuli such tones lights predictive reinforcement previous models these issues have been successful more phenomenological than level they reproduce experimental findings but formal foundations provide basis understanding why animals behave they do present theory seemingly arbitrary aspects previous models while capturing broader set data key patterns data eg concerning animals distinguish patterns varying degrees overlap shown follow statistical inference
growing evidence psychophysical neurophysiological studies brain utilizes bayesian principles inference decision making important open question bayesian inference arbitrary graphical models implemented networks spiking neurons paper show recurrent networks noisy integrate fire neurons perform approximate bayesian inference dynamic hierarchical graphical models membrane potential dynamics neurons used implement belief propagation log domain spiking probability neuron shown approximate posterior probability preferred state encoded neuron given past inputs illustrate model using two examples motion detection network spiking probability direction selective neuron becomes proportional posterior probability motion preferred direction two level hierarchical network produces attentional effects similar those observed visual cortical areas hierarchical model offers new bayesian interpretation attentional modulation
study synthesis neural coding selective attention perceptual decision making hierarchical neural architecture proposed implements bayesian integration noisy sensory input attentional priors leading sound perceptual discrimination model offers explicit explanation experimentally observed modulation prior information stimulus feature location have independent feature orientation networks intermediate levels representation known physiological properties visual cortical neurons model illustrates possible cortical neuromodulatory representations uncertainty
consider problem structured classification task predict label input has meaningful internal structure our framework includes supervised training markov random fields weighted context free grammars special cases describe algorithm solves large margin optimization problem defined using exponential family gibbs distribution representation structured objects algorithm efficient even cases number labels exponential size provided certain expectations under gibbs distributions calculated efficiently method structured labels relies more general result specifically application exponentiated gradient updates quadratic programs
investigate approach simultaneously multiple activities each modeled temporally extended action semi markov decision process each activity define set admissible solutions consisting redundant set optimal policies those policies optimal function associated them plan generated merging them such way solutions activities realized set admissible solutions satisfying superior activities present our theoretical results empirically evaluate our approach simulated domain
present algorithm perform blind microphone speech separation our algorithm separates mixtures speech without modeling individual speakers instead formulate problem speech separation problem segmenting spectrogram signal into two more disjoint sets build feature sets our using classical cues speech psychophysics combine these features into parameterized affinity matrices take advantage fact generate training examples segmentation artificially separately recorded signals thus parameters affinity matrices tuned using recent work learning spectral clustering yields adaptive speech specific segmentation algorithm successfully separate microphone speech mixtures
embedding algorithms search low dimensional structure complex data but most algorithms handle objects single type pairwise distances specified paper describes method embedding objects different types such images text into single common euclidean space based their co occurrence statistics joint distributions modeled euclidean distances low dimensional embedding space links problem convex optimization over positive semidefinite matrices local structure our embedding corresponds statistical correlations via random walks euclidean space quantify performance our method two text datasets show consistently significantly outperforms standard methods statistical correspondence modeling such multidimensional scaling correspondence analysis
schema learning way discover probabilistic predictive action models schemas experience includes methods finding using hidden state make predictions more accurate extend original schema mechanism handle arbitrary discrete valued sensors improve original learning criteria handle pomdp domains better maintain hidden state using schema predictions these extensions show large improvement over original schema mechanism several pomdps achieve low prediction error difficult speech modeling task further compare extended schema learning recently introduced predictive state representations find their predictions next step action effects approximately equal accuracy work foundation schema based system integrated learning planning
derive optimal learning rule sense mutual information maximization spiking neuron model under assumption small fluctuations input find spike timing dependent plasticity stdp function depends time course excitatory postsynaptic potentials function postsynaptic neuron show stdp function has both positive negative phases positive phase related shape epsp while negative phase controlled neuronal
many works have shown strong connections relate learning examples regularization techniques ill posed inverse problems nevertheless now formal evidence neither learning examples could seen inverse problem nor theoretical results learning theory could independently derived using tools regularization theory paper provide positive answer both questions indeed considering square loss learning problem language regularization theory show consistency results optimal regularization parameter choice derived discretization corresponding inverse problem
statistical approaches language learning typically focus either short range syntactic dependencies long range semantic dependencies between words present generative model uses both kinds dependencies used simultaneously find syntactic classes semantic topics despite having representation semantics beyond statistical dependency model competitive tasks like part speech document classification models use long range dependencies respectively
examine problem approximating norm sense positive semidefinite symmetric matrix rank matrix upper bound cardinality its eigenvector problem arises decomposition covariance matrix into sparse factors has wide applications ranging biology use modification classical variational representation largest eigenvalue symmetric matrix cardinality constrained derive semidefinite programming based relaxation our problem
have constructed system uses array spiking silicon neurons fast digital memory implement network integrate fire neurons system designed rapid spiking neural networks require high throughput communication external address event hardware arbitrary network topologies implemented selectively routing address events specific internal external targets according memory based projective field mapping utility system demonstrated three stage network accepts input address event detects salient regions image performs spatial modulation around high resolution centered location highest
successful application reinforcement learning algorithms often involves considerable hand necessary non linear features reduce complexity value functions hence convergence algorithm contrast human brain readily autonomously finds complex features provided sufficient training recent work machine learning neurophysiology has demonstrated role basal ganglia frontal cortex mammalian reinforcement learning paper develops explores new reinforcement learning algorithms inspired evidence provides potential new approaches feature construction problem algorithms compared evaluated acrobot task
investigate problem reducing complexity graphical model finding chosen class such optimal respect kl divergence do first defining decomposition tree representation closely related junction tree representation give algorithm uses representation compute optimal have used graph separation properties solve several combinatorial optimization problems size minimal graph bounded present extension technique applies important choices even size minimal arbitrarily large particular applies problems such finding optimal model over tree graphical model over tree arbitrary selecting optimal model constant fewer edges respect kl divergence solved time polynomial using formulation
bayesian regularization nonnegative deconvolution proposed estimating time delays acoustic signals environments sparsity nonnegative filter coefficients using norm regularization probabilistic generative model used simultaneously estimate regularization parameters filter coefficients signal data iterative update rules derived under bayesian framework using expectation maximization procedure resulting time delay estimation algorithm demonstrated noisy acoustic data
paper use method policy improvement analyze version version sometimes called has revealed player but follows usual rules strategy establish using iterated about twice many games average expert human player does
describe approach building brain computer interfaces bci based graphical models probabilistic inference learning show dynamic bayesian network dbn used infer probability distributions over body states during planning execution actions dbn learned directly observed data allows measured signals such eeg emg interpreted terms internal states such move activity movement execution unlike traditional classification based approaches bci proposed approach allows continuous tracking prediction internal states over time generates control signals based entire probability distribution over states rather than binary decisions present preliminary results body state estimation using simultaneous eeg emg signals recorded during self hand movement task
paper investigates effect kernel principal component analysis within classification framework essentially regularization properties dimensionality reduction method has been previously used pre processing step before applying svm but point out method somewhat redundant regularization point view propose new algorithm called kernel projection machine avoid redundancy based analogy statistical framework regression gaussian white noise model preliminary experimental results show algorithm reaches same performances svm
areas brain involved various forms memory exhibit patterns neural activity quite unlike those canonical computational models show use well bayesian probabilistic recall derive biologically reasonable neuronal dynamics coupled models together appropriate values parameters such membrane time constant inhibition explicitly treat two cases arises standard hebbian learning rule involves activity patterns coded graded firing rates other arises spike timing dependent learning rule involves patterns coded phase spike times relative coherent local field potential oscillation our model offers new more complete understanding neural dynamics may support
existing algorithms discrete partially observable markov decision processes best solve problems few thousand states due two important sources intractability curse dimensionality policy space complexity paper describes new algorithm both sources intractability combining value directed compression technique bounded policy iteration demonstrated synthetic network management problems up million states
introduce new algorithm based linear programming approximates differential value function average cost markov decision process via linear combination pre selected basis functions algorithm carries out form cost minimizes version bellman error establish error bound scales number states without strong lyapunov condition required its counterpart propose path following method selection important algorithm parameters represent counterparts state relevance weights studied
online mechanism design addresses problem sequential decision making stochastic environment multiple self interested agents goal make value maximizing decisions despite self interest previous work presented markov decision process mdp based approach large scale problem domains practice underlying mdp needed solve large hence mechanism consider approximations possibility agents may able exploit approximation gain adopt sparse sampling based mdp algorithms implement efficient policies truth approximate equilibrium our approach empirically illustrated context dynamic allocation connectivity users
area under roc curve has been evaluation criterion bipartite ranking problem study large deviation properties particular derive distribution free large deviation bound serves bound expected accuracy ranking function terms its empirical independent test sequence comparison our result corresponding large deviation result classification error rate suggests test sample size required obtain accurate estimate expected accuracy ranking function confidence larger than required obtain accurate estimate expected error rate classification function same confidence simple application union bound allows large deviation bound extended learned ranking functions chosen finite function classes
claim present arguments effect large class manifold learning algorithms essentially local kernel learning algorithms suffer curse dimensionality dimension true underlying manifold observation suggests explore non local manifold learning algorithms attempt discover shared structure tangent different positions criterion such algorithm proposed experiments estimating tangent plane prediction function presented showing its advantages respect local manifold learning algorithms able generalize far training data learning handwritten character image rotations local non parametric method fails
analog system chip kernel based pattern classification sequence estimation presented state transition probabilities conditioned input data generated integrated support vector machine dot product based kernels support vector coefficients implemented analog programmable floating gate circuits probabilities propagated normalized using sub threshold current mode circuits input state support vector forward decoding kernel machine integrated chip cmos technology experiments processor trained speaker verification phoneme sequence estimation demonstrate real time recognition accuracy floating point software sub power
seek both detect segment objects images exploit both local image data well contextual information introduce boosted random fields uses boosting learn graph structure local evidence conditional random field crf graph structure learned graph additive model connections between individual pixels informative but using dense graphs pool information large regions image dense models support efficient inference show contextual information other objects improve detection performance both terms accuracy speed using computational cascade apply our system detect office street scenes
propose novel framework deriving approximations intractable probabilistic models framework based free energy negative log marginal likelihood seen generalization adaptive tap expectation propagation ep free energy constructed two approximating distributions encode different aspects intractable model such single node constraints construction consistent chosen set moments test framework difficult benchmark problem binary variables fully connected graphs grid graphs find good performance using sets moments either specify nodes spanning tree nodes structured approximation surprisingly bethe approximation gives inferior results even grids
clustering prediction sets curves important problem many areas science engineering often case curves tend each other continuous manner either space across measurements time develop probabilistic framework allows joint clustering continuous alignment sets curves curve space opposed fixed dimensional space proposed methodology integrates new probabilistic alignment models model based curve clustering algorithms probabilistic approach allows derivation consistent em learning algorithms joint clustering alignment problem experimental results shown alignment human growth data joint clustering alignment gene expression time course data
present graphical model tracking recorded music using probabilistic graphical model allows us incorporate local information global smoothness constraints principled manner evaluate our model set varied difficult examples achieve impressive results using fast dual tree algorithm graphical model inference our system runs less time than duration music being processed
study method optimal data driven aggregation classifiers convex combination establish tight upper bounds its excess risk respect convex loss function under assumption solution optimal aggregation problem sparse use boosting type algorithm optimal aggregation develop aggregate classifiers activation patterns fmri based locally trained svm classifiers aggregation coefficients used design boosting map brain needed identify regions most significant impact classification
call behavior intrinsically motivated its own rather than step toward solving specific problem clear practical value but what learn during intrinsically motivated behavior essential our development autonomous entities able efficiently solve wide range practical problems they arise paper present initial results computational study intrinsically motivated reinforcement learning aimed allowing artificial agents construct extend hierarchies skills needed
directed graphical models layer observed random variables more layers hidden random variables have been dominant modelling paradigm many research fields although approach has met considerable success causal semantics these models make difficult infer posterior distribution over hidden variables paper propose alternative two layer model based exponential family distributions semantics undirected models inference these exponential family fast while learning performed minimizing divergence member family studied alternative probabilistic model latent semantic experiments shown they perform well document retrieval tasks provide elegant solution searching keywords
many sequential prediction tasks involve locating instances patterns sequences generative probabilistic language models such hidden markov models hmms have been successfully applied many these tasks limitation these models they cannot naturally handle cases pattern instances overlap arbitrary ways present alternative approach based conditional markov networks naturally represent arbitrarily overlapping elements show efficiently train perform inference these models experimental results domain show our models more accurate locating instances overlapping patterns than baseline models based hmms
go game whose complexity has attempts suggest using probability bayesian sense model uncertainty arising vast complexity game tree present simple conditional markov random field model predicting outcome game topology model reflects spatial structure go board describe version process sampling model during learning apply loopy belief propagation rapid inference prediction model trained several hundred records games our experimental results indicate model successfully learns predict despite its simplicity
has been suggested primary goal sensory system represent input such way reduce high degree redundancy given noisy neural representation solely reducing redundancy desirable redundancy reduce effects noise here propose model best redundancy reduction redundant representation like previous models our model accounts localized oriented structure simple cells but predicts different organization population noisy limited capacity units optimal representation becomes overcomplete multi scale representation compared previous models closer agreement physiological data these results offer new perspective expansion number neurons retina provide theoretical model incorporating useful redundancy into efficient neural representations
correction bias magnetic resonance images important problem medical image processing most previous approaches have used maximum likelihood method increase likelihood pixels single image adaptively estimating correction unknown image bias field pixel likelihoods defined either terms pre existing tissue model non terms images own pixel values both cases specific location pixel image used calculate likelihoods suggest new approach simultaneously eliminate bias set images same anatomy but different patients use statistics same location across different images rather than within image eliminate bias fields images simultaneously method builds multi resolution non parametric tissue model conditioned image location while eliminating bias fields associated original image set present experiments both synthetic real data sets present comparisons other methods
propose hierarchical dirichlet process nonparametric bayesian model clustering problems involving multiple groups data each group data modeled mixture number components being open inferred automatically model further components shared across groups allowing dependencies across groups modeled effectively well generalization new groups such clustering problems occur often practice eg problem topic discovery document corpora report experimental results three text corpora showing effective superior performance over previous models
present novel method learning gaussian process regression hierarchical bayesian framework first step kernel matrices fixed set input points learned data using simple efficient em algorithm step nonparametric does require parametric form covariance function second step kernel functions fitted approximate learned covariance matrix using generalized method results complex data driven kernel evaluate our approach engine art images proposed hierarchical bayesian method leads excellent prediction performance
various problems machine learning databases statistics involve pairwise distances among set objects often desirable these distances satisfy properties metric especially triangle inequality applications metric data useful include clustering classification metric based approximation algorithms various graph problems paper presents metric problem given dissimilarity matrix find nearest matrix distances satisfy triangle inequalities measures paper develops efficient triangle fixing algorithms compute globally optimal solutions exploiting inherent structure problem empirically algorithms have time storage costs linear number triangle constraints methods easily additional speed
context binary classification define disagreement measure often two independently trained models differ their classification unlabeled data explore use disagreement error estimation model selection call procedure co validation two models effectively another comparing results unlabeled data assume relatively cheap compared labeled data show per instance disagreement unbiased estimate variance error instance show disagreement provides lower bound prediction generalization error tight upper bound variance prediction error variance average error across instances variance measured across training sets present experimental results several data sets exploring co validation error estimation model selection procedure especially effective active learning settings training sets drawn random cross validation error
problem learning sparse conic combination kernel functions kernel matrices classification regression achieved via regularization block norm paper present algorithm computes entire regularization path these problems path obtained using numerical techniques involves running time complexity constant times complexity solving problem value regularization parameter working setting kernel linear regression kernel logistic regression show empirically effect block norm regularization differs notably non block norm regularization commonly used variable selection regularization path particular value block case
consider mdp setting reward function allowed change during each time step play possibly manner yet dynamics remain fixed similar experts setting address question well agent do compared reward achieved under best stationary policy over time provide efficient algorithms have regret bounds dependence size state space instead these bounds depend certain horizon time process number actions show case dynamics change over time problem becomes computationally hard
many machine learning algorithms clustering dimensionality reduction take input points euclidean space construct graph input data points vertices graph partitioned clustering used metric information dimensionality reduction has been much recent work new methods graph based clustering dimensionality reduction but much constructing graph itself graphs typically used include graph local fixed grid graph image segmentation nearest neighbor graph suggest graph should adapt locally structure data achieved graph ensemble combines multiple minimum spanning trees each fit version data set show such graph ensemble usually produces better representation data manifold than standard methods provides robustness subsequent clustering dimensionality reduction algorithm based graph
study gender discrimination human faces using combination psychophysical classification discrimination experiments together methods machine learning reduce dimensionality set face images using principal component analysis train set linear classifiers reduced representation linear support vector machines svms relevance vector machines fisher linear discriminant prototype classifiers using human classification data because combine linear linear classifiers entire system acts linear classifier allowing us decision image corresponding normal vector separating hyperplanes each classifier predict female transition along normal vector classifiers closely human classification svm rvm should faster than transition along other direction psychophysical discrimination experiment using decision images stimuli consistent prediction
repeated spike patterns have often been taken evidence chain phenomenon stable spike synchrony through feedforward network inter spike intervals represent repeated spike pattern influenced propagation speed spike packet relation between propagation speed network structure well understood while apparent propagation speed depends excitatory synapse strength might related spike patterns analyze feedforward network connectivity using equation show both uniform localized spike packet stable certain parameter region demonstrate propagation speed depends distinct firing patterns same network
survey propagation powerful technique statistical physics has been applied solve sat problem both principle practice give using probability arguments common derivation survey propagation belief propagation several interesting hybrid methods present numerical experiments use widely used random walk based sat quantify complexity sat formulae function their parameters both randomly generated after guided survey propagation properties have previously been reported make ideal tool purpose its mean cost proportional number variables formula xed ratio clauses variables easy sat regime slightly beyond its behavior regime appears underlying structure solution space has been predicted replica symmetry breaking arguments analysis between various methods search satisfying assignments shows far more powerful than has been suggests interesting new directions practical algorithm development
paper provides foundation multi task learning using reproducing kernel hilbert spaces vector valued functions setting kernel matrix valued function explicit examples described go beyond our earlier results particular characterize classes kernels linear dot product translation invariant type discuss these kernels used model relations between tasks present linear multi task learning algorithms finally present novel proof theorem regularization functional based notion minimal norm interpolation
present discriminative part based approach recognition object classes cluttered scenes objects modeled flexible parts conditioned local observations found interest operator each object class probability given assignment parts local features modeled conditional random field crf propose extension crf framework incorporates hidden variables combines class conditional crfs into unified framework part based object recognition parameters crf estimated maximum likelihood framework recognition proceeds finding most likely class under our model main advantage proposed crf framework allows us relax assumption conditional independence observed data ie local features often used generative approaches assumption might restrictive considerable number object classes
consider multi agent systems whose agents resources group agents adapt environment reinforcement learning preferences policies they hold diversity preferences policies introduced adding random biases initial cumulative payoffs their policies explain provide evidence agent cooperation becomes increasingly important diversity increases analyses these mechanisms yield excellent agreement simulations over decades data
finding minimum norm representation signal given overcomplete basis vectors important problem many application domains unfortunately required optimization problem often intractable because combinatorial increase number local minima number candidate basis vectors increases deficiency has most researchers instead minimize surrogate measures such norm lead more tractable computational methods procedure have now introduced mismatch between our goal our objective function paper demonstrate sparse bayesian learning based method minimizing norm while reducing number local minima moreover derive necessary conditions local minima occur via approach empirically demonstrate typically many fewer general problems interest
present competitive analysis bayesian learning algorithms online learning setting show many simple bayesian algorithms such gaussian linear regression bayesian logistic regression perform favorably compared single best model model class analysis does assume bayesian algorithms modeling assumptions correct our bounds hold even data chosen gaussian linear regression using our error bounds comparable best bounds online learning literature provide lower bound showing gaussian linear regression optimal certain worst case sense give bounds widely used maximum posteriori map estimation algorithms including regularized logistic regression
multi bandit problem online algorithm choose set strategies sequence trials so minimize total cost chosen strategies while nearly tight upper lower bounds known case strategy set finite much less known infinite strategy set here consider case set strategies subset rd cost functions continuous case improve best known upper lower bounds gap factor consider case cost functions convex adapting recent online convex optimization algorithm feedback model multi bandit problem
representation acoustic signals cochlear nerve serve wide range auditory tasks require sensitivity both time frequency demonstrated many filtering properties cochlea could explained terms efficient coding natural sounds model did account properties such phase locking sound could encoded terms action potentials here extend theoretical approach algorithm learning efficient auditory codes using spiking population code here propose algorithm learning efficient auditory codes using theoretical model coding sound terms spikes model each spike encodes precise time position magnitude localized time varying kernel function adapting kernel functions statistics natural sounds show compared conventional signal representations spike code achieves far greater coding efficiency furthermore inferred kernels show both striking similarities measured cochlear filters similar bandwidth versus frequency dependence
propose new method clustering based finding maximum margin hyperplanes through data problem terms equivalence relation matrix pose problem convex integer program although still yields difficult computational problem hard clustering constraints soft clustering formulation solved semidefinite program our clustering technique depends data through kernel matrix easily achieve nonlinear same manner spectral clustering experimental results show our maximum margin clustering technique often obtains more accurate results than conventional clustering methods real benefit our approach leads naturally semi supervised training method support vector machines maximizing margin simultaneously labeled unlabeled training data achieve state art performance using single integrated learning principle
describe way using multiple different types similarity relationship learn low dimensional embedding dataset our method chooses different possibly overlapping representations similarity individually dimensions common underlying latent space applied single similarity relation based euclidean distances between input data points method reduces simple dimensionality reduction additional information available about dataset about subsets use information clean up otherwise improve embedding demonstrate potential usefulness form semi supervised dimensionality reduction simple examples
propose family kernels based theorem its extension operators includes special cases currently known kernels derived behavioral framework diffusion processes kernels kernels graphs kernels sets arising subspace angle approach many these kernels seen new continuum kernel functions leads numerous new special cases application apply new class kernels problem clustering video sequences encouraging results
choice based analysis builds models preferences over products answers gathered our main goal bring tools machine learning community solve problem more efficiently thus propose two algorithms quickly accurately estimate preferences
provide principle semi supervised learning based optimizing rate labels unlabeled points side information side information expressed terms identities sets points regions purpose labels each region same resulting regularization objective convex has unique solution solution found pair local propagation operations graphs induced regions analyze properties algorithm demonstrate its performance document classification tasks
protein interactions typically arise physical interaction more small sites surface two proteins identifying these sites important drug protein design paper propose computational method based probabilistic relational model attempts address task using high throughput protein interaction data set short sequence learn model using em algorithm branch bound algorithm approximate inference step our method searches whose presence pair interacting proteins explain their observed interaction tries determine motif pairs have high affinity therefore lead interaction show our method more accurate than others predicting new protein protein interactions more importantly examining solved structures protein find predicted active correspond actual interaction sites
while clustering usually unsupervised operation circumstances believe varying degrees certainty items should assigned same cluster while items should would like such pairwise relations influence cluster assignments out sample data manner consistent prior knowledge expressed training set our starting point probabilistic clustering based gaussian mixture models data distribution express clustering preferences prior distribution over assignments data points clusters prior cluster assignments according degree they preferences fit model parameters em experiments variety data sets show consistently improve clustering results
have recently proposed extension adaboost regression uses median base final paper extend theoretical results obtained adaboost median boosting its localized variant first extend recent results efficient margin maximizing show algorithm converge maximum achievable margin within precision finite number steps provide confidence interval type bounds generalization error
describe three dimensional geometric hand model suitable visual tracking applications kinematic constraints models have probabilistic structure well described graphical model inference model complicated many degrees freedom well multimodal likelihoods caused ambiguous image measurements use nonparametric belief propagation nbp develop tracking algorithm exploits graphs structure control complexity while avoiding costly discretization while kinematic constraints naturally have local structure created imaging process lead complex color edge based likelihood functions show local structure may recovered introducing binary hidden variables describing occlusion state each pixel nbp algorithm infer these occlusion variables distributed fashion analytically over them produce hand position estimates properly account occlusion events provide simulations showing nbp may used refine inaccurate model well track hand motion through extended image sequences
semantic such provide rich source knowledge natural language processing applications but expensive build maintain extend motivated problem automatically constructing extending such paper present new algorithm automatically learning relations text our method generalizes earlier work had using small numbers hand regular expression patterns identify pairs using dependency path features extracted parse trees introduce general purpose generalization these patterns given training set text containing known pairs our algorithm automatically extracts useful dependency paths applies them new corpora identify novel pairs our evaluation task determining whether two news article relationship our automatically extracted database attains both higher precision higher recall than
first order markov models have been successfully applied many problems example modeling sequential data using markov chains modeling control problems using markov decision processes mdp formalism first order markov models parameters estimated data standard maximum likelihood estimator considers first order single step transitions but many problems conditional independence assumptions satisfied result higher order transition probabilities may poorly approximated motivated problem learning mdps parameters control propose algorithm learning first order markov model explicitly takes into account higher order interactions during training our algorithm uses optimization criterion different maximum likelihood allows us learn models capture longer range effects but without giving up benefits using first order markov models our experimental results show new algorithm outperforming conventional maximum likelihood estimation number control problems mdps parameters estimated data
consider problem set together set external acoustic events eg hand unknown times unknown locations propose solution approximates problem under far field approximation defined calculus affine geometry relies singular value decomposition svd recover affine structure problem define low dimensional optimization techniques embedding solution into euclidean geometry further techniques recovering locations emission times acoustic events approach useful ad hoc microphone arrays sensor networks
integrate fire type models usually because their simplicity other hand integrate fire model basis most theoretical studies spiking neuron models here develop sequential procedure quantitatively evaluate equivalent integrate fire type model based intracellular recordings cortical pyramidal neurons find resulting effective model sufficient predict spike train real pyramidal neuron high accuracy vivo like regimes predicted recorded traces almost significant part spikes predicted correct timing slow processes like spike frequency adaptation shown key feature context they necessary model between different driving regimes
training learning algorithm costly task major goal active learning reduce cost paper introduce new algorithm capable actively learning large scale problems using selective sampling algorithm overcomes costly sampling step well known query committee algorithm projecting onto low dimensional space enables use kernels providing simple way extending non linear scenario sampling low dimension space done using run random walk demonstrate success novel algorithm applying both artificial real world problems
analog focal plane processor having array has been developed directional edge filtering perform pixel kernel convolution entire pixels steps simple analog processing newly developed cyclic line access row parallel processing scheme conjunction nearest neighbor architecture has simple implementation proof concept chip fabricated poly cmos technology edge filtering rate has been experimentally demonstrated
paper presents diffusion based probabilistic interpretation spectral clustering dimensionality reduction algorithms use eigenvectors normalized graph laplacian given pairwise matrix points define diffusion distance between two data points show low dimensional representation data first few eigenvectors corresponding markov matrix optimal under certain mean squared error criterion furthermore assuming data points random samples density px identify these eigenvectors discrete approximations eigenfunctions operator potential reflecting boundary conditions finally applying known results regarding eigenvalues eigenfunctions continuous operator provide mathematical justification success spectral clustering dimensional reduction algorithms based these first few eigenvectors analysis terms characteristics diffusion processes many empirical findings regarding spectral clustering algorithms keywords algorithms architectures learning theory
paper presents representation logic labeling contrast edges visual scenes terms both surface occlusion objects natural scenes objects include while human graphical communication include other abstract devices our analysis directed both natural graphical domains basic problem formulate logic interactions among local image events specifically contrast edges junctions alignment relations such encode natural constraints among these events visual scenes sparse heterogeneous markov random field framework define set interpretation nodes functions among them minimum energy configuration found loopy belief propagation shown correspond preferred human interpretation across wide range prototypical examples including important illusory contour figures such triangle well more difficult examples practical terms approach correct interpretations inherently ambiguous hand drawn box diagrams low computational cost
based large scale spiking neuron model input layers macaque identify neural mechanisms observed contrast dependent receptive field size cells observe rich variety mechanisms phenomenon analyze them based relative gain excitatory inhibitory synaptic inputs observe average growth spatial extent excitation inhibition low contrast predicted phenomenological models contrary phenomenological models our simulation results suggest neither sufficient nor necessary explain phenomenon
paper aim analyzing characteristic neuronal population responses instantaneous time dependent inputs role synapses neural information processing have derived evolution equation membrane potential density function synaptic depression obtain formulas analytic computing response instantaneous re rate through technical analysis arrive several signi cant conclusions background inputs play important role information processing act switch temporal integration detection role synapses regarded spatio temporal important neural information processing spatial distribution synapses spatial temporal relation inputs instantaneous input frequency affect response amplitude phase delay
paper present our design experiments planar biped robot under pure reflexive neuronal control goal study combine neuronal mechanisms obtain fast speed line learning circuit parameters our controller built biologically inspired motor neuron models including local employing kind position trajectory tracking control algorithm instead reflexive controller allows exploit its own natural dynamics during critical stages its walking cycle our knowledge first time dynamic biped walking achieved using pure reflexive controller addition structure allows using policy gradient reinforcement learning algorithm tune parameters reflexive controller real time during walking way reach relative speed leg lengths per second after few minutes online learning faster than other biped robot comparable relative speed human walking addition stability domain stable walking quite large supporting design strategy
define probability distribution over equivalence classes binary matrices finite number rows unbounded number columns distribution suitable use prior probabilistic models represent objects using potentially infinite array features identify simple generative process results same distribution over equivalence classes call process illustrate use distribution prior infinite latent feature model deriving markov chain monte carlo algorithm inference model applying algorithm image dataset
prove known bound risk hypotheses selected ensemble generated running learning algorithm incrementally training data our result based proof techniques different standard risk analysis based uniform convergence arguments
consider regularized least squares rls gaussian kernel prove let gaussian bandwidth while regularization parameter rls solution tends polynomial whose order controlled rates decay rls solution tends kth order polynomial minimal empirical error illustrate result example
supervised learning scenarios feature selection has been studied widely literature selecting features unsupervised learning scenarios much harder problem due absence class labels would guide search relevant information almost previous unsupervised feature selection methods techniques require learning algorithm evaluate candidate feature subsets paper propose filter method feature selection independent learning algorithm our method performed either supervised unsupervised fashion proposed method based observation many real world classification problems data same class often close each other importance feature evaluated its power locality preserving laplacian score compare our method data variance unsupervised fisher score supervised two data sets experimental results demonstrate effectiveness efficiency our algorithm
propose mean field approximation dramatically reduces computational complexity solving stochastic dynamic games provide conditions guarantee our method approximates equilibrium number agents grow derive performance bound assess well approximation performs given number agents apply our method important class problems applied show numerical experiments able greatly expand set economic problems analyzed computationally
given redundant basis vectors our goal find maximally sparse representations signals previously have argued sparse bayesian learning sbl framework particularly well suited task showing has far fewer local minima than other bayesian inspired strategies paper provide further evidence claim proving restricted equivalence condition based distribution nonzero generating model weights whereby sbl solution equal maximally sparse representation prove these nonzero weights drawn approximate prior probability our equivalence condition satisfied finally motivate worst case scenario sbl demonstrate still better than most widely used sparse representation algorithms these include basis pursuit bp based convex relaxation quasi norm orthogonal matching pursuit simple greedy strategy iteratively selects basis vectors most aligned current residual
variational bayesian framework has been widely used approximate bayesian learning various applications has provided computational good generalization performance paper discuss variational bayesian learning mixture exponential families provide additional theoretical support deriving asymptotic form stochastic complexity stochastic complexity corresponds minimum free energy lower bound marginal likelihood key quantity model selection enables us discuss effect hyperparameters accuracy variational bayesian approach approximation true bayesian learning
considered gamma distribution interspike intervals statistical model neuronal spike generation model parameters consist time dependent firing rate shape parameter characterizes spiking individual neurons because environment changes time observed data generated time dependent firing rate unknown function statistical model unknown function called model problem statistics generally difficult solve used novel method estimating functions information geometry estimate shape parameter without estimating unknown function analytically obtained optimal estimating function shape parameter independent functional form firing rate estimation efficient without fisher information loss better than maximum likelihood estimation
multiple visual cues used visual system analyze scene cues include luminance texture contrast motion recordings have shown mammalian visual cortex contains neurons respond similarly scene structure eg orientation boundary regardless cue type information paper shows cue invariant response properties complex type cells learned natural image data unsupervised manner order do extend previous conceptual model cue invariance so applied model complex cell responses our results relate cue invariant response properties natural image statistics thereby showing statistical modeling approach used model processing beyond elemental response properties visual neurons work demonstrates learn natural image data more sophisticated feature detectors than those based changes mean luminance thereby way new data driven approaches image processing computer vision
fast heuristics well studied models bounded psychological research has proposed take best heuristic successful strategy decision making limited resources take searches sufficiently good ordering cues features task objects compared investigate complexity problem approximating optimal cue permutations strategies show efficient algorithm approximate optimum within constant factor np further consider greedy approach building strategies derive tight bounds performance ratio new simple algorithm algorithm proven perform better than take best
recent experimental results suggest dendritic back propagating spikes influence synaptic plasticity different ways study investigate these signals could temporally interact dendrites leading changing plasticity properties local synapse clusters similar previous study employ differential hebbian plasticity rule spike timing dependent plasticity use dendritic back propagating bp spikes post synaptic signals learning rule investigate their interaction influence plasticity analyze situation synapse plasticity characteristics change course time depending type post synaptic activity elicited starting weak synapses local spikes slow growth process induced begins spike process replaced fast synaptic changes consequence much stronger bp spike now plasticity rule way winner take mechanism emerges two stage process enhancing best correlated inputs these results suggest synaptic plasticity temporal changing process computational properties dendrites complete neurons substantially augmented
present competitive analysis non parametric bayesian algorithms worst case online learning setting probabilistic assumptions about generation data made consider models use gaussian process prior over space functions provide bounds regret under log loss commonly used non parametric bayesian algorithms including gaussian regression logistic regression show these algorithms perform favorably under rather general conditions these bounds explicitly handle infinite dimensionality these non parametric classes natural way make formal connections minimax minimum description length mdl framework here show precisely bayesian gaussian regression minimax strategy
consider problem constructing estimator finite class base functions approximately minimizes convex risk functional under constraint purpose propose stochastic procedure descent performs gradient descent dual space generated estimates additionally averaged recursive fashion specific weights descent algorithms have been developed different contexts they known particularly efficient high dimensional problems moreover their implementation adapted online setting main result paper upper bound convergence rate generalization error
separation music signals interesting but difficult problem helpful many other music such audio content analysis paper new music signal separation method proposed based harmonic structure modeling main idea harmonic structure modeling harmonic structure music signal stable so music signal represented harmonic structure model accordingly corresponding separation algorithm proposed main idea learn harmonic structure model each music signal mixture separate signals using these models distinguish harmonic structures different signals experimental results show algorithm separate signals obtain high noise ratio snr but rather good subjective audio quality
survival natural world demands selection relevant visual cues rapidly reliably guide attention towards cluttered environments investigate whether our visual system selects cues guide search optimal manner formally obtain optimal cue selection strategy maximizing signal noise ratio between search target surrounding optimal strategy successfully accounts several phenomena visual search behavior including effect target discriminability uncertainty targets features heterogeneity linear furthermore theory generates new prediction verify through psychophysical experiments human subjects our results provide direct experimental evidence humans select visual cues so maximize between targets surrounding clutter
present non linear simple yet effective feature subset selection method regression use analyzing cortical neural activity our algorithm involves feature weighted version nearest neighbor algorithm able capture complex dependency target function its input makes use leave out error natural regularization explain characteristics our algorithm synthetic problems use context predicting hand velocity spikes recorded motor cortex behaving monkey applying feature selection able improve prediction quality suggest novel way exploring neural data
images represent important source data understanding their statistical structure has important applications such image compression restoration paper propose particular kind probabilistic model products edge model describe structure wavelet transformed images develop practical denoising algorithm based single edge show state ofthe art denoising performance benchmark images
perceptron algorithm despite its simplicity often performs well online classification tasks perceptron becomes especially effective used conjunction kernels common difficulty encountered implementing kernel based online algorithms amount memory required store online hypothesis may grow paper present analyze algorithm kernel based online learning fixed memory our knowledge first online learning algorithm hand maintains strict limit number examples stores while other hand relative mistake bound addition formal results present experiments real datasets merits our approach
present new connectionist model constructive intuitionistic modal reasoning use ensembles neural networks represent intuitionistic modal theories show each intuitionistic modal program exists corresponding neural network ensemble computes program provides massively parallel model intuitionistic modal reasoning sets scene integrated reasoning knowledge representation learning intuitionistic theories neural networks networks ensemble trained examples using standard neural learning algorithms
nonnegative matrix approximation recent technique dimensionality reduction data analysis yields parts based sparse nonnegative representation nonnegative input data has found wide variety applications including text analysis document clustering recognition language modeling speech processing many others despite these numerous applications algorithmic development computing factors has been relatively paper makes algorithmic progress modeling solving using multiplicative updates new generalized problems minimize divergences between input matrix its approximation multiplicative update formulae work lee arise special case our algorithms addition paper shows use penalty functions incorporating constraints other than into problem further interesting extensions use link functions modeling nonlinear relationships discussed
spectral clustering its success both data clustering learning but most spectral clustering algorithms cannot handle multi class clustering problems directly additional strategies needed extend spectral clustering algorithms multi class clustering problems furthermore most spectral clustering algorithms employ hard cluster membership likely local optimum paper present new spectral clustering algorithm named soft cut improves normalized cut algorithm introducing soft membership efficiently computed using bound optimization algorithm our experiments variety datasets have shown promising performance proposed clustering algorithm
propose probabilistic model based independent component analysis learning multiple related tasks our model task parameters assumed generated independent sources account tasks use laplace distributions model hidden sources makes possible identify hidden independent components instead modeling correlations furthermore our model sparsity property makes both parsimonious robust propose efficient algorithms both empirical bayes method point estimation our experimental results two multi label text classification data sets show proposed approach promising
paper presents new framework based walks graph analysis inference gaussian graphical models key idea decompose correlations between variables sum over walks between those variables graph weight each walk given product partial correlations provide walk sum interpretation gaussian belief propagation trees approximate method loopy belief propagation graphs cycles perspective leads better understanding gaussian belief propagation its convergence loopy graphs
kernel methods make relatively easy define complex highdimensional feature spaces question identify relevant subspaces particular learning task two views same phenomenon available kernel canonical correlation analysis has been shown effective preprocessing step improve performance classification algorithms such support vector machine svm paper takes observation its logical conclusion proposes method combines two stage learning followed svm into single optimisation termed svm present both experimental theoretical analysis approach showing encouraging results insights
propose algorithm uses gaussian process regression learn common hidden structure shared between corresponding sets observations observation spaces linked via single reduced dimensionality latent variable space present results two datasets demonstrating ability synthesize novel data learned correspondences first show method learn nonlinear mapping between corresponding views objects filling missing data needed synthesize novel views show method learn mapping between human degrees freedom robotic degrees freedom robot allowing robotic imitation human poses motion capture data
present method nonparametric regression performs bandwidth selection variable selection simultaneously approach based technique incrementally decreasing bandwidth directions gradient estimator respect bandwidth large unknown function satisfies sparsity condition our approach avoids curse dimensionality achieving optimal minimax rate convergence up logarithmic factors relevant variables known advance method called regularization derivative expectation operator sequence hypothesis tests easy implement modified version replaces hard soft effectively solves sequence lasso problems
study statistical convergence consistency regularized boosting methods samples independent distributed iid but come empirical processes stationary mixing sequences utilizing technique constructs sequence independent blocks close distribution original samples prove consistency composite classifiers resulting regularization achieved norm base classifiers weights compared iid case nature sampling consistency result through generalization original condition growth regularization parameter
given directed graphical model binary valued hidden nodes real valued noisy observations consider deciding upon maximum posteriori map maximum posterior marginal mpm assignment under restriction each node its children exactly single bit message present variational formulation viewing processing rules local nodes degrees freedom minimizes loss expected map mpm performance subject such online communication constraints approach leads novel message passing algorithm executed offline before observations realized performance loss iteratively coupling rules manner implicitly driven global statistics provide illustrative examples ii assumptions guarantee convergence efficiency iii connections active research areas
long standing site cerebellar motor learning different theories experimental results suggest either cerebellar brainstem learns task stores memory dynamical system approach clarify mechanism memory generated brainstem so called savings phenomena brainstem learning sort hebbian rule depending cell activities contrast earlier numerical models our model simple but explanations predictions experimental situations qualitative features trajectories phase space synaptic weights without fine parameter tuning
study problem maximum entropy density estimation presence known sample selection bias propose three bias correction approaches first takes advantage unbiased sufficient statistics obtained biased samples second estimates biased distribution factors bias out third approximates second using samples sampling distribution provide guarantees first two approaches evaluate performance three approaches synthetic experiments real data species modeling has been successfully applied sample selection bias significant problem
paper presents new filter online data association problems high dimensional spaces key representation data association posterior information form proximity objects tracks expressed numerical links updating these links requires linear time compared exponential time required computing exact posterior probabilities paper algorithm formally provides comparative results using data obtained real world camera array large scale sensor network simulation
previous work has demonstrated image variations many objects human faces particular under variable lighting effectively modeled low dimensional linear spaces typical linear subspace learning algorithms include principal component analysis pca linear discriminant analysis lda locality preserving projection lpp these methods consider image high dimensional vector rn while image represented plane intrinsically matrix paper propose new algorithm called tensor subspace analysis tsa tsa considers image second order tensor rn rn rn rn two vector spaces relationship between column vectors image matrix between row vectors naturally characterized tsa tsa detects intrinsic local geometrical structure tensor space learning lower dimensional tensor subspace compare our proposed approach pca lda lpp methods two standard databases experimental results demonstrate tsa achieves better recognition rate while being much more efficient
propose consensus propagation asynchronous distributed protocol averaging numbers across network establish convergence characterize convergence rate regular graphs demonstrate protocol exhibits better scaling properties than pairwise averaging alternative has received much recent attention consensus propagation viewed special case belief propagation our results contribute belief propagation literature particular beyond singly connected graphs few classes relevant problems belief propagation known converge
investigate learning appearance object single image instead using large number pictures object recognize use labeled reference database pictures other objects learn invariance noise variations pose illumination acquired knowledge used predict two pictures new objects do appear training pictures actually display same object propose generic scheme called address task relies hundreds random binary splits training set chosen keep together images given object those splits extended complete image space simple learning algorithm given two images responses split predictors combined bayesian rule into posterior probability similarity experiments database database graded symbols compare our method classical learning several examples positive class direct learning similarity
fisher linear discriminant analysis lda sensitive problem data robust fisher lda systematically sensitivity problem explicitly incorporating model data uncertainty classification problem optimizing worst case scenario under model main contribution paper show general convex uncertainty models problem data robust fisher lda carried out using convex optimization certain type product form uncertainty model robust fisher lda carried out cost comparable standard fisher lda method demonstrated numerical examples finally show extend these results robust kernel fisher discriminant analysis ie robust fisher lda high dimensional feature space
present generalization temporal difference networks include temporally abstract options links question network temporal difference td networks have been proposed way representing learning wide variety predictions about interaction between agent its environment these predictions compositional their targets defined terms other predictions they about what would happen action sequence actions taken conventional td networks inter related predictions successive time steps single action here generalize them accommodate extended time intervals whole ways behaving our generalization based options framework temporal abstraction primary contribution paper introduce new algorithm intra option learning td networks function approximation eligibility traces present empirical examples our algorithms effectiveness greater representational td networks primary distinguishing feature temporal difference td networks sutton they permit general compositional specification goals learning goals learning thought predictive questions being asked agent learning problem such what see step forward look right open see bottle seeing bottle course complicated perceptual act might thought obtaining set predictions about what would happen certain reaching grasping actions taken about what would happen bottle turned down what bottle would look like viewed various angles predict seeing bottle thus make prediction about set other predictions target overall prediction composition mathematical sense first prediction each other predictions td networks first framework representing goals predictive learning compositional machine form each node td network represents individual question something predicted has associated value representing answer question prediction something questions represented set directed links between nodes node linked node node re question incorporating node question its value prediction about node prediction higher level predictions composed several ways lower ones producing powerful structured representation language targets learning compositional structure human head expressed links thus agent its learning algorithm network these links referred question network entirely separate set directed links between nodes used compute values predictions answers associated each node these links referred answer network computation answer network compositional conventional way node values computed other node values essential insight td networks notion should apply questions well answers secondary distinguishing feature td networks predictions node values each time used representation state world time way they instance idea predictive state representations psrs introduced littman sutton singh schapire representing state its predictions potentially powerful strategy state abstraction et al note questions used previous work psrs defined terms concrete actions observations other predictions they compositional sense td network questions questions have discussed so far meaning they conditional certain way behaving predict what would see step forward look right open questions conventional td networks but they conditional primitive actions open loop sequences primitive actions conventional psrs natural generalize have examples above questions conditional closed loop temporally extended ways behaving example complex high level action arm hand shaped grasping handle etc ask questions like go room would see would require substantial temporal abstraction addition state abstraction options framework sutton precup singh straightforward way about temporally extended ways behaving about predictions their outcomes paper extend options framework so applied td networks significant extensions original options framework needed novel features our option extended td networks they predict components option outcomes rather than full outcome probability distributions learn according first intra option method use eligibility traces see sutton barto include possibility options whose policies several actions selected options framework section present essential elements options framework sutton precup singh need our extension td networks framework agent environment interact discrete time steps each state st agent selects action determining next state st action way behaving time step options framework lets us about temporally extended ways behaving individual option consists three parts first set subset states option started second component option its policy agent behaves although options framework includes rewards omit them here because concerned prediction control option finally termination function specifies option ends denotes probability state option thus completely formally defined conventional td networks section briefly present details structure learning algorithm comprising td networks introduced sutton td networks address prediction problem agent may have direct access state environment instead each time step agent receives observation ot dependent state experience stream thus consists sequence alternating actions observations td network consists set nodes each representing single scalar prediction question answer networks suggested previously network nodes vector predictions time step denoted yt yt yt predictions estimates expected value scalar quantity typically bit case they interpreted estimates probabilities predictions updated each time step according vector valued function parameter often taken linear form yt ot wt wt xt xt vector features created yt ot wt matrix whose elements sometimes referred weights vector form either identity function shaped logistic function feature vector arbitrary vector valued function yt ot example simplest case feature vector unit basis vector location current state partially observable environment feature vector may combination agents action observations predictions previous time step overall update defines answer network question network consists set target functions condition functions ci define zt ot target prediction yt similarly define ct yt condition time learning algorithm ij each component wt wt written ij wt positive step size parameter note targets here functions observation predictions exactly time step later conditions functions single primitive action what makes algorithm suitable learning about step td relationships together multiple nodes sutton used predict steps ahead various particular values predict outcome specific action sequences psrs eg littman et al singh et al now consider extension temporally abstract actions ij ij wt wt zt yt ci yt option extended td networks section present our intra option learning algorithm td networks options eligibility traces suggested earlier each nodes link question quantity almost same think them identical here difference calculated weights step out date compared ie ot wt cf equation now correspond option applying over possibly many steps policy ith nodes option corresponds condition function ci think recognizer option each action taken assess whether option being followed ci agent acting consistently option policy ci wise intermediate values possible agent act consistently option policy say option has possibility recognizing more than action consistent option significant generalization original idea options actions recognized acceptable state option cannot followed thus cannot here take set states least recognized action set option option termination function generalizes naturally td networks each node given corresponding termination function ot yt probability time indicates option has time indicates has intermediate values correspond soft stochastic termination conditions option zt acts target but option ongoing without termination nodes own next value yt should target termination function specifies two targets mixture two targets used produce form td error each node ii ii zt yt our option extended algorithm incorporates eligibility traces see sutton barto short term memory variables organized matrix weight matrix traces record effect each weight could have had each nodes prediction during time agent has been acting consistently nodes option components eligibility matrix updated ci yt ij wt trace decay parameter familiar td learning algorithm because ci factor nodes traces immediately reset zero whenever agent nodes options policy agent follows policy option does trace gradient way typical eligibility traces policy followed option does trace reset zero immediately following time step new trace start building finally our algorithm updates weights each time step ij ij wt wt fully observable experiment experiment designed test correctness algorithm simple gridworld environmental state observable applied options extended td network problem learning predict observations interaction gridworld environment shown left figure empty squares indicate spaces agent move colored squares shown figure indicate agent each time step agent receives environment six bits representing color facing red green orange yellow white first experiment provided other bits directly indicating complete state environment square orientation fact option depends current predictions action observation means considering markov options fifigure test world left question network right used experiments triangle world indicates location orientation agent labeled representing colors red orange yellow green note left wall mostly but partly green right shows full portion question network corresponding red bit structure repeated but shown other four non white colors primitive actions forward wander options three possible actions actions selected according fixed stochastic policy independent state probability actions respectively cause agent degrees left right causes agent move ahead square probability stay same square probability probability called probability forward movement would cause agent move into wall agent does move experiment used addition these primitive actions provided two temporally abstract options forward wander forward option takes action every state agent senses wall color front policy wander option same actually followed agent wander probability wall probability otherwise used question network shown right figure predictions nodes estimates probability red bit would observed corresponding primitive action taken node prediction whether agent see red bit upon termination wander option taken node predicts probability observing red bit given forward option followed until termination nodes represent predictions outcome primitive action followed forward option nodes take step further they represent predictions red bit forward option followed termination primitive action taken forward option followed again termination applied our algorithm learn parameter answer network question network step size parameter trace decay parameter initial each run agent state indicated figure left experiment identity function each value ran runs time steps each time step root rms error each nodes prediction computed averaged over nodes nodes corresponding wander option included average because difficulty calculating their correct predictions average fi fully observable rms error partially observable rms error steps steps figure learning curves fully observable experiment each probability left partially observable experiment right itself averaged over runs bins time steps produce learning curves shown left figure probabilities error predictions almost zero after approximately trials agent made almost perfect predictions cases surprisingly learning slower higher probabilities these results show our augmented td network able make complete temporally abstract model world partially observable experiment our second experiment six color observation bits available agent experiment provides more challenging test our algorithm model environment well td network construct representation state sparse information fact completely accurate prediction possible problem our question network experiment input vector consisted three groups components each total action first components set node values six observation bits other components action next group components same way first third groups zero action third group technique enables answer network function approximator represent wider class functions linear form than would otherwise possible experiment shaped logistic function probability our performance measure used rms error first experiment except predictions primitive actions nodes included these predictions never become completely accurate because agent cant detail located open space before averaged rms error over runs time step bins produce learning curve shown right figure before rms error approached zero node figure holds prediction red agent forward wall ahead corresponding nodes other subnetworks hold predictions other colors upon forward make these predictions accurately agent keep track wall facing even many steps away has learn sort keep updated turns middle space figure demonstration learned after representative run time steps end run agent driven manually state shown first row relative index steps agent place third column shows prediction node each portion question network predictions shown each color observation bit termination forward option agent facing orange wall predicts forward option would result seeing orange bit none other over steps see predictions accurately agent spins despite fact its observation bits remain same even after steps agent exactly way facing while agent correctly never predicts seeing green bit after forward but driven up turned last row figure green bit accurately predicted fourth column shows prediction node each portion question network recall these nodes correspond sequence forward forward time agent accurately predicts forward bring orange third column predicts forward forward bring green predictions made node each subsequent step sequence correct these results show agent able accurately maintain its long term predictions without directly sensory verification much larger would td network have handle gridworld answer same question network applies size problem layout colored remain same even answer network across widely varying sizes other experiments training successively larger problems have shown same td network used here learn make long term predictions correctly version gridworld used here st bg bg bg bg bg bg bg bg bg bg bg bg bg bg figure illustration part what agent learns partially observable environment second column sequence states relative time index given first column sequence generated controlling agent manually steps agent place trajectory after shown line last state third fourth columns show values nodes corresponding figure each color observation bit fi conclusion our experiments show option extended td networks learn effectively they learn about their environments conventional td networks other method learning models world our intra option learning algorithm off policy learning method incorporating function approximation bootstrapping learning predictions combination these three known produce convergence problems methods see sutton barto they may arise here sound solution may require modifications incorporate importance sampling see precup sutton paper have considered intra option eligibility traces traces extending over time span within option but across options sutton have proposed method inter option traces could perhaps combined our intra option traces primary contribution paper
classical bayes rule computes posterior model probability prior probability data likelihood generalize rule case prior density matrix symmetric positive definite trace data likelihood covariance matrix classical bayes rule special case matrices diagonal classical setting calculation probability data expected likelihood expectation over prior distribution generalized setting replaced expected variance calculation variance computed along eigenvectors prior density matrix expectation over eigenvalues density matrix form probability vector variances along direction determined covariance matrix enough expected variance calculation quantum measurement co variance matrix specifies prior density matrix mixture state particle motivate both classical generalized bayes rule minimum relative entropy principle leibler version gives classical bayes rule quantum relative entropy new bayes rule density matrices
show learn distance metric nearest neighbor knn classification semidefinite programming metric trained goal nearest neighbors always belong same class while examples different classes separated large margin seven data sets varying size difficulty find metrics trained way lead significant improvements knn classification example achieving test error rate mnist handwritten digits support vector machines svms learning problem reduces convex optimization based hinge loss unlike learning svms our framework requires modification extension problems opposed binary classification
functional magnetic resonance imaging fmri has look into active brain between functional brain regions still little studied paper contribute novel framework modeling interactions between multiple active brain regions using dynamic bayesian networks dbns generative models brain activation patterns framework applied modeling neuronal circuits associated reward novelty our framework machine learning perspective lies use dbns reveal brain connectivity such models derived fmri data validated through group classification task employ compare four different types dbns parallel hidden markov models coupled hidden markov models fully linked hidden markov models dynamically hmms hmm moreover propose compare two schemes learning hmms experimental results show using dbns group classification performed even dbns constructed few brain regions demonstrate using proposed learning algorithms different dbn structures characterize drug subjects vs control subjects finding provides independent test effect brain function general demonstrate incorporation computer science principles into functional clinical studies provides novel approach human brain function
layer neuromorphic vision processor whose components communicate spike events using address demonstrated system includes retina chip two convolution chips winner take chip delay line chip learning classifier chip set computer address space components use mixture analog digital computation learn classify trajectories moving object complete experimental setup measurements results shown
paper provide general theorem establishes correspondence between surrogate loss functions classification family divergences moreover provide constructive procedures determining divergence induced given surrogate loss conversely finding surrogate loss functions realize given divergence next introduce notion universal equivalence among loss functions corresponding divergences provide necessary sufficient conditions universal equivalence hold these ideas have applications classification problems involve component experiment design particular our results prove consistency procedure learning classifier under requirements
introduce technique dimensionality estimation based notion quantization dimension asymptotic optimal quantization error probability distribution manifold its intrinsic dimension definition quantization dimension yields family estimation algorithms whose limiting case equivalent recent method based numbers using formalism high rate vector quantization address issues statistical consistency analyze behavior our scheme presence noise
active learning problem supervised learning design locations training input points so generalization error minimized existing active learning methods often assume model used learning correctly specified ie learning target function expressed model hand many practical situations assumption may paper first show existing active learning method theoretically justified under slightly weaker condition model does have correctly specified but slightly models allowed turns out condition still restrictive practice cope problem propose alternative active learning method theoretically justified wider class models thus proposed method has broader range applications than existing method numerical studies show proposed active learning method robust against models thus reliable
analyze classification error unseen cases ie cases different those training set unlike standard generalization error off training set error may differ significantly empirical error high probability even large sample sizes derive bound difference between off training set standard generalization error our result based new bound missing mass small samples stronger than existing bounds based good estimators demonstrate uci data sets our bound gives nontrivial generalization guarantees many practical cases light these results show certain claims made free literature
propose new linear method dimension reduction identify nongaussian components high dimensional data our method non gaussian component analysis uses general semi parametric framework contrast existing projection methods define what gaussian projecting out estimate relevant non gaussian subspace show estimation error finding non gaussian components tends zero parametric rate once components identified extracted various tasks applied data analysis process like data visualization clustering denoising classification numerical study demonstrates usefulness our method
present novel approach characterization complex sensory neurons main goals characterizing sensory neurons characterize dimensions stimulus space neurons highly sensitive large gradients neural responses alternatively dimensions stimulus space neuronal response invariant defining response manifolds formulate problem learning geometry stimulus space compatible neural responses distance between stimuli should large responses they different small responses they similar here show successfully train such distance functions using rather limited amount information data consisted responses neurons primary auditory cortex cats stimuli derived natural sounds each neuron subset pairs stimuli selected such responses two stimuli pair either similar distance function trained fit these constraints resulting distance functions generalized predict distances between responses test stimulus trained stimuli
paper provides system level analysis scalable distributed sensing model sensors our system model data center data sensors each independently encode their noisy observations original binary sequence transmit their encoded data sequences data center combined rate limited sensors use independent rate distortion codes show system performance evaluated given finite number sensors goes infinity analysis shows optimal strategy distributed sensing problem changes critical values data rate noise level
paper show hinge loss interpreted log likelihood semi parametric model posterior probabilities point view svms represent parametric component semi parametric model fitted maximum posteriori estimation procedure connection enables derive mapping svm scores estimated posterior probabilities unlike previous proposals suggested mapping interval valued providing set posterior probabilities compatible each svm score framework offers new way adapt svm optimization problem classification decisions result unequal asymmetric experiments show improvements over state art procedures
brain computer interface bci systems create novel communication channel brain output device conventional motor output pathways muscles therefore they could provide new communication control option patients modern bci technology essentially based techniques classification single trial brain signals here present novel technique allows simultaneous optimization spatial spectral filter enhancing discriminability multi channel eeg single trials evaluation experiments involving different subjects demonstrates superiority proposed algorithm apart enhanced classification spatial andor spectral filter determined algorithm used further analysis data eg source localization respective brain rhythms
given set points set prototypes representing them create graph prototypes whose topology accounts points problem had yet been explored framework statistical learning theory work propose generative model based graph prototypes algorithm learn parameters work first step towards construction topological model set points grounded statistics
while kernel canonical correlation analysis kernel cca has been applied many problems asymptotic convergence functions estimated finite sample true functions has yet been established paper gives rigorous proof statistical convergence kernel cca related method provides theoretical justification these methods result gives sufficient condition decay regularization coefficient methods ensure convergence
female locate song they produce behaviour underlying physiology has been studied depth showing cricket auditory system solves complex problem unique manner present analogue large scale integrated avlsi circuit model process show results testing circuit agree simulation what known behaviour physiology cricket auditory system avlsi circuitry now being extended use robot along previously modelled neural circuitry better understand complete sensorimotor pathway understanding insects carry out complex sensorimotor tasks help design simple sensory robotic systems often insect sensors have evolved into filters matched extract highly specific data environment solves particular problem directly little need further processing examples include head fly uses vision other senses estimate self rotation thus its head flight cricket because cricket body few interaural time difference itd sounds arriving two sides head small even located they cricket itd reaches about low detect directly neural spikes because cricket calling song significantly greater than width cricket body interaural intensity difference iid low absence itd iid information cricket uses phase determine direction possible because male cricket produces almost pure tone its calling song school electrical information engineering institute perception action behaviour fifigure cricket auditory system four acoustic inputs channel sounds directly through onto two sound contralateral inputs has pass double central membrane medial inducing phase delay reduction gain sound transmission contralateral weak making each effectively input system physics cricket auditory system well understood system see figure uses pair sound receivers four acoustic inputs two external surfaces two body acoustic connecting such interference occurs sounds travel inside cricket producing directional response frequencies near calling song amplitude vibration hence firing rate auditory afferent neurons attached them vary sound source moved around cricket sounds different inputs move out phase outputs two match sound straight ahead inputs symmetric respect sound source sound calling song frequency off centre phase signals closer side comes better into alignment signal increases side conversely decreases other vibration amplitudes allows cricket track sound source see figure example simplified version auditory system using two acoustic inputs implemented hardware simple neuron network required direct robot carry out towards species specific calling song simple simulator created model behaviour auditory system figure different frequencies data et al figures used together average typical values paper choose gains delays simulation figure shows model internal auditory system cricket sound arriving acoustic inputs through transmission down auditory receptor fibres simulator implements model up summing delayed inputs well modelling external sound transmission results simulator used check system different frequencies gain better understanding its response impractical check effect leg movements complex sounds simulator due simulating sound production transmission avlsi chip designed implement same model both allowing more complex experiments such leg movements run experiments run real world fifigure model auditory system cricket used build simulator avlsi implementation shown boxes these experiments simulator circuits being published referred those papers more details present paper present details circuits used avlsi implementation circuits chip implementing avlsi box figure comprises two pass delay filters three gain circuits second order narrow band band pass filter first order wide band band pass filter first order high pass filter well supporting circuitry including reference voltages currents etc single avlsi chip mosis chip thus includes half necessary circuitry model complete auditory system cricket complete model auditory system obtained using two appropriately connected chips two pass delay filters need implemented instead three suggested figure because relative delay between three pathways arriving summing node counts delay circuits implemented fully differential filters order extend frequency range delay first order pass delay circuit cascaded second order pass delay circuit resulting addition first order delay second order delay allowed approximately flat delay response wider bandwidth decreased delay around corner frequency first order filter increased delay second order filter around its resonant frequency figure shows second order sections pass delay circuit two these used based data presented designed delays way bias current manipulation amplifier figure standard includes common mode feedback necessary fully differential designs figure simple differential pairs ii ii ii ii ii ii figure first order pass delay circuit left second order pass delay right differential output delay circuits converted into current multiplied variable gain implemented shown figure gain cell includes differential pair source via transistors source improves linearity current three gain cells implemented avlsi have default gains set holding default input high appropriately bias currents through value correct chip andor explore other gain configurations current cell figure allows gain digital means post fabrication current takes input current figure into branches recursively current ie first branch gives second branch third branch so these currents used together controlled switches digital analogue holding default low setting cc appropriately gain set output program bits cc each three gain cells set via single bit shift bit serial fashion summing output three gain circuits current domain simply involves connecting three together therefore natural option filters follow use current domain filters our case have chosen implement log domain filters using mos transistors operating weak inversion figure shows basic building blocks filters tau cell multiplier cell block diagrams showing these blocks connected create necessary filtering blocks tau cell log domain filter has response out slope factor vt thermal voltage ca bias current figure input currents tau cell used building second order filter multiplier cell simply loop out out ai configurations tau cell get particular responses along corresponding equations high frequency filter figure implemented high pass filter figure corner frequency khz low frequency filter divided into two parts biological filters response see example figure separates well into narrow second order band pass filter khz resonant frequency wide band pass filter made first order high pass filter khz corner frequency followed first order low pass filter khz corner frequency these filters added together reproduce biological filter filters responses adjusted post fabrication via their bias currents allows due processing matching errors figure gain cell above used convert differential voltage input delay cells into single current output gain each cell via programmable current cell chip bias generator used create necessary current biases chip main blocks delays gain cells filters have their chip bias currents through external chip chip fabricated using mosis technology designed using custom ic design tools methods chip tested using sound generated computer played through chip responses chip recorded back computer completion given output gain circuits current external current sense circuit built discrete components used enable output figure circuit diagrams log domain filter building blocks tau cell multiplier along block diagrams three filters used avlsi model initial experiments performed tune delays gains after recordings taken directional frequency responses sounds generated computer each chip input simulate moving sound appropriate amount time much simpler solution than using moving them using results avlsi chip tested measure its gains delays successfully tuned appropriate values chip compared simulation check faithfully modelling system result test khz approximately cricket calling song frequency shown figure apart amplitude signal response circuit similar simulator differences expected because avlsi circuit has deal real world noise whereas simulated version has perfect signals examples gain versus frequency response two log domain band pass filters shown figure note narrow band filter peaks khz significantly above song frequency cricket around khz mistake but observed real well stated
investigate under what conditions neuron learn experimentally supported rules spike timing dependent plasticity stdp predict arrival times strong teacher inputs same neuron turns out contrast perceptron convergence theorem predicts convergence perceptron learning rule simplified neuron model whenever stable solution exists equally strong convergence guarantee given spiking neurons stdp but derive criterion statistical dependency structure input spike trains characterizes exactly learning stdp converge average simple model spiking neuron criterion reminiscent linear criterion perceptron convergence theorem but applies here rows correlation matrix related spike inputs addition show through computer simulations more realistic neuron models resulting analytically predicted positive learning results hold common interpretation stdp stdp changes weights synapses but more realistic interpretation suggested experimental data stdp initial release probability dynamic synapses
standard statistical models language fail capture most striking properties natural languages power law distribution frequencies word present framework developing statistical models produce power laws augmenting standard generative models produces appropriate pattern frequencies show taking particular stochastic process process appearance type frequencies formal analyses natural language improves performance model unsupervised learning morphology
present model learns influence interacting markov chains within team proposed model dynamic bayesian network dbn two level structure individual level group level individual level models actions each player group level models actions team whole experiments synthetic multi player games multi corpus show effectiveness proposed model
paper derive algorithm computes entire solution path support vector regression essentially same computational cost fitting svr model propose unbiased estimate degrees freedom svr model allows convenient selection regularization parameter
discuss method obtaining subjects priori beliefs behavior psychophysics context under assumption behavior nearly optimal bayesian perspective method nonparametric sense do assume prior belongs fixed class distributions eg gaussian despite increased generality method relatively simple implement being based simplest case linear programming algorithm more generally straightforward maximum likelihood maximum posteriori formulation turns out convex optimization problem non global local maxima many important cases addition develop methods analyzing uncertainty these estimates demonstrate accuracy method simple simulated setting particular method able precisely track evolution subjects posterior distribution more more data observed close briefly interesting connection recent models neural population coding
present efficient algorithm actively select queries learning boundaries separating function domain into regions function above below given threshold develop experiment selection methods based entropy misclassification rate variance their combinations show they perform number data sets show these algorithms used determine simultaneously valid confidence intervals seven parameters shows algorithm reduces computation necessary parameter estimation problem order magnitude
while classical experiments spike timing dependent plasticity analyzed synaptic changes function timing pairs preand postsynaptic spikes more recent experiments point effect spike here develop mathematical framework allows us characterize timing based learning rules moreover identify candidate learning rule five variables free parameters captures variety experimental data including dependence potentiation depression upon preand postsynaptic firing frequencies relation bienenstock cooper rule well timing based rules discussed
problem resource allocation sparse graphs real variables studied using methods statistical physics efficient distributed algorithm devised basis insight gained analysis examined using numerical simulations showing excellent performance full agreement theoretical results
our understanding input output function single cells has been substantially advanced biophysically accurate multi compartmental models large number parameters hand tuning these models has somewhat their applicability here propose simple well method automatic estimation many these key parameters spatial distribution channel densities cells membrane spatiotemporal pattern synaptic input channels potentials conductances noise level each compartment assume experimental access spatiotemporal voltage signal dendrite contiguous eg via voltage sensitive imaging techniques approximate description channels synapses present each compartment morphology part neuron under investigation key observation given data parameters may simultaneously inferred version constrained linear regression regression turn efficiently solved using standard algorithms without local minima problems despite large number parameters complex dynamics noise level may estimated standard techniques demonstrate methods accuracy several model datasets describe techniques quantifying uncertainty our estimates
network topology neurons brain exhibits abundance feedback connections but computational function these feedback connections largely unknown present computational theory characterizes gain computational power achieved through feedback dynamical systems memory implies many such systems acquire through feedback universal computational capabilities analog computing non memory particular show feedback enables such systems process time varying input streams diverse ways according rules implemented through internal states dynamical system contrast previous attractor based computational models neural networks these flexible internal states high dimensional attractors circuit dynamics still allow circuit state new information online input streams way novel models working memory integration evidence reward expectation cortical circuits show they applicable circuits conductance based hodgkin huxley neurons high levels noise reflect experimental data conditions
argue objects characterized many attributes clustering them basis relatively small random subset these attributes capture information unobserved attributes well moreover show under mild technical conditions clustering objects basis such random subset performs almost well clustering full attribute set prove finite sample generalization theorems novel learning scheme extends analogous results supervised learning setting scheme demonstrated collaborative filtering users rating attributes
long distance language modeling important speech recognition machine translation but high dimensional discrete sequence modeling general problem context length has almost been so far bag words history has been employed natural language processing contrast paper view topic shifts within text latent stochastic process give explicit probabilistic generative model has partial propose online inference algorithm using particle filters recognize topic shifts employ most appropriate length context automatically experiments corpus showed consistent improvement over previous methods involving order
motor imagery eeg rhythms over sensorimotor these amplitude changes most successfully captured method common spatial patterns widely used interfaces bci bci methods based amplitude information have rich phase dynamics eeg rhythm study reports bci method based phase synchrony rate computed phase locking value describes number discrete synchronization events within window statistical nonparametric tests show contain significant differences between types motor classifiers trained consistently demonstrate results subjects further observed subjects phase more discriminative than amplitude first suggests phase has potential boost information transfer rate
theories visual attention commonly early parallel processes extract features such color contrast motion visual field these features combined into saliency map attention directed most salient regions first top down attentional control achieved modulating contribution different feature types saliency map key source data concerning attentional control comes behavioral studies effect recent experience examined individuals repeatedly perform perceptual discrimination task eg what shape odd colored object robust finding features recent trials eg target color facilitates performance view adaptation statistical structure environment propose probabilistic model environment updated after each trial under assumption attentional control operates so make performance more efficient more likely environmental states obtain parsimonious explanations data four different experiments further our model provides rational explanation why influence past experience attentional control short
present series theoretical arguments supporting claim large class modern learning algorithms rely solely smoothness prior similarity between examples expressed local kernel sensitive curse dimensionality more precisely variability target our discussion covers supervised unsupervised learning algorithms these algorithms found local sense crucial properties learned function depend mostly neighbors training set makes them sensitive curse dimensionality well studied classical non parametric statistical learning show case gaussian kernel function learned has many variations these algorithms require number training examples proportional number variations could large even though may exist short descriptions target function ie their complexity may low suggests exist non local learning algorithms least have potential learn about such structured but apparently complex functions because locally they have many variations while using specific prior domain knowledge
study learning multiple sources limited data each may corrupted different rate develop complete theory data sources should used two fundamental problems estimating bias learning classifier presence label noise both cases efficient algorithms provided computing optimal subset data
design new learning algorithm set covering machine pac bayes perspective propose pac bayes risk bound minimized classifiers achieving non trivial margin sparsity trade off
paper consider problem finding sets points given underlying model within dense noisy set observations problem motivated task efficiently linking but applicable range spatial queries survey current tree based approaches showing trade off exists between single tree multiple tree algorithms end present new type multiple tree algorithm uses variable number trees exploit advantages both approaches empirically show algorithm performs well using both simulated data
present method performing transductive inference large datasets our algorithm based multiclass gaussian processes effective whenever multiplication kernel matrix its inverse vector computed sufficiently fast holds instance certain graph string kernels transduction achieved variational inference over unlabeled data subject balancing constraint
under natural viewing conditions small movements eye body prevent steady direction gaze known stimuli tend they retina several seconds whether physiological self motion retinal image serves visual purpose during brief periods natural visual study examines impact instability statistics visual input retina structure neural activity early visual system instability introduces fluctuations retinal input signals presence natural images lack spatial correlations these input fluctuations strongly influence neural activity model lgn they cell responses even contrast sensitivity functions simulated cells perfectly tuned balance power law spectrum natural images decorrelation neural activity has been proposed statistical input signals instability might therefore contribute establishing efficient representations natural stimuli
paper propose new basis selection criterion building sparse gp regression models provides promising gains accuracy well efficiency over previous methods our algorithm much faster than bartlett while generalization greatly outperforms information gain approach proposed et al especially quality predictive distributions
present conditional temporal probabilistic framework reconstructing human motion monocular video based descriptors encoding image observations computational efficiency restrict visual inference low dimensional kernel induced non linear state spaces our methodology combines kernel pca based non linear dimensionality reduction conditional bayesian mixture experts order learn complex predictors between observations model hidden states necessary accurate inverse visual perception inferences several probable distant solutions exist due noise uncertainty monocular perspective projection low dimensional models appropriate because many visual processes exhibit strong non linear correlations both image observations target hidden state variables learned predictors temporally combined within conditional graphical model order allow principled propagation uncertainty study several predictors empirically show proposed algorithm positively compares techniques based regression kernel dependency estimation pca alone gives results competitive those high dimensional mixture predictors fraction their computational cost show method successfully complex motion humans real monocular video sequences
present new kernel method extracting semantic relations between entities natural language text based generalization kernels kernel uses three types patterns typically employed natural language relationships between two entities experiments extracting protein interactions corpora top level relations corpora demonstrate advantages approach
dynamic texture video model treats video sample spatio temporal stochastic process specifically linear dynamical system problem associated dynamic texture cannot model video multiple regions distinct motion work introduce layered dynamic texture model addresses problem introduce variant model present em algorithm learning each models finally demonstrate efficacy proposed model tasks segmentation synthesis video
propose fast manifold learning algorithm based methodology domain decomposition starting set sample points partitioned into two develop solution interface problem two into embedding whole domain provide detailed analysis assess errors produced process using matrix perturbation theory numerical examples given illustrate efficiency effectiveness proposed methods
experimental data indicate norepinephrine critically involved aspects vigilance attention previously considered function neuromodulatory system time scale minutes longer suggested signals global uncertainty arising changes environmental norepinephrine known activated familiar stimuli tasks here extend our uncertainty based treatment norepinephrine mode involved detection reaction state uncertainty within task role norepinephrine understood through neural
although non parametric tests have already been proposed purpose statistical significance tests non standard measures different classification error less often used literature paper attempt empirically these tests compare more classical tests various conditions more precisely using large dataset estimate whole population analyzed behavior several statistical test varying class compared models performance measure sample size main result providing big enough evaluation sets non parametric tests relatively reliable conditions
characterize sample complexity active learning problems terms parameter takes into account distribution over input space specific target hypothesis desired accuracy
paper presents non asymptotic statistical analysis kernel pca focus different proposed previous work topic here instead considering reconstruction error interested approximation error bounds themselves prove upper bound depending spacing between eigenvalues but dimensionality consequence allows infer stability results these estimated spaces
describe hierarchy motif based kernels multiple biological sequences particularly suitable process regulatory regions genes kernels incorporate more information most complex kernel accounting multiple alignment regions tree relating species prior knowledge relevant sequence patterns occur motif blocks these kernels used presence known transcription factor binding sites de over given length latter mode discriminative classifier built such kernel recognizes given class regions but side effect simultaneously identifies collection relevant discriminative sequence demonstrate utility motif based multiple alignment kernels using collection aligned regions five species recognize classes cell cycle genes data available
neurons have rapidly changing spike train statistics underlying network excitability behavioural state animal estimate time course such state dynamics multiple neuron recordings have developed algorithm maximizes likelihood observed spike trains optimizing state state conditional interspike interval isi distributions our nonparametric algorithm free time spike counting problems has computational complexity mixed state markov model operating state sequence length equal total number recorded spikes example fit two state model paired recordings neurons find two state conditional isi functions highly similar ones measured during respectively
investigate problem automatically constructing efficient representations basis functions approximating value functions based analyzing structure topology state space particular two novel approaches value function approximation explored based automatically constructing basis functions state spaces represented graphs manifolds approach uses eigenfunctions laplacian effect performing global fourier analysis graph second approach based diffusion wavelets generalize classical wavelets graphs using multiscale induced diffusion operator random walk graph together these approaches form foundation new generation methods solving large markov decision processes underlying representation policies simultaneously learned
consider framework semi supervised learning using spectral decomposition based un supervised kernel design approach class previously proposed semi supervised learning methods data graphs examine various theoretical properties such methods particular derive generalization performance bound obtain optimal kernel design minimizing bound based theoretical analysis able demonstrate why spectral kernel design based methods often improve predictive performance experiments used illustrate main consequences our analysis
propose new bayesian method spatial cluster detection bayesian spatial scan statistic compare method standard frequentist scan statistic approach demonstrate bayesian statistic has several advantages over frequentist approach including increased power detect clusters testing much faster evaluate bayesian frequentist methods task disease detecting spatial clusters disease cases resulting emerging disease demonstrate our bayesian methods successful rapidly detecting while keeping number false positives low
given probability measure reference measure often interested minimum measure set measure least minimum volume sets type summarize regions probability mass useful detecting anomalies constructing confidence regions paper addresses problem estimating minimum volume sets based independent samples distributed according other than these samples other information available regarding but reference measure assumed known introduce rules estimating minimum volume sets parallel empirical risk minimization structural risk minimization principles classification classification show performances our estimators controlled rate uniform convergence empirical true probabilities over class estimator drawn thus obtain finite sample size performance bounds terms vc dimension related quantities demonstrate strong universal consistency inequality estimators based histograms dyadic partitions illustrate proposed rules
recurrent networks perform winner take computation have been studied extensively although these studies include spiking networks they consider analog input rates present results winner take computation network integrate fire neurons receives spike trains inputs show connectivity network so winner selected after pre determined number input spikes discuss spiking inputs both regular frequencies poisson distributed rates robustness computation tested implementing winner take network analog vlsi array integrate fire neurons have variance their operating parameters
consider problem joint parameter estimation prediction markov random field ie model parameters estimated basis initial set data fitted model used perform prediction eg smoothing denoising interpolation new noisy observation working computation limited setting analyze joint method same convex variational relaxation used construct estimator fitting parameters perform approximate prediction step key result paper computation limited setting using parameter estimator ie estimator returns wrong model even infinite data limit provably resulting errors partially compensate errors made using approximate prediction technique en route result analyze asymptotic properties estimators based convex variational relaxations establish stability property holds broad class variational methods show joint based sum product algorithm substantially outperforms commonly used heuristic based ordinary sum product keywords markov random fields variational method message passing algorithms sum product belief propagation parameter estimation learning
reinforcement learning models have long unify computational psychological neural accounts conditioned behavior data animal conditioning comes free experiments measuring fast animals work reinforcement existing reinforcement learning rl models about these tasks because they lack notion they thus fail address simple observation animals work harder well such their sometimes greater even working irrelevant outcomes such water here develop rl framework free behavior suggesting subjects choose perform selected actions optimally balancing costs benefits quick responding motivational states such shift these factors tradeoff accounts effects motivation response rates well many other classic findings finally suggest levels dopamine may involved computation linking motivational state optimal responding thereby explaining complex related effects manipulation dopamine
trying understand brain fundamental importance analyse eg measurements what parts cortex interact each other order infer more accurate models brain activity common techniques like blind source separation bss estimate brain sources single out artifacts using underlying assumption source signal independence physiologically interesting brain sources typically interact so bss construction fail characterize them properly interacting sources signals seemingly interact due effects volume conduction work aims contribute distinguishing these effects new bss technique proposed uses anti cross correlation matrices subsequent resulting decomposition consists interacting brain sources suppresses spurious interaction volume conduction our new concept interacting source analysis successfully demonstrated meg data
two power law empirical law inverse non linear relationship between hand speed curvature its trajectory during curved motion widely invariant upper movement has been shown exist locomotion even demonstrated motion perception prediction has various attempts empirical relationship these generally either smoothness joint space result mechanisms noise inherent motor system produce smooth trajectories human motion show here white gaussian noise obeys power law analysis signal noise combinations shows trajectories created power law transformed power law ones after combination low levels noise furthermore exist colored noise types drive non power law trajectories power law affected smoothing these results suggest running experiments aimed power law assuming its underlying existence without proper analysis noise our results could suggest power law might derived smoothness smoothness inducing mechanisms operating noise inherent our motor system but rather correlated noise inherent motor system
escape curse dimensionality claim learn non local functions sense value shape learned function inferred using examples may far objective present non local non parametric density estimator builds upon previously proposed gaussian mixture models regularized covariance matrices take into account local shape manifold builds upon recent work non local estimators tangent plane manifold able generalize places little training data unlike traditional local non parametric models
present new gaussian process gp regression model whose covariance parameterized locations pseudo input points learn gradient based optimization take number real data points hence obtain sparse regression method has om training cost om prediction cost per test case find hyperparameters covariance function same joint optimization method viewed bayesian regression model particular input dependent noise method turns out closely related several other sparse gp approaches discuss relation detail finally demonstrate its performance large data sets make direct comparison other sparse gp methods show our method match full gp performance small ie sparse solutions significantly outperforms other approaches regime
describe hierarchical compositional system detecting objects images objects represented graphical models algorithm uses hierarchical tree root tree corresponds full object lower level elements tree correspond simpler features algorithm proceeds passing simple messages up down tree method works rapidly under second images demonstrate approach detecting cats method works presence background clutter occlusions our approach more traditional methods such dynamic programming belief propagation
standard method obtain stochastic models symbolic time series train state hidden markov models hmms baum welch algorithm based observable operator models last few number novel learning algorithms similar purposes have been developed two versions efficiency es algorithm iteratively improves statistical efficiency sequence estimators constrained gradient descent ml estimator transition hmms hmms give these algorithms compare them learning synthetic real life data
paper introduces gaussian process dynamical models nonlinear time series analysis comprises low dimensional latent space associated dynamics map latent space observation space out model parameters closed form using gaussian process gp priors both dynamics observation mappings results nonparametric model dynamical systems accounts uncertainty model demonstrate approach human motion capture data each pose dimensional despite use small data sets learns effective representation nonlinear dynamics these spaces
have been many graph based approaches semi supervised classification problem learning performance depends greatly hyperparameters similarity graph transformation graph laplacian noise model present bayesian framework learning hyperparameters graph based classification given labeled data contain inaccurate labels pose semi supervised classification inference problem over unknown labels expectation propagation used approximate inference mean posterior used classification hyperparameters learned using em evidence maximization show posterior mean written terms kernel matrix providing bayesian classifier classify new points tests synthetic real datasets show cases significant improvements performance over existing approaches
online learning algorithms typically fast memory efficient simple implement many common learning problems fit more naturally batch learning setting power online learning algorithms exploited batch settings using online batch conversions techniques build new batch algorithm existing online algorithm first give unified three existing online batch conversion techniques do use training data conversion process build upon these data independent conversions derive analyze data driven conversions our conversions find hypotheses small risk explicitly minimizing generalization bounds experimentally demonstrate usefulness our approach particular show data driven conversions consistently outperform data independent conversions
present novel spectral clustering method enables users incorporate prior knowledge size clusters into clustering process cost function named size regularized cut defined sum inter cluster similarity regularization term measuring relative size two clusters finding partition data set minimize proved np complete approximation algorithm proposed solve version optimization problem eigenvalue problem evaluations over different data sets demonstrate method sensitive outliers performs better than normalized cut
derive bayesian ideal observer bio detecting motion solving correspondence problem obtain classic model approximation our psychophysical experiments show trends human performance similar bayesian ideal but overall human performance far worse investigate ways bayesian ideal but show even extreme do approach human performance instead propose humans perform motion tasks using generic general purpose models motion perform more psychophysical experiments consistent humans using slow smooth model rule out alternative model using
propose simple information theoretic approach soft clustering based maximizing mutual information ix between unknown cluster labels training patterns respect parameters specifically constrained encoding distributions constraints chosen such patterns likely clustered similarly they lie close specific unknown vectors feature space method may applied learning optimal affinity matrix corresponds learning parameters encoder procedure does require computations eigenvalues gram matrices makes potentially attractive clustering large data sets
show linear generalizations rescorla wagner perform maximum likelihood estimation parameters generative models causal reasoning our approach involves augmenting variables deal conjunctions causes similar model rescorla our results involve assumptions distributions causes these assumptions violated example causal power theory show linear rescorla wagner estimate parameters model up nonlinear moreover nonlinear rescorla wagner able estimate parameters directly within arbitrary accuracy previous results used determine convergence estimate convergence rates
well known learnable difficult online setting arbitrary sequences examples labeled time learnable batch setting examples drawn independently distribution show result opposite direction give efficient conversion algorithm batch online transductive uses future unlabeled data demonstrates equivalence between what properly efficiently learnable batch model transductive online model
convexity has recently received lot attention machine learning community lack convexity has been seen major disadvantage many learning algorithms such multi layer artificial neural networks show training multi layer neural networks number hidden units learned viewed convex optimization problem problem involves infinite number variables but solved incrementally hidden unit time each time finding linear classifier minimizes weighted sum errors
paper explores statistical relationship between natural images their underlying range depth images look relationship changes over scale information used enhance low resolution range data using full resolution intensity image based our findings propose extension existing technique known shape success two methods compared using images laser scans real scenes our extension shown provide two fold improvement over current method furthermore demonstrate ideal linear shape shading filters learned natural scenes may derive even more strength cues than traditional linear shading cues
present improvement dp algorithm simultaneous localization mapping maintains multiple hypotheses about densely maps full map per particle particle filter time linear significant algorithm parameters takes constant time per iteration means asymptotic complexity algorithm greater than pure localization algorithm using single map same number particles present hierarchical extension dp uses two level particle filter models drift particle filtering process itself hierarchical approach enables recovery drift results using finite number particles particle filter permits use dp more challenging domains while maintaining linear time asymptotic complexity
determine asymptotic limit function computed support vector machines svm related algorithms minimize regularized empirical convex loss function reproducing kernel hilbert space gaussian rbf kernel situation number examples tends infinity bandwidth gaussian kernel tends regularization parameter held fixed non asymptotic convergence bounds limit sense provided together upper bounds classification error shown converge bayes risk therefore proving bayes consistency variety methods although regularization term does vanish these results particularly relevant class svm regularization vanish construction shown first time consistent density level set estimator
good image object detection algorithm accurate fast does require exact locations objects training set create such object detector taking architecture viola jones detector cascade training new variant boosting call uses cost functions multiple instance learning literature combined framework adapt feature selection criterion optimize performance viola jones cascade experiments show detection rate up times better using increased detection rate shows advantage simultaneously learning locations scales objects training set along parameters classifier
probabilistic modeling correlated neural population firing activity central understanding neural code building practical decoding algorithms parametric models currently exist modeling multivariate correlated neural data high dimensional nature data makes fully non parametric methods impractical address these problems propose energy based model joint probability neural activity represented using learned functions marginal histograms data parameters model learned using divergence optimization procedure finding appropriate marginal directions evaluate method using real data recorded population motor cortical neurons particular model joint probability population spiking times hand position show likelihood test data under our model significantly higher than under other models these results suggest our model captures correlations firing activity our rich probabilistic model neural population activity step towards both measurement importance correlations neural coding improved decoding population activity
observed physiological dynamics receiving intensive affected many possible factors including operation monitoring state factorial switching kalman filter used infer presence such factors sequence observations estimate true values these observations have been corrupted apply model clinical time series data show effective identifying number physiological patterns
hybrid integrated circuits combining cmos simple two terminal promise extend exponential moore law development into sub nm range developing neuromorphic network architectures future technology neural cell implemented cmos used axons dendrites while bistable switches used elementary synapses have shown may trained perform pattern recovery classification despite limitations imposed hardware preliminary estimates have shown may extremely dense cells per cm operate approximately million times faster than biological neural networks power consumption conclusion discuss brief possible short term long term applications emerging technology
general analysis limiting distribution neural network functions performed emphasis non gaussian limits show iid symmetric stable output weights more generally weights distributed normal domain attraction stable variable neural functions converge distribution stable processes conditions investigated under gaussian limits do occur weights independent but distributed particularly tractable classes stable distributions examined possibility learning such processes
compressed sensing emerging field based small group linear projections sparse signal contains enough information reconstruction paper introduce new theory distributed compressed sensing dcs enables new distributed coding algorithms multi signal ensembles exploit both inter signal correlation structures dcs theory new concept term joint sparsity signal ensemble study three simple models jointly sparse signals propose algorithms joint recovery multiple signals projections characterize theoretically empirically number measurements per sensor required accurate reconstruction sense dcs framework distributed compression sources memory has challenging problem information theory time dcs immediately applicable range problems sensor networks arrays
paper presents rigorous statistical analysis characterizing regimes active learning significantly outperforms classical passive learning active learning algorithms able make queries select sample locations online fashion depending results previous queries regimes extra flexibility leads significantly faster rates error decay than those possible classical passive learning settings nature these regimes explored studying fundamental performance limits active passive learning two illustrative nonparametric function classes addition examining theoretical potential active learning paper describes practical algorithm capable exploiting extra flexibility active setting provably improving upon classical passive techniques our active learning theory methods show promise number applications including field estimation using wireless sensor networks fault line detection
predictive state representations psrs method modeling dynamical systems using observable data such actions observations describe their model psrs use predictions about outcome future tests summarize system state best existing techniques discovery learning psrs use monte carlo approach explicitly estimate these outcome probabilities paper present new algorithm discovery learning psrs uses gradient descent approach compute predictions current state algorithm takes advantage large amount structure inherent valid prediction matrix constrain its predictions furthermore algorithm used online agent improve its prediction quality something current state art discovery learning algorithms unable do give empirical results show our constrained gradient algorithm able discover core tests using small amounts data larger amounts data compute accurate predictions system dynamics
introduce method automatically improve character models handwritten without use using minimum document specific training data show use searches words identify portions document whose using templates extracted those regions our character prediction model drastically improve our search retrieval performance words document
develop approach estimation gaussian markov processes imposes smoothness prior while allowing discontinuities instead propagating information between neighboring nodes graph study posterior distribution hidden nodes whole discontinuities edges graph show resulting computation amounts feed forward fan operations reminiscent neurons moreover using suitable matrix matrix inverse determinant approximated without iteration same computational style simulation results illustrate merits approach
linear implementations efficient coding hypothesis such independent component analysis ica sparse coding models have provided functional explanations properties simple cells these models ignore non linear behavior neurons fail match individual population properties neural receptive fields subtle but important ways hierarchical models including gaussian scale mixtures other generative statistical models capture higher order regularities natural images explain nonlinear aspects neural processing such normalization context effects previously had been assumed lower level representation independent hierarchy had been fixed training these models here examine optimal lower level representations derived context hierarchical model find resulting representations different those based linear models unlike basis functions filters learned ica sparse coding these functions individually more closely resemble simple cell receptive fields span broad range spatial scales our work several related approaches observations about natural image structure suggests hierarchical models might yield better representations image structure throughout hierarchy
extend radial basis function rbf networks scenario multiple correlated tasks learned simultaneously present corresponding learning algorithms develop algorithms learning network structure either supervised unsupervised manner training data may actively selected improve networks generalization test data experimental results based real data demonstrate advantage proposed algorithms support our conclusions
propose simple clustering framework graphs encoding pairwise data similarities unlike usual similarity based methods approach assigns data clusters probabilistic way more importantly hierarchical clustering naturally derived framework gradually merge lower level clusters into higher level ones random walk analysis indicates algorithm clustering structures various resolutions ie higher level statistically models longer term diffusion graphs thus discovers more global clustering structure finally provide encouraging experimental results
paper propose general framework study generalization properties binary classifiers trained data may dependent but generated upon sample independent examples provides generalization bounds binary classification cases ranking problems relationship between these learning tasks
has been interest learning non linear manifold models approximate high dimensional data both computational complexity reasons generalization capability sparsity desired feature such models usually means dimensionality reduction naturally implies estimating intrinsic dimension but mean selecting subset data use especially important because many existing algorithms have quadratic complexity number observations paper presents algorithm selecting based lasso regression well known sparse approximations because uses regularization norm added benefit continuous manifold parameterization based found experimental results synthetic real data illustrate algorithm
present computational model human eye movements object class detection task model combines state art computer vision object class detection methods features trained using adaboost biologically plausible model human eye movement produce sequence simulated acquisition target validated model comparing its behavior behavior human observers performing identical object class detection task looking among visually complex objects found considerable agreement between model human data multiple eye movement measures including number cumulative probability target distance
sparse pca seeks approximate sparse eigenvectors whose projections capture maximal variance data cardinality constrained non convex optimization problem np hard encountered wide range applied fields bio recent progress has focused mainly continuous approximation convex relaxation hard cardinality constraint contrast consider alternative discrete spectral formulation based variational eigenvalue bounds provide effective greedy strategy well provably optimal solutions using branch bound search moreover exact methodology used reveals simple step improves approximate solutions obtained continuous method resulting performance gain discrete algorithms demonstrated real world benchmark data extensive monte carlo evaluation trials
lasso regression tends zero weights most irrelevant redundant features hence promising technique feature selection its limitation offers solutions linear models kernel machines feature scaling techniques have been studied feature selection non linear models such approaches require solve hard non convex optimization problems paper proposes new approach named feature vector machine standard lasso regression into form isomorphic svm form easily extended feature selection non linear models introducing kernels defined feature vectors generates sparse solutions nonlinear feature space much more tractable compared feature scaling kernel machines our experiments simulated data show encouraging results identifying small number features non linearly correlated response task standard lasso fails complete
experimental evidence cortical neurons show activity intensity firing events being distributed power law present biologically plausible extension neural network exhibits power law distribution wide range connectivity parameters
address problem robust computationally efficient design biological experiments classical optimal experiment design methods have been widely adopted biological practice part because resulting designs parameter estimates model poor part because computational constraints present method robust experiment design based semidefinite programming relaxation present application method design experiments complex calcium signal transduction pathway have found parameter estimates obtained robust design better than those obtained optimal design
increasing number neuroscience requires statistical analysis high dimensional data sets instance predicting behavior neural firing operating artificial devices brain recordings brain machine interfaces linear analysis techniques remain such cases but classical linear regression approaches often numerically high dimensions paper address question whether emg data collected arm movements monkeys faithfully reconstructed linear approaches neural activity primary motor cortex achieve robust data analysis develop full bayesian approach linear regression automatically detects irrelevant features data against overfitting comparison ordinary least squares regression partial least squares lasso regression force combinatorial search most predictive input features data demonstrate new bayesian method offers superior mixture characteristics terms regularization against overfitting computational efficiency ease use demonstrating its potential replacement other linear regression techniques results our analyses demonstrate emg data well predicted neurons further path possible real time interfaces between machines
show algorithm minimizing symmetric functions used clustering variety different objective functions two specific criteria consider paper single minimum description length criteria first criterion tries maximize minimum distance between elements different clusters inherently discriminative known optimal into clusters given polynomial time criterion computed second criterion seeks minimize description length clusters given probabilistic generative model show optimal partitioning into clusters approximate partitioning guaranteed within factor optimal more clusters computed best our knowledge first time tractable algorithm finding optimal clustering respect mdl criterion clusters has been given optimality result mdl criterion contribution paper show same algorithm used optimize broad class criteria hence used many application specific criterion efficient algorithm known
consider scaling number examples necessary achieve good performance distributed cooperative multi agent reinforcement learning function number agents prove lower bound showing algorithms rely solely global reward signal learn policies fundamental limit they require number real world examples scales roughly linearly number agents settings interest large number agents impractical demonstrate class algorithms taking advantage local reward signals large distributed markov decision processes able ensure good performance number samples scales makes them applicable even settings large number agents
humans extremely learning new skills actions others progression abilities has been observed children ranging imitation simple body movements imitation based inferring paper show problem goal based imitation formulated inferring goals selecting actions using learned probabilistic graphical model environment first describe algorithms planning actions achieve goal state using probabilistic inference describe planning used bootstrap learning goal dependent policies utilizing feedback environment resulting graphical model shown powerful enough allow goal based imitation using simple navigation task illustrate agent infer goals observed teacher imitate teacher even goals uncertain demonstration incomplete
paper propose new receiver digital communications focus application gaussian processes gps detection code division multiple access systems solve near far problem hence aim reduce interference other users sharing same frequency band while usual approaches minimize mean square error linearly retrieve user interest exploit same criteria but design nonlinear optimal solution known nonlinear performance novel method clearly improves detectors furthermore gp based achieves excellent interference suppression even short training sequences include experiments illustrate other nonlinear detectors such those based support vector machines svms exhibit worse performance
humans make optimal perceptual decisions noisy ambiguous conditions computations underlying such optimal behavior have been shown rely probabilistic inference according generative models whose structure usually taken known priori argue bayesian model selection ideal inferring similar even more complex model structures experience find experiments humans learn subtle statistical properties visual scenes completely unsupervised manner show these findings well captured bayesian model learning within class models seek explain observed variables independent hidden causes
calculations quantify dependencies between variables many operations graphical models eg active learning sensitivity analysis previously pairwise information gain calculation has involved cost quadratic network size work show perform similar computation cost linear network size loss function allows form computation dynamic programming message passing algorithm results described empirical results demonstrate large without decrease accuracy cost sensitive domains examined superior accuracy achieved
while classical kernel based learning algorithms based single kernel practice often desirable use multiple kernels et al considered conic combinations kernel matrices classification leading convex constraint quadratic program show semi infinite linear program efficiently solved standard svm implementations moreover generalize formulation our method larger class problems including regression class classification experimental results show proposed algorithm helps automatic model selection improving learning result works hundred thousands examples hundreds kernels combined
paper presents novel technique analyzing imaging data obtained using stimulus evoked experimental paradigm technique based probabilistic graphical model describes data terms underlying evoked interference sources explicitly models stimulus evoked paradigm variational bayesian em algorithm infers model data suppresses interference sources activity separated individual brain sources new algorithm outperforms existing techniques two real datasets well simulated data
most nervous systems encode information about stimuli responding activity large neuronal networks activity often itself dynamically coordinated sequences action potentials multiple electrode recordings now standard tool neuroscience research important have measure such network wide behavioral coordination information sharing applicable multiple neural spike train data propose new statistic informational coherence measures much better unit predicted dynamical state another argue informational coherence measure association shared information superior traditional pairwise measures synchronization correlation find dynamical states use recently introduced algorithm effective state spaces stochastic time series extend pairwise measure multivariate analysis network estimating network multi information illustrate our method testing detailed model transition gamma beta rhythms much most important information neural systems shared over multiple neurons cortical areas such forms population codes distributed representations behavioral time scales neural information stored temporal patterns activity opposed static markers therefore information shared between neurons brain regions physically instantiated coordination between entire sequences neural spikes furthermore neural systems regions brain often require coordinated neural activity perform important functions acting requires multiple neurons cortical areas share information thus want measure dynamic network wide behavior neurons test hypotheses about them need reliable practical methods detect quantify behavioral coordination associated information sharing across multiple neural units these would especially useful testing ideas about particular forms coordination relate distributed coding eg current techniques analyze relations among spike trains handle pairs neurons so further need method analyze coordination network system region whole here propose new measure behavioral coordination information sharing informational coherence based notion dynamical state section coordinated behavior neural systems often captured measures synchronization correlation something sensitive nonlinear stochastic predictive relationships needed section defines informational coherence normalized mutual information between dynamical states two systems explains looking states rather than observables needs out section rarely know right states section briefly describes reconstruct effective state spaces data section gives details about calculate informational coherence approximate global information stored network section applies our method model system biophysically detailed conductance based model comparing our results those more familiar second order statistics interest space omit proofs full discussion existing literature giving minimal references here proofs references appear longer paper now preparation synchrony coherence most hypotheses involve idea information sharing reflected coordinated activity across neural units specific notion coordinated activity namely strict synchrony units should doing exactly same eg spiking exactly same time measure coordination measuring close units come being strictly synchronized eg variance spike times informational point view reason strict synchrony over other kinds coordination neuron consistently spiking ms after another informative relationship two simultaneously spiking but such stable phase relations strict synchrony approaches indeed exact nature neural code uses temporally extended patterns activity so information sharing should reflected coordination those patterns rather than instantaneous activity three common ways going beyond strict synchrony cross correlation related second order statistics mutual information topological generalized synchrony cross correlation function normalized covariance function includes present purposes joint time histogram most widespread measures synchronization efficiently calculated observable series handles statistical well deterministic relationships between processes incorporating variable reduces problem phase locking fourier transformation covariance function xy yields cross spectrum turn gives spectral coherence fx normalized correlation between fourier components integrated over frequencies spectral coherence measures essentially degree linear cross two series applies spectral coherence coordinated neural activity such second order statistics handle linear relationships neural processes known strongly nonlinear little reason think these statistics adequately measure coordination synchrony neural systems mutual information attractive because handles both nonlinear stochastic relationships has natural appealing interpretation unfortunately often seems fail practice being small even between signals known tightly coupled major reason neural codes use distinct patterns activity over time rather than many different instantaneous actions usual approach these extended patterns consider two neurons other spike ms after does driving neuron spiking once every ms these tightly coordinated but whether first neuron time little information about what second neuron doing its spiking but its spiking most time mutual information calculated direct observations spike second neuron fire its around spike here mutual information could find coordination used ms but work general take two rate coding neurons base line firing rates hz suppose stimulus hz suppresses other hz spiking rates thus share lot information but whether neuron about what other neuron did help generalized synchrony based idea establishing relationships between states various units state here taken sense physics dynamics control theory state time variable distribution observables times past system irrelevant state allows us predict well possible system evolve respond external forces two coupled systems said exhibit generalized synchrony state system given mapping state other applications data employ reconstruction state evolves according smooth dimensional deterministic dynamics observe generic function space time delay vectors yt yt yt generic choices various versions generalized synchrony differ precisely quantify mappings between reconstructed state spaces but they appear empirically equivalent another notions phase synchronization based hilbert transforms thus these measures accommodate nonlinear relationships potentially flexible unfortunately essentially reason believe neural systems have deterministic dynamics experimentally levels detail much less deterministic relationships among such states different units what want but none these alternatives provides quantity measures predictive relationships among states but allows those relationships nonlinear stochastic next section introduces such measure call informational coherence states informational coherence alternatives calculating surface mutual information between sequences observations themselves described fails capture coordination know units phase oscillators rate estimate their instantaneous phase rate calculating mutual information between those variables see coordinated units patterns activity phases rates do neural patterns more general common scheme desirable most general notion pattern activity simply dynamical state system sense mentioned above now formalize assuming usual notation shannon information information content state variable hx mutual information between ix well known ix min hx use normalize mutual state information scale informational coherence ic ix min hx interpreted follows ix kullback leibler divergence between joint distribution product their marginal distributions indicating error involved ignoring dependence between mutual information between predictive dynamical states thus error involved assuming two systems independent ie much predictions could improve taking into account dependence hence measures amount dynamically relevant information shared two systems simply value indicates degree two systems have coordinated patterns behavior cf although uses directly observable quantities reconstruction estimation effective state spaces mentioned state space deterministic dynamical system reconstructed sequence observations main tool experimental nonlinear dynamics but assumption crucial false almost interesting neural system while classical state space reconstruction work stochastic processes such processes do have state space representations special case discrete time series ways reconstruct state space here use algorithm introduced code available produces causal state models stochastic automata capable statistically optimal nonlinear prediction state machine minimal sufficient statistic future observable process basic idea form set states should markovian sufficient statistics next observable have deterministic transitions automata theory sense algorithm begins minimal state iid model whether these properties hold means hypothesis tests they fail model modified generally but always adding more states new model again each state model corresponds distinct distribution over future events ie statistical pattern behavior under mild conditions do involve prior knowledge state space converges probability unique causal state model data generating process practice quite fast linear data size generalizes least well training hidden markov models em algorithm using cross validation selection standard heuristic advantage causal state approach shares classical state space reconstruction state estimation greatly simplified general case nonlinear state estimation necessary know form stochastic dynamics state space observation function but their precise parametric values distribution observation driving estimating state observable time series becomes computationally intensive application rule due way causal states built statistics data probability finite time causal state time certain degree belief confidence because way states constructed impossible process other state time once causal state has been established updated recursively ie causal state time explicit function causal state time observation causal state model automatically converted therefore into finite state observation time series outputs corresponding series states our implementation filters its training data automatically result new time series states non predictive components have been filtered out estimating coherence our algorithm estimating matrix informational follows each unit reconstruct causal state model filter observable time series produce series causal states each pair neurons construct joint histogram causal state models have same expressive power observable operator models predictive state representations greater power than variable length markov models figure neuronal spike times network excitatory pyramidal neurons numbers shown green inhibitory interneurons numbers red during first seconds current connections among pyramidal cells suppressed gamma rhythm emerges left those connections become active leading beta rhythm right state distribution estimate mutual information between states normalize single unit state gives symmetric matrix values even two systems independent their estimated ic average positive because while they should have zero mutual information empirical estimate mutual information non negative thus significance ic values assessed against null hypothesis system independence way do so take reconstructed state models two systems run them forward independently another generate large number simulated state sequences these calculate values ic procedure approximate sampling distribution ic under null model preserves dynamics each system but their interaction find values usual omit them here space approximating network multi information broad agreement analyses networks should analysis pairs neurons averaged over pairs analysis information sharing network would look over structure statistical dependence between various units reflected complete joint probability distribution states would allow us instance calculate fold multi information ix xn dp kullback leibler divergence between joint distribution product marginal distributions analogous pairwise mutual information calculated over predictive states multi information would give total amount shared dynamical information system normalized mutual information ix its maximum possible value min hx hx normalize its maximum smallest sum marginal ix xn min unfortunately distribution over high dimensional space so hard estimate well without strong parametric constraints thus consider approximations lowest order approximation treats units independent distribution step up tree distributions global distribution function joint distributions pairs units every pair units needs enter into such distribution every unit part pair tree distribution corresponds spanning tree edges linking units whose interactions enter into global probability conversely spanning trees determine tree distributions writing et set pairs xn xn has xi xi xj xj xi xi xi xi xj xj marginal distributions xi pair distributions xi xj estimated empirical marginal pair distributions now pick edges et so best approximates true global distribution natural approach minimize dp divergence between its tree approximation chow liu showed maximum weight spanning tree gives divergence minimizing distribution taking edges weight mutual information between variables links three advantages using chow liu approximation estimating empirical probabilities gives consistent maximum likelihood estimator ideal tree reasonable rates convergence so reliably known even cannot efficient algorithms constructing maximum weight spanning trees such algorithm sec runs time log thus approximation computationally tractable kl divergence chow liu distribution gives lower bound network multi information bound sum mutual along edges tree ix xn dt xj even exactly eq would useful alternative calculating dp directly evaluating log exponentially many configurations natural seek higher order approximations eg using three way interactions into pairwise interactions but hard do so effectively because finding optimal approximation such interactions allowed np analytical formulas like eq generally do exist therefore chow liu approximation here example model gamma beta rhythms use simulated data test case instead empirical multiple electrode recordings allows us try method system over neurons compare measure against expected results model taken originally designed study episodes gamma hz beta hz oscillations mammalian nervous system often occur successively spontaneous transition between them more rhythms studied those displayed hippocampal ca slice vivo neocortical model contains two neuron populations excitatory pyramidal neurons inhibitory interneurons defined conductance based hodgkin huxley style equations simulations carried out network pyramidal cells interneurons each cell modeled compartment neuron coupling basic potassium spiking currents external applied current gaussian input noise first seconds simulation correspond gamma rhythm group neurons made spike via linearly increasing applied current beta rhythm figure maps coordination network measured zero cross correlation top row informational coherence bottom gamma rhythm left column beta right colors run red coordination through yellow maximum subsequent seconds obtained activating pyramidal pyramidal recurrent connections hebbian preprocessing result synchrony during gamma rhythm slow after hyper ahp current current suppressed during gamma due activation used generation rhythm during beta rhythm pyramidal cells during gamma rhythm fire subset interneurons cycles fig fig compares zero cross correlation second order method quantifying coordination informational coherence calculated reconstructed states simulation could have calculated actual states model neurons directly rather than reconstructing them but purposes testing our method did finds relationships visible fig but instance phase shifts between pyramidal cells surface mutual information shown gives similar results informational coherence has recognizing two populations effectively coordinated blocks presence dynamical noise problematic ordinary state reconstruction issue average ic inactive low neurons tree estimate global informational multi information bits global coherence right half analysis beta rhythm stage average ic tree estimate global multi information bits though estimated global coherence falls slightly because low neurons before now active global information but over pattern somewhat weaker more noisy seen fig so expected total information content higher but overall coordination across network lower conclusion informational coherence provides measure neural information sharing coordinated activity nonlinear stochastic relationships between extended patterns spiking robust dynamical noise leads multivariate measure global coordination across networks regions applied data multi electrode recordings should valuable tool evaluating hypotheses about distributed neural representation function acknowledgments page support institute under grants nsf foundation agreement foundation foundation references sejnowski eds neural codes distributed representations mit press nature neuroscience rao rao olshausen eds probabilistic models brain pp mit press eds practice data analysis pp princeton up et al physical review statistical dynamics press littman sutton singh becker ghahramani eds advances neural information processing systems pp mit press nonlinear time series analysis cambridge up cover elements information theory et al physical review probability eds uncertainty artificial intelligence proceedings conference pp press journal statistical physics neural computation singer tishby machine learning statistics linear nonlinear filtering world scientific upper university california still bialek physical review letters chow liu ieee transactions information theory et al
consider task depth estimation single monocular image take supervised learning approach problem begin training set monocular images unstructured outdoor environments include trees etc their corresponding ground truth apply supervised learning predict function image depth estimation challenging problem local features alone insufficient estimate depth point needs consider global context image our model uses trained markov random field mrf incorporates multiscale global image features models both individual points well relation between different points show even unstructured scenes our algorithm frequently able recover fairly accurate
probabilistic temporal planning attempts find good policies acting domains concurrent tasks multiple uncertain outcomes limited resources these domains typically modelled markov decision problems solved using dynamic programming methods paper demonstrates application reinforcement learning form policy gradient method these domains our emphasis large domains dynamic programming our approach construct simple policies agents each planning task result general probabilistic temporal planner named factored policy gradient planner planner handle hundreds tasks probability success duration resource use
present family approximation techniques probabilistic graphical models based use graphical developed scientific computing literature our framework yields rigorous upper lower bounds event probabilities log partition function undirected graphical models using non iterative procedures have low time complexity mean field approaches approximations built upon tractable problem optimizing tractable distribution parameters approximate inference terms well studied linear systems problem obtaining good matrix experiments presented compare new approximation schemes variational methods
diffusion tensor magnetic resonance imaging dt mri non method brain neuronal fibers here show modification dt mri allows neuronal fibers use tensor variational framework replaces diffusion model dt mri multiple component model fits signal variational regularization mechanism order reduce free water contamination estimate free water compartment volume fraction each remove calculate remaining compartment variational framework applied data collected conventional clinical parameters containing six diffusion directions using variational framework able overcome highly ill posed fitting results show able find fibers found dt mri
paper describes highly successful application mrfs problem generating high resolution range images new generation range sensors combines capture low resolution range images acquisition high resolution camera images mrf paper exploits fact discontinuities range tend co enables generate high resolution low noise range images integrating regular camera images into range data show using such mrf substantially improve over existing range imaging technology
describe novel method learning templates recognition localization objects drawn categories generative model represents configuration multiple object parts respect object coordinate system these parts turn generate image features complexity model number features low meaning our model much more efficient train than comparative methods moreover variational approximation introduced allows learning orders magnitude faster than previous approaches while incorporating many more features results both accuracy localization improvements our model has been tested standard datasets compare number recent template models particular demonstrate state art results detection localization
inspired sets consider problem retrieving items concept cluster given query consisting few items cluster formulate bayesian inference problem describe simple algorithm solving our algorithm uses concept cluster ranks items using score evaluates marginal probability each item belongs cluster containing query items exponential family models conjugate priors marginal probability simple function sufficient statistics focus sparse binary data show our score evaluated exactly using single sparse matrix multiplication making possible apply our algorithm large datasets evaluate our algorithm three datasets retrieving finding author sets nips dataset finding sets words compare sets show bayesian sets gives reasonable set
category visual stimuli has been reliably decoded patterns neural activity visual cortex has yet seen whether object identity inferred activity present fmri data measuring responses human cortex set distinct object images use simple winner take classifier using half data each recording session training set evaluate encoding object identity across fmri voxels approach sensitive inclusion noisy voxels describe two methods identifying subsets voxels data optimally distinguish object identity method characterizes reliability each within subsets data while another estimates mutual information each stimulus set find both metrics identify subsets data reliably encode object identity even noisy measurements artificially added data mutual information metric less efficient task likely due constraints fmri data
demonstrate first fully hardware implementation retinotopic self organization transduction neural map formation silicon retina illumination into correlated spike trains drive population silicon growth automatically wire topographic mapping toward sources guidance cue postsynaptic spikes varied pattern illumination growth projected different retinal ganglion cell types self organize coordinated retinotopic maps
nested sampling new monte carlo method intended general bayesian computation nested sampling provides robust alternative annealing based methods computing normalizing constants generate estimates other quantities such posterior expectations key technical requirement ability draw samples uniformly prior subject constraint likelihood provide demonstration model undirected graphical model
linear text classification algorithms work computing inner product between test document vector parameter vector many such algorithms including naive bayes most variants parameters determined simple closed form function training set statistics call mapping mapping statistics parameters parameter function much research text classification over last few decades has consisted efforts identify better parameter functions paper propose algorithm automatically learning function related classification problems parameter function found our algorithm defines new learning algorithm text classification apply novel classification tasks find our learned classifier outperforms existing methods variety multiclass text classification tasks
paper explores two aspects social network modeling first generalize successful static model relationships into dynamic model accounts over time second show make tractable learn such models data even number entities gets large generalized model each entity point dimensional latent space points move time progresses but large moves latent space observed links between entities more likely entities close latent space show make such model tractable number entities use appropriate kernel functions similarity latent space use low dimensional trees new efficient dynamic adaptation multidimensional scaling first pass approximate projection entities into latent space efficient conjugate gradient update rule non linear local optimization time per entity during update use both synthetic real world data entities indicate linear scaling computation time improved performance over four alternative approaches illustrate system operating years nips co data present detailed version work
investigate top down td bottom up bu information weighted guidance human search behavior manipulated proportions bu td components saliency based model model biologically plausible implements artificial retina neuronal population code bu component based td component defined feature template match stored target representation compared models behavior different mixtures td bu components eye movement behavior human observers performing identical search task found purely td model provides much closer match human behavior than mixture model using bu information biological constraints removed eg eliminating retina did mixture model begin approximate human behavior
many real world classification problems involve prediction multiple inter dependent variables forming structural dependency recent progress machine learning has mainly focused supervised classification such structured variables paper investigate structured classification semi supervised setting present discriminative approach utilizes intrinsic geometry input patterns revealed unlabeled data points derive maximum margin formulation semi supervised learning structured variables unlike transductive algorithms our formulation naturally extends new test points
paper proposes algorithm convert stage stochastic decision problem continuous state space sequence supervised learning problems optimization problem associated trajectory tree random trajectory methods solved using method algorithm breaks reinforcement learning problem into sequence single stage reinforcement learning each solved via exact reduction weighted classification problem solved using off self methods thus algorithm reinforcement learning problem into simpler supervised learning shown method converges finite number steps solution cannot further improved optimization proposed algorithm classification methods applied find policies reinforcement learning problem
present model edge region grouping using conditional random field built over scale invariant representation images integrate multiple cues our model includes potentials capture low level similarity mid level continuity high level object shape maximum likelihood parameters model learned human labeled large collection images using belief propagation using held out test data quantify information gained incorporating generic mid level cues high level shape
biological sensory systems faced problem encoding high fidelity sensory signal population noisy low fidelity neurons problem expressed information theoretic terms coding multi dimensional analog signal over set noisy channels previously have shown robust overcomplete codes learned minimizing reconstruction error constraint channel capacity here present theoretical analysis characterizes optimal linear decoder twodimensional data analysis allows arbitrary number coding units thus including both over complete representations provides number important insights into optimal coding strategies particular show form code adapts number coding units different data noise conditions achieve robustness report numerical solutions robust coding highdimensional image data show these codes substantially more robust compared against other image codes such ica wavelets
describe vision based obstacle avoidance system off road mobile robots system trained end end map raw input images angles trained supervised mode predict angles provided human during training runs collected wide variety weather conditions lighting conditions obstacle types robot cm off road two wireless color remote computer processes video controls robot via learning system large layer convolutional network whose input single pair low resolution images robot exhibits excellent ability detect obstacles around them real time speeds ms
arm highly complex controls such hyper redundant arm eight them yet unknown robotic arms based same mechanical principles may present day robotic arms paper tackle control problem using online reinforcement learning algorithm based bayesian approach policy evaluation known gaussian process temporal difference learning our real arm computer simulation dimensional model arm even inherent model state space face high dimensional apply algorithm domain demonstrate its operation several learning tasks varying degrees difficulty
propose efficient algorithms learning ranking functions order constraints between sets ie classes training samples our algorithms may used maximizing generalized statistic accounts partial ordering classes special cases include maximizing area under roc curve binary classification its generalization ordinal regression experiments public benchmarks indicate proposed algorithm least accurate current state art computationally several orders magnitude faster unlike current methods easily able handle even large datasets over samples
introduce new model genetic diversity summarizes large input dataset into epitome short sequence small set short sequences probability distributions capturing many overlapping subsequences dataset epitome representation has already been used modeling real valued signals such images audio discrete sequence model introduce paper targets applications multiple alignment mutation inference our experiments concentrate modeling diversity epitome emerges natural model producing relatively small covering large number system targets known our experiments show epitome includes more than other designs similar length including consensus tree centers observed discuss epitome designs take into account uncertainty about cross presentation our experiments find optimization fairly robust these uncertainties
paper proposes new approach feature selection based statistical feature mining technique sequence tree kernels natural language data take discrete structures convolution kernels such sequence tree kernels both concept accuracy many natural language processing tasks experiments have shown best results achieved limited small sub structures these kernels paper discusses issue convolution kernels proposes statistical feature selection enable us use larger sub structures effectively proposed method order efficiently embedded into original kernel calculation process using sub structure mining algorithms experiments real tasks confirm problem conventional method compare performance conventional method proposed method
paper addresses issue numerical computation machine learning domains based similarity metrics such kernel methods spectral techniques gaussian processes presents general solution strategy based subspace iteration fast body learning methods experiments show significant gains computation storage datasets arising image segmentation object detection dimensionality reduction paper presents theoretical bounds stability these methods
although variants value iteration have been proposed finding nash correlated equilibria general sum markov games these variants have been shown effective general paper demonstrate construction existing variants value iteration cannot find stationary equilibrium policies arbitrary general sum markov games instead propose alternative interpretation output value iteration based new non stationary equilibrium concept call cyclic equilibria prove value iteration identifies cyclic equilibria class games fails find stationary equilibria demonstrate empirically value iteration finds cyclic equilibria nearly examples drawn random distribution markov games
learning patterns human behavior sensor data extremely important high level activity inference show extract label activities significant places traces gps data contrast existing techniques our approach simultaneously detects classifies significant locations person takes context into account our system uses relational markov networks represent hierarchical activity model encodes complex relations among gps activities significant places apply fft based message passing perform efficient summation over large numbers nodes networks present experiments show significant improvements over existing techniques
topic models such latent dirichlet allocation lda useful tools statistical analysis document other discrete data lda model assumes words each document arise mixture topics each distribution over vocabulary limitation lda inability model topic correlation even though example document about more likely about disease than limitation use dirichlet distribution model variability among topic proportions paper develop correlated topic model topic proportions exhibit correlation via logistic normal distribution derive mean field variational inference algorithm approximate posterior inference model complicated fact logistic normal conjugate multinomial gives better fit than lda collection journal science furthermore provides natural way visualizing exploring other unstructured data sets
motivated problem learning detect recognize objects minimal supervision develop hierarchical probabilistic model spatial structure visual scenes contrast most existing models our approach explicitly captures uncertainty number object instances depicted given image our scene model based transformed dirichlet process novel extension hierarchical dp set stochastically transformed mixture components shared between multiple groups data visual scenes mixture components describe spatial structure visual features object centered coordinate frame while transformations model object positions particular image learning inference has many potential applications beyond computer vision based empirically effective gibbs applied dataset partially labeled street scenes show inclusion spatial structure improves detection performance exploiting partially labeled training images
present infinite mixture model each component comprises multivariate gaussian distribution over input space gaussian process model over output space our model able deal non stationary covariance functions discontinuities overlapping output signals work similar ghahramani use full generative model over input output space rather than conditional model allows us deal incomplete data perform inference over inverse functional mappings well regression leads more powerful consistent bayesian specification effective gating network different experts
clustering fundamental problem machine learning has been approached many ways two general quite different approaches include iteratively fitting mixture model eg using em linking together pairs training cases have high affinity eg using spectral methods pair wise clustering algorithms need compute sufficient statistics avoid poor solutions directly placing similar examples same cluster many applications require each cluster data accurately described prototype model so affinity based clustering its benefits cannot directly realized describe technique called affinity propagation combines advantages both approaches method learns mixture model data recursively propagating affinity messages demonstrate affinity propagation problems clustering image patches image segmentation learning mixtures gene expression models microarray data find affinity propagation obtains better solutions than mixtures gaussians algorithm spectral clustering hierarchical clustering both able find pre specified number clusters able automatically determine number clusters interestingly affinity propagation viewed belief propagation graphical model accounts pairwise training case likelihood functions identification cluster centers
paper presents new sampling algorithm approximating functions variables undirected graphical models arbitrary connectivity pairwise potentials well estimating notoriously difficult partition function graph algorithm fits into framework sequential monte carlo methods rather than more widely used mcmc relies constructing sequence intermediate distributions get closer desired while idea using proposals known construct novel sequence target distributions rather than global temperature parameter sequentially individual pairs variables initially sampled exactly spanning tree variables present experimental results inference estimation partition function sparse densely connected graphs
consider criteria variational representations non gaussian latent variables derive variational em algorithms general form establish general equivalence among convex bounding methods evidence based methods ensemble bayes methods has previously been demonstrated particular cases
problem semi supervised learning construction graph underlying data propose use method optimally combines number differently constructed graphs each these graphs associate basic graph kernel compute optimal combined kernel kernel solves extended regularization problem requires joint minimization over both data set graph kernels present encouraging results different ocr tasks optimal combined kernel computed graphs constructed variety distances functions nearest neighbors
present probabilistic generative model entity relationships their attributes simultaneously discovers groups among entities topics among corresponding attributes block models relationship data have been studied social network analysis time here simultaneously cluster several modalities once incorporating attributes here words associated certain relationships significantly joint inference allows discovery topics guided emerging groups present experimental results two large data sets years put before us comprising their corresponding text records years similar data show comparison traditional separate latent variable models words group topic models joint inference discovers more groups improved topics
spiking activity neurophysiological experiments often exhibits dynamics beyond driven external stimulation presumably reflecting extensive neural circuitry characterizing these dynamics may reveal important features neural computation particularly during internally driven cognitive operations example activity cortex neurons during delay period separating movement target specification cue involved motor planning show dynamics underlying activity captured non linear dynamical systems model underlying recurrent structure stochastic point process output present validate latent variable methods simultaneously estimate system parameters trial trial dynamical trajectories these methods applied characterize dynamics data recorded electrode array while monkeys perform delayed reach tasks
present bayesian framework explaining people reason about predict actions agent based observing its behavior action understanding cast problem probabilistic generative model assumes agents tend act order achieve their goals given constraints their environment working simple world domain show model used infer goal agent predict agent act novel situations environmental constraints change model provides qualitative account several kinds inferences have been shown perform fits quantitative predictions adult observers make new experiment
consider problem modeling dynamics based state action trajectories collected contribution paper two fold first consider linear models such learned standard helicopter identification show linear parameterization makes certain properties dynamical systems such difficult capture propose alternative acceleration based parameterization does suffer deficiency learned efficiently data second markov decision process model dynamics would explicitly model step transitions but often interested models predictive performance over longer paper present efficient algorithm approximately minimizing prediction error over long time scales present empirical results two different although work motivated problem modeling ideas presented here general applied modeling large classes dynamics
reinforcement learning direct policy gradient estimation attractive theory but practice leads notoriously ill optimization problems improve its robustness speed convergence stochastic meta descent gain vector adaptation method employs fast hessian vector products our experiments resulting algorithms outperform previously employed online stochastic offline conjugate natural policy gradient methods
problem computing estimate reconstruction error pca inference problem help replica method using expectation consistent ec approximation intractable inference problem solved efficiently using two variational parameters correction result computed alternative simplified derivation presented
present mixed signal vlsi hardware real time blind separation localization acoustic sources gradient flow representation traveling wave signals acquired over cm array four yields linearly mixed instantaneous observations time sources separated localized independent component analysis ica gradient flow ica processors each measure mm mm cmos power respectively supply sampling rate experiments demonstrate clear separation precise localization two speech sources presented through speakers array conference room table analysis residuals shows they direct path
present simple scalable algorithm large margin estimation structured models including important class markov networks combinatorial models formulate estimation problem convex concave saddle point problem apply method yielding algorithm linear convergence using simple gradient projection calculations projection step solved using combinatorial algorithms min cost quadratic flow makes approach efficient alternative formulations based quadratic program qp present experiments two different structured prediction tasks image segmentation word alignment illustrating favorable scaling properties our algorithm
multiple information sources yield significant benefits successfully learning tasks many studies have information supervised learning contexts present approach utilize multiple information sources form similarity data unsupervised learning based similarity information clustering task non negative matrix factorization problem mixture similarity measurements tradeoff between data sources sparseness their mixture controlled entropy based weighting mechanism purpose model selection stability based approach employed ensure selection most self consistent hypothesis experiments demonstrate performance method toy well real world data sets
use core decomposition develop algorithms analysis large scale complex networks decomposition based recursive pruning least connected vertices allows hierarchical structure networks focusing their central using strategy develop general visualization algorithm used compare structural properties various networks highlight their hierarchical structure low computational complexity algorithm size network number edges makes suitable visualization large sparse networks show proposed visualization tool allows find specific structural networks
