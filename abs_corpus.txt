how does the connectivity of a neural network number of synapses per neuron relate to the complexity of the problems it can handle measured by the entropy switching theory would suggest no relation at all since all boolean functions can be implemented using a circuit with very low connectivity eg using two input nand gates however for a network that learns a problem from examples using a local learning rule we prove that the entropy of the problem becomes a lower bound for the connectivity of the network
we describe a family of learning algorithms that operate on a recurrent symmetrically connected neuromorphic network that like the boltzmann machine settles in the presence of noise these networks learn by modifying synaptic connection strengths on the basis of correlations seen locally by each synapse we describe a version of the supervised learning algorilhm for a network with analog activation functions we also demonstrate unsupervised competitive learning with this approach where weight saturation and decay play an important role and describe preliminary experiments in reinforcement earning where noise is used in the search procedure we identify the above described phenomena as elements that can unify learning techniques at a physical microscopic level these algorilhms were chosen for ease of implementation in vlsi we have designed a cmos test chip in micron rules that can speed up the learning about a millionfold over an equivalent simulation on a vax the speedup is due to parallel analog computation for summing and multiplying weights and activations and the use of physical processes for generating random noise the components of the test chip are a noise amplifier a neuron amplifier and a transistor adaptive synapse each of which is separately testable these components are also integrated into a neuron and synapse network finally we point out techniques for reducing the area of the electronic correlational synapse both in technology and design and show how the algorithm we study can be implemented naturally in electronic systems
this paper generalizes the backpropagation method to a general network containing feedback connections the network model considered consists of interconnected groups of neurons where each group could be fully interconnected it could have feedback connections with possibly asymmetric weights but no loops between the groups are allowed a stochastic descent algorithm is applied under a certain inequality constraint on each intra group weight matrix which ensures for the network to possess a unique equilibrium state for every input
an artificial neural network is developed to recognize spatioqemporal bipolar patterns associatively the function of a formal neuron is generalized by replacing multiplication with convolution weights with transfer functions and thresholding with nonlinear transform following adaptation the hebbian learning rule and the delta learning rule are generalized accordingly resulting in the learning of weights and delays the neural network which was first developed for spatial patterns was thus generalized for spario temporal patterns it was tested using a set of bipolar input patterns derived from speech signals showing robust classification of model phonemes
the complexity and computational capacity of multi layered feedforward neural networks is examined neural networks for special purpose structured functions are examined from the perspective of circuit complexity known results in complexity theory are applied to the special instance of neural network circuits and in particular classes of functions that can be implemented in shallow circuits characterised some conclusions are also drawn about learning complexity and some open problems raised the dual problem of determining the computational capacity of a class of multi layered networks with dynamics regulated by an algebraic hamiltoninn is considered formal results are presented on the storage capacities of programmed higher order structures and a tradeoff between ease of programming and capacity is shown a precise determination is made of the static fixed point structure of random higher order constructs and phase transitions laws are shown

in the visual cortex of the monkey the horizontal organization of the preferred orientations of orientation selective cells follows two opposing rules neighbors tend to have similar orientation preferences and many different orientations are observed in a local region several orientation models which satisfy these constraints are found to differ in the spacing and the topological index of their singularities using the rate of orientation change as a measure the models are compared to published experimental results
we investigate the behavior of different learning algorithms for networks of neuron like units as test cases we use simple pattern association problems such as the xor problem and symmetry detection problems the algorithms considered are either versions of the boltzmann machine learning rule or based on the backpropagation of errors we also propose and analyze a generalized delta rule for linear threshold units we find that the performance of a given learning algorithm depends strongly on the type of units used in particular we observe that networks with units quite generally exhibit a significantly better learning behavior than the corresponding versions we also demonstrate that an adaption of the weight structure to the symmetries of the problem can lead to a drastic increase in learning speed

being able to record the electrical activities of a number of neurons simultaneously is likely to be important in the study of the functional organization of networks of real neurons using one extracellular microelectrode to record from several neurons is one approach to studying the response properties of sets of adjacent and therefore likely related neurons however to do this it is necessary to correctly classify the signals generated by these different neurons this paper considers this problem of classifying the signals in such an extracellular recording based upon their shapes and specifically considers the classification of signals in the case when spikes overlap temporally
much experimental study of real neural networks relies on the proper classification of extracellulary sampled neural signals ie action potentials recorded from the brains of experimental animals in most neurophysiology laboratories this classification task is simplified by limiting investigations to single electrically well isolated neurons recorded one at a time however for those interested in sampling the activities of many single neurons simultaneously waveform classification becomes a serious concern in this paper we describe and constrast three approaches to this problem each designed not only to recognize isolated neural events but also to separately classify temporally overlapping events in real time first we present two formulations of waveform classification using a neural network template matching approach these two formulations are then compared to a simple template matching implementation analysis with real neural signals reveals that simple template matching is a better solution to this problem than either neural network approach
based on anatomical and physiological data we have developed a computer simulation of piriform olfactory cortex which is capable of reproducing spatial and temporal patterns of actual cortical activity under a variety of conditions using a simple hebb type learning rule in conjunction with the cortical dynamics which emerge from the anatomical and physiological organization of the model the simulations are capable of establishing cortical representations for different input patterns the basis of these representations lies in the interaction of sparsely distributed highly divergentconvergent interconnections between modeled neurons we have shown that different representations can be stored with minimal interference and that following learning these representations are resistant to input degradation allowing reconstruction of a representation following only a partial presentation of an original training stimulus further we have demonstrated that the degree of overlap of cortical representations for different stimuli can also be modulated for instance similar input patterns can be induced to generate distinct cortical representations discrimination while dissimilar inputs can be induced to generate overlapping representations accommodation both features are presumably important in classifying olfactory stimuli
the simd parallelism of the connection machine cm allows the construction of neural network simulations by the use of simple data and control structures two approaches are described which allow parallel computation of a models nonlinear functions parallel modification of a models weights and parallel propagation of a models activation and error each approach also allows a models interconnect structure to be physically dynamic a hopfield model is implemented with each approach at six sizes over the same number of cm processors to provide a performance comparison

artificial neural networks anns are capable of accurate recognition of simple speech vocabularies such as isolated digits this paper looks at two more difficult vocabularies the alphabetic e set and a set of polysyllabic words the e set is difficult because it contains weak discriminants and polysyllables are difficult because of timing variation polysyllabic word recognition is aided by a time pre alignment technique based on dynamic programming and e set recognition is improved by focusing attention recognition accuracies are better than for both vocabularies when implemented with a single layer perceptron
the potential for presynaptic information processing within the arbor of a single axon will be discussed in this paper current knowledge about the activity dependence of the firing threshold the conditions required for conduction failure and the similarity of nodes along a single axon will be reviewed an electronic circuit model for a site of low conduction safety in an axon will be presented in response to single frequency stimulation the electronic circuit acts as a lowpass filter i
in this paper we wish to analyze the convergence behavior of a number of neuronal plasticity models recent neurophysiological research suggests that the neuronal behavior is adaptive in particular memory stored within a neuron is associated with the synaptic weights which are varied or adjusted to achieve learning a number of adaptive neuronal models have been proposed in the literature three specific models will be analyzed in this paper specifically the hebb model the sutton barto model and the mot recent trace model in this paper we will examine the conditions for convergence the position of convergence and the rate at convergence of these models as they applied to classical conditioning simulation results are also presented to verify the analysis
the new neural network classifier we propose transforms the classification problem into the coding theory problem of decoding a noisy codeword an input vector in the feature space is transformed into an internal representation which is a codeword in the code space and then error correction decoded in this space to classify the input feature vector to its class two classes of codes which give high performance are the hadamard matrix code and the maximal length sequence code we show that the number of classes stored in an n neuron system is linear in n and significantly more than that obtainable by using the hopfield type memory as a classifier i

various simulations o cortical subnetworks have evidenced something like phase transitions with respect to key parameters we demonstrate that such transitions must indeed exist in analogous ininite array models for related inite array models classical phase transitions which describe steady state behavior may not exist but there can be distinct qualitative changes in metastable transient behavior as key system parameters pass through critical values
transient phenomena associated with forward biased silicon p n n structures at k show remarkable similarities with biological neurons the devices play a role similar to the two terminal switching elements in hodgkin huxley equivalent circuit diagrams the devices provide simpler and more realistic neuron emulation than transistors or op amps they have such low power and current requirements that they could be used in massive neural networks some observed properties of simple circuits containing the devices include action potentials refractory periods threshold behavior excitation inhibition summation over synaptic inputs synaptic weights temporal integration memory network connectivity modification based on experience pacemaker activity firing thresholds coupling to sensors with graded signal outputs and the dependence of firing rate on input current transfer functions for simple artificial neurons with spiketrain inputs and spiketrain outputs have been measured and correlated with input coupling



we propose learning rules for recurrent neural networks with high order interactions between some or all neurons the designed networks exhibit the desired associative memory function perfect storage and retrieval of pieces of information andor sequences of information of any complexity
we report a study bn the relationship between eeg amplitude values and unit spike output in the prepyriform cortex of awake and motivated rats this relationship takes the form of a sigmoid curve that describes normalized pulse output for normalized wave input the curve is fitted using nonlinear regression and is described by its slope and maximum value measurements were made for both excitatory and inhibitory neurons in the cortex these neurons are known to form a monosynaptic negative feedback loop both classes of cells can be described by the same parameters the sigmoid curve is asymmetric in that the region of maximal slope is displaced toward the excitatory side the data are compatible with freemans model of prepyriform burst generation other analogies with existing neural nets are being discussed and the implications for signal processing are reviewed in particular the relationship of sigmoid slope to efficiency of neural computation is examined
advances in brain theory need two complementary approaches analytical investigations by in situ measurements and as well synthetic modelling supported by computer simulations to generate suggestive hypothesis on purposeful structures in the neural tissue in this paper research of the second line is described starting from a neurophysiologically inspired model of stimulusresponse s r andor associative memorization and a psychologically motivated ministructure for basic control tasks pre conditions and conditions are studied for cooperation of such units in a hierarchical organisation as can be assumed to be the general layout of macrostructures in the brain i
the interaction of a set of tropisms is sufficient in many cases to explain the seemingly complex behavioral responses exhibited by varied classes of biological systems to combinations of stimuli it can be shown that a straightforward generalization of the tropism phenomenon allows the efficient implementation of effective algorithms which appear to respond intelligently to changing environmental conditions examples of the utilization of troplstic processing techniques will be presented in this paper in applications entailing simulated behavior synthesis path planning pattern analysis clustering and engineering design optimization
intracellular recordings in spinal cord motoneurons and cerebral cortex neurons have provided new evidence on the correlational strength of monosynaptic connections and the relation between the shapes of postsynaptic potentials and the associated increased firing probability in these cells excitatory postsynaptic potentials epsps produce crosscorrelogram peaks which resemble in large part the derivative of the epsp additional synaptic noise broadens the peak but the peak area ie the number of above chance firings triggered per epsp remains proportional to the epsp amplitude a typical epsp of gv triggers about firings per epsp the consequences of these data for information processing by polysynaptic connections is discussed the effects of sequential polysynaptic links can be calculated by convolving the effects of the underlying monosynaptic connections the net effect of parallel pathways is the sum of the individual contributions
the hopfield neural network model for associative memory is generaezed the generalization replaces two state neurons by neurons taking a richer set of values two classes of neuron input output relations we developed guaranteeing convergence to stable states the fu st is a class of continuous relations and the second is a class of allowed quantization rules for the neurons the information capacity for networks from the second class is found to be of order s bits for a network with s neurons a generalization of the sum of outer products learning rule is developed and investigated as well american institute of physics i
a computer program has been designed and implemented to allow a researcher to analyze the oscillatory behavior of simulated neural networks with cyclic connectivity the computer program implemented on the texas instruments explorerodyssey system and the results of numerous experiments are discussed the program cycles allows a user to construct operate and inspect neural networks containing cyclic connection paths with the aid of a powerful graphicsbased interface numerous cycles have been studied including cycles with one or more activation points non interruptible cycles cycles with variable path lengths and interacting cycles the final class interacting cycles is important due to its ability to implement time dependent goal processing in neural networks

we describe a method of constructing higher order neural networks that respond invariantly under geometric transformations on the input space by requiring each unit to satisfy a set of constraints on the interconnection weights a particular structure is imposed on the network a network built using such an architecture maintains its invariant performance independent of the values the weights assume of the learning rules used and of the form of the nonlinearities in the network the invariance exhibited by a firstorder network is usually of a trivial sort eg responding only to the average input in the case of translation invariance whereas higher order networks can perform useful functions and still exhibit the invariance we derive the weight constraints for translation rotation scale and several combinations of these transformations and report results of simulation studies
information retrieval in a neural network is viewed as a procedure in which the network computes a most probable or map estimate of the unknown information this viewpoint allows the class of probability distributions p the neural network can acquire to be explicitly specified learning algorithms for the neural network which search for the most probable member of p can then be designed statistical tests which decide if the true or environmental probability distribution is in p can also be developed example applications of the theory to the highly nonlinear back propagation learning algorithm and the networks of hop field and anderson are discussed
to process sensory data sensory brain areas must preserve information about both the similarities and differences among learned cues without the latter acuity would be lost whereas without the former degraded versions of a cue would be erroneously thought to be distinct cues and would not be recognized we have constructed a model of piriform cortex incorporating a large number of biophysical anatomical and physiological parameters such as two step excitatory firing thresholds necessary and suicient conditions for long term potentiation ltp of synapses three distinct types of inhibitory currents short ipsps long hyperpolarizing currents lhp and long cellspecific afterhyperpolarization ahp sparse connectivity between bulb and layer ii cortex caudally fiowing excitatory collateral fibers nonlinear dendritic summation etc we have tested the model for its ability to learn similarityand difference preserving encodings of incoming sensory cues the biological characteristics of the model enable it to produce multiple encodings of each input cue in such a way that different readouts of the cell firing activity of the model preserve both similarity and difference iuiormation in particular probabilistic quant al transmitter release properties of pitiform synapses give rise to probabilistic postsynaptic voltage levels which in combination with the activity of local patches of inhibitory interneurons in layer h differentially select bursting rs single pulsing layer ii cells time locked firing to the theta rhythm larson and lynch enables distinct spatial patterns to be read out against a relatively quiescent background firing rate raining trials using the physiological rules for induction of ltp yield stable layer ii cell spatial firing patterns for learned cues multiple simulated olfactory input patterns ie those that share many chemical features will give rise to strongly overlapping bulb firing patterns activating many shared lateral olfactory tract lot axons innervating layer ia of pitiform cortex which in turn yields highly overlapping layer h cell excitatory potentials enabling this spatial layer ii cell encoding to preserve the overlap similarity among similar inputs at the same time those synapses that are enhanced by the learning process cause stronger cell firing yielding strong cell specific afterhyperpolarizing ahp currents local inhibitory interneurons effectively select alternate cells to fire once strongly firing cells have undergone ahp these alternate cells then activate their caudally fiowing recurrent collaterals activating distinct populations of synapses in caudal layer ib potentiation of these synapses in combination with those of still active lot axons selectively enhance the response of caudal cells that tend to accentuate the differences among even very similar cues empirical tests of the computer simulation have shown that after training the initial spatial layer ii cell firing responses to similar cues enhance the similarity of the cues such that the overlap in response is equal to or greater than the overlap in thls research was supported in part by the otce of naval lesearch under grants n k and n k and by the national science foundation under grant ist american institute of physics input cell firing in the bulb eg two cues that overlap by give rise to response patterns that overlap by or more reciprocally later cell firing patterns after ahp increasingly enhance the differences among even very similar patterns so that cues with input overlap give rise to output responses that overlap by less than this difference enhancing response can be measured with respect to its acuity since input overlaps are reduced to near zero response overlaps it enables the structure to distinguish between even very similar cues on the other hand the similarity enhancing response is properly viewed as a partitioning mechanism mapping quite distinct input cues onto nearly identical response patterns or category indicators we therefore use a statistical metric for the information value of categorizations to measure the value of partitionings produced by the pitiform simulation network
the efficient realization using current silicon technology of very large connection networks vlcn with more than a billion connections requires that these networks exhibit a high degree of communication locmity real neural networks exhibit significant locality yet most connectionistneurm network models have little in this paper the connectivity requirements of a simple associative network are analyzed using communication theory several techniques based on communication theory are presented that improve the robustness of the network in the face of sparse local interconnect structures also discussed are some potential problems when information is distributed too widely
many connectionist learning models are implemented using a gradient descent in a least squares error function of the output and teacher signal the present model generalizes in particular back propagation by using minkowski r power metrics for small rs a city block error metric is approximated and for large rs the maximum or suprcmum metric is approached while for r the standard backpropagation model results an implementation of minkowski r back propagation is described and several experiments are done which show that different values of r may be desirable for various purposes different r values may be appropriate for the reduction of the effects of outliers noise modeling the input space with more compact clusters or modeling the statistics of a particular domain more naturally or in a way that may be more perccptually or psychologically meaningful eg speech or vision
we describe a new learning procedure for networks that contain groups of nonlinear units arranged in a closed loop the aim of the learning is to discover codes that allow the activity vectors in a visible group to be represented by activity vectors in a hidden group one way to test whether a code is an accurate representation is to try to reconstruct the visible vector from the hidden vector the difference between the original and the reconstructed visible vectors is called the reconstruction error and the learning procedure aims to minimize this error the learning procedure has two passes on the first pass the original visible vector is passed around the loop and on the second pass an average of the original vector and the reconstructed vector is passed around the loop the learning procedure changes each weight by an amount proportional to the product of the presynaptic activity and the difference in the post synaptic activity on the two passes this procedure is much simpler to implement than methods like back propagation simulations in simple networks show that it usually converges rapidly on a good set of codes and analysis shows that in certain restricted cases it performs gradient descent in the squared reconstruction error
this paper outlines a schema for movement control based on two stages of signal processing the higher stage is a neural network model that treats the cerebellum as an array of adjustable motor pattern generators this network uses sensory input to preset and to trigger elemental pattern generators and to evaluate their performance the actual patterned outputs however are produced by intrinsic circuitry that includes recurrent loops and is thus capable of self sustained activity these patterned outputs are sent as motor commands to local feedback systems called motor servos the latter control the forces and lengths of individual muscles overall control is thus achieved in two stages an adaptive cerebellar network generates an array of feedforward motor commands and a set of local feedback systems translates these commands into actual movements
we describe two expriments in optical neural computing in the first a closed optical feedback loop is used to implement auto associative image recall in the second a perceptron like learning algorithm is implemented with photorefractive holography
previous work on nets with continuous valued inputs led to generative procedures to construct convex decision regions with two layer percepttons one hidden layer and arbitrary decision regions with three layer percepttons two hidden layers here we demonstrate that two layer perceptton classifiers trained with back propagation can form both convex and disjoint decision regions such classifiers are robust train rapidly and provide good performance with simple decision regions when complex decision regions are required however convergence time can be excessively long and performance is often no better than that of k nearest neighbor classifiers three neural net classifiers are presented that provide more rapid training under such situations two use fixed weights in the first one or two layers and are similar to classifiers that estimate probability density functions using histograms a third feature map classifier uses both unsupervised and supervised training it provides good performance with little supervised training in situations such as speech recognition where much unlabeled training data is available the architecture of this classifier can be used to implement a neural net k nearest neighbor classifier
inverse matrix calculation can be considered as an optimization we have demonstrated that this problem can be rapidly solved by highly interconnected simple neuron like analog processors a network for matrix inversion based on the concept of hopfields neural network was designed and implemented with electronic hardware with slight modifications the network is readily applicable to solving a linear simultaneous equation efficiently notable features of this circuit are potential speed due to parallel processing and robustness against variations of device parameters
ictalurid catfish use a highly developed gustatory system to localize track and acquire food from their aquatic environment the neural organization of the gustatory system illustrates well the importance of the four fundamental ingredients representation architecture search and knowledge of an intelligent system in addition the pipelined design of architecture illustrates how a goal directed system effectively utilizes interactive feedback from its environment anatomical analysis of neural networks involved in target tracking indicated that reticular neurons within the medullary region of the brainstem mediate connections between the gustatory sensory inputs and the motor outputs of the spinal cord ele ctrophysiological analysis suggested that these neurons integrate selective spatic temporal patterns of sensory input transduced through a rapidly adapting type peripheral filter responding tonically only to a continuously increasing stimulus concentration the connectivity and response patterns of reticular cells and the nature of the peripheral taste response suggest a unique gustation seeking function of reticulospinal cells which may enable a catfish to continuously track a stimulus source once its directionality has been computed
the information capacity of kanervas sparse distributed memory sdm and hopfield type neural networks is investigated under the approximations used here it is shown that the total information stored in these systems is proportional to the number connections in the network the proportionality constant is the same for the sdm and hopfield type models independent of the particular model or the order of the model the approximations are checked numerically this same analysis can be used to show that the sdm can store sequences of spatiotemporal patterns and the addition of time delayed connections allows the retrieval of context dependent temporal patterns a minor modification of the sdm can be used to store correlated patterns

recently many modifications to the mccullochpitts model have been proposed where both learning and forgetting occur given that the network never saturates ceases to function effectively due to an overload of information the learning updates can continue indefinitely for these networks we need to introduce performance measures in addition to the information capacity to evaluate the different networks we mathematically define quantities such as the plasticity of a network the efficacy of an information vector and the probability of network saturation from these quantities we analytically compare different networks
there is presently great interest in the abilities of neural networks to mimic qualitative reasoningby manipulating neural incodings of symbols less work has been performed on using neural networks to process floating point nurnbers and it is sometimes stated that neural networks are somehow inherently inaccurate and therefore best suited for fuzzyqualitative reasoning nevertheless the potential speed of massively parallel operations make neural net number crunchingan interesting topic to explore in this paper we discuss some of our work in which we demonstrate that for certain applications neural networks can achieve significantly higher numerical accuracy than more conventional techniques in particular prediction of future values of a chaotic time series can be performed with exceptionally high accuracy we analyze how a neural net is able to do this and in the process show that a large class of functions from r r n may be accurately approximated by a backpropagation neural net with just two hiddenlayers the network uses this functional approximation to perform either interpolation signal processing applications or extrapolation symbol processing applications i neural nets therefore use quite familiar methods to perform their tasks the geometrical viewpoint advocated here seems to be a useful approach to analyzing neural network operation and relates neural networks to well studied topics in functional approximation
a new distributed neural information processing model is proposed to explain the response characteristics of the vestibulo ocular system and to reflect more accurately the latest anatomical and neurophysiological data on the vestibular afferent fibers and vestibular nuclei in this model head motion is sensed topographically by hair cells in the semicircular canals hair cell signals are then processed by multiple synapses in the primary afferent neurons which exhibit a continuum of varying dynamics the model is an application of the concept of multilayered neural networks to the description of findings in the bullfrog vestibular nerve and allows us to formulate mathematically the behavior of an assembly of neurons whose physiological characteristics vary according to their anatomical properties

general formulae for mapping optimization problems into systems of ordinary differential equations associated with artificial neural networks are presented a comparison is made to optimization using gradient search methods the performance measure is the settling time from an initial state to a target state a simple analytical example illustrates a situation where dynamical systems representing artificial neural network methods would settle faster than those representing gradientsearch settling time was investigated for a more complicated optimization problem using computer simulations the problem was a simplified version of a problem in medical imaging determining loci of cerebral activity from electromagnetic measurements at the scalp the simulations showed that gradient based systems typically setfled to times faster than systems based on current neural network optimization methods
an information theoretic optimization principle is proposed for the development of each processing stage of a multilayered perceptual network this principle of maximum information preservation states that the signal transformation that is to be realized at each stage is one that maximizes the information that the output signal values from that stage convey about the input signals values to that stage subject to certain constraints and in the presence of processing noise the quantity being maximized is a shannon information rate i provide motivation for this principle and for some simple model cases derive some of its consequences discuss an algorithmic implementation and show how the principle may lead to biologically relevant neural architectural features such as topographic maps map distortions orientation selectivity and extraction of spatial and temporal signal correlations a possible connection between this information theoretic principle and a principle of minimum entropy production in nonequilibrium thermodynamics is suggested
in the synchronous discrete model the average memory capacity of bidirectional associative memories bams is compared with that of hopfield memories by means of a calculation of the percentage of good recall for random bams of dimension x for different numbers of stored vectors the memory capacity is found to be much smaller than the kosko upper bound which is the lesser of the two dimensions of the bam on the average a x bam has about of the capacity of the corresponding hopfield memory with the same number of neurons orthonormal coding of the bam increases the effective storage capacity by only the memory capacity limitations are due to spurious stable states which arise in bams in much the same way as in hopfield memories occurrence of spurious stable states can be avoided by replacing the thresholding in the backlayer of the bam by another nonlinear process here called dominant label selection dls the simplest dls is the winner take all net which gives a fault sensitive memory fault tolerance can be improved by the use of an orthogonal or unitary transformation an optical application of the latter is a fourier transform which is implemented simply by a lens
lecently there has been renewed interest in neural like processing systems evidenced for example in the two volumes parallel distributed processing edited by pumelhart and mcclelland and discussed as parallel distributed systems connectionist models neural nets value passing systems and multiple context systems dissatisfaction with symbolic manipulation paradigms for artificial intelligence seems partly responsible for this attention encouraged by the promise of massively parallel systems implemented in hardware this paper relates simple neural like systems based on multiple context to some other well known formalisms namely production systems k length sequence prediction finite state machines and turing machines and presents earlier sequence prediction results in a new light
in this paper we discuss why special purpose chips are needed for useful implementations of connectionist neural networks in such applications as pattern recognition and classification three chip designs are described a hybrid digitalanalog programmable connection matrix an analog connection matrix with adjustable connection strengths and a digital pipelined best match chip the common feature of the designs is the distribution of arithmetic processing power amongst the data storage to minimize data movement ams distributed computation chip s i jj conventional cpus node complexity no of transistors figure a schematic graph of addressable node complexity and size for conventional computer chips memories can contain millions of very simple nodes each with a very few transistors but with no processing power cpu chips are essentially one very complex node neural network chips are in the distributed computation region where chips contain many simple fixed instruction processors local to data storage after reece and treleaven american institute of physics
we have studied the basins of attraction for fixed point and oscillatory attractors in an electronic analog neural network basin measurement circuitry periodically opens the network feedback loop loads raster scanned initial conditions and examines the resulting attractor plotting the basins for fixed points memories we show that overloading an associative memory network leads to irregular basin shapes the network also includes analog time delay circuitry and we have shown that delay in symmetric networks can introduce basins for oscillatory attractors conditions leading to oscillation are related to the presence of frustration reducing frustration by diluting the connections can stabilize a delay network
we donsider a class of neural networks whose performance can be analyzed and geometrically visualized in a signal space environment alternating projection neural networks apnns perform by alternately projecting between two or more constraint sets criteria for desired and unique convergence are easily established the network can be configured in either a homogeneous or layered form the number of patterns that can be stored in the network is on the order of the number of input and hidden neurons if the output neurons can take on only one of two states then the trained layered apnn can be easily configured to converge in one iteration more generally convergence is at an exponential rate convergence can be improved by the use of sigmoid type nonlinearities network relaxation andor increasing the number of neurons in the hidden layer the manner in which the network responds to data for which it was not specifically trained ie how it generalizes can be directly evaluated analytically

in the present paper we survey mad utilize results from the qualitative theory of large scale interconnected dynamical systems in order to develop a qualitative theory for the hop field model of neural networks in our approach we view such networks as an interconnection of many single neurons our results are phrased in terms of the qualitative properties of the individual neurons and in terms of the properties of the interconnecting structure of the neural networks aspects of neural networks which we address include asymptotic stability exponential stability and instability of an equilibrium estimates of trajectory bounds estimates of the domain of attraction of an asymptotically stable equilibrium and stability of neural networks under structural perturbations
a binary synaptic matrix chip has been developed for electronic neural networks the matrix chip contains a programmable x array of long channel nmosfet binary connection elements implemented in a um bulk cmos process since the neurons are kept offchip the synaptic chip serves as a cascadable building block for a multi chip synaptic network as large as x in size as an alternative to the programmable nmosfet long channel connection elements tailored thin film resistors are deposited in series with fet switches on some cmos test chips to obtain the weak synaptic connections although deposition and patterning of the resistors require additional processing steps they promise substantial savings in silcon area the performance of a synaptic chip in a neuron breadboard system in an associative memory test application is discussed
a bit serial vlsi neural network is described from an initial architecture for a synapse array through to silicon layout and board design the issues surrounding bit serial computation and analogdigital arithmetic are discussed and the parallel development of a hybrid analogdigital neural network is outlined learning and recall capabilities are reported for the bit serial network along with a projected specification for a neuron bit serial board operating at mhz this technique is extended to a synapses network with an update time of ms using a paging technique to time multiplex calculations through the synapse array
a novel network type is introduced which uses unit length vectors for local variables as an example of its applications associative memory nets are defined and their performance analyzed real systems corresponding to such phasor models can be eg neurobiological networks of limit cycle oscillators or optical resonators that have a hologram in their feedback path
we have developed a neural network which consists of cooperatively interconnected grossberg on center off surround subnets and which can be used to optimize a function related to the log likelihood function for decoding convolutional codes or more general fir signal deconvolution problems connections in the network are confined to neighboring subnets and it is representative of the types of networks which lend themselves to vlsi implementation analytical and experimental results for convergence and stability of the network have been found the structure of the network can be used for distributed representation of data items while allowing for fault tolerance and replacement of faulty units
a general method for deriving backpropagation algorithms for networks with recurrent and higher order networks is introduced the propagation of activation in these networks is determined by dissipative differential equations the error signal is backpropagated by integrating an associated differential equation the method is introduced by applying it to the recurrent generalization of the feedforward backpropagation network the method is extended to the case of higher order networks and to a constrained dynamical system for training a content addressable memory the essential feature of the adaptive algorithms is that adaptive equation has a simple outer product form preliminary experiments suggest that learning can occur very rapidly in networks with recurrent connections the continuous formalism makes the new approach more suitable for implementation in vlsi
many optimization models of neural networks need constraints to restrict the space of outputs to a subspace which satisfies external criteria optimizations using energy methods yield forces which act upon the state of the neural network the penalty method in which quadratic energy constraints are added to an existing optimization energy has become popular recently but is not guaranteed to satisfy the constraint conditions when there are other forces on the neural model or when there are multiple constraints in this paper we present the basic differential multiplier method bdmm which satisfies constraints exactly we create forces which gradually apply the constraints over time using neurons that estimate lagrange multipliers the basic differential multiplier method is a differential version of the method of multipliers from numerical analysis we prove that the differential equations locally converge to a constrained minimum examples of applications of the differential method of multipliers include enforcing permutation codewords in the analog decoding problem and enforcing valid tours in the traveling salesman problem

error propagation nets have been shown to be able to learn a variety of tasks in which a static input pattern is mapped onto a static output pattern this paper presents a generalisation of these nets to deal with time varying or dynanic patterns and three possible architectures are explored as an example dynanic nets are applied to the problem of speech coding in which a time sequence of speech data are coded by one net and decoded by another the use of dynamic nets gives a better signal to noise ratio than that achieved using static nets

coarse coded symbol memories have appeared in several neural network symbol processing models in order to determine how these models would scale one must first have some understanding of the mathematics of coarse coded representations we define the general structure of coarse coded symbol memories and derive mathematical relationships among their essential parameters memort size slmbol set size and capacitor the computed capacity of one of the schemes agrees well with actual measurements of the coarse coded working memory of dcps touretzky and hintons distributed connectionist production system

the study of distributed memory systems has produced a number of models which work well in limited domains however until recently the application of such systems to realworld problems has been difficult because of storage limitations and their inherent architectural and for serial simulation computational complexity recent development of memories with unrestricted storage capacity and economical feedforward architectures has opened the way to the application of such systems to complex pattern recognition problems however such problems are sometimes underspecified by the features which describe the environment and thus a significant portion of the pattern environment is often non separable we will review current work on high density memory systems and their network implementations we will discuss a general learning algorithm for such high density memories and review its application to separable point sets finally we will introduce an extension of this method for learning the probability distributions of non separable point sets

we have developed a methodology for manually training autonomous control systems based on artificial neural systems ans in applications where the rule set governing an experts decisions is difficult to formulate ans can be used to extract rules by associating the information an expert receives with the actions he takes properly constructed networks imitate rules of behavior that permits them to function autonomously when they are trained on the spanning set of possible situations this training can be provided manually either under the direct supervision of a system trainer or indirectly using a background mode where the network assimilates training data as the expert performs his day to day tasks to demonstrate these methods we have trained an ans network to drive a vehicle through simulated freeway traffic
the ability to obtain three dimensional structure from visual motion is important for survival of human and non human primates using a parallel processing model the current work explores how the biological visual system might solve this problem and how the neurophysiologist might go about understanding the solution
self organization of multi layered networks can be realized by time sequential organization of successive neural layers lateral inhibition operating in the surround of firing cells in each layer provides for unsupervised capture of excitation patterns presented by the previous layer by presenting patterns of increasing complexity in co ordination with network selforganization higher levels of the hierarchy capture concepts implicit in the pattern set

a general method the tensor product representation is described for the distributed representation of valuevariable bindings the method allows the fully distributed representation of symbolic structures the roles in the structures as well as the fillers for those roles can be arbitrarily non local fully and partially localized special cases reduce to existing cases of connectionist representations of structured data the tensor product representation generalizes these and the few existing examples of fully distributed representations of structures the representation saturates gracefully as larger structures are represented it permits recursive construction of complex representations from simpler ones it respects the independence of the capacities to generate and maintain multiple bindings in parallel it extends naturally to continuous structures and continuous representational patterns it permits values to also serve as variables it enables analysis of the interference of symbolic structures stored in associative memories and it leads to characterization of optimal distributed representations of roles and a recirculation algorithm for learning them
the aim of this paper is to explore the spatial organization of neural networks under markovian assumptions in what concerns the behaviour of individual cells and the interconnection mechanism spaceorganizational properties of neural nets are very relevant in image modeling and pattern analysis where spatial computations on stochastic two dimensional image fields are involved as a first approach we develop a random neural network model based upon simple probabilistic assumptions whose organization is studied by means of discrete event simulation we then investigate the possibility of approximating the random networks behaviour by using an analytical approach originating from the theory of general product form queueing networks the neural network is described by an open network of nodes in which customers moving from node to node represent stimulations and connections between nodes are expressed in terms of suitably selected routing probabilities we obtain the solution of the model under different disciplines affecting the time spent by a stimulation at each node visited results concerning the distribution of excitation in the network as a function of network topology and external stimulation arrival pattern are compared with measures obtained from the simulation and validate the approach followed
recognizing patterns with temporal context is important for such tasks as speech recognition motion detection and signature verification we propose an architecture in which time serves as its own representation and temporal context is encoded in the state of the nodes we contrast this with the approach of replicating portions of the architecture to represent time as one example of these ideas we demonstrate an architecture with capacitive inputs serving as temporal feature detectors in an otherwise standard back propagation model experiments involving motion detection and word discrimination serve to illustrate novel features of the system finally we discuss possible extensions of the architecture
we propose a new scheme to construct neural networks to classify patterns the new scheme has several novel features we focus attention on the important attributes of patterns in ranking order extract the most important ones first and the less important ones later in training we use the information as a measure instead of the error function a multi perceptron like architecture is formed auomatically decision is made according to the tree structure of learned attributes this new scheme is expected to self organize and perform well in large scale problems american institute of physics i
an efficient method of self organizing associative databases is proposed together with applications to robot eyesight systems the proposed databases cn associate ny input with some output in the first half prt of discussion n algorithm of self organization is proposed from an aspect of hardware it produces a new style of neural network in the latter half part an pplicability to handwritten letter recognition and that to an autonomous mobile robot system are demonstrated
networks of simple analog processors having neuron like properties have been employed to compute good solutions to a variety of optimization problems this paper presents a neural net solution to a resource allocation problem that arises in providing local access to the backbone of a wide area communication network the problem is described in terms of an energy function that can be mapped onto an analog computational network simulation results characterizing the performance of the neural computation are also presented
an increasing number of prfoundly deaf patients suffering from sensorineural deafness are using cooblear implants as prostheses after the implant sound can be detected through the electrical stimulation of the remaining peripheral auditory nervous system although great progress has been achieved in this area no useful speech recognition has been attained with either single or multiple channel cooblear implants coding evidence suggests that it is necessary for any implant which would effectively couple with the natural speech perception system to simulate the temporal dispersion and other phenomena found in the natural receptors and currently not implemented in any cooblear implants to this end it is presented here a computational model using artificial neural networks ann to incorporate the natural phenomena in the artificial cochlear the ann model presents a series of advantages to the implementation of such systems first the hardware requirements with constraints on power size and processing speeds can be taken into account together with the development of the underlining software before the actual neural structures are totally defined second the ann model since it is an abstraction of natural neurons carries the necessary ingredients and is a close mapping for implementing the necessary functions third some of the processing like sorting and majority functions could be implemented more efficiently requiring only local decisions fourth the ann model allows function modifications through parametric modification no software recoding which permits a variety of fine tuning experiments with the opinion of the patients to be conceived some of those will permit the user some freedom in system modification at real time allowing finer and more subjective adjustments to fit differences on the condition and operation of individuals remaining peripheral auditory system


a family of neuromorphic networks specifically designed for communications and optical signal processing applications is presented the information is encoded utilizing sparse optical orthogonal code sequences on the basis of unipolar binary signals the generalized synaptic connectivity matrix is also unipolar and clipped to binary values in addition to high capacity associative memory the resulting neural networks can be used to implement general functions such as code filtering code mapping code joining code shifting and code projecting
the paper presents an artificial neural network concept the synchronizable oscillator networks where the instants of individual firings in the form of point processes constitute the only form of information transmitted between joining neurons this type of communication contrasts with that which is assumed in most other models which typically are continuous or discrete value passing networks limiting the messages received by each processing unit to time markers that signal the firing of other units presents significant implementation advantages in our model neurons fire spontaneously and regularly in the absence of perturbation when interaction is present the scheduled firings are advanced or delayed by the firing of neighboring neurons networks of such neurons become global oscillators which exhibit multiple synchronizing attractors from arbitrary initial states energy minimization learning procedures can make the network converge to oscillatory modes that satisfy multi dimensional constraints such networks can directly represent routing and scheduling problems that consist of ordering sequences of events
this paper describes an approach to dimensional object recognition complex log conformal mapping is combined with a distributed associative memory to create a system which recognizes objects regardless of changes in rotation or scale recalled information from the memorized database is used to classify an object reconstruct the memorized version of the object and estimate the magnitude of changes in scale or rotation the system response is resistant to moderate amounts of noise and occlusion several experiments using real gray scale images are presented to show the feasibility of our approach
this paper presents a model of nondeterministic adaptive automata that are constructed from simpler nondeterministic adaptive information processing elements the first half of the paper describes the model the second half discusses some of its significant adaptive properties using computer simulation examples chief among these properties is that network aggregates of the model elements can adapt appropriately when a single reinforcement channel provides the same positive or negative reinforcement signal to all adaptive elements of the network at the same time this holds for multiple input multiple output multiple layered combinational and sequential networks it also holds when some network elements are hidden in that their outputs are not directly seen by the external environment


the potential of adaptive networks to learn categorization rules and to model human performance is studied by comparing how natural and artificial systems respond to new inputs ie how they generalize like humans networks can learn a deterministic categorization task by a variety of alternative individual solutions an analysis of the constraints imposed by using networks with the minimal number of hidden units shows that this minimal configuration conslxaint is not sufficient to explain and predict human performance only a few solutions were found to be shared by both humans and minimal adaptive networks a further analysis of human and network generalizations indicates that initial conditions may provide important constraints on generalization a new technique which we call reversed learning is described for finding appropriate initial conditions
we propose an optimality principle for training an unsupervised feedforward neural network based upon maximal ability to reconstruct the input data from the network outputs we describe an algorithm which can be used to train either linear or nonlinear networks with certain types of nonlinearity examples of applications to the problems of image coding feature detection and analysis of randomdot stereograms are presented
alvis is a reinforcement based connectionist architecture that learns associative maps in continuous multidimensional environments the discovered locations of positive and negative reinforcements are recorded in do be and dont be subnetworks respectively the outputs of the subnetworks relevant to the current goal are combined and compared with the current location to produce an error vector this vector is backpropagated through a motor perceptual mapping network to produce an action vector that leads the system towards do be locations and away from dont be locations alvis is demonstrated with a simulated robot posed a target seeking task
a class of fast supervised learning algorithms is presented they use local representations hashing aild multiple scales of resolution to approximate functions which are piece wise continuous inspired by albuss cmac model the algorithms learn orders of magnitude more rapidly than typical implementations of back propagation while often achieving comparable qualities of generalization furthermore unlike most traditional function approximation methods the algorithms are well suited for use in real time adaptive signal processing unlike simpler adaptive systems such as linear predictive coding the adaptive linear combinet and the kalman filter the new algorithms are capable of efficiently capturing the structure of complicated non linear systems as an illustration the algorithm is applied to the prediction of a chaotic timeseries
parallelizable optimization techniques are applied to the problem of learning in feedforward neural networks in addition to having superior convergence properties optimization techniques such as the polakribiere method are also significantly more efficient than the backpropagation algorithm these results are based on experiments performed on small boolean learning problems and the noisy real valued learning problem of hand written character recognition
classifier systems are machine learning systems incorporating a genetic algorithm as the learning mechanism although they respond to inputs that neural networks can respond to their intemal structure representation formalisms and learning mechanisms differ markedly from those employed by neural network researchers in the same sorts of domains as a result one might conclude that these two types of machine learning formalisms are intrinsically different this is one of two papers that taken together prove instead that classifier systems and neural networks are equivalent in this paper half of the equivalence is demonstrated through the description of a transformation procedure that will map classifier systems into neural networks that are isomorphic in behavior several alterations on the commonly used paradigms employed by neural network researchers are required in order to make the transformation work these alterations are noted and their appropriateness is discussed the paper concludes with a discussion of the practical import of these results and with comments on their extensibility
this work introduces a new method called self organizing neural network sonn algorithm and demonstrates its use in a system identification task the algorithm constructs the network chooses the neuron functions and adjusts the weights it is compared to the back propagation algorithm in the identification of the chaotic time series the results shows that sonn constructs a simpler more accurate model requiring less training data and epochs the algorithm can be applied and generalized to appilications as a classifier i

we introduce a learning algorithm for multilayer neural networks composed of binary linear threshold elements whereas existing algorithms reduce the learning process to minimizing a cost function over the weights our method treats the internal representations as the fundamental entities to be determined once a correct set of internal representations is arrived at the weights are found by the local ahd biologically plausible perceptton learning rule plr we tested our learning algorithm on four problems adjacency symmetry parity and combined symmetry parity i
we address the question of when a network can be expected to generalize from m random training examples chosen from some arbitrary probability distribution assuming that future test examples are drawn from the same distribution among our results are the following bounds on appropriate sample rs network size assume e we show that if m o w v ogt random examples can be loaded on a feedforward network of linear threshold functions with n nodes and w weights so that at least a fraction of the examples are correctly classified then one has confidence approaching certainty that the network will correctly classify a fraction i e of future test examples drawn from the same distribution conversely for fully connected feedforward nets with one hidden layer any learning algorithm using fewer than f random training examples will for some distributions of examples consistent with an appropriate weight choice fail at least some fixed fraction of the time to find a weight choice that will correctly classify more than a e fraction of the future test examples

a new training paradigm called the coznparison paradigm is introduced for tasks in which a network must learn to choose a preferred pattern from a set of n alternatives based on examples of human expert preferences in this paradigm the input to the network consists of two of the n alternatives and the trained outtrot is the experts judgement of which iattcrn is better this paradigm is applied to the learning of backgammon a difflcult board game in which the expert selects a move from a set of legal moves with comparison training much higher levels of performance can be achieved wih networks that are much smaller and with coding schemes that arc much simpler and easier to understand furthermore it is possible to set up the network so that it always produces consistent rank orderings
this paper proposes a means of using the knowledge in a network to determine the functionality or relevance of individual units both for the purpose of understanding the networks behavior and improving its performance the basic idea is to iteratively train the network to a certain performance criterion compute a measure of relevance that identifies which input or hidden units are most critical to performance and automatically trim the least relevant units this skeletonization technique can be used to simplify networks by eliminating units that convey redundant information to improve learning performance by first learning with spare hidden units and then trimming the unnecessary ones away thereby constraining generalization and to understand the behavior of networks in terms of minimal rules
the concept of the stochastic boltzmann machine bm is attractive for decision making and pattern classification purposes since the probability of attaining the network states is a function of the network energy hence the probability of attaining particular energy minima may be associated with the probabilities of making certain decisions or classifications however because of ill stochastic nature the complexity of the bm is fairly high and therefore such networks are not very likely to be used in practice in this paper we suggest a way to alleviate this drawback by converting the stoelmstic bm into a deterministic network which we call the boltzmann perceplxon network bpn the bpn is functionally equivalent to the bm but has a feed forward structure and low complexity no annealing is required the conditions under which such a conversion is feasible are given a learning algorithm for the bpn based on the conjugate gradient method is also provided which is somewhat akin to the backpropagation algorithm
a nonlinearity is required before matched filtering in minimum error receivers when additive noise is present which is impulsive and highly non gaussian experiments were performed to determine whether the correct clipping nonlinearity could be provided by a single input singleoutput multi layer perceptron trained with back propagation it was found that a multi layer perceptron with one input and output node nodes in the first hidden layer and nodes in the second hidden layer could be trained to provide a clipping nonlinearity with fewer than presentations of noiseless and corrupted waveform samples a network trained at a relatively high signal to noise sn ratio and then used as a front end for a linear matched filter detector greatly reduced the probability of error the clipping nonlinearity formed by this network was similar to that used in current receivers designed for impulsive noise and provided similar substantial improvements in performance
a large fraction of recent work in artificial neural nets uses multilayer perceptrons trained with the back propagation algorithm described by rumelhart et al this algorithm converges slowly for large or complex problems such as speech recognition where thousands of iterations may be needed for convergence even with small data sets in this paper we show that training multilayer perceptrons is an identification problem for a nonlinear dynamic system which can be solved using the extended kalman algorithm although computationally complex the kalman algorithm usually converges in a few iterations we describe the algorithm and compare it with back propagation using twodimensional examples

this paper provides a systematic analysis of the recurrent backpropagation rbp algorithm introducing a number of new results the main limitation of the rbp algorithm is that it assumes the convergence of the network to a stable fixed point in order to backpropagate the error signals we show by experiment and eigenvalue analysis that this condition can be violated and that chaotic behavior can be avoided next we examine the advantages of rbp over the standard backpropagation algorithm rbp is sitown to build stable fixed points corresponding to the input patterns this makes it an appropriate tool for content addressable memories one to many function learning and inverse problems
the issues of scaling and generalization have emerged as key issues in current studies of supervised learning from examples in neural networks questions such as how many training patterns and training cycles are needed for a problem of a given size and difficulty how to represent the intnt and how to choose useful training exemplars are of considerable theoretical and practical importance several intuitive rules of thumb have been obtained from empirical studies but as ye there are few rigorous results in this paper we summarize a study of generalization in the simplest possible case perceptron networks learning linearly separable functions the task chosen was the majority function ie return a if a majority of the input units are on a predicate with a number of useful properties we find that many aspects ofgeneraiization in multilayer networks learning large difficult tasks are reproduced in this simple domain in which concrete numerical results and even some analytic understanding can be achieved
an improved learning paradigm that offers a significant reduction in computation time during the supervised lewnlng phase is described it is based on extending the role that the neuron plays in artificial neural systems prior work has regarded the neuron as a strictly passive non linear processing element and the synapse on the other hand as the primary source of information processing and knowledge retention in this work the role of the neuron is extended insofar as mlowing its pararaeters to adaptively participate in the learning phase the temperature of the sigmoid function is an exaraple of such a parameter during learning both the synaptic interconnection weights w and the neuronal temperatures tm are optimized so as to capture the knowledge contained within the training set the method allows each neuron to possess and update its own characteristic local temperature this algorithm has been applied to logic type of problems such as the xor or parity problem resulting in a significant decrease in the required number of training cycles
rumelhart has proposed a method for choosing minimal or simple representations during learning in back propagation networks this approach can be used to a dynamically select the number of hidden units co construct a representation that is appropriate for the problem and c thus improve the generalization ability of back propagation networks the method rumelhart suggests involves adding penalty terms to the usual error function in this paper we introduce rumelharts minimal networks idea and compare two possible biases on the weight search space these biases are compared in both simple counting problems and a speech recognition problem in general the constrained search does seem to minimize the number of hidden units required with an expected increase in local minima
this paper addresses the problem of determining the weights for a set of linear filters model cells so as to maximize the ensemble averaged information that the cells output values jointly convey about their input values given the statistical properties of the ensemble of input vectors the quantity that is maximized is the shannon information rate or equivalently the average mutual information between input and output several models for the role of processing noise are analyzed and the biological motivation for considering them is described for simple models in which nearby input signal values in space or time are correlated the cells resulting from this optimization process include center surround cells and cells sensitive to temporal variations in input signal
a number of learning models have recently been proposed which involve calculations of temporal differences or derivatives in continuous time models these models like most adaptive network models are formulated in terms of frequency or activation a useful abstraction of neuronal firing rates to more precisely evaluate the implications of a neuronal model it may be preferable to develop a model which transmits discrete pulse coded information we point out that many functions and properties of neuronal processing and learning may depend in subtle ways on the pulse coded nature of the information coding and transmission properties of neuron systems when compared to formulations in terms of activation computing with temporal derivatives or differences as proposed by kosko klopf and sutton is both more stable and easier when reformulated for a more neuronally realistic pulse coded system in reformulating these models in terms of pulse coding our motivation has been to enable us to draw further parallels and connections between real time behavioral models of learning and biological circuit models of the substrates underlying learning and memory

in this paper we show that neural networks for speech recognition can be constructed in a modular fashion by exploiting the hidden structure of previously trained phonetic subcategory networks the performance of resulting larger phonetic nets was found to be as good as the performance of the subcomponent nets by themselves this approach avoids the excessive learning times that would be necessary to ixain larger networks and allows for incremental learning large time delay neural networks constructed incrementally by applying these modular training techniques achieved a recognition performance of for all consonants
preliminary results on speaker independant speech recognition are reported a method that combines expertise on neural networks with expertise on speech recognition is used to build the recognition systems for transient sounds eventdriven property extractors with variable resolution in the time and frequency domains are used for sonorant speech a model of the human auditory system is preferred to fft as a front end module
we propose a new neural network model and its learning algorithm the proposed neural network consists of four layers input hidden output and final output layers the hidden and output layers are multiple using the proposed siclspread pattern information and cooperative learning algorithm it is possible to learn analog data accurately and to obtain smooth outputs using this neural network we have developed a speech production system consisting of a phonemic symbol production subsystem and a speech parameter production subsystem we have succeeded in producing natural speech waves with high accuracy
syren is a connectionist model that uses temporal information in a speech signal for syllable recognition it classifies the rates and directions of formant center transitions and uses an adaptive method to associate transition events with each syllable the system uses explicit spatial temporal representations through delay lines syren uses implicit parametric temporal representations in formant transition classification through node activation onset decay and transition delays in sub networks analogous to visual motion detector cells syren recognizes of six repetitions of consonant vowel syllables when tested on unseen data and recognizes of its training syllables
the space environment laboratory in boulder has collaborated with the university of colorado to construct a small expert system for solar flare forecasting called theo it performed as well as a skilled human forecaster we have constructed theonet a three layer back propagation connectionist network that learns to forecast flares as well as theo does theonets success suggests that a connectionist network can perform the task of knowledge engineering automatically a study of the internal representations constructed by the network may give insights to the microstructure of reasoning processes in the human brain
we discuss in this paper architectures for executing probabilistic rule bases in a parallel manner using as a theoretical basis recently introduced information theoretic models we will begin by describing our non neural learning algorithm and theory of quantitative rule modelling followed by a discussion on the exact nature of two particular models finally we work through an example of our approach going from database to rules to inference network and compare the networks performance with the theoretical limits for specific problems

the application of neural networks to the demodulation of spread spectrum signals in a multiple access environment is considered this study is motivated in large part by the fact that in a multiuser system the conventional matched filter receiver suffers severe performance degradation as the relative powers of the interfering signals become large the near far problem furthermore the optimum receiver which alleviates the near far problem is too complex to be of practical use receivers based on multi layer percepttons are considered as a simple and robust alternative to the optimum solution the optimum receiver is used to benchmark the performance of the neural net receiver in particular it is proven to be instrumental in identifying the decision regions of the neural networks the back propagation algorithm and a modified version of it are used to train the neural net an importance sampling technique is introduced to reduce the number of simulations necessary to evaluate the performance of neural nets in all examples considered the proposed neural net receiver significantly outperforms the conventional receiver
this study evaluates the performance of the multilayer perceptron and the frequency sensitive competitive learning network in identifying five commercial aircraft from radar backscatter measurements the performance of the neural network classifiers is compared with that of the nearest neighbor and maximum likelihood classifiers our results indicate that for this problem the neural network classifiers are relatively insensitive to changes in the network topology and to the noise level in the training data while for this problem the traditional algorithms outperform these simple neural classifiers we feel that neural networks show the potential for improved performance
a new class of neural network aimed at early visual processing is described we call it a neural analog diffusion enhancement layer or nadel the network consists of two levels which are coupled through feedfoward and shunted feedback connections the lower level is a two dimensional diffusion map which accepts visual features as input and spreads activity over larger scales as a function of time the upper layer is periodically fed the activity from the diffusion layer and locates local maxima in it an extreme form of contrast enhancement using a network of local comparators these local maxima are fed back to the diffusion layer using an on centeroff surround shunting anatomy the maxima are also available as output of the network the network dynamics serves to cluster features on multiple scales as a function of time and can be used in a variety of early visual processing tasks such as extraction of comers and high curvature points along edge contoms line end detection gap filling in contoms generation of fixation points perceptual grouping on multiple scales correspondence and path impletion in long range apparent motion and building d shape representations that are invariant to location orientation scale and small deformation on the visual field
we propose a parallel network of simple processors to find color boundaries irrespective of spatial changes in illumination and to spread uniform colors within marked regions
alvinn autonomous land vehicle in a neural network is a layer back propagation network designed for the task of road following curten fly alvinn takes images from a camera and a laser range finder as input and produces as output the direction the vehicle should travel in order to follow the road training has been conducted using simulated road images successful tests on the carnegie mellon autonomous navigation test vehicle indicate that the network can effectively follow real roads under certain field conditions the tepresentation developed to perform the task differs dramatically when the network is trained under various conditions suggesting the possibility of a novel adaptive autonomous navigation system capable of tailoring its processing to the conditions at hand
currently the most complex spacecraft attitude determination and control tasks are ultimately governed by ground based systems and personnel conventional on board systems face severe computational bottlenecks introduced by serial microprocessors operating on inherently parallel problems new computer architectures based on the anatomy of the human brain seem to promise high speed and fault tolerant solutions to the limitations of serial processing this paper discusses the latest applications of artificial neural networks to the problem of star pattern recognition for spacecraft attitude determination

a neural network is applied to the problem of recognizing kanji characters using a b ac k propagation network learning algorithm a threelayered feed forward network is trained to recognize similar handwritten kanji characters in addition two new methods are utilized to make training effective the recognition accuracy was higher than that of conventional methods an analysis of connection weights showed that trained networks can discern the hierarchical structure of kanji characters this strategy of trained networks makes high recognition accuracy possible our results suggest that neural networks are very effective for kanji character recognition
a pool of handwritten signatures is used to train a neural network for the task of deciding whether or not a given signature is a forgery the network is a feedforward net with a binary image as input there is a hidden layer with a single unit output layer the weights are adjusted according to the backpropagation algorithm the signatures are entered into a c software program through the use of a datacopy electronic digitizing camera the binary signatures are normalized and centered the performance is examined as a function of the training set and network structure the best scores are on the order of true signature rejection with false signature acceptance
murphy is a vision based kinematic controller and path planner based on a connectionist architecture and implemented with a video camera and rhino xr series robot arm imitative of the layout of sensory and motor maps in cerebral cortex murphys internal representations consist of four coarse coded populations of simple units representing both static and dynamic aspects of the sensory motor environment in previously reported work murphy first learned a direct kinematic model of his camera arm system during a period of extended practice and then used this mental model to heuristically guide his hand to unobstructed visual targets murphy has since been extended in two ways first he now learns the inverse differential kinematics of his arm in addition to ordinary direct kinematics which allows him to push his hand directly towards a visual target without the need for search secondly he now deals with the much more difficult problem of reaching in the presence of obstacles
computing the inverse dynamics of a robot arm is an active area of research in the control literature we hope to learn the inverse dynamics by training a neural network on the measured response of a physical ann the input to the network is a temporal window of measured positions output is a vector of torques we train the network on data measured from the first two joints of the cmu direct drive arm ii as it moves through a randomly generated sample of pick and place trajectories we then test generalization with a new trajectory and compare its output with the torque measured at the physical arm the network is shown to generalize with a root mean square errorstandard deviation rmss of we interpreted the weights of the network in terms of the velocity and acceleration filters used in conventional control theory
the barn owl has fused visualauditorymotor representations of space in its midbrain which are used to orient the head so that visual or auditory stimuli are centered in the visual field of view we present models and computer simulations of these structures which address various problems includi g the construction of a map of space from auditory sensory infolation and the problem of driving the motor system from these maps we compare the results with biological data
we have previously developed a simple mathematical model for formation of ocular dominance columns in msmmalinn visual cortex the model provides a common framework in which a variety of activity dependent biological machnnisms can be studied analytic and computational results together now reveal the following if inputs specific to each eye are locally correlated in their firing and are not anticorrelated within an arbor radius monocular cells will robustly form and be organized by intra cortical interactions into column broader correlations within each eye or anti correlations between the eyes create a more purely monocular cortex positive correlation over an arbor radius yields an almost perfectly monocular cortex most features of the model can be understood analytically through decomposition into eigenfunctions and linear stability analysis this allows prediction of the widths of the cohmns and other features from measurable biological parameters

in modeling studies of memory based on neural networks both the selective enhancement and depression of synaptic strengths are required for efficient storage of information sejnowski a b kohonen bienenstock et al sejnowski and tesauro we have tested this assumption in the hippocampus a cortical structure of the brain that is involved in long term memory a brief high frequency activation of excitatory synapses in the hippocampus produces an increase in synaptic strength known as long term potentiation or ltp bliss and lomo that can last for many days ltp is known to be hebbian since it requires the simultaneous release of neurotransmitter from presynaptic terminals coupled with postsynaptic alepolarization kelso et al mannow and miller gustaffson et al however a mechanism for the persistent reduction of synaptic strength that could balance ltp has not yet been demonstrated we studied the associative interactions between separate inputs onto the same dendritic trees of hippocampal pyramidal cells of field ca and found that a low frequency input which by itself does not persistently change synaptic strength can either increase associative ltp or decrease in strength associative longterm depression or ltd depending upon whether it is positively or negatively correlated in ome with a second high frequency bursting input ltp of synaptic strength is hebbian and ltd is antihebbian since it is elicited by pairing presynaptic firing with postsynaptic hyperpolarization sufficient to block postsynaptic activity thus associative ltp and associative ltd are capable of storing information contained in the covariance between separate converging hippocampal inputs present address departments of neuroscience and neurology albert einstein college of medicine pelham parkway south bronx ny usa present address computational neurobiology laboratory the salk institute po box san diego ca usa storing covariance by synaptic strengths in the hippocampus
the olfactory bulb of mammals aids in the discrimination of odors a mathematical model based on the bulbar anatomy and electrophysiology is described simulations produce a hz modulated activity coherent across the bulb mimicing the observed field potentials the decision states for the odor information here can be thought of as stable cycles rather than point stable states typical of simpler neuro computing models analysis and simulations show that a group of coupled non linear oscillators are responsible for the oscillatory activities determined by the odor input and that the bulb with appropriate inputs from higher centers can enhance or suppress the sensitivity to particular odors the model provides a framework in which to understand the transform between odor input and the bulbar output to olfactory cortex
we present a new hypothesis that the cerebellum plays a key role in actively controlling the acquisition of sensory information by the nervous system in this paper we explore this idea by examining the function of a simple cerebellar related behavior the vestibulo ocular reflex or vor in which eye movements are generated to minimize image slip on the retina during rapid head movements considering this system from the point of view of statistical estimation theory our results suggest that the transfer function of the vor often regarded as a static or slowly modifiable feature of the system should actually be continuously and rapidly changed during head movements we further suggest that these changes are under the direct control of the cerebellar cortex and propose experiments to test this hypothesis

the weakly electric fish gnathonemus petersii explores its environment by generating pulsed electric fields and detecting small perturbations in the fields resulting from nearby objects accordingly the fish detects and discriminates objects on the basis of a sequence of electric images whose temporal and spatial properties depend on the timing of the fishs electric organ discharge and its body position relative to objects in its environment we are interested in investigating how these fksh utilize timing and body position during exploration to aid in object discrimination we have developed a fmite element simulation of the fishs self generated electric fields so as to reconstruct the electrosensory consequences of body position and electric organ discharge timing in the fish this paper describes this finite element simulation system and presents preliminary electric field measurements which are being used to tune the simulation
tteiligenberg recently proposed a model to explain how sensory maps could enhance resolution through orderly arrangement of broadly tuned receptors we have extended this model to the general case of polynomial weighting schemes and proved that the response function is also a polynomial of the same order we further demonstrated that the hermitian polynomials are eigenfunctions of the system finally we suggested a biologically plausible mechanism for sensory representation of external stimuli with resolution far exceeding the inter receptor separation

a new learning algorithm for the storage of static and periodic attractors in biologically inspired recurrent analog neural networks is introduced for a network of n nodes n stati c or n periodic attractors may be stored the aigorithm ailows programming of the network vector fieid independent of the patterns to be stored stability of patterns basin geometry and rates of convergence may be controlled for orthonormal patterns the legming operation reduces to a kind of periodic outer product ruie that allows iocal additive commutative incremental learning standing or traveiing wave cycles may be stored to mimic the kind of osciilating spatial patterns that appear in the neural activity of the olfactory bulb and prepyriform cortex during inspiration and suffice in the buib to predict the pattern recognition behavior of rabbits in ciassical conditioning experiments these attractors arise during simuiated inspiration through a muitipie hopf bifurcation which can act as a criticai decision point for their selection by a very smaii input pattern
the primate visual system learns to recognize the true direction of pattern motion using local detectors only capable of detecting the component of motion perpendicular to the orientation of the moving edge a multilayer feedforward network model similar to linskers model was presented with input patterns each consisting of randomly oriented contours moving in a particular direction input layer units are granted component direction and speed tuning curves similar to those recorded from neurons in primate visual area v that project to area mt the network is trained on many such patterns until most weights saturate a proportion of the units in the second layer solve the aperture problem eg show the same direction tuning curve peak to plaids as to gratings resembling pattern direction selective neurons which first appear in area mt
we analyze a mathematical model for retinal directionally selective cells based on recent electrophysiological data and show that its computation of motion direction is robust against noise and speed
we have developed a graphically oriented general purpose simulation system to facilitate the modeling of neural networks the simulator is implemented under unix and x windows and is designed to support simulations at many levels of detail specifically it is intended for use in both applied network modeling and in the simulation of detailed realistic biologicallybased models examples of current models developed under this system include mammalian olfactory bulb and cortex invertebrate central pattern generators as well as more abstract connectionist simulations
we consider a layer node n input neural network whose nodes compute linear threshold functions of their inputs we show that it is np complete to decide whether there exist weights and thresholds for the three nodes of this network so that it will produce output consistent with a given set of training examples we extend the result to other simple networks this result suggests that those looking for perfect training algorithms cannot escape inherent computational difficulties just by considering only simple or very regular networks it also suggests the importance given a training problem of finding an appropriate network and input encoding for that problem it is left as an open problem to extend our result to nodes with non linear functions such as sigmoids
hidden markov models are widely used for automatic speech recognition they inherently incorporate the sequential character of the speech signal and are statistically trained however the a priori choice of the model topology limits their flexibility another drawback of these models is their weak discriminating power multilayer perceptrons are now promising tools in the connectionist approach for classification problems and have already been successfully tested on speech recognition problems however the sequential nature of the speech signal remains difficult to handle in that kind of machine in this paper a discriminant hidden markov model is defined and it is shown how a particular multilayer perceptron with contextual and extra feedback input units can be considered as a general form of such markov models
the boltzmann machine has been introduced as a means to perform global optimization for multimodal objective functions using the principles of simulated annealing in this paper we consider its utility as a spurious free content addressable memory and provide bounds on its performance in this context we show how to exploit the machines ability to escape local minima in order to use it at a constant temperature for unambiguous associative pattern retrieval in noisy environments an association rule which creates a sphere of influence around each stored pattern is used along with the machines dynamics to match the machines noisy input with one of the pre stored patterns spurious fixed points whose ragions of attraction are not recognized by the rule are skipped due to the machines finite probability to escape from any state the results apply to the boltzmann machine and to the asynchronous net of binary threshold elements rlopfield model they provide the network designer with worst case and best case bounds for the networks performance and allow polynomial time tradeoff studies of design parameters i

i will describe my recent results on the automatic development of fixedwidth recursive distributed representations of variable sized hierarchal data structures one implication of this work is that certain types of ai style data structures can now be represented in fixed width analog vectors simple inferences can be performed using the type of pattern associations that neural networks excel at another implication arises from noting that these representations become self similar in the limit once this door to chaos is opened many interesting new questions about the representational basis of intelligence emerge and can and will be discussed
the parsing and learning systempals is a massively parallel self tuning context free parser it is capable of parsing sentences of unbounded length mainly due to its parse tree representation scheme the system is capable of improving its parsing performance through the presentation of training examples

one attempt at explaining human inferencing is that of spreading activation particularly in the structured connectionist paradigm this has resulted in the building of systems with semantically nameable nodes which perform inferencing by examining the patterns of activation spread in this paper we demonstrate that simple struct ured network inferencing can be performed by passing activation over the weights learned by a distributed algorit hm thus an account is provided which explains a wellbehaved relationship between structured and distributed connectionist approaches

a time delay in the response of the neurons in a network can induce sustained oscillation and chaos we present a stability criterion based on local stability analysis to prevent sustained oscillation in symmetric delay networks and show an example of chaotic dynamics in a non symmetric delay network i
research in artificial neural networks has generally emphasized homogeneous architectures in conuast the nervous systems of natural animals exhibit gmat heterogeneity in both their elements and patterns of interconnection this heterogeneity is crucial to the flexible generation of behavior which is essential for survival in a complex dynamic environment it may also provide powerful insights into the design of artificial neural networks in this paper we describe a heterogeneous neural network for controlling the walking of a simulated insect this controller is inspired by the neuroethological and neurobiological literature on insect locomotion it exhibits a variety of statically stable gaits at different speeds simply by varying the tonic activity of a single cell it can also adapt to perturbations as a natural consequence of its design

a new optimization strategy mean field annealing is presented its application to map restoration of noisy range images is derived and experimentally verified
this research involves a method for finding global maxima in constraint safi sfacfion networks it is an anneahng process but unlike most others reqmres no annealing schedule temperature is instead determined locally by units at each update and thus all processing is done at the unit level there are two major practical benefits to processing this way processing can continue in bad areas of the network while good areas remain stable and processing continues in the bad areas as long as the constraints remain poorly satisfied ie it does not stop after some predetermined number of cycles as a result this method not only avoids the kludge of requiring an externally determined annealing schedule but it also finds global maxima more quickly and consistently than externally scheduled systems a comparison to the boltzmann machine ackley et al is made finally implementation of this method is computationally trivial

we introduce an optimization approach for solving problems in computer vision that involve multiple levels of abstraction our objective functions include compositional and specialization hierarchies we cast vision problems as inexact graph matching problems formulate graph matching in terms of constrained optimization and use analog neural networks to perform the optimization the method is applicable to perceptual grouping and model matching preliminary experimental results are shown i
dcps the distributed connectionist production system is a neural network with complex dynamical properties visualizing the energy landscapes of some of its component modules leads to a better intuitive understanding of the model and suggests ways in which its dynamics can be controlled in order to improve performance on difficult cases


we describe an adaptive network tin that learns the transition function of a sequential system from observations of its behavior it integrates two subnets tin winter ryan and turner and tin tin constructs state representations from examples of system behavior and its dynamics are the main topics of the paper tin abstracts transition functions from noisy state representations and environmental data during training while in operation it produces sequences of transitions in response to variations in input dynamics of both nets are based on the adaptive resonance theory of carpenter and grossberg we give results from an experiment in which tin learned the behavior of a system that recognizes strings with an even number of ls
edwin lewis dept elect eng uc berkeley we present a simplified model of the micromechanics of the human cochlea realized with electrical elements simulation of the model shows that it retains four signal processing features whose importance we argue on the basis of engineering logic and evolutionary evidence furthermore just as the cochlea does the model achieves massively parallel signal processing in a stmcturally economic way by means of shared elements by extracting what we believe are the five essential features of the cochlea we hope to design a useful front end filter to process acoustic images and to obtain a better understanding of the auditory system
we describe pulse stream firing integrated circuits that implement asynchronous analog neural networks synaptic weights are stored dynamically and weighting uses time division of the neural pulses from a signalling neuron to a receiving neuron mos transistors in their on state act as variable resistors to control a capacitive discharge and time division is thus achieved by a small synapse circuit cell the vlsi chip set design uses xm cmos technology
this paper describes a cmos artificial neuron the circuit is directly derived from the voltage gated channel model of neural membrane has low power dissipation and small layout geometry the principal motivations behind this work include a desire for high performance more accurate neuron emulation and the need for higher density in practical neural network implementations
reconstructing a surface from sparse sensory data is a well known problem in computer vision this paper describes an experimental analog vlsi chip for smooth surface interpolation from sparse depth data an eight node d network was designed in m cmos and successfully tested the network minimizes a second order or thinplate energy of the surface the circuit directly implements the coupled depthslope model of surface reconstruction harris in addition this chip can provide gaussian like smoothing of images
an extremely compact all analog and fully parallel implementation of a class of shunting recurrent neural networks that is applicable to a wide variety of fet based integration technologies is proposed while the contrast enhancement data compression and adaptation to mean input intensity capabilities of the network are well suited for processing of sensory information or feature extraction for a content addressable memory cam system the network also admits a global liapunov function and can thus achieve stable cam storage itself in addition the model can readily function as a front end processor to an analog adaptive resonance circuit



a digital realisation of two dimensional self organising feature maps is presented the method is based on subspace classification using an n tuple technique weight vector approximation and orthogonal projections to produce a winnertakes all network are also discussed over one million effective binary weights can be applied in ms using a conventional microcomputer details of a number of image recognition tasks including character recognition and object centring are described
a design for a fully analog version of a self organizing feature map neural network has been completed several parts of this design are in fabrication the feature map algorithm was modified to accommodate circuit solutions to the various computations required performance effects were measured by simulating the design as part of a frontend for a speech recognition system circuits are included to implement both activation computations and weight adaption or learning external access to the analog weight values is provided to facilitate weight initialization testing and static storage this fully analog implementation requires an order of magnitude less area than a comparable digitalanalog hybrid version developed earlier
we have fabricated a test chip in micron cmos that can perform supervised learning in a manner similar to the boltzmann machine patterns can be presented to it at per second the chip learns to solve the xor problem in a few milliseconds we also have demonstrated the capability to do unsupervised competitive learning with it the functions of the chip components are examined and the performance is assessed



we discuss synthetic receptors for haptic sensing these are based on magnetic field sensors hall effect structures fabricated using standard cmos technologies these receptors biased with a small permanent magnet can detect the presence of ferro or ferri magnefic objects in the vicinity of the sensor they can also detect the magnitude and direction of the magnetic field


automatic speech recognition asr is an artificial perception problem the input is raw continuous patterns no symbols and the desired output which may be words phoneroes meaning or text is symbolic the most successful approach to automatic speech recognition is based on stochastic models a stochastic model is a theoretical system whose internal state and output undergo a series of transformations governed by probabilistic laws in the application to speech recognition the unknown patterns of sound are treated as if they were outputs of a stochastic system information about the classes of patterns is encoded as the structure of these laws and the probabilities that govern their operation the most popular type of sm for asr is also known as a hidden markov model there are several reasons why the sm approach has been so successful for asr it can describe the shape of the spectrum and has a principled way of describing temporal order together with variability of both it is compatible with the hierarchical nature of speech structure there are powerful algorithms for decoding with respect to the model recognition and for adapting the model to fit significant amounts of example data learning firm theoretical mathematical foundations enable extensions to be accommodated smoothly eg there are many deficiencies however in a typical system the speech signal is first described as a sequence of acoustic vectors spectrum cross sections or equivalent at a rate of say per second the pattern is assumed to consist of a sequence of segments corresponding to discrete states of the model in each segment the acoustic vectors are drawn from a distribution characteristic of the state but otherwise independent of one another and of the states before and after in some systems there is a controlled relationship between states and the phonernes or phones of speech science but most of the properties and notions which speech scientists assume are important are ignored most sm approaches are also deficient at a pattern recognition theory level the parameters of the models are usually adjusted using the baum welch re estimation method so as to maximise the likelihood of the data given the model this is the right thing to do if the form of the model is actually appropriate for the data but if not the parameter optimisation method needs to be concerned with speech recognition discrimination between classes phonemes words meanings a hmm recognition algorithm is designed to find the best explanation of the input in terms of the model it tracks scores for all plausible current states of the generator and throws away explanations which lead to a current state for which there is a better explanation bellmans dynamic programming it may also throw away explanations which lead to a current state much worse than the best current state score pruning producing a beam search method it is important to keep many hypotheses in hand particularly when the current input is ambiguous connectionist or neural network approaches start with a strong pre conception of the types of process to be used they can claim some legitimacy by reference to new or renewed theories of cognitive processing the actual mechanisms used are usually simpler than those of the sm methods but the mathematical theory of what can be learnt or computed for instance is more difficult particularly for structures which have been proposed for dealing with temporal structure one of the dreams for connectionist approaches to speech is a network whose inputs accept the speech data as it arrives it would have an internal state which contains all necessary information about the past input and the output would be as accurate and early as it could be the training of networks with their own dynamics is particularly difficult especially when we are unable to specify what the internal state should be some are working on methods for training the fixed points of continuousvalued recurrent non hnear networks prager has attempted to train various types of network in a full state feedback arrangement watrous limits his recurrent connections to self loops on hidden and output units but even so the theory of such recursive non linear filters is formidable at the other extreme are systems which treat a whole time frequency amplitude array resulting from initial acoustic analysis as the input to a network and require a label as output for example the performance that peeling et al report on multi speaker small vocabulary isolated word recognition tasks approach those of the best hmm techniques available on the same data invariance to temporal position was trained into the network by presenting the patterns at random positions in a fixed time window waibel et al use a powerful compromise arrangement which can be thought of either as the replication of smaller networks across the timewindow a time spread network or as a single small network with internal delay lines a time delay neural network there are no recurrent links except for trivial ones at the output so training using backpropagation is no great problem we may think of this as a finite impulse response non linear filter reported results on consonant discrimination are encouraging and better than those of a hmm system on the same data the system is insensitive to position by virtue of its construction kohonen has constructed and demonstrated large vocabulary isolated word and unrestricted vocabulary continuous speech transcription systems which are inspired by neural network ideas but implemented as algorithms more suitable for bridle current programmed digital signal processor and cpu chips kohonens phonotopic map technique can be thought of as an unsupervised adaptive quantiser constrained to put its reference points in a non linear low dimensional sub space his learning vector quantiser technique used for initial labeling combines the advantages of the classic nearest neighbor method and discriminant training among other types of network which have been applied to speech we must mention an interesting class based not on correlations with weight vectors dot product but on distances from reference points radial basis function theory was developed for multi dimensional interpolation and was shown by broomhead and lowe to be suitable for many of the jobs that feed forward networks are used for the advantage is that it is not difficult to find useful positions for the reference points which define the first non linear transformation if this is followed by a linear output transformation then the weights can be found by methods which are fast and straightforward the reference points can be adapted using methods based on backpropagation related methods include potential functions kernel methods and the modified kanerva network there is much to be gained form a careful comparison of the theory of stochastic model and neural network approaches to speech recognition if a nn is to perform speech decoding in a way anything like a sm algorithm it will have a state which is not just one of the states of the hypothetical generarive model the state must include information about the distribution of possible generator states given the pattern so far and the state transition function must update this distribution depending on the current speech input it is not clear whether such an internal representation and behavior can be learned from scratch by an otherwise unstructured recurrent network stochastic model based algorithms seem to have the edge at present for dealing with temporal sequences discrimination based training inspired by nn techniques may make a significant difference in performance it would seem that the area where nns have most to offer is in finding non linear transformations of the data which take us to a space perhaps related to formant or articuatory parameters where comparisons are more relevant to phonetic decisions than purely auditory ones eg the resulting transformation could also be viewed as a set of feature detectors or perhaps the nn should deliver posterior probabilities of the states of a sm directly the art of applying a stochastic model or neural network approach is to choose a class of models or networks which is realistic enough to be likely to be able to capture the distinctions between speech sounds or words for instance and yet have a structure which makes it amenable to algorithms for building the detail of the models based on examples and for interpreting particular unknown patterns future systems will need to exploit the regularities described by phonetics to allow the construction of high performance systems with large vocabularies and their adaptation to the characteristics of each new user speech recognition there is no doubt that the stochastic model based methods work best at present but current systems are generally far inferior to humans even in situations where the usefulness of higher level processing in minimal i predict that the next generation of asr systems will be based on a combination of connectionist and sm theory and techniques with mainstream speech knowledge used in a rather soft way to decide the structure it should not be long before the distinction i have been making will disappear references lo d r cox and h d millar the theory of stochastic processes methuen pp s e levinson l r rabiner and m m sohndi an


the midbrain of the barn owl contains a map like representation of sound source direction which is used to precisely orient the head toward targets of interest elevation is computed from the interaural difference in sound level we present models and computer simulations of two stages of level difference processing which qualitatively agree with known anatomy and physiology and make several striking predictions i
the pyloric central pattern generator of the crustacean stomatogastric ganglion is a well defined biological neural network this neuron network is modulated by many inputs these inputs reconfigure the network to produce multiple output patterns by three simple mechanisms determining which cells are active modulating the synaptic efficacy changing the intrinsic response properties of individual neurons the importance of modifiable intrinsic response properties of neurons for network function and modulation is discussed i
interneurons in leech ganglia receive multiple sensory inputs and make synaptic contacts with many motor neurons these hidden units coordinate several different behaviors we used physiological and anatomical constraints to construct a model of the local bending reflex dynamical networks were trained on experimentally derived input output patterns using recurrent back propagation units in the model were modified to include electrical synapses and multiple synaptic time constants the properties of the hidden units that emerged in the simulations matched those in the leech the model and data support distributed rather than ocalist representations in the local bending reflex these results also explain counterintuitive aspects of the local bending circuitry
traditional inethods of studying neural coding characterize the cncoding of known stimuli in average neural responses organisms face nearly the opposite task decoding short segments of a spike train to extract information about an unknown time varying stimulus here we present strategies for characterizing the neural code from the point of view of the organism culminating in algorithms for real time stimulus reconstruction based on a single sample of the spike train these methods are applied to the design and analysis of experiments on an identified movement sensitive neuron in the fly visual system as far as we know this is the first instance in which a direct reading of the neural code has been accomplished
most complex behaviors appear to be governed by internal motivational states or drives that modify an animals responses to its environment it is therefore of considerable interest to understand the neural basis of these motivational states drawing upon work on the neural basis of feeding in the marine mollusc aplysia we have developed a heterogeneous artificial neural network for controlling the feeding behavior of a simulated insect we demonstrate that feeding in this artificial insect shares many characteristics with the motivated behavior of natural animals
the brain represents the skin surface as a topographic map in the somatosensory cortex this map has been shown experimentally to be modifiable in a use dependent fashion throughout life we present a neural network simulation of the competitive dynamics underlying this cortical plasticity by detailed analysis of receptive field properties of model neurons during simulations of skin coactivation cortical lesion digit amputation and nerve section
it is well known that neural responses in particular brain regions are spatially organized but no general principles have been developed that relate the structure of a brain map to the nature of the associated computation on parallel computers maps of a sort quite similar to brain maps arise when a computation is distributed across multiple processors in this paper we will discuss the relationship between maps and computations on these computers and suggest how similar considerations might also apply to maps in the brain
a generic model of oscillating cortex which assumes minimal coupling justified by known anatomy is shown to function as an sociative memory using previously developed theory the network has explicit excitatory neurons with local inhibitory interneuron feedback that forms a set of nonlinear oscillators coupled only by long range excitatogy connections using a local hebb like learning rule for primary and higher order synapses at the ends of the long range connections the system learns to store the kinds of oscillation amplitude patterns observed in olfactory and visual cortex this rule is derived from a more general projection algorithm for recurrent analog networks that analytically guarantees content addressable memory storage of continuous periodic sequences capacity n fourier components for an n node network no spurious attractors
the firing patterns of populations of cells in the cat visual cortex can exhibit oscillatory responses in the range of hz furthermore groups of neurons many toms apart can be highly synchronized as long as the cells have similar orientation tuning we investigate two basic network architectures that incorporate either nearest neighbor or global feedback interactions and conclude that non local feedback plays a fundamental role in the initial synchronization and dynamic stability of the oscillations i
it has been known for many years that specific regions of the working cerebral cortex display periodic variations in correlated cellular activity while the olfactory system has been the focus of much of this work similar behavior has recently been observed in primary visual cortex we have developed models of both the olfactory and visual cortex which replicate the observed oscillatory properties of these networks using these models we have examined the dependence of oscillatory behavior on single cell properties and network architectures we discuss the idea that the oscillatory events recorded from cerebral cortex may be intrinsic to the architecture of cerebral cortex as a whole and that these rhythmic patterns may be important in coordinating neuronal activity during sensory processing i
we outline a computational model of the development and regeneration of specific eye brain circuits the model comprises a self organizing map forming network which uses local hebb rules constrained by molecular markers various simulations of the development of eyebrain maps in fish and frogs are described
at the level of individual neurons catecholamine release increases the responsivity of cells to excitatory and inhibitory inputs we present a model of catecholamine effects in a network of neural like elements we argue that changes in the responsivity of individual elements do not affect their ability to detect a signal and ignore noise however the same changes in cell responsivity in a network of such elements do improve the signal detection performance of the network as a whole we show how this result can be used in a computer simulation of behavior to account for the effect of cns stimulants on the signal detection performance of human subjects
we study networks of spiking neurons in which spikes are fired as a poisson process the state of a cell is determined by the instantaneous firing rate and in the limit of high firing rates our model reduces to that studied by hopfield we find that the inclusion of spiking results in several new features such as a noise induced asymmetry between on and off states of the cells and probability currents which destroy the usual description of network dynamics in terms of energy surfaces taking account of spikes also allows us to calibrate network parameters such as synaptic weights against experiments on real synapses realistic forms of the post synaptic response alters the network dynamics which suggests a novel dynamical learning mechanism i
this paper presents the results of a simulation of the spatial relationship between the inferior olivary nucleus and folium crus iia of the lateral hemisphere of the rat cerebellum the principal objective of this modeling effort was to resolve an apparent conflict between a proposed zonal organization of olivary projections to cerebellar cortex suggested by anatomical tract tracing experiments brodal kawamura campbell armstrong and a more patchy organization apparent with physiological mapping robertson the results suggest that several unique features of the olivocerebellar circuit may contribute to the appearance of zonal organization using anatomical techniques but that the detailed patterns of patchy tactile projections seen with physiological techniques are a more accurate representation of the afferent organization of this region of cortex
in the mammalian visual cortex orientation selective simple cells which detect straight lines may be adapted to detect curved lines instead we test a biologically plausible hebbian single neuron model which learns oriented receptive fields upon exposure to unstructured noise input and maintains orientation selectivity upon exposure to edges or bars of all orientations and positions this model can also learn arc shaped receptive fields upon exposure to an environment of only circular rings thus new experiments which try to induce an abnormal curved receptive field may provide insight into the plasticity of simple cells the model suggests that exposing cells to only a single spatial frequency may induce more striking spatial frequency and orientation dependent effects than heretofore observed i

spiking neurons which integrate to threshold and fire were used to study the transmission of frequency modulated fm signals through layered networks firing correlations between cells in the input layer were found to modulate the transmission of fm signals under certain dynamical conditions a tonic level of activity was maintained by providing each cell with a source of poissondistributed synaptic input when the average membrane depolarization produced by the synaptic input was sufficiently below threshold the firing correlations between cells in the input layer could greatly amplify the signal present in subsequent layers when the depolarization was sufficiently close to threshold however the firing synchrony between cells in the initial layers could no longer effect the propagation of fm signals in this latter case integrateand fire neurons could be effectively modeled by simpler analog elements governed by a linear input output relation
the inputoutput properties of a compartment model neuron are systematically explored taken from the work of macgregor macgregor the model neuron compartments contain several active conductances including a potassium conductance in the dendritic compartment driven by the accumulation of intradendritic calcium dynamics of the conductances and potentials are governed by a set of coupled first order differential equations which are integrated numerically there are a set of internal parameters to this model specificying conductance rate constants time constants thresholds etc to study parameter sensitivity a set of trials were run in which the input driving the neuron is kept fixed while each internal parameter is varied with all others left fixed to study the inputoutput relation the input to the rendrite a square wave was varied in frequency and magnitude while all internal parameters of the system were left fixed and the resulting output firing rate and bursting rate was counted the inputoutput relation of the model neuron studied turns out to be much more sensitive to modulation of certain dendritic potassium current parameters than to plasticity of synapse efficacy per se the amount of current influx due to synapse activation this would in turn suggest as has been recently observed experimentally that the potassium current may be as or more important a focus of neural plasticity than synaptic efficacy
analytic solutions to the information theoretic evolution equation of the connection strength of a three layer feedforward neural net for visual information processing are presented the results are the receptive fields of the feature analysing cells correspond to the eigenvector of the maximum eigenvalue of the fredholm integral equation of the first kind derived from the evolution equation of the connection strength a symmetry breaking mechanism parity violation has been identified to be responsible for the changes of the morphology of the receptive field the conditions for the formation of different morphologies are explicitly identified
eight neural net and conventional pattern classifiers bayesianunimodal gaussian k nearest neighbor standard back propagation adaptive stepsize back propagation hypersphere feature map learning vector quantizer and binary decision tree were implemented on a serial computer and compared using two speech recognition and two artificial tasks error rates were statistically equivalent on almost all tasks but classifiers differed by orders of magnitude in memory requirements training time classification time and ease of adaptivity nearest neighbor classifiers trained rapidly but required the most memory tree classifiers provided rapid classification but were complex to adapt back propagation classifiers typically required long training times and had intermediate memory requirements these results suggest that classifier selection should often depend more heavily on practical considerations concerning memory and computation resources and restrictions on training and classification times than on error rate this work was sponsored by the department of the air force and the air force office of scientific research practical characteristics of neural network i
it is well known that when an automatic learning algorithm is applied to a fixed corpus of data the size of the corpus places an upper bound on the number of degrees of freedom that the model can contain if it is to generalize well because the amount of hardware in a neural network typically increases with the dimensionality of its inputs it can be challenging to build a high performance network for classifying large input patterns in this paper several techniques for addressing this problem are discussed in the context of an isolated word recognition task
we are developing a phoneme based speaker dependent continuous speech recognition system embedding a multilayer perceptton mlp ie a feedforward artificial neural network into a hidden markov model hmm approach in bourlard wellekens it was shown that mlps were approximating maximum a posteriori map probabilities and could thus be embedded as an emission probability estimator in hmms by using contextual information from a sliding window on the input frames we have been able to improve frame or phoneme classiftcation performance over the corresponding performance for simple maximum likelihood ml or even map probabilities that are estimated without the benefit of context however recognition of words in continuous speech was not so simply improved by the use of an mlp and several modifications of the original scheme were necessary for getting acceptable performance it is shown here that word recognition performance for a simple discrete density hmm system appears to be somewhat better when mlp methods are used to estimate the emission probabilities
two approaches were explored which integrate neural net classifiers with hidden markov model hmm speech recognizers both attempt to improve speech pattern discrimination while retaining the temporal processing advantages of ltmms one approach used neural nets to provide second stage discrimination following an itmm recognizer on a small vocabulary task radial basis function rbf and back propagation neural nets reduced the error rate substantially from to for the rbf classifier in a larger vocabulary task neural net classifiers did not reduce the error rate they however outperformed gaussian gaussian mixture and knearest neighbor knn classifiers in another approach neural nets functioned as low level acoustic phonetic feature extractors when classifying phonemes based on single msec frames discriminant rbf neural net classifiers outperformed gaussian mixture classifiers performance however differed little when classifying phones by accumulating scores across all frames in phonetic segments using a single node itmm recognizer this work was sponsored by the department of the air force and the air force office of scientific research hmm speech recognition with neural net discrimination cepstral sequence second stage classifier node averages viterbi segmentation figure second stage discrimination system itmm recognition is based on the accumulated scores from each node a second stage classifier can adjust the weights from each node to provide improved discrimination i
we present a number of time delay neural network tdnn based architectures for multi speaker phoneme recognition fodgtask we use speech of two females and four males to compare the performance of the various architectures against a baseline recognition rate of for a single tdnn on the six speaker fodgtask this series of modular designs leads to a highly modular multi network architecture capable of performing the six speaker recognition task at the speaker dependent rate of in addition to its high recognition rate the so called meta pi architecture learns without direct supervision to recognize the speech of one particular male speaker using internal models of other male speakers exclusively
one of the attractions of neural network approaches to pattern recognition is the use of a discrimination based training method we show that once we have modified the output layer of a multilayer perceptron to provide mathematically correct probability distributions and replaced the usual squared error criterion with a probability based score the result is equivalent to maximum mutual information training which has been used successfully to improve the performance of hidden markov models for speech recognition if the network is specially constructed to perform the recognition computations of a given kind of stochastic model based classifier then we obtain a inethod for discrimination based training of the parameters of the models examples include an hmm based word discriminator which we call an alphanet i
we attempt to combine neural networks with knowledge from speech science to build a speaker independent speech recognition system this knowledge is utilized in designing the preprocessing input coding output coding output supervision and architectural constraints to handle the temporal aspect of speech we combine delays copies of activations of hidden and output units at the input level and back propagation for sequences bps a learning algorithm for networks with local self loops this strategy is demonstrated in several experiments in particular a nasal discrimination task for which the application of a speech theory hypothesis dramatically improved generalization
the effects of parameter modifications imposed by hardware constraints on a self organizing feature map algorithm were examined performance was measured by the error rate of a speech recognition system which included this algorithm as part of the front end processing system parameters which were varied included weight connection strength quantization adaptation quantization distance measures and circuit approximations which include device characteristics and process variability experiments using the t isolated word database for speakers demonstrated degradation in performance when weight quantization fell below bits the competitive nature of the algorithm relaxes constraints on uniformity and linearity which makes it an excellent candidate for a fully analog circuit implementation prototype circuits have been fabricated and characterized following the constraints established through the simulation efforts
acoustic speech recognition degrades in the presence of noise compensatory information is available from the visual speech signals around the speakers mouth previous attempts at using these visual speech signals to improve automatic speech recognition systems have combined the acoustic and visual speech information at a symbolic level using heuristic rules in this paper we demonstrate an alternative approach to fusing the visual and acoustic speech information by training feedforward neural networks to map the visual signal onto the corresponding short term spectral amplitude envelope stsae of the acoustic signal this information can be directly combined with the degraded acoustic stsae significant improvements are demonstrated in vowel recognition from noise degraded acoustic signals these results are compared to the performance of humans as well as other pattern matching and estimation algorithms i
distinctive electrocardiogram ecg patterns are created when the heart is beating normally and when a dangerous arrhythmia is present some devices which monitor the ecg and react to arrhythmias parameterize the ecg signal and make a diagnosis based on the parameters the author discusses the use of a neural network to classify the ecg signals directly without parameterization the input to such a network must be translation invariant since the distinctive features of the ecg may appear anywhere in an arbritrarily chosen ecg segment the input must also be insensitive to the episode to episode and patient to patient variability in the rhythm pattern
this paper describes a neural network algorithm that performs temporal pattern matching in real time is trained on line with a single pass requires only a single template for training of each representative class is continuously adaptable to changes in background noise deals with transient signals having low signalto noise ratios works in the presence of non gaussian noise makes use of context dependencies and outputs bayesian probability estimates the algorithm has been adapted to the problem of passive sonar signal detection and classification it runs on a connection machine and correctly classifies within ms of onset signals embedded in noise and subject to considerable uncertainty i
in our effort to develop a modular neural system for invariant learning and recognition of d objects we introduce here a new module architecture called an aspect network constructed around adaptive axo axo dendritic synapses this builds upon our existing system seibert waxman which processes d shapes and classifies them into view categories ie aspects invariant to illumination position orientation scale and projective deformations from a sequenceof views the aspect network learns the transitions between these aspects crystallizing a graph like structure from an initially amorphous network object recognition emerges by accumulating evidence over multiple views vhich activate competing object hypotheses
we describe a model that can recognize two dimensional shapes in an unsegmented image independent of their orientation position and scale the model called traffic efficiently represents the structural relation between an object and each of its component features by encoding the fixed viewpoint invariant transformation from the features reference frame to the objects in the weights of a connectionist network using a hierarchy of such transformations with increasing complexity of features at each successive layer the network can recognize multiple objects in parallel an implementation of traffic is described along with experimental results demonstrating the networks ability to recognize constellations of stars in a viewpoint invariant manner

contour maps provide a general method for recognizing two dimensional shapes all but blank images give rise to such maps and people are good at recognizing objects and shapes from them the maps are encoded easily in long feature vectors that are suitable for recognition by an associative memory these properties of contour maps suggest a role for them in early visual perception the prevalence of direction sensitive neurons in the visual cortex of mammals supports this view
we have constructed a two axis camera positioning system which is roughly analogous to a single human eye this artificial eye aeye combines the signals generated by two rate gyroscopes with motion information extracted from visual analysis to stabilize its camera this stabihzation process is similar to the vestibulo ocular response vor hke the vor a eye learns a system model that can be incrementally modified to adapt to changes in its structure performance and environment a eye is an example of a robust sensory system that performs computations that can be of significant use to the designers of mobile robots
to achieve high rate image data compression while maintainig a high quality reconstructed image a good image model and an efficient way to represent the specific data of each image must be introduced based on the physiological knowledge of multichannel characteristics and inhibitory interactions between them in the human visual system a mathematically coherent parallel architecture for image data compression which utilizes the markov random field image model and interactions between a vast number of filter banks is proposed

this paper explores the use of a model neural network for motor learning steinbuch and taylor presented neural network designs to do nearest neighbor lookup in the early s in this paper their nearest neighbor network is augmented with a local model network which fits a local model to a set of nearest neighbors the network design is equivalent to local regression this network architecture can represent smooth nonlinear functions yet has simple training rules with a single global optimum the network has been used for motor learning of a simulated arm and a simulated running machine

the cmac storage scheme has been used as a basis for a software implementation of an associative memory system ams which itself is a major part of the learning control loop lenas a major disadvantage of this emac concept is that the degree of local generalization area of interpolation is fixed this paper deals with an algorithm for self organizing variable generalization for the ams based on ideas of t kohonen i
the performance sensitivity of albus cmac network was studied for the scenario in which faults are introduced into the adjustable weights after training has been accomplished it was found that fault sensitivity was reduced with increased generalization when loss of weight faults were considered but sensitivity was increased for saturated weight faults
given a set of input output training samples we describe a procedure for determining the time sequence of weights for a dynamic neural network to model an arbitrary input output process we formulate the input output mapping problem as an optimal control problem defining a performance index to be minimized as a function of time varying weights we solve the resulting nonlinear two point boundary value problem and this yields the training rule for the performance index chosen this rule turns out to be a continuous time generalization of the outer product rule earlier suggested heuristically by hopfield for designing associative memories learning curves for the new technique are presented i
a nonlinear neural framework called the generalized hopfield network is proposed which is able to solve in a parallel distributed manner systems of nonlinear equations the method is applied to the general nonlinear optimization problem we demonstrate ghns implementing the three most important optimization algorithms namely the augmented lagrangian generalized reduced gradient and successive quadratic programming methods the study results in a dynamic view of the optimization problem and offers a straightforward model for the parallelization of the optimization computations thus significantly extending the practical limits of problems that can be formulated as an optimization problem and which can gain from the
we present a novel modular recurrent connectionist network architecture which learns to robustly perform incremental parsing of complex sentences from sequential input one word at a time our networks learn to do semantic role assignment noun phrase attachment and clause saucture recognition for sentences with passive consauctions and center embedded clauses the networks make syntactic and semantic predictions at every point in time and previous predictions are revised as expectations are affirmed or violated with the arrival of new infomarion our networks induce their own grammar rules for dynamically transforming an input sequence of words into a syntacticsemantic interpretafion these networks generalize and display tolerance to input which has been corrupted in ways common in spoken language
the phonological structure of human languages is intricate yet highly constrained through a combination of connectionist modeling and linguistic analysis we are attempting to develop a computational basis for the nature of phonology we present a connectionist architecture that performs multiple simultaneous insertion deletion and mutation operations on sequences of phonemes and introduce a novel additional primitive clustering clustering provides an interesting alternative to both iterative and relaxation accounts of assimilation processes such as vowel harmony our resulting model is efficient because it processes utterances entirely in parallel using only feed forward circuitry
order single layer recursive network easily learns to a deterministic finite state machine and recognize regular grammars when an enhanced version of this neural net state machine is connected through a common error term to an external analog stack memory the combination can be interpreted as a neural net pushdown automata the neural net finite state machine is given the primitives push and pop and is able to read the top of the stack through a gradient descent learning rule derived from the common error function the hybrid network learns to effectively use the stack actions to manipulate the stack memory and to learn simple contextfree grammars

we present an application of back propagation networks to handwritten digit recognition minimal preprocessing of the data was required but architecture of the network was highly constrained and specifically designed for the task the input of the network consists of normalized images of isolated digits the method has error rate and about a reject rate on zipcode digits provided by the us postal service

we propose a new way to construct a large scale neural network for handwritten kanji characters recognition this neural network consists of parts a collection of small scale networks which are trained individually on a small number of kanji characters a network which integrates the output from the small scale networks and a process to facilitate the integration of these neworks the recognition rate of the total system is comparable with those of the small scale networks our results indicate that the proposed method is effective for constructing a large scale network without loss of recognition performance
in order to detect the presence and location of immunoglobulin ig domains from amino acid sequences we built a system based on a neural network with one hidden layer trained with back propagation the program was designed to efficiently identify proteins exhibiting such domains characterized by a few localized conserved regions and a low overall homology when the national biomedical research foundation nbrf new protein sequence database was scanned to evaluate the programs performance we obtained very low rates of false negatives coupled with a moderate rate of false positives
we present two connectionist architectures for chunking of symbolic rewrite rules one uses backpropagation learning the other competitive learning although they were developed for chunking the same sorts of rules the two differ in their representational abilities and learning behaviors
consider a robot wandering around an unfamiliar environmere performing actions and sensing the resulting environmental states the robots task is to construct an internal model of its environment a model that will allow it to predict the consequences of its actions and to deumnine what sequences of actions to take to reach particular goal states rivest and schapire b schapire have studied this problem and have designed a symbolic algorithm to strategically explore and infer the structure of finite state environments the heart of this algorithm is a clever representation of the environment called an update graph we have developed a connectionist implementation of the update graph using a highly specialized network architecture with back propagation learning and a uivial exploration strategy choosing random tions the connectiordst network can outperform the rivest and schapire algorithm on simple problems the network has the additional strength that it can accommodate stochastic environments perhaps the greatest virtue of the connectiordst appwach is that it suggests generalizations of the update graph representation that do not arise from a traditional symbolic perspective
we present a general and systematic method for neural network design based on the genetic algorithm the technique works in conjunction with network learning rules addressing aspects of the networks gross architecture connectivity and learning rule parameters networks can be optimized for various applicationspecific criteria such as learning speed generalization robustness and connectivity the approach is model independent we describe a prototype system neurogeness that employs the backpropagation learning rule experiments on several small problems have been conducted in each case neurogenesys has produced networks that perform significantly better than the randomly generated networks of its initial population the computational feasibility of our approach is discussed
kanervas sparse distributed memory sdm is an associative memory model based on the mathematical properties of high dimensional binary address spaces hollands genetic algorithms are a search technique for high dimensional spaces inspired by evolutionary processes of dna genetic memory is a hybrid of the above two systems in which the memory uses a genetic algorithm to dynamically reconfigure its physical storage locations to reflect correlations between the stored addresses and data for example when presented with raw weather station data the genetic memory discovers specific features in the weather data which correlate well with upcoming rain and reconfigures the memory to utilize this information effectively this architecture is designed to maximize the ability of the system to scale up to handle real world problems
we have developed graphics to visualize static and dynamic information in layered neural network learning systems emphasis was placed on creating new visuals that make use of spatial arrangements size information animation and color wc applied these tools to the study of back propagation learning of simple boolean predicates and have obtained new insights into the dynamics of the learning process
the goal in this work has been to identify the neuronal elements of the cortical column that are most likely to support the learning of nonlinear associative maps we show that a particular style of network learning algorithm based on locally tuned receptive fields maps naturally onto cortical hardware and gives coherence to a variety of features of cortical anatomy physiology and biophysics whose relations to learning remain poorly understood
in this paper we present upper bounds for the learning rates for hybrid models that employ a combination of both self organized and supervised learning using radial basis functions to build receptive field representations in the hidden units the learning performance in such networks with nearest neighbor heuristic can be improved upon by multiplying the individual receptive field widths by a suitable overlap factor we present results indicating optimal values for such overlap factors we also present a new algorithm for determining receptive field centers this method negotiates more hidden units in the regions of the input space as a function of the output and is conducive to better learning when the number of patterns hidden units is small
if neurons sum up their inputs in a non linear way as some simulations suggest how is this distributed fine grained non linearity exploited during learning how are all the small sigmoids in synapse spine and dendritic tree lined up in the right areas of their respective input spaces in this report i show how an abstract atemporal highly nested tree snucture with a quadratic transfer function associated with each branchpoint can self organise using only a single global reinforcement scalar to perform binary classification tasks the procedure works well solving the multiplexer and a difficult phoneme classification task as well as back propagation does and faster furthermore it does not calculate an error gradient but uses a statistical scheme to build moving models of the reinforcement signal
a methodology for faster supervised learning in dynamical nonlinear neural networks is presented it exploits the concept of adjoint operators to enable computation of changes in the networks response due to perturbations in all system parameters using the solution of a single set of appropriately constructed linear equations the lower bound on speedup per learning iteration over conventional methods for calculating the neuromorphic energy gradient is on where n is the number of neurons in the network i
a new form of the deterministic boltzmann machine dbm learning procedure is presented which can efficiently train network modules to discriminate between input vectors according to some criterion the new technique directly utilizes the free energy of these mean field modules to represent the probability that the criterion is met the free energy being readily manipulated by the learning procedure although conventional deterministic boltzmann learning fails to extract the higher order feature of shift at a network bottleneck combining the new mean field modules with the mutual information objective function rapidly produces modules that perfectly extract this important higher order feature without direct external supervision
a new learning algorithm learning by choice of internal represetations chir was recently introduced whereas many algorithms reduce the learning process to minimizing a cost function over the weights our method treats the internal representations as the fundamental entities to be determined the algorithm applies a search procedure in the space of internal representations and a cooperative adaptation of the weights eg by using the perceptton learning rule since the

a central problem in connectionist modelling is the control of network and architectural resources during learning in the present approach weights reflect a coarse prediction history as coded by a distribution of values and parameterized in the mean and standard deviation of these weight distributions weight updates are a function of both the mean and standard deviation of each connection in the network and vary as a function of the error signal stochastic delta rule hanson consequently the weights maintain information on their central tendency and their uncertainty in prediction such information is useful in establishing a policy concerning the size of the nodal complexity of the network and growth of new nodes for example during problem solving the present network can undergo meiosis producing two nodes where there was one overtaxed node as measured by its coefficient of variation it is shown in a number of benchmark problems that meiosis networks can find minimal architectures reduce computational complexity and overall increase the efficiency of the representation learning interaction also a member of the cognitive science laboratory princeton university princeton nj hanson
this work introduces a new method called self organizing neural network sonn algorithm and compares its performance with back propagation in a signal separation application the problem is to separate two signals a modem data signal and a male speech signal added and transmitted through a khz channel the signals are sampled at khz and using supervised learning an attempt is made to reconstruct them the sonn is an algorithm that constructs its own network topology during training which is shown to be much smaller than the bp network faster to trained and free from the trial anderror network design that characterize bp

a simple method for training the dynamical behavior of a neural network is derived it is applicable to any training problem in discrete time networks with arbitrary feedback the algorithm resembles back propagation in that an error function is minimized using a gradient based method but the optimization is carried out in the hidden part of state space either instead of or in addition to weight space computational results are presented for some simple dynamical training problems one of which requires response to a signal time steps in the past
selective sampling is a form of directed search that can greatly increase the ability of a connectionist network to generalize accurately based on information from previous batches of samples a network may be trained on data selectively sampled from regions in the domain that are unknown this is realizable in cases when the distribution is known or when the cost of drawing points from the target distribution is negligible compared to the cost of labeling them with the proper classification the approach is justified by its applicability to the problem of training a network for power system security analysis the benefits of selective sampling are studied analytically and the results are confirmed experimentally i
one popular class of unsupervised algorithms are competitive algorithms in the traditional view of competition only one competitor the winner adapts for any given case i propose to view competitive adaptation as attempting to fit a blend of simple probability generators such as gaussians to a set of data points the maximum likelihood fit of a model of this type suggests a softer form of competition in which all competitors adapt in proportion to the relative probability that the input came from each competitor i investigate one application of the soft competitive model placement of radial basis function centers for function interpolation and show that the soft model can give better performance with little additional computational cost i

a method for storing analog vectors in hopfields continuous feedback model is proposed by analog vectors we mean vectors whose components are real valued the vectors to be stored are set as equilibria of the network the network model consists of one layer of visible neurons and one layer of hidden neurons we propose a learning algorithm which results in adjusting the positions of the equilibria as well as guaranteeing their stability simulation results confirm the effectiveness of the method
we have used information theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network by removing unimportant weights from a network several improvements can be expected better generalization fewer training examples required and improved speed of learning andor classification the basic idea is to use second derivative information to make a tradeoff between network complexity and training set error experiments confirm the usefulness of the methods on a real world application
we have calculated both analytically and in simulations the rate of convergence at long times in the backpropagation learning algoritlun for networks with and without hidden units our basic finding for units using the standard sigrnoid transfer function is it convergence of the error for large t with at most logarithmic corrections for networks with hidden units other transfer functions may lead to a slower polynomial rate of convergence our analytic calculations were presented in tesauro he ahamd here we focus in more detail on our empirical measurements of the convergence rate in numerical simulations which corm our analytic results
ht the development of an image segmentation system for real time image processing applications we apply the classical decision analysis paradigan by viewing inmge segmentation as a pixel classification task we use supervised training to derive a classifier for our system from a set of examples of a particular pixel classification problem in this study we test the suitability of a connectionist method against two statistical methods gaussian maximum likelihood classifier and first second and third degree polynomial classifiers for the solution of a real world image segmentation problem taken from combustion research classifiers are derived using ah three methods and the performance of all of the classifiers on the training data set as well as on separate entire test images is measured i
multi layer percepttons and trained classification trees are two very different techniques which have recently become popular given enough data and time both methods are capable of performing arbitrary non linear classification we first consider the important differences between multi layer percepttons and classification trees and conclude that there is not enough theoretical basis for the clearcut superiority of one technique over the other for this reason we performed a number of empirical tests on three real world problems in power system load forecasting power system security prediction and speaker independent vowel identification in all cases even for piecewise linear trees the multi layer perceptron performed as well as or better than the trained classification trees performance comparisons
we have done an empirical study of the relation of the number of parameters weights in a feedforward net to generalization performance two experiments are reported in one we use simulated data sets with well controlled parameters such as the signal to noise ratio of continuous valued data in the second we train the network on vector quantized mel cepstm from real speech samples in each case we use back propagation to train the feedforward net to discriminate in a multiple class pattern classification problem we report the results of these studies and show the application of cross validation techniques to prevent overfitting i

the learning dynamics of the back propagation algorithm are investigated when complexity constraints are added to the standard least mean square lms cost function it is shown that loss of generalization performance due to overtraining can be avoided when using such complexity constraints furthermore energy hidden representations and weight distributions are observed and compared during learning an attempt is made at explaining the results in terms of linear and non linear effects in relation to the gradient descent learning algorithm
the properties of a cluster of multiple back propagation bp networks are examined and compared to the performance of a single bp network the underlying idea is that a synergistic effect within the cluster improves the performance and fault tolerance five networks were initially trained to perform the same input output mapping following training a cluster was created by computing an average of the outputs generated by the individual networks the output of the cluster can be used as the desired output during training by feeding it back to the individual networks in comparison to a single bp network a cluster of multiple bps generalization and significant fault tolerance it appear that cluster advantage follows from simple maxim you can fool some of the single bps in a cluster all of the time but you cannot fool all of them all of the time lincoln
in recent years many researchers have investigated the use of markov random fields mrfs for computer vision they can be applied for example to reconstruct surfaces from sparse and noisy depth data coming from the output of a visual process or to integrate early vision processes to label physical discontinuities in this paper we show that by applying mean field theory to those mrfs models a class of neural networks is obtained those networks can speed up the solution for the mrfs models the method is not restricted to computer vision
a rigorous analysis on the finite precision computational spects of neural network as a pattern classifier via a probabilistic approach is presented even though there exist negative results on the capability of perceptron we show the following positive results given n pattern vectors each represented by cn bits where c that are uniformly distributed with high probability the perceptron can perform all possible binary classifications of the patterns moreover the resulting neural network requires a vanishingly small proportion olog nn of the memory that would be required for complete storage of the patterns further the perceptron algorithm takes on arithmetic operations with high probability whereas other methods such as linear programming takes on in the worst case we also indicate some mathematical connections with vlsi circuit testing and the theory of random matrices i
within the context of valiants protocol for learning the perceptron algorithm is shown to learn an arbitrary half space in time or if d the probability distribution of examples is taken uniform over the unit sphere n here is the accuracy parameter this is surprisingly fast as standard approaches involve solution of a linear programming problem involving constraints in n dimensions a modification of valiants distribution independent protocol for learning is proposed in which the distribution and the function to be learned may be chosen by adversaries however these adversaries may not communicate it is argued that this definition is more reasonable and applicable to real world learning than valiants under this definition the perceptron algorithm is shown to be a distribution independent learning algorithm in an appendix we show that for uniform distributions some classes of infinite v c dimension including convex sets and a class of nested differences of convex sets are learnable
decision making tasks that involve delayed consequences are very common yet difficult to address with supervised learning methods if there is an accurate model of the underlying dynamical system then these tasks can be formulated as sequential decision problems and solved by dynamic programming this paper discusses reinforcement learning in terms of the sequential decision framework and shows how a learning algorithm similar to the one implemented by the adaptive critic element used in the pole balancer of barto sutton and anderson and further developed by sutton fits into this framework adaptive neural networks can play significant roles as modules for approximating the functions required for solving sequential decision problems

experimental evidence has shown analog neural networks to be extremely fault tolerant in particular their performance does not appear to be significantly impaired when precision is limited analog neurons with limited precision essentially compute k ary weighted multilinear threshold functions which divide r n into k regions with k hyperplanes the behaviour of k ary neural networks is investigated there is no canonical set of threshold values for k although they exist for binary and ternary neural networks the weights can be made integers of only o z klog z k bits where z is the number of processors without increasing hardware or running time the weights can be made while increasing running time by a constant multiple and hardware by a small polynomial in z and k binary neurons can be used if the running time is allowed to increase by a larger constant multiple and the hardware is allowed to increase by a slightly larger polynomial in z and k any symmetric k ary function can be computed in constant depth and size n k and any k ary function can be computed in constant depth and size o nk the alternating neural networks of olafsson and abu mostafa and the quantized neural networks of fleisher are closely related to this model analog neural networks of limited precision i
a comparison of algorithms that minimize error functions to train the trajectories of recurrent networks reveals how complexity is traded off for causality these algorithms are also related to time independent formalisms it is suggested that causal and scalable algorithms are possible when the activation dynamics of adaptive neurons is fast compared to the behavior to be learned standard continuous time recurrent backpropagation is used in an example
the paper suggests a statistical framework for the parameter estimation problem associated with unsupervised learning in a neural network leading to an exploratory projection pursuit network that performs feature extraction or dimensionality reduction
i
we introduce a cost function for learning in feed forward neural networks which is an explicit function of the internal representation in addition to the weights the learning problem can then be formulated as two simple perceptrons and a search for internal representations back propagation is recovered as a limit the frequency of successful solutions is better for this algorithm than for back propagation when weights and hidden units are updated on the same timescale ie once every learning step i
the vestibulo ocular reflex vor is the primary mechanism that controls the compensatory eye movements that stabilize retinal images during rapid head motion the primary pathways of this system are feed forward with inputs from the semicircular canals and outputs to the oculomotor system since visual feedback is not used directly in the vor computation the system must exploit motor learning to perform correctly lisberger has proposed a model for adapting the vor gain using image slip information from the retina we have designed and tested analog very largescale integrated vlsi circuitry that implements a simplified version of lisbergers adaptive vor model i
the long term goal of our laboratory is the development of analog resistive network based vlsi implementations of early and intermediate vision algorithms we demonstrate an experimental circuit for smoothing and segmenting noisy and sparse depth data using the resistive fuse and a d edge detection circuit for computing zero crossings using two resistive grids with different spaceconstants to demonstrate the robustness of our algorithms and of the fabricated analog cmos vlsi chips we are mounting these circuits onto small mobile vehicles operating in a real time laboratory environment i
distributed neuron synapses have been integrated in an active area of mm x mm using a ttm double metal single poly n well cmos technology the distributed neuron synapses are arranged in blocks of which we call x tiles switch matrices are interleaved between each of these tiles to provide programmability of interconnections with a small area overhead the units of the network can be rearranged in various configurations some of the possible configurations are a network a network two networks etc the numbers separated by dashes indicate the number of units per layer including the input layer weights are stored in analog form on mos capacitors the synaptic weights are usable to a resolution of of their full scale value the limitation arises due to charge injection from the access switch and charge leakage other parameters like gain and shape of nonlinearity are also programmable
cascadable cmos synapse chips containing a cross bar array of x programmable synapses have been fabricated as uilding blocks for fully parallel implementation of neural networks the synapses are based on a hybrid digital analog design which utilizes on chip bit data latches to store quantized weights and two quadrant multiplying dacs to compute weighted outputs the synapses exhibit bit resolution and excellent monotonicity and consistency in their transfer characteristics a neuron hardware incorporating four synapse chips has been fabricated to investigate the performance of feedback networks in optimization problem solving in this study a x one to one assignment net and the hopfield tank city traveling salesman problem net have been implemented in the hardware the networks ability to obtain optimum or near optimum solutions in real time has been demonstrated
this paper explores whether analog circuitry can adequately perform constrained optimization constrained optimization circuits are designed using the differential multiplier method these circuits fulfill time varying constraints correctly example circuits include a quadratic programming circuit and a constrained flip flop


in this paper we present a novel implementation of the widely used back propagation neural net learning algorithm on the connection machine cm a general purpose massively parallel computer with a hypercube topology this implementation runs at about million interconnections per second ips on a k processor cm the main interprocessor communication operation used is d nearest neighbor communication the techniques developed here can be easily extended to implement other algorithms for layered neural nets on the cm or on other massively parallel computers which have d or higher degree connections among their processors i
the mapping of the back propagation and mean field theory learning algorithms onto a generic d simd computer is described this architecture proves to be very adequate for these applications since efficiencies close to the optimum can be attained expressions to find the learning rates are given and then particularized to the dap array procesor i

a short account is given of various investigations of neural network properties beginning with the classic work of mcculloch pitts early work on neurodynamics and statistical mechanics analogies with magnetic materials fault tolerance via parallel distributed processing memory learning and pattern recognition is described
we describe a computational model of the development and regeneration of specific eye brain circuits the model comprises a self organizing map forming network which uses local hebb rules constrained by genetically determined molecular markers various simulations of the development and regeneration of eye brain maps in fish and frogs are described in particular successful simulations of experiments by schmidt cicerone easter meyer and yoon
feature selective cells in the primary visual cortex of several species are organized in hierarchical topographic maps of stimulus features like position in visual space orientation and ocular dominance in order to understand and describe their spatial structure and their development we investigate a self organizing neural network model based on the feature map algorithm the model explains map formation as a dimension reducing mapping from a high dimensional feature space onto a two dimensional lattice such that similarity between features or feature combinations is translated into spatial proximity betveen the corresponding feature selective cells the model is able to reproduce several aspects of the spatial structure of cortical maps in the visual cortex
the development of projections from the retinas to the cortex is mathematically analyzed according to the previously proposed thermodynamic formulation of the self organization of neural networks three types of submodality included in the visual afferent pathways are assumed in two models model a in which the ocularity and retinotopy are considered separately and model b in which on centeroff center pathways are considered in addition to ocularity and retinotopy model a shows striped ocular dominance spatial patterns and in ocular dominance histograms reveals a dip in the binocular bin model b displays spatially modulated irregular patterns and shows single peak behavior in the histograms when we compare the simulated results with the observed results it is evident that the ocular dominance spatial patterns and histograms for models a and b agree very closely with those seen in monkeys and cats
simple classical spin models well known to physicists as the annni and heisenberg xy models in which long range interactions occur in a pattern given by the mexican hat operator can generate many of the structural properties characteristic of the ocular dominance columns and iso orientation patches seen in cat and primate visual cortex

we are exploring the significance of biological complexity for neuronal computation here we demonstrate that hebbian synapses in realistically modeled hippocampal pyramidal cells may give rise to two novel forms of self organization in response to structured synaptic input first on the basis of the electrotonic relationships between synaptic contacts a cell may become tuned to a small subset of its input space second the same mechanisms may produce clusters of potentiated synapses across the space of the dendrites the latter type of self organization may be functionally significant in the presence of nonlinear dendritic conductanceso
combining neuropharmacological experiments with computational modeling we have shown that cholinergic modulation may enhance associative memory function in pitiform olfactory cortex we have shown that the acetylcholine analogue carbachol selectively suppresses synaptic transmission between cells within pitiform cortex while leaving input connections unaffected when tested in a computational model of piriform cortex this selective suppression applied during learning enhances associative memory performance i
we have devised a scheme to reduce the complexity of dynamical systems belonging to a class that includes most biophysically realistic neural models the reduction is based on transformations of variables and perturbation expansions and it preserves a high level of fidelity to the original system the techniques are illustrated by reductions of the hodgkin huxley system and an augmented hodgkin huxley system
the main point of this paper is that stochastic neural networks have a mathematical structure that corresponds quite closely with that of quantum field theory neural network liouvillians and lagrangians can be derived just as can spin hamiltonians and lagrangians in qft it remains to show the efficacy of such a description
the self organization of recurrent feature discovery networks is studied from the perspective of dynamical systems bifurcation theory reveals parameter regimes in which multiple equilibria or limit cycles coexist with the equilibrium at which the networks perform principal component analysis i
we present a new way to derive dissipative optimizing dynamics from the lagrangian formulation of mechanics it can be used to obtain both standard and novel neural net dynamics for optimization problems to demonstrate this we derive standard descent dynamics as well as nonstandard variants that introduce a computational attention mechanism i
the hopfield network hopfield provides a simple model of an associative memory in a neuronal structure this model however is based on highly artificial assumptions especially the use of formal two state neurons hopfield or graded response neurons hopfield what happens if we replace the formal neurons by real biological neurons we address this question in two steps first we show that a simple model of a neuron can capture all relevant features of neuron spiking ie a wide range of spiking frequencies and a realistic distribution of interspike intervals second we construct an associative memory by linking these neurons together the analytical solution for a large and fully connected network shows that the hopfield solution is valid only for neurons with a short refractory period if the refractory period is longer than a critical duration c the solutions are qualitatively different the associative character of the solutions however is preserved
a simple architecture and algorithm for analytically guaranteed associative memory storage of analog patterns continuous sequences and chaotic attractors in the same network is described a matrix inversion determines network weights given prototype patterns to be stored there are n units of capacity in an n node network with n weights it costs one unit per static attractor two per fourier component of each sequence and four per chaotic attractor there are no spurious attractors and there is a liapunov function in a special coordinate system which governs the approach of transient states to stored trajectories unsupervised or supervised incremental learning algorithms for pattern classification such as competitive learning or bootstrap widrow hoff can easily be implemented the architecture can be folded into a recurrent network with higher order weights that can be used as a model of cortex that stores oscillatory and chaotic attractors by a hebb rule hierarchical sensory motor control networks may be constructed of interconnected cortical patches of these network modules network performance is being investigated by application to the problem of real time handwritten digit recognition
we show analytically how the stability of two dimensional lateral inhibition neural networks depends on the local connection topology for various network topologies we calculate the critical time delay for the onset of oscillation in continuous time networks and present analytic phase diagrams characterizing the dynamics of discrete time networks
bernard victorri elsap universit de caen caen cedex france fully recurrent asymmetrical networks can be thought of as dynamic systems the dynamics can be shaped to perform content addressable memories recognize sequences or generate trajectories unfortunately several problems can arise first the convergence in the state space is not guaranteed second the learned fixed points or trajectories are not necessarily stable finally there might exist spurious fixed points andor spurious attracting trajectories that do not correspond to any patterns in this paper we introduce a new energy function that presents solutions to all of these problems we present an efficient gradient descent algorithm which directly acts on the stability of the fixed points and trajectories and on the size and shape of the corresponding basin and valley of attraction the results are illustrated by the simulation of a small content addressable memory
the development of learning algorithms is generally based upon the minimization of an energy function it is a fundamental requirement to compute the gradient of this energy function with respect to the various parameters of the neural architecture eg synaptic weights neural gainetc in principle this requires solving a system of nonlinear equations for each parameter of the model which is computationally very expensive a new methodology for neural learning of time dependent nonlinear mappings is presented it exploits the concept of adjoint operators to enable a fast global computation of the networks response to perturbations in all the systems parameters the importance of the time boundary conditions of the adjoint functions is discussed an algorithm is presented in which the adjoint sensitivity equations are solved smultaneously ie forward in time along with the nonlinear dynamics of the neural networks this methodology makes real time applications and hardware implementation of temporal learning feasible
coherent oscillatory activity in large networks of biological or artificial neural units may be a useful mechanism for coding information pertaining to a single perceptual object or for detailing regularities within a data set we consider the dynamics of a large array of simple coupled oscillators under a variety of connection schemes of particular interest is the rapid and robust phase locking that results from a sparse scheme where each oscillator is strongly coupled to a tiny randomly selected subset of its neighbors
this paper studies dynamical aspects of neural systems with delayed negative feedback modelled by nonlinear delay differential equations these systems undergo a hopf bifurcation from a stable fixed point to a stable limit cycle oscillation as certain parameters are varied it is shown that their frequency of oscillation is robust to parameter variations and noisy fluctuations a property that makes these systems good candidates for pacemakers the onset of oscillation is postponed by both additive and parametric noise in the sense that the state variable spends more time near the fixed point than it would in the absence of noise this is also the case when noise affects the delayed variable ie when the system has a faulty memory finally it is shown that a distribution of delays rather than a fixed delay also stabilizes the fixed point solution
we show that a simple spin system biased at its critical point can encode spatial characteristics of external signals such as the dilnensions of objects in the visual field in the temporal correlation functions of individual spins qualitative arguments suggest that regularly firing neurons should be described by a planar spin of unit length and such xy models exhibit critical dynamics over a broad range of paralneters we show how to extract these spins from spike trains and then measure the interaction hamiltonia n using simulations of small clusters of cells static correlations among spike trains obtained kom silnulations of large arrays of cells are in agreement with the predictions from these hamiltonians and dynamic correlations display the predict ed encoding of spatial inforlnation we suggest that this novel representation of object dilnensions in temporal correlations may be relevant to recent experiments on oscillatory neural firing in the visual cortex
multi layered neural networks have recently been proposed for nonlinear prediction and system modeling although proven successful for modeling time invariant nonlinear systems the inability of neural networks to characterize temporal variability has so far been an obstacle in applying them to complicated nonstationary signals such as speech in this paper we present a network architecture called hidden control neural network hcnn for modeling signals generated by nonlinear dynamical systems with restricted time variability the approach taken here is to allow the mapping that is implemented by a multi layered neural network to change with time as a function of an additional control input signal this network is trained using an algorithm that is based on back propagation and segmentation algorithms for estimating the unknown control together with the networks parameters the hcnn approach was applied to several tasks including modeling of time varying nonlinear systems and speaker independent recognition of connected digits yielding a word accuracy of i
in this work we describe a new method that adjusts time delays and the widths of time windows in artificial neural networks automatically the input of the units are weighted by a gaussian input window over time which allows the learning rules for the delays and widths to be derived in the same way as it is used for the weights our results on a phoneme classification task compare well with results obtained with the tdnn by waibel et al which was manually optimized for the same task
we present a new neural network model for processing of temporal patterns this model the gamma neural model is as general as a convolution delay model with arbitrary weight kernels wt we show that the gamma model can be formulated as a partially prewired additive model a temporal hebbian learning rule is derived and we establish links to related existing models for temporal processing
the goal has been to construct a supervised artificial neural network that learns incrementally an unknown mapping as a result a network consisting of a combination of art and backpropagation is proposed and is called an artbp network the art network is used to build and focus a supervised backpropagation network the artbp network has the advantage of being able to dynamically expand itself in response to input patterns containing new information simulation results show that the artbp network outperforms a classical maximum likelihood method for the estimation of a discrete dynamic and nonlinear transfer function i
we study the representation of static patterns and temporal associations in neural networks with a broad distribution of signal delays for a certain class of such systems a simple intuitive understanding of the spatio temporal computation becomes possible with the help of a novel lyapunov functional it allows a quantitative study of the asymptotic network behavior through a statistical mechanical analysis we present analytic calculations of both retrieval quality and storage capacity and compare them with simulation results i
this work extends computational learning theory to situations in which concepts vary over time eg system identification of a time varying plant we have extended formal definitions of concepts and learning to provide a framework in which an algorithm can track a concept as it evolves over time given this framework and focusing on memory based algorithms we have derived some pac style sample complexity results that determine for example when tracking is feasible we have also used a similar framework and focused on incremental tracking algorithms for which we have derived some bounds on the mistake or error rates for some specific concept classes

we present a large vocabulary continuous speech recognition system based on linked predictive neural networks lpnns the system uses neural networks as predictors of speech frames yielding distortion measures which are used by the one stage dtw algorithm to perform continuous speech recognition the system already deployed in a speech to speech translation system currently achieves and word accuracy on tasks with perplexity and respectively outperforming several simple hmms that we tested we also found that the accuracy and speed of the lpnn can be slightly improved by the judicious use of hidden control inputs we conclude by discussing the strengths and weaknesses of the predictive approach
a neural network architecture was designed for locating word boundaries and identifying words from phoneme sequences this architecture was tested in three sets of studies first a highly redundant corpus with a restricted vocabulary was generated and the network was trained with a limited number of phonemic variations for the words in the corpus tests of network performance on a transfer set yielded a very low error rate in a second study a network was trained to identify words from expert transcriptions of speech on a transfer test error rate for correct simultaneous identification of words and word boundaries was the third study used the output of a phoneme classifier as the input to the word and word boundary identification network the error rate on a transfer test set was for this task overall these studies provide a first step at identifying words in connected discourse with a neural network
previous work has shown the ability of multilayer perceptrons mlps to estimate emission probabilities for hidden markov models hmms the advantages of a speech recognition system incorporating both mlps and hmms are the best discrimination and the ability to incorporate multiple sources of evidence features temporal context without restrictive assumptions of distributions or statistical independence this paper presents results on the speaker dependent portion of darpas english language resource management database results support the previously reported utility of mlp probability estimation for continuous speech recognition an additional approach we are pursuing is to use mlps as nonlinear predictors for autoregressive hmms while this is shown to be more compatible with the hmm formalism it still suffers from several limitations this approach is generalized to take account of time correlation between successive observations without any restrictive assumptions about the driving noise
through the use of neural network classifiers and careful feature selection we have achieved high accuracy speaker independent spoken letter recognition for isolated letters a broad category segmentation is performed location of segment boundaries allows us to measure features at specific locations in the signal such as vowel onset where important information resides letter classification is performed with a feed forward neural network recognition accuracy on a test set of speakers was neural network classifiers are also used for pitch tracking and broad category segmentation of letter strings our research has been extended to recognition of names spelled with pauses between the letters when searching a database of names we achieved first choice name retrieval work has begun on a continuous letter classifier which does frame by frame phonetic classification of spoken letters i
the neural prediction model is the speech recognition model based on pattern prediction by multilayer percepttons its effectiveness was confirmed by the speaker independent digit recognition experiments this paper presents an improvement in the model and its application to large vocabulary speech recognition based on subword units the improvement involves an
stephen j cox british telecom research labs ipswich uk ip re a particular form of neural network is described which has terminals for acoustic patterns class labels and speaker parameters a method of training this network to tune in the speaker parameters to a particular speaker is outlined based on a trick for converting a supervised network to an unsupervised mode we describe experiments using this approach in isolated word recognition based on whole word hidden markov models the results indicate an improvement over speaker independent performance and for unlabelled data a performance close to that achieved on labelled data
a novel unsupervised neural network for dimensionality reduction which seeks directions emphasizing multimodality is presented and its connection to exploratory projection pursuit methods is discussed this leads to a new statistical insight to the synaptic modification equations governing learning in bienenstock cooper and munro bcm neurons the importance of a dimensionality reduction principle based solely on distinguishing features is demonstrated using a linguistically motivated phoneme recognition experiment and compared with feature extraction using back propagation network i
in this paper we will describe several extensions to our earlier work utilizing a segment based approach we will formulate our segmental framework and report our study on the use of multi layer perceptrons for detection and classification of phoneroes we will also examine the outputs of the network and compare the network performance with other classifiers our investigation is performed within a set of experiments that attempts to recognize vowels and consonants in american english independent of speaker when evaluated on the timit database our system achieves an accuracy of i
spoken language is one of the mot natural efficient flexible and economical means of communication among humans as computers play an ever increasing role in our lives it is important that we address the issue of providing a graceful human machine interface through spoken language in this paper we will describe our recent efforts in moving beyond the scope of speech recognition into the realm of spoken language understanding specifically we report on the development of an urban navigation and exploration system called voyager an application which we have used as a basis for performing research in spoken language understanding i
this paper is a summary of sprint project aims and results the project focus on the use of neuro computing techniques to tackle various problems that remain unsolved in speech recognition first results concern the use of feedforward nets for phonetic units classification isolated word recognition and speaker adaptation
we have been studying the performance of a bottlenosed dolphin on a delayed matching to sample task to gain insight into the processes and mechanisms that the animal uses during echolocation the dolphin recognizes targets by emitting natural sonar signals and listening to the echoes that return this paper describes a novel neural network architecture called an integrator gateway network that we have developed to account for this performance the integrator gateway network combines information from multiple echoes to classify targets with about accuracy in contrast a standard backpropagation network performed with only about accuracy
signal processing capabilities of biological neurons are investigated temporally coded signals in neurons can be multiplexed to increase the transmission capacity multiplexing of signal is suggested in bi threshold neurons with high threshold and low threshold for switching firing modes to extract the signal embedded in the interspikeintervals of firing the encoded signal are demultiplexed and multiplexed by a network of neurons with delayed line circuitry for signal processing the temporally coded input signal is transformed spatially by mapping the firing intervals topographically to the output of the network thus decoding the specific firing interspike intervals the network also provides a band pass filtering capability where the variability of the timing of the original signal can be decoded

using an unsupervised learning procedure a network is trained on an ensemble of images of the same two dimensional object at different positions orientations and sizes each half of the network sees one fragment of the object and tries to produce as output a set of parameters that have high mutual information with the parameters output by the other half of the network given the ensemble of training patterns the parameters on which the two halves of the network can agree are the position orientation and size of the whole object or some recoding of them after training the network can reject instances of other shapes by using the fact that the predictions made by its two halves disagree if two competing networks are trained on an unlabelled mixture of images of two objects they cluster the training cases on the basis of the objects shapes independently of the position orientation and size
the model based neural vision system presented here determines the position and identity of three dimensional objects two stereo images of a scene are described in terms of shape primitives line segments derived from edges in the scenes and their relational structure a recurrent neural matching network solves the correspondence problem by assigning corresponding line segments in right and left stereo images a d relational scene description is then generated and matched by a second neural network against models in a model base the quality of the solutions and the convergence speed were both improved by using mean field approximations
a second order architecture is presented here for translation rotation and scale invariant processing of d images mapped to n input units this new architecture has a complexity of on weights as opposed to the on weights usually required for a third order rotation invariant architecture the reduction in complexity is due to the use of discrete frequency information simulations show favorable comparisons to other neural network architectures
previous work mi sereno cf me sereno showed that a feedforward network with area vl like input layer units and a hebb rule can develop area mt like second layer units that solve the aperture problem for pattern motion the present study extends this earlier work to more complex motions saito et al showed that neurons with large receptive fields in macaque visual area mst are sensitive to different senses of rotation and dilation irrespective of the receptive field location of the movement singularity a network with an mt like second layer was trained and tested on combinations of rotating dilating and translating patterns third layer units learn to detect specific senses of rotation or dilation in a position independent fashion despite having position dependent direction selectivity within their receptive fields
this paper presents a neural network nn approach to the problem of stereopsis the correspondence problem finding the correct matches between the pixels of the epipolar lines of the stereo pair from amongst all the possible matches is posed as a non iterative many to one mapping a two layer feed forward nn architecture is developed to learn and code this nonlinear and complex mapping using the back propagation learning rule and a training set the important aspect of this technique is that none of the typical constraints such as uniqueness and continuity are explicitly imposed all the applicable constraints are learned and internally coded by the inn enabling it to be more flexible and more accurate than the existing methods the approach is successfully tested on several randomdot stereograms it is shown that the net can generalize its learned mapping to cases outside its training set advantages over the marr poggio algorithm are discussed and it is shown that the inn performance is superior i

a neural network model of motion segmentation by visual cortex is described the model clarifies how preprocessing of motion signals by a motion oriented contrast filter moc filter is joined to long range cooperative motion mechanisms in a motion cooperative competitive loop cc loop to control phenomena such as as induced motion motion capture and motion aftereffects the total model system is a motion boundary contour system bcs that is computed in parallel with a static bcs before both systems cooperate to generate a boundary representation for three dimensional visual form perception the present investigations clarify how the static bcs can be modified for use in motion segmentation problems notably for analyzing how ambiguous local movements the aperture problem on a complex moving shape are suppressed and actively reorganized into a coherent global motion signal

exact structure from motion is an ill posed computation and therefore very sensitive to noise in this work i describe how a qualitative shape representation based on the sign of the gaussian curvature can be computed directly from motion disparities without the computation of an exact depth map or the directions of surface normals i show that humans can judge the curvature sense of three points undergoing d motion from two three and four views with success rate significantly above chance a simple rbf net has been trained to perform the same task
we formulate the problem of optimizing the sampling of natural images using an array of linear filters optimization of information capacity is constrained by the noise levels of the individual channels and by a penalty for the construction of long range interconnections in the array at low signal to noise ratios the optimal filter characteristics correspond to bound states of a schrsdinger equation in which the signal spectrum plays the role of the potential the resulting optimal filters are remarkably similar to those observed in the mammalian visual cortex and the retinal ganglion cells of lower vertebrates the observed scale invariance of natural images plays an essential role in this construction bialek ruderman and zee

the dark adapted visual system can count photons with a reliability limited by thermal noise in the rod photoreceptors the processing circuitry between the rod cells and the brain is essentially noiseless and in fact may be close to optimal here we design an optimal signal processor which estimates the time varying light intensity at the retina based on the rod signals we show that the first stage of optimal signal processing involves passing the rod cell output through a linear filter with characteristics determined entirely by the rod signal and noise spectra this filter is very general in fact it is the first stage in any visual signal processing task at low photon flux we identifv the output of this first stage filter with the intracellular voltage response of the bipolar cell the first anatomical stage in retinal signal processing from recent data on tiger salamander photoreceptors we extract the relevant spectra and make parameter free quantitative predictions of the bipolar cell response to a dim diffuse flash agreement with experiment is essentially perfect as far as we know this is the first successtiff predictive theory for neural dynamics
in salamander retina the response of on off ganglion cells to a central flash is reduced by movement in the receptive field surround through computer simulation of a d model which takes into account their anatomical and physiological properties we show that interactions between four neuron types two bipolar and two amacrine may be responsible for the generation and lateral conductance of this change sensitive inhibition the model shows that the four neuron circuit can account for previously observed movement sensitive reductions in ganglion cell sensitivity and allows visualization and prediction of the spatio temporal pattern of activity in change sensitive retinal cells
light adaptation la allows cone vision to remain functional between twilight and the brightest time of day even though at any one time their intensity response i r characteristic is limited to log units of the stimulating light one mechanism underlying la was localized in the outer segment of an isolated cone we found that by adding annular illtmination an i r characteristic of a cone can be shifted along the intensity domain neural network involving feedback synapse from horizontal cells to cones is involved to be in register with ambient light level of the periphery an equivalent electrical circuit with three different transmembrane channels leakage photocurrent and feedback was used to model static behavior of a cone spice simulation showed that interactions between feedback synapse and the light sensitive conductance in the outer segment can shift the i r curves along the intensity domain provided that phototransduction mechanism is not saturated during maximally hyperpolarizcd light response
we have designed and tested a one dimensional pixel analog cmos vlsi chip which localizes intensity edges in real time this device exploits on chip photoreceptors and the natural filtering properties of resistive networks to implement a scheme similar to and motivated by the difference of gaussians dog operator proposed by marr and hildreth our chip computes the zero crossings associated with the difference of two exponential weighting functions if the derivative across this zero crossing is above a threshold an edge is reported simulations indicate that this technique will extend well to two dimensions i
inspired by a visual motion detection model for the rabbit retina and by a computational architecture used for early audition in the barn owl we have designed a chip that employs a correlation model to report the one dimensional field motion of a scene in real time using subthreshold analog vlsi techniques we have fabricated and successfully tested a transistor chip using a standard mosis process
we present a generic neural network architecture capable of controlling non linear plants the network is composed of dynamic parallel linear maps gated by non linear switches using a recurrent form of the back propagation algorithm control is achieved by optimizing the control gains and task adapted switch parameters a mean quadratic cost function computed across a nominal plant trajectory is minimized along with performance constraint penalties the approach is demonstrated for a control task consisting of landing a commercial aircraft in difficult wind conditions we show that the network yields excellent performance while remaining within acceptable damping response constraints
we describe a real time robot navigation system based on three vlsi neural network modules these are a resistive grid for path planning a nearest neighbour classifier for localization using range data from a timeof flight infra red sensor and a sensory motor associative network for dynamic obstacle avoidance i
the alvinn autonomous land vehicle in a neural network project addresses the problem of training artificial neural networks in real time to perform difficult perception tasks alvinnis a back propagation network that uses inputs from a video camera and an imaging laser rangefinder to drive the cmu naylab a modified chevy van this paper describes training techniques which allow alvinn to learn in under minutes to autonomously control the naylab by watching a human drivers response to new situations using these techniques alvinn has been trained to drive in a variety of circumstances including single lane paved and unpaved roads multilane lined and unlined roads and obstacle ridden onand off road environments at speeds of up to miles per hour
we propose a new parallel hierarchical neural network model to enable motor learning for simultaneous control of both trajectory and force by integrating hogans control method and our previous neural network control model using a feedback error learning scheme furthermore two hierarchical control laws which apply to the model are derived by using the moore penrose pseudoinverse matrix one is related to the minimum muscle tension change trajectory and the other is related to the minimum motor command change trajectory the human arm is redundant at the dynamics level since joint torque is generated by agnist and antagonist muscles therefore acquisition of the inverse model is an ill posed problem however the combination of these control laws and feedback error learning resolve the ill posed problem finally the efficiency of the parallel hierarchical neural network model is shown by learning experiments using an artificial muscle arm and computer simulations
we have used a neural network to compute corrections for images written by electron beams to eliminate the proximity effects caused by electron scattering iterative methods are effective but require prohibitively computation time we have instead trained a neural network to perform equivalent corrections resulting in a significant speed up we have examined hardware implementations using both analog and digital electronic networks both had an acceptably small error of compared to the iterative results additionally we verified that the neural network correctly generalized the solution of the problem to include patterns not contained in its training set we have experimentally verified this approach on a cambridge instruments ebmf exposure system
we present a new connectionist planning method tml by interaction with an unknown environment a world model is progressively constructed using gradient descent for deriving optimal actions with respect to future reinforcement planning is applied in two steps an experience network proposes a plan which is subsequently optimized by gradient descent with a chain of world models so that an optimal reinforcement may be obtained when it is actually run the appropriateness of this method is demonstrated by a robotics application and a pole balancing task

barto sutton and watkins introduced a grid task as a didactic example of temporal difference planning and asynchronous dynamical programming this paper considers the effects of changing the coding of the input stimulus and demonstrates that the self supervised learning of a particular form of hidden unit representation improves performance
this is a summary of results with dyna a class of architectures for intelligent systems based on approximating dynamic programming methods dyna architectures integrate trial and error reinforcement learning and execution time planning into a single process operating alternately on the world and on a learned forward model of the world we describe and show results for two dyna architectures dyna ahc and dyna q using a navigation task results are shown for a simple dyna ahc system which simultaneously learns by trial and error learns a world model and plans optimal routes using the evolving world model we show that dyna q architectures based on watkinss q learning are easy to adapt for use in changing environments
we present an algorithm based on reinforcement and state recurrence learning techniques to solve control scheduling problems in particular we have devised a simple learning scheme called handicapped learning in which the weights of the associative search element are reinforced either positively or negatively such that the system is forced to move towards the desired setpoint in the shortest possible trajectory to improve the learning rate a variable reinforcement scheme is employed negative reinforcement values are varied depending on whether the failure occurs in handicapped or normal mode of operation furthermore to realize a simulated annealing scheme for accelerated learning if the system visits the same failed state successively the negative reinforcement value is increased in examples studied these learning schemes have demonstrated high learning rates and therefore may prove useful for in situ learning
this paper examines a class of neuron based learning systems for dynamic control that rely on adaptive range coding of sensor inputs sensors are assumed to provide binary coded range vectors that coarsely describe the system state these vectors are input to neuron like processing elements output decisions generated by these neurons in turn affect the system state subsequently producing new inputs reinforcement signals from the environment are received at various intervals and evaluated the neural weights as well as the range boundaries determining the output decisions are then altered with the goal of maximizing future reinforcement from the environment preliminary experiments show the promise of adapting neural receptive fields when learning dynamical control the observed performance with this method exceeds that of earlier approaches adaptive range coding
a feedforward layered network implements a mapping required to control an unknown stochastic nonlinear dynamical system training is based on a novel approach that combines stochastic approximation ideas with backpropagation the method is applied to control admission into a queueing system operating in a time varying environment
this work addresses three problems with reinforcement learning and adaptive neuro control non markovian interfaces between learner and environment on line learning based on system realization vectorvalued adaptive critics an algorithm is described which is based on system realization and on two interacting fully recurrent continually running networks which may learn in parallel problems with parallel learning are attacked by adaptive randomness it is also described how interacting modelcontroller systems can be combined with vector valued adaptive critics previous critics have been scalar i
in response to a puff of wind the american cockroach turns away and runs the circuit underlying the initial turn of this escape response consists of three populations of individually identifiable nerve cells and appears to employ distributed representations in its operation we have reconstructed several neuronal and behavioral properties of this system using simplified neural network models and the backpropagation learning algorithm constrained by known structural characteristics of the circuitry in order to test and refine the model we have also compared the models responses to various lesions with the insects responses to similar lesions
neural network simulations of the dragonfly flight neurocontrol system have been developed to understand how this insect uses complex unsteady aerodynamics the simulation networks account for the ganglionic spatial distribution of cells as well as the physiologic operating range and the stochastic cellular firing history of each neuron in addition the motor neuron firing patterns flight command sequences were utilized simulation training was targeted against both the cellular and flight motor neuron firing patterns the trained networks accurately resynthesized the intraganglionic cellular firing patterns these in turn controlled the motor neuron firing patterns that drive wing musculature during flight such networks provide both neurobiological analysis tools and first generation controls for the use of unsteady aerodynamics
three dimensional d structures of protein backbones have been predicted using neural networks a feed forward neural network was trained on a class of functionally but not structurally homologous proteins using backpropagation learning the network generated tertiary structure information in the form of binary distance constraints for the c atoms in the protein backbone the binary distance between two c atoms was if the distance between them was less than a certain threshold distance and otherwise the distance constraints predicted by the trained neural network were utilized to generate a folded conformation of the protein backbone using a steepest descent minimization approach
jude w shavlik computer sciences university of wisconsin madison wi we describe the application of a hybrid symbolicconnectionist machine learning algorithm to the task of recognizing important genetic sequences the symbolic portion of the kbann system utilizes inference rules that provide a roughly correct method for recognizing a class of dna sequences known as eukarotic splice junctions we then map this domain theory into a neural network and provide training examples using the samples the neural networks learning algorithm adjusts the domain theory so that it properly classifies these dna sequences our procedure constitutes a general method for incorporating preexisting knowledge into artificial neural networks we present an experiment in molecular genetics that demonstrates the value of doing so
diagnosis of faults in complex real time control systems is a complicated task that has resisted solution by traditional methods we have shox that neural networks can be successfully employed to diagnose faults in digitally controlled powertrain systems this paper discusses the means we use to develop the appropriate databases for training and testing in order to select the optimum network architectures and to provide reasonable estimates of the classification accuracy of these networks on new samples of dam recent work applying neural nets to adaptive control of an active suspension system is presented
this study has demonstrated how artificial neural networks anns can be used to characterize seismic sources using high frequency regional seismic data we have taken the novel approach of using an ns as a research tool for obtaining seismic source information specifically depth of focus for earthquakes and ripple fire characteristics for economic blasts rather than as just a feature classifier between earthquake and explosion populations overall we have found that anns have potential applications to seismic event characterization and identification beyond just as a feature classifier in future studies these techniques should be applied to actual data of regional seismic events recorded at the new regional seismic arrays the results of this study indicates that an ann should be evaluated as part of an operational seismic event identification system
an artificial neural network ann is trained to recognize a buysell longshort pattern for a particular commodity future contract the backpropagation of errors algorithm was used to encode the relationship between the longshort desired output and fundamental variables plus or technical variables into the year of past data the ann longshort market positions for future that would have made investment of less than ann trained on one is able to predict months in the profit on an
wee kheng leow mcc and university of texas austin tx neural network algorithms have proven useful for recognition of individual segmented characters however their recognition accuracy has been limited by the accuracy of the underlying segmentation algorithm conventional rule based segmentation algorithms encounter difficulty if the characters are touching broken or noisy the problem in these situations is that often one cannot properly segment a character until it is recognized yet one cannot properly recognize a character until it is segmented we present here a neural network algorithm that simultaneously segments and recognizes in an integrated system this algorithm has several novel features it uses a supervised learning algorithm backpropagation but is able to take position independent information as targets and self organize the activities of the units in a competitive fashion to infer the positional information we demonstrate this ability with overlapping hand printed numerals
the dimensionality of a set of face images of i male and female subjects is reduced from to via an autoencoder network the extracted features do not correspond to the features used in previous face recognition systems kanade such as ratios of distances between facial elements rather they are whole face features we call holons the holons are given to and layer back propagation networks that are trained to classify the input features for identity feigned emotional state and gender the automatically extracted holons provide a sufficient basis for all of the gender discriminations of the identity discriminations and several of the emotion discriminations among the training set network and human judgements of the emotions are compared and it is found that the networks tend to confuse more distant emotions than humans do i
sex identification in animals has biological importance humans are good at making this determination visually but machines have not matched this ability a neural network was trained to discriminate sex in human faces and performed as well as humans on a set of exemplars images sampled at x were compressed using a xx fully connected back propagation network activities of hidden units served as input to a back propagation sexnet trained to produce values of i for male and for female faces the networks average error rate of compared favorably to humans who averaged some sexnet errors mimicked those of humans
this paper proposes a fuzzy neural expert system fnes with the following two functions generalization of the information derived from the training data and embodiment of knowledge in the form of the fuzzy neural network extraction of fuzzy hen rules with linguistic relative importance of each proposition in an antecedent part from a trained neural network this paper also gives a method to extract automatically fuzzy if then rules from the trained neural network to prove the effectiveness and validity of the proposed fuzzy neural expert system a fuzzy neural expert system for medical diagnosis has been developed
analog neural networks with feedback can be used to implement kwinner take all kwta networks in turn kwta networks can be used as decoders of a class of nonlinear error correcting codes by interconnecting such kwta networks xve can construct decoders capable of decoding more powerful codes we consider several families of interconnected kwta networks analyze their performance in terms of coding theory metrics and consider the feasibility of embedding such networks in vlsi technologies
harmonic grammar legendre et al is a connectionist theory of linguistic well formedness based on the assumption that the well formedness of a sentence can be measured by the harmony negative energy of the corresponding connectionist state assuming a lower level connectionist network that obeys a few general connectionist principles but is otherwise unspecified we construct a higher level network with an equivalent harmony function that captures the most linguistically relevant global aspects of the lower level network in this paper we extend the tensor product representation smolensky to fully recursire representations of recursively structured objects like sentences in the lower level network we show theoretically and with an example the power of the new technique for parallel distributed structure processing
a network was trained by back propagation to map locative expressions of the form noun preposition noun to a semantic representation as in cosic and munro the networks performance was analyzed over several simulations with training sets in both english and german translation of prepositions was attempted by presenting a locative expression to a network trained in one language to generate a semantic representation the semantic representation was then presented to the network trained in the other language to generate the appropriate preposition

in a previous paper touretzky wheeler a we showed how adding a clustering operation to a connectionist phonology model produced a parallel processing account of certain itemfive phenomena in this paper we show how the addition of a second structuring primitive syllabification greatly increases the power of the model we present examples from a non indo european language that appear to require rule ordering to at least a depth of four by adding syllabification circuitry to structure the models perception of the input string we are able to handle these examples with only two derivational steps we conclude that in phonology derivation can be largely replaced by structuring
a higher order recurrent neural network architecture learns to recognize and generate languages after being trained on categorized exemplars studying these networks from the perspective of dynamical systems yields two interesting discoveries first a longitudinal examination of the learning process illustrates a new form of mechanical inference induction by phase transition a small weight adjustment causes a bifurcation in the limit behavior of the network this phase transition corresponds to the onset of the networks capacity for generalizing to arbitrary length strings second a study of the automata resulting from the acquisition of previously published languages indicates that while the architecture is not guaranteed to find a minimal finite automata consistent with the given exemplars which is an np hard problem the architecture does appear capable of generating nonregular languages by exploiting fractal and chaotic dynamics i end the paper with a hypothesis relating linguistic generafive capacity to the behavioral regimes of non linear dynamical systems
competitive learning is an unsupervised algorithm that classifies input patterns into mutually exclusive clusters in a neural net framework each cluster is represented by a processing unit that competes with others in a winnertake all pool for an input pattern i present a simple extension to the algorithm that allows it to construct discrete distributed representations discrete representations are useful because they are relatively easy to analyze and their information content can readily be measured distributed representations are useful because they explicitly encode similarity the basic idea is to apply competitive learning iteratively to an input pattern and after each stage to subtract from the input pattern the component that was captured in the representation at that stage this component is simply the weight vector of the winning unit of the competitive pool the subtraction procedure forces competitive pools at different stages to encode different aspects of the input the algorithm is essentially the same as a traditional data compression technique known as multistep vector quantization although the neural net perspective suggests potentially powerful extensions to that approach
for lack of alternative models search and decision processes have provided the dominant paradigm for human memory access using two or more cues despite evidence against search as an access process humphreys wiles bain we present an alternative process to search based on calculating the intersection of sets of targets activated by two or more cues two methods of computing the intersection are presented one using information about the possible targets the other constraining the cue target strengths in the memory matrix analysis using orthogonal vectors to represent the cues and targets demonstrates the competence of both processes and simulations using sparse distributed representations demonstrate the performance of the latter process for tasks involving and cues

alcove is a connectionist model of human category learning that fits a broad spectrum of human learning data its architecture is based on wellestablished psychological theory and is related to networks using radial basis functions from the perspective of cognitive psychology alcove can be construed as a combination of exemplar based representation and errordriven learning from the perspective of connectionism it can be seen as incorporating constraints into back propagation networks appropriate for modelling human learning


a network based on splines is described it automatically adapts the number of units unit parameters and the architecture of the network for each application i
multi layer percepttons are often slow to learn nonlinear functions with complex local structure due to the global nature of their function approximations it is shown that standard multi layer percepttons are actually a special case of a more general network formulation that incorporates b splines into the node computations this allows novel spline network architectures to be developed that can combine the generalization capabilities and scaling properties of global multi layer feedforward networks with the computational efficiency and learning speed of local computational paradigms simulation results are presented for the well known spiral problem of weiland and of lang and witbrock to show the effectiveness of the spline net approach

local variable selection has proven to be a powerful technique for approximating functions in high dimensional spaces it is used in several statistical methods including cart id c mars and others see the bibliography for references to these algorithms in this paper i present a tree structured network which is a generalization of these techniques the network provides a framework for understanding the behavior of such algorithms and for modifying them to suit particular applications i
we examine the ability of radial basis functions rbfs to generalize we compare the performance of several types of rbfs we use the inverse dynamics of an idealized two joint arm as a test case we find that without a proper choice of a norm for the inputs rbfs have poor generalization properties a simple global scaling of the input variables greatly improves performance we suggest some efficient methods to approximate this distance metric
we have created a radial basis function network that allocates a new computational unit whenever an unusual pattern is presented to the network the network learns by allocating new units and adjusting the parameters of existing units if the network performs poorly on a presented pattern then a new unit is allocated which memorizes the response to the presented pattern if the network performs well on a presented pattern then the network parameters are updated using standard lms gradient descent for predicting the mackey glass chaotic time series our network learns much faster than do those using back propagation and uses a comparable number of synapses i
f fallside we develop a sequential adaptation algorithm for radial basis function rbf neural networks of gaussian nodes based on the method of succesrove projections this method makes use of each observation efficiently in that the network mapping function so obtained is consistent with that information and is also optimal in the least la norm sense the rbf network with the projections adaptation algorithm was used for predicting a chaotic time series we compare its performance to an adaptation scheme based on the method of stochastic approximation and show that the projections algorithm converges to the underlying model much faster i
we introduce oriented non radial basis function networks onrbf as a generalization of radial basis function networks rbfwherein the euclidean distance metric in the exponent of the gaussian is replaced by a more general polynomial this permits the definition of more general regions and in particularhyper ellipses with orientations in the case of hyper surface estimation this scheme requires a smaller number of hidden units and alleviates the curse of dimensionality associated kernel type approximatorsin the case of an image the hidden units correspond to features in the image and the parameters associated with each unit correspond to the rotation scaling and translation properties of that particular feature in the context of the onbf scheme this means that an image can be represented by a small number of features since transformation of an image by rotation scaling and translation correspond to identical transformations of the individual features the onbf scheme can be used to considerable advantage for the purposes of image recognition and analysis
we consider feed forward neural networks with one non linear hidden layer and linear output units the transfer function in the hidden layer are either bell shaped or sigmoid in the bell shaped case we show how bernstein polynomials on one hand and the theory of the heat equation on the other are relevant for understanding the properties of the corresponding networks in particular these techniques yield simple proofs of universal approxhnation properties ie of the fact that any reasonable function can be approximated to any degree of precision by a linear combination of bellshaped functions in addition in this framework the problem of learning is equivalent to the problem of reversing the time course of a diffusion process the results obtained in the bell shaped case can then be applied to the case of sigmoid transfer functions in the hidden layer yielding similar universality results a conjecture related to the problem of generalization is briefly examined i
in this paper we show that discrete affine wavelet transforms can provide a tool for the analysis and synthesis of standard feedforward neural networks it is shown that wavelet frames for li can be constructed based upon sigmoids the spatio spectral localization property of wavelets can be exploited in defining the topology and determining the weights of a feedforward network training a network constructed using the synthesis procedure described here involves minimization of a convex cost functional and therefore avoids pitfalls inherent in standard backpropagation algorithms extension of these methods to li n is also discussed i
bruno caprile irst povo italy learning an input output mapping fiom a set of examples can be regarded as synthesizing an approxilnation of a lnulti dilnensional fimction front this point of view this form of learning is closely related to regula rization theory and we have previously shown poggio and girosi a b the equivalence between regularization and a class of three layer networks that we call regularization networks in this note we extend the theory by introducing ways of dealing with two aspects of learning learning in presence of unreliable examples or outliers and learning fiom positive and negative examples

we describe a multi network or modular connectionist architecture that captures that fact that many tasks have structure at a level of granularity intermediate to that assumed by local and global function approximation schemes the main innovation of the architecture is that it combines associative and competitive learning in order to learn task decompositions a task decomposition is discovered by forcing the networks comprising the architecture to compete to learn the training patterns as a result of the competition different networks learn different training patterns and thus learn to partition the input space the performance of the architecture on a what and where vision task and on a multi payload robotics task are presented
we compare the performance of the modular architecture composed of competing expert networks suggested by jacobs jordan nowlan and hinton to the performance of a single back propagation network on a complex but low dimensional vowel recognition task simulations reveal that this system is capable of uncovering interesting decompositions in a complex task the type of decomposition is strongly influenced by the nature of the input to the gating network that decides which expert to use for each case the modular architecture also exhibits consistently better generalization on many variations of the task i
we introduce a framework for training architectures composed of several modules this framework which uses a statistical formulation of learning systems provides a unique formalism for describing many classical connectionist algorithms as well as complex systems where several algorithms interact it allows to design hybrid systems which combine the advantages of connectionist algorithms as well as other learning algorithms
we describe a recurrent connectionist network called concert that uses a set of melodies written in a given style to compose new melodies in that style concert is an extension of a traditional algorithmic composition technique in which transition tables specify the probability of the next note as a function of previous context a central ingredient of concert is the use of a psychologically grounded representation of pitch
genetic algorithms were used to select and create features and to select reference exemplar patterns for machine vision and speech pattern classification tasks for a complex speech recognition task genetic algorithms required no more computation time than traditional approaches to feature selection but reduced the number of input features required by a factor of five from to features on a difficult artificial machine vision task genetic algorithms were able to create new features polynomial functions of the original features which reduced classification error rates from to almost neural net and k nearest neighbor knn classifiers were unable to provide such low error rates using only the original features genetic algorithms were also used to reduce the number of reference exemplar patterns for a knn classifier on a training pattern vowel recognition problem with classes genetic algorithms reduced the number of stored exemplars from to without significantly increasing classification error rate in all applications genetic algorithms were easy to apply and found good solutions in many fewer trials than would be required by exhaustive search run times were long but not unreasonable these results suggest that genetic algorithms are becoming practical for pattern classification problems as faster serial and parallel computers are developed
dept of electrical engineering stanford university stanford ca storkpsych stanfordedu learning can increase the rate of evolution of a population of biological organisms the baldwin effect our simulations show that in a population of artificial neural networks solving a pattern recognition problem no learning or too much learning leads to slow evolution of the genes whereas an intermediate amount is optimal moreover for a given total number of training presentations fastest evoution occurs if different individuals within each generation receive different numbers of presentations rather than equal numbers because genetic algorithms gas help avoid local minima in energy functions our hybrid learning ga systems can be applied successfully to complex highdimensional pattern recognition problems

given some training data how should we choose a particular network classifier from a family of networks of different complexities in this paper we discuss how the application of stochastic complexity theory to classifier design problems can provide some insights into this problem in particular we introduce the notion of admissible models whereby the complexity of models under consideration is affected by among other factors the class entropy the amount of training data and our prior belief in particular we discuss the implications of these results with respect to neural architectures and demonstrate the approach on real data from a medical diagnosis task i
we introduce a method for the efficient design of a boltzmann machine or a hopfield net that computes an arbitrary given boolean function f this method is based on an efficient simulation of acyclic circuits with threshold gates by boltzmann machines as a consequence we can show that various concrete boolean functions f that are relevant for classification problems can be computed by scalable boltzmann machines that are guaranteed to converge to their global maximum configuration with high probability after constantly many steps i
we present and compare learning rate schedules for stochastic gradient descent a general algorithm which includes lms on line backpropagation and k means clustering as special cases we introduce search thenconverge type schedules which outperform the classical constant and running average lt schedules both in speed of convergence and quality of solution
in this paper we prove that the vectors in the lvq learning algorithm converge we do this by showing that the learning algorithm performs stochastic approximation convergence is then obtained by identifying the appropriate conditions on the learning rate and on the underlying statistics of the classification problem we also present a modification to the learning algorithm which we argue results in convergence of the lvq error to the bayesian optimal error as the appropriate parameters become large i


this paper explores the effect of initial weight selection on feed forward networks leaming simple functions with the back propagation technique we first demonstrate through the use of monte carlo techniques that the magnitude of the initial condition vector in weight space is a very significant parameter in convergence time variability in order to further understand this result additional deterministic experiments were performed the results of these experiments demonstrate the extreme sensitivity of back propagation to initial weight configuration i

inspired by the information theoretic idea of minimum description length we add a term to the back propagation cost function that penalizes network complexity we give the details of the procedure called weight elimination describe its dynamics and clarify the meaning of the parameters involved from a bayesian perspective the complexity term can be usefully interpreted as an assumption about prior distribution of the weights we use this procedure to predict the sunspot time series and the notoriously noisy series of currency exchange rates i

for a simple linear case a mathematical analysis of the training and generalization validation performance of networks trained by gradient descent on a least mean square cost function is provided as a function of the learning parameters and of the statistics of the training data base the analysis predicts that generalization error dynamics are very dependent on a priori initial weights in particular the generalization error might sometimes weave within a computable range during extended training in some cases the analysis provides bounds on the optimal number of training cycles for minimal validation error for a speech labeling task predicted weaving effects were qualitatively tested and observed by computer simulations in networks trained by the linear and non linear back propagation algorithm i
we study the evolution of the generalization ability of a simple linear perceptron with n inputs which learns to imitate a teacher perceptron the system is trained on p an binary example inputs and the generalization ability measured by testing for agreement with the teacher on all n possible binary input patterns the dynamics may be solved analytically and exhibits a phase transition from imperfect to perfect generalization at a except at this point the generalization ability approaches its asymptotic value exponentially with critical slowing down near the transition the relaxation time is o x right at the critical point the approach to perfect generalization follows a power law o t in the presence of noise the generalization ability is degraded by an amount o xrx just above a
while the network loading problem for layer threshold nets is np hard when learning from examples alone as with backpropagation baum has now proved that a learner can employ queries to evade the hidden unit credit assignment problem and pac load nets with up to four hidden units in polynomial time empirical tests show that the method can also learn far more complicated functions such as randomly generated networks with hidden units the algorithm easily approximates wielands spirals function using a single layer of hidden units and requires only minutes of cpu time to learn bit parity to accuracy
we tiescribe a series of careful nulnerical experiments which measure tile average geueralization capability of neural networks trained on a variety of simple functions these experiments are designed to test whether average generalization performance can surpass the worst case bounds obtained from formal learning theory using the vapnik chervonenkis dilnension blumer et al we indeed find that in some cases the average generalization is significantly better than the vc bound the approach to perfect performance is exponential in the number of examples m rather than the m result of the bound in other cases we do find the behavior of the vc bound and in these cases the numerical prefactor is closely related to prefactor contained in the bound
sara a solla att bell laboratories crawfords corner rd holmdel nj usa the learning time of a simple neural network model is obtained through an analytic computation of the eigenvalue spectrum for the hessian matrix which describes the second order properties of the cost function in the space of coupling coefficients the form of the eigenvalue distribution suggests new techniques for accelerating the learning process and provides a theoretical justification for the choice of centered versus biased state variables i
ronald rosenfeld school of computer science carnegie mellon university pittsburgh pa we present a unified framework for a number of different ways of failing to generalize properly during learning sources of random information contaminate the network effectively augmenting the training data with random information the complexity of the function computed is therefore increased and generalization is degraded we analyze replicated networks in which a number of identical networks are independently trained on the same data and their results averaged we conclude that replication almost always results in a decrease in the expected complexity of the network and that replication therefore increases expected generalization simulations confirming the effect are also presented i broken symmetry considered harmful consider a one unit backpropagation network trained on exclusive or without hidden units the problem is insoluble one point where learning would stop is resulting in an mean squared when all weights are zero and the output is always error of but this is a saddle point by placing the discrimination boundary and one with error properly one point can be gotten correctly two with errors of of giving an mse of as shown in figure networks are initialized with small random weights or noise is injected during training to break symmetries of this sort but in breaking this symmetry something has been lost consider a knn classifier constructed from a knn program and the training data anyone who has a copy of the knn program can construct an idenlical classifier if they receive the training data thus considering the classification pearlmutter and rosenteld as an abstract entity we know its complexity cannot exceed that of the training data plus the overhead of the complexity of the program which is fixed but this is not necessarily the case for the backpropagation network we saw because of the
if patterns are drawn from an n dimensional feature space according to a probability distribution that obeys a weak smoothness criterion we show that the probability that a random input pattern is misclassified by a nearest neighbor classifier using m random reference patterns asymptotically satisfies a pmerror poerror for sufficiently large values of m here pooerror denotes the probability of error in the infinite sample limit and is at most twice the error of a bayes classifier although the value of the coefficient a depends upon the underlying probability distributions the exponent of m is largely distribution free we thus obtain a concise relation between a classifiers ability to generalize from a finite reference sample and the dimensionality of the feature space as well as an analytic validation of bellmans well known curse of dimensionality i
we consider different types of single hidden layer feedforward nets with or without direct input to output connections and using either threshold or sigmoidal activation functions the main results show that direct connections in threshold nets double the recognition but not the interpolation power while using sigmoids rather than thresholds allows at least doubling both various results are also given on vc dimension and other measures of recognition capabilities
we develop a new feedforward neural network representation of lipschitz functions from p into based on the level sets of the function we show that npll npl is an upper bound on tile nulnber of nodes needed to represent f to within uniform error st where l is the lipschitz constant ve also show that the number of bits needed to represent the weights in the network in order to achieve this approximation is given by vze conapare this bound with the e entropy of the functional class under consideration i
we introduce a geometric approach for investigating the power of threshold circuits viewing n variable boolean functions as vectors in we invoke tools from linear algebra and linear programming to derive new results on the realizability of boolean functions using threshold gates using this approach one can obtain upper bounds oil the number of spurious memories in iiopfield networks and on the number of functions implementable by a depth d threshold circuit a lower bound on the number of orthogonal input fuuctions required to ilnplement a threshold function a necessary condition for all arbitrary set of input fimctions to implement a threshold filnction a lower bound on the error introduced in approximating boolean functions using sparse polynomials a limit on the effectiveness of the only known lower bound technique based on computing correlations among booleall fimctions for the depth of threshold circuits implementing boolean fimctions and a constructive proof that every boolean function f of n input variables is a threshold function of polynomially many input functions none of which is significantly correlated with f some of these restilts lead to genera lizations of key restilts concerning threshold circuit complexity particularly those that are based on tile so called spectral or iiarmonic analysis approach moreover our geometric approach yields simple proofs based on elementary resnlts fiom linear algebra for many of these earlier results roychowdhury orlitsky siu and kailath
in this paper after some introductory remarks into the classification problem as considered in various research communities and some discussions concerning some of the reasons for ascertaining the performances of the three chosen algorithms viz cart classification and regression tree c one of the more recent versions of a popular induction tree technique known as id and a multi layer perceptron mlp it is proposed to compare the performances of these algorithms under two criteria classification and generalisation it is found that in general the mlp has better classification and generalisation accuracies compared with the other two algorithms i
richard p lippmann lincoln laboratory mit lexington ma seven different pattern classifiers were implemented on a serial computer and compared using artificial and speech recognition tasks two neural network radial basis function and high order polynomial gmdh network and five conventional classifiers gaussian mixture linear tree k nearest neighbor kd tree and condensed k nearest neighbor were evaluated classifiers were chosen to be representative of different approaches to pattern classification and to complement and extend those evaluated in a previous study lee and lippmann this and the previous study both demonstrate that classification error rates can be equivalent across different classifiers when they are powerful enough to form minimum error decision regions when they are properly tuned and when sufficient training data is available practical characteristics such as training time classification time and memory requirements however can differ by orders of magnitude these results suggest that the selection of a classifier for a particular task should be guided not so much by small differences in error rate but by practical considerations concerning memory usage computational resources ease of implementation and restrictions on training and classification times
the performance of seven minimization algorithms are compared on five neural network problems these include a variable step size algorithm conjugate gradient and several methods with explicit analytic or numerical approximations to the hessian i
the problem of color clustering is defined and shown to be a problem of assigning a large number hundreds of thousands of vectors to a small number of clusters finding those clusters in such a way that they best represent a full color image using only distinct colors is a burdensome computational problem in this paper the problem is solved using classical techniques k means clustering vector quantization which tums out to be the same thing in this application competitive learning and kohonen self organizing feature maps quality of the result is judged subjectively by how much the pseudo color result resembles the true color image by rms quantization error and by run time the kohonen map provides the best solution



feedback connections are required so that the teacher signal on the output neurons can modify weights during supervised learning relaxation methods are needed for learning static patterns with full time feedback connections feedback network learning techniques have not achieved wide popularity because of the still greater computational efficiency of back propagation we show by simulation that relaxation networks of the kind we are implementing in vlsi are capable of learning large problems just like back propagation networks a microchip incorporates deterministic mean field theory learning as well as stochastic boltzmann learning a multiple chip electronic system implementing these networks will make high speed parallel learning in them feasible in the future
a high speed implementation of the cmac neural network was designed using dedicated cmos logic this technology was then used to implement two general purpose cmac associative memory boards for the vme bus each board implements up to independent cmac networks with a total of one million adjustable weights each cmac network can be configured to have from i to integer inputs and from i to integer outputs response times for typical cmac networks are well below i millisecond making the networks sufficiently fast for most robot control problems and many pattern recognition and signal processing problems
the adaptive solutions cnaps architecture chip is a general purpose neurocomputer chip it has processors each with k bytes of local memory running at megahertz it is capable of implementing most current neural network algorithms with on chip learning this paper discusses the implementation of the back propagation algorithm on an array of these chips and shows performance figures from a clock accurate hardware simulator an eight chip configuration on one board can update billion connections per second in learning mode and process billion connections per second in feed forward mode
we describe a cmos neural net chip with a reconfigurable network architecture it contains binary programmable connections arranged in building block neurons several building blocks can be connected to form long neurons with up to binary connections or to form neurons with analog connections singleor multi layer networks can be implemented with this chip we have integrated this chip into a board system together with a digital signal processor and fast memory this system is currently in use for image processing applications in which the chip extracts features such as edges and corners from binary and gray level images i
the neocognitron is a neural network for pattern recognition and feature extraction an analog ccd parallel processing architecture developed at lincoln laboratory is particularly well suited to the computational requirements of shared weight networks such as the neocognitron and implementation of the neocognitron using the ccd architecture was simulated a modification to the neocognitron training procedure which improves network performance under the limited arithmetic precision that would be imposed by the ccd architecture is presented
a massively parallel all digital stochastic architecture tinman hi is described which performs competitive and kohonen types of learning a vlsi design is shown for a tlnmahihl neuron which fits within a small inexpensive mosis tinychip frame yet which can be used to build larger networks of several hundred neurons the neuron operates at a speed of mhz which allows the network to process training examples per second use of level sensitive scan logic provides the chip with fault coverage permitting very reliable neural systems to be built
during waking and sleep the brain and mind undergo a tightly linked and precisely specified set of changes in state at the level of neurons this process has been modeled by variations of volterra lotka equations for cyclic fluctuations of brainstem cell populations however neural network models based upon rapidly developing knowledge of the specific population connectivities and their differential responses to drugs have not yet been developed furthermore only the most preliminary attempts have been made to model across states some of our own attempts to link rapid eye movement rem sleep neurophysiology and dream cognition using neural network approaches are summarized in this paper
based on a general non stationary point process model we computed estimates of the synaptic coupling strength efficacy as a function of time after stimulus onset between an inhibitory interneuron and its target postsynaptic cell in the feline dorsal cochlear nucleus the data consist of spike trains from pairs of neurons responding to brief tone bursts recorded in vivo our results suggest that the synaptic efficacy is non stationary further synaptic efficacy is shown to be inversely and approximately linearly related to average presynaptic spike rate a second order analysis suggests that the latter result is not due to non linear interactions synaptic efficacy is less strongly correlated with postsynaptic rate and the correlation is not consistent across neural pairs
recently linsker and mackay and miller have analysed hebbian correlational rules for synaptic development in the visual system and miller has studied such rules in the case of two populations of fibres particularly two eyes millers analysis has so far assumed that each of the two populations has exactly the same correlational structure relaxing this constraint by considering the effects of small perturbative correlations within and between eyes permits study of the stability of the solutions we predict circumstances in which qualitative changes are seen including the production of binocularly rather than monocularly driven units
we develop a model independent method for characterizing the reliability of neural responses to brief stimuli this approach allows us to measure the discriminability of similar stimuli based on the real time response of a single neuron neurophysiological data were obtained from a movementsensitive neuron h in the visual system of the blowfly caiiiphora erythrocephaia furthermore recordings were made from blowfly photoreceptor cells to quantify the signal to noise ratios in the peripheral visual system as photoreceptors form the input to the visual system the reliability of their signals ultimately determines the reliability of any visual discrimination task for the case of movement detection this limit can be computed and compared to the h neurons reliability under favorable conditions the performance of the h neuron closely approaches the theoretical limit which means that under these conditions the nervous system adds little noise in the process of computing movement from the correlations of signals in the photoreceptor array i
are single neocortical neurons as powerful as multi layered networks a recent compartmental modeling study has shown that voltage dependent membrane nonlinearities present in a complex dendritic tree can provide a virtual layer of local nonlinear processing elements between synaptic inputs and the final output at the cell body analogous to a hidden layer in a multi layer network in this paper an abstract model neuron is introduced called a clusteron which incorporates aspects of the dendritic cluster sensitivity phenomenon seen in these detailed biophysical modeling studies it is shown using a clusteron that a hebb type learning rule can be used to extract higher order statistics from a set of training patterns by manipulating the spatial ordering of synaptic connections onto the dendritic tree the potential neurobiological relevance of these higher order statistics for nonlinear pattern discrimination is then studied within a full compartmental model of a neocortical pyramidal cell using a training set of high dimensional sparse random patterns i
single nerve cells with static properties have traditionally been viewed as the building blocks for networks that show emergent phenomena in contrast to this approach we study here how the overall network activity can control single cell parameters such as input resistance as well as time and space constants parameters that are crucial for excitability and spariotemporal integration using detailed computer simulations of neocortical pyramidal cells we show that the spontaneous background firing of the network provides a means for setting these parameters the mechanism for this control is through the large conductance change of the membrane that is induced by both non nmda and nmda excitatory and inhibitory synapses activated by the spontaneous background activity
the dendritic trees of cortical pyramidal neurons seem ideally suited to perform local processing on inputs to explore some of the implications of this complexity for the computational power of neurons we simulated a realistic biophysical model of a hippocampal pyramidal cell in which a cold spot a high density patch of inhibitory ca dependent k channels and a colocalized patch of ca channels was present at a dendritic branch point the cold spot induced a nonmonotonic relationship between the strength of the synaptic input and the probability of neuronal fn ing this effect could also be interpreted as an analog stochastic xor
ion channels are the dynamical systems of the nervous system their distribution within the membrane governs not only communication of information between neurons but also how that information is integrated within the cell here an argument is presented for an anti hebbian rule for changing the distribution of voltage dependent ion channels in order to fiatten voltage curvatures in dendrites simulations show that this rule can account for the self organisation of dynamical receptive field properties such as resonance and direction selectivity it also creates the conditions for the faithful conduction within the cell of signals to which the cell has been exposed various possible cellular implementations of such a learning rule are proposed including activity dependent migration of channel proteins in the plane of the membrane i
we consider a noisy bistable single neuron model driven by a periodic external modulation the modulation introduces a correlated switching between states driven by the noise the information flow through the system from the modulation to the output switching events leads to a succession of strong peaks in the power spectrum the signal to noise ratio snr obtained from this power spectrum is a measure of the information content in the neuron response with increasing noise intensity the snr passes through a maximum an effect which has been called stochastic resonance we treat the problem within the framework of a recently developed approximate theory valid in the limits of weak noise intensity weak periodic forcing and low forcing frequency a comparison of the results of this theory with those obtned from a linear system fft is also presented


during visual development projections from retinal ganglion cells rgcs to the lateral geniculate nucleus lgn in cat are refined to produce ocular dominance layering and precise topographic mapping normal development depends upon activity in rgcs suggesting a key role for activity dependent synaptic plasticity recent experiments on prenatal retina show that during early development waves of activity pass across rgcs meister et al we provide the first simulations to demonstrate that such retinal waves in conjunction with hebbian synaptic competition and early arrival of contralateral axons can account for observed patterns of retinogeniculate projections in normal and experimentally treated animals
to test whether the known connectivies of neurons in the lamprey spinal cord are sufficient to account for locomotor rhytbmogenesis a connectionist neural network simulation was done using identical cells connected according to experimentally established patterns it was demonstrated that the network oscillates in a stable manner with the same phase relationships among the neurons as observed in the lamprey the model was then used to explore coupling between identical oscillators it was concluded that the neurons can have a dual role as rhythm generators and as coordinators between oscillators to produce the phase relations observed among segmental oscillators during swimming
animal locomotion patterns are controlled by recurrent neural networks called central pattern generators gpgs although a gpg can oscillate autonomously its rhythm and phase must be well coordinated with the state of the physical system using sensory inputs in this paper we propose a learning algorithm for synchronizing neural and physical oscillators with specific phase relationships sensory input connections are modified by the correlation between cellular activities and input signals simulations show that the learning rule can be used for setting sensory feedback connections to a gpg as well as coupling connections between gpgs central and sensory mechanisms in locomotion control patterns of animal locomotion such as walking swimming and flying are generated by recurrent neural networks that are located in segmental ganglia of invertebrates and spinal cords of vertebrates barnes and gladden these networks can produce basic rhythms of locomotion without sensory inputs and are called central pattern generators cpgs the physical systems of locomotion such as legs fins and wings combined with physical environments have their own oscillatory characteristics therefore in order to realize efficient locomotion the frequency and the phase of oscillation of a gpg must be well coordinated with the state of the physical system for example the bursting patterns of motoneurons that drive a leg muscle must be coordinated with the configuration of the leg its contact with the ground and the state of other legs doya and yoshizawa the oscillation pattern of a cpg is largely affected by proprioceptive inputs it has been shown in crayfish siller et al and lamprey grillnet et al that the oscillation of a cpg is entrained by cyclic stimuli to stretch sensory neurons over a wide range of frequency both negative and positive feedback pathways are found in those systems elucidation of the function of the sensory inputs to gpgs requires computational studies of neural and physical dynamical systems algorithms for the learning of rhythmic patterns in recurrent neural networks have been derived by doya and yoshizawa pearlmutter and williams and zipset in this paper we propose a learning algorithm for synchronizing a neural oscillator to rhythmic input signals with a specific phase relationship it is well known that a coupling between nonlinear oscillators can entrainment their frequencies the relative phase between oscillators is determined by the parameters of coupling and the difference of their intrinsic frequencies for example either in phase or anti phase oscillation results from symmetric coupling between neural oscillators with similar intrinsic frequencies kawato and suzuki efficient locomotion involves subtle phase relationships between physical variables and motor commands accordingly our goal is to derive a learning algorithm that can finely tune the sensory input connections by which the relative phase between physical and neural oscillators is kept at a specific value required by the task learning of synchronization we will deal with the following continuous time model of a cpg network d c s vyt j m kl where xt and gxit i c represent the states and the outputs of cpg neurons and yt k represents sensory inputs we assume that the connection weights w wij are already established so that the network oscillates without sensory inputs the goal of learning is to find the input connection weights v vii that make the network state xt xt xct entrained to the input signal yt yt yst with a specific relative phase an objective function for phase locking the standard way to derive a learning algorithm is to find out an objective function to be minimized if we can approximate the waveforms of xit and yt by sine waves a linear relationship xt pyt specifies a phase locked oscillation of xt and yt for example if we have y sinwt and y coswt then a matrix p vrgspecifies xsinwtrand sinwt even when the wavefor are not sinusoidal nimization of an objective function xictpiyct zt ilxtpytll i kl adaptive synchronization of neural and physical oscillators determines a specific relative phase between xt and yt thus we call p pit a phase lock matrix learning procedure using the above objective function we will derive a learning procedure for phaselocked oscillation of xt and yt first an appropriate phase lock matrix p is identified while the relative phase between xt and yt changes gradually in time then a feedback mechanism can be applied so that the network state xt is kept close to the target waveform p yt suppose we actually have an appropriate phase relationship between xt and yt then the phase lock matrix p can be obtained by gradient descent of et with respect to pit as follows widrow and stearns d oet s pit rl o i rl xit zpisystytt j if the coupling between xt and yt are weak enough their relative phase changes in time unless their intrinsic frequencies are exactly equal and the systems are completely noiseless by modulating the learning coefficient by some performance index of the total system for example the speed of locomotion it is possible to obtain a matrix p that satisfies the requirement of the task once a phase lock matrix is derived we can control xt close to p yt using the gradient of et with respect to the network state s oet xitzpitytt kml the simplest feedback algorithm is to add this term to the cpg dynamics as follows d c s ri d xit xit z wljgjxjt axit zpitytt jl kl the feedback gain a o must be set small enough so that the feedback term does not destroy the intrinsic oscillation of the cpg in that case by neglecting the small additional decay term axit we have d c rixt xit z wisgsxst z apitytt jl kl which is equivalent to the equation with input weights vii apit doya and yoshizawa delayed synchronization we tested the above learning scheme on a delayed synchronization task to find coupling weights between neural oscillators so that they synchronize with a specific time delay we used the following coupled cpg model c c zx t riy t jl kl yt gxt i c where superscripts denote the indices of two cpgs n the goal of learning was to synchronize the waveforms ylt and yt with a time delay at we used as the performance index the learning coefficient u of equation was modulated by the deviation of zt from its running average t using the following equations t o zt t t t t t a b yl y o y c y d figure learning of delayed synchronization of neural oscillators the dotted and solid curves represent yt and yt respectively awithout coupling bat cat cat dat adaptive synchronization of neural and physical oscillators first two cpgs were trained independently to oscillate with sinusoidal waveforms of period t and t using continuous time back propagation learning doya and yoshizawa each cpg was composed of two neurons c with time constants r and output functions g tanh instead of following the two step procedure described in the previous section the network dynamics and the learning equations and were simulated concurrently with parameters a uo and ra figure a shows the oscillation of two cpgs without coupling figures b through e show the phase locked waveforms after learning for time units with different desired delay times zero legged locomotion next we applied the learning rule to the simplest locomotion system that involves a critical phase lock between the state of the physical system and the motor command a zero legged locomotion system as shown in figure a the physical system is composed of a wheel and a weight that moves back and forth on a track fixed radially in the wheel it rolls on the ground by changing its balance with the displacement of the weight in order to move the wheel in a given direction the weight must be moved at a specific phase with the rotation angle of the wheel the motion equations are shown in appendix first a cpg network was trained to oscillate with a sinusoidal waveform of period t doya and yoshizawa the network consisted of one output and two hidden units c with time constants ri and output functions gi tanh next the output of the cpg was used to drive the weight with a force f fmax gxl t the position r and the velocity of the weight and the rotation angle cos sin and the angular velocity of the wheel j were used as sensory feedback inputs yit k after scaling to in order to eliminate the effect of biases in xt and yt we used the following learning equations the rotation speed of the wheel was employed as the performance index zt after smoothing by the following equation d rzt zt tt the learning coefficient was modulated by equations the time constants were rt ry rs and ra each training run was started from a random configuration of the wheel and was finished after ten seconds doya and yoshizawa a b d c o o o figure learning of zero legged locomotion adaptive synchronization of neural and physical oscillators figure b is an example of the motion of the wheel without sensory feedback the rhythms of the cpg and the physical system were not entrained to each other and the wheel wandered left and right figure c shows an example of the wheel motion after runs of training with parameters and a at first the oscillation of the cpg was slowed down by the sensory inputs and then accelerated with the rotation of the wheel in the right direction we compared the patterns of sensory input connections made after learning with wheels of different sizes table i shows the connection weights to the output unit the positive connection from sin forces the weight to the right hand side of the wheel and stabilize clockwise rotation the negative connection from cos with smaller radius fastens the rhythm of the cpg when the wheel rotates too fast and the weight is lifted up the positive input from r with larger radius makes the weight stickier to both ends of the track and slows down the rhythm of the cpg table sensory input weights to the output unit pt k radius cm cm cm cm cm r cos sin j discussion the architectures of cpgs in lower vertebrates and invertebrates are supposed to be determined by genetic information nevertheless the way an animal utilizes the sensory inputs must be adaptive to the characteristics of the physical environments and the changing dimensions of its body parts back propagation through forward models of physical systems can also be applied to the learning of sensory feedback jordan and jacobs however learning of nonlinear dynamics of locomotion systems is a difficult task moreover multi layer back propagation is not appropriate as a biological model of learning the learning rule is similar to the covariance learning rule sejnowski and stanton which is a biological model of long term potentiation of synapses acknowledgements the authors thank allen severston peter rowat and those who gave comments to our poster at nips conference this work was partly supported by grants from the ministry of education culture and science of japan doya and yoshizawa references barnes w jp gladden m h feedback and motor control in invertebrates and vertebrates beckenham britain croom helm doya k yoshizawa s adaptive neural oscillator using continuous time back propagation learning neural networks grillnet s matsushima t the neural network underlying locomotion in lamprey synaptic and cellular mechanisms neuron july jordan m i jacobs r a learning to control an unstable system with forward modeling in touretzky d s ed advances in neural information processing systems san mateo ca morgan kaufmann kawato m suzuki r two coupled neural oscillators as a model of the circadian pacemaker journal of theoretical biology pearlmutter b a learning state space trajectories in recurrent neural networks neural computation sejnowski t j stanton p k covariance storage in the hippocampus in zornetzer s f et al eds an
the dynamic behavior of a network model consisting of all to all excitatory coupled binary neurons with global inhibition is studied analytically and numerically we prove that for random input signals the output of the network consists of synchronized bursts with apparently random intermissions of noisy activity our results suggest that synchronous bursts can be generated by a simple neuronal architecture which amplifies incoming coincident signals this synchronization process is accompanied by dampened oscillations which by themselves however do not play any constructive role in this and can therefore be considered to be an epiphenomenon i
we investigate a model in which excitatory neurons have dynamical thresholds which display both fatigue and potentiation the fatigue property leads to oscillatory behavior it is responsible for the ability of the model to perform segmentation ie decompose a mixed input into staggered oscillations of the activities of the cell assemblies memories affected by it potentiation is responsible for sustaining these staggered oscillations after the input is turned off ie the system serves as a model for short term lnemory it has a limited stm capacity reminiscent of the magical number i
we present the multi state time delay neural network ms tdnn as an extension of the tdnn to robust word recognition unlike most other hybrid methods the ms tdnn embeds an alignment search procedure into the connectionist architecture and allows for word level supervision the resulting system has the ability to manage the sequential order of subword units while optimizing for the recognizer performance in this paper we present extensive new evaluations of this approach over speaker dependent and speaker independent connected alphabet
the focused gamma network is proposed as one of the possible implementations of the gamma neural model the focused gamma network is compared with the focused backpropagation network and tdnn for a time series prediction problem and with adaline in a system identification problem
recently much interest has been generated regarding speech recognition systems based on hidden markov models hmms and neural network nn hybrids such systems attempt to combine the best features of both models the temporal structure of hmms and the discriminative power of neural networks in this work we define a time warping tw neuron that extends the operation of the formal neuron of a back propagation network by warping the input pattern to match it optimally to its weights we show that a single layer network of tw neurons is equivalent to a gaussian nsity hmmbased recognition system and we propose to unprove the discriminative power of this system by using back propagation discriminative training andor by generalizing the structure of the recognizer to a multi layered net the performance of the proposed network was evaluated on a highly confusable isolated word multi speaker recognition task the results indicate that not only does the recognition performance improve but the separation between classes is enhanced also allowing us to set up a rejection criterion to improve the confidence of the system i

issues relating to the estimation of hidden markov model hmm local probabilities are discussed in particular we note the isomorphism of radial basis functions rbf networks to tied mixture density modelling additionally we highlight the differences between these methods arising from the different training criteria employed we present a method in which connectionist training can be modified to resolve these differences and discuss some preliminary experiments finally we discuss some outstanding problems with discriminative training i
the subject of this paper is the integration of multi layered artificial neural networks ann with probability density functions such as gaussian mixtures found in continuous density hidden markov models hmm in the first part of this paper we present an annhmm hybrid in which all the parameters of the the system are simultaneously optimized with respect to a single criterion in the second part of this paper we study the relationship between the density of the inputs of the network and the density of the outputs of the networks a few experiments are presented to explore how to perform density estimation with anns i
we present janus a speech to speech translation system that utilizes diverse processing strategies including connectionist learning traditional ai knowledge representation approaches dynamic programming and stochastic techniques janus translates continuously spoken english and german into german english and japanese janus currently achieves translation fidelity from english speech and from german speech we present the janus system along with comparative evaluations of its interchangeable processing components with special emphasis on the connectionist modules also with university of karlsruhe karlsruhe gemany now with aliiant techsystems research and technology center hopkins minnesota waibel et al
we propose a paradigm for modeling speech production based on neural networks we focus on characteristics of the musculoskeletal system using real physiological data articulator movements and emg from muscle activity a neural network learns the forward dynamics relating motor commands to muscles and the ensuing articulator behavior after learning simulated perturbations were used to asses properties of the acquired model such as natural frequency damping and interarticulator couplings finally a cascade neural network is used to generate continuous motor commands from a sequence of discrete articulatory targets
a recognition system is reported which recognizes names spelled over the telephone with brief pauses between letters the system uses separate neural networks to locate segment boundaries and classify letters the letter scores are then used to search a database of names to find the best scoring name the speaker independent classification rate for spoken letters is the system retrieves the correct name spelled with pauses between letters of the time from a database of names i
this paper presents parsec a system for generating connectionist parsing networks from example parses parsec is not based on formal grammar systems and is geared toward spoken language tasks parsec networks exhibit three strengths important for application to speech processing they learn to parse and generalize well compared to handcoded grammars they tolerate several types of noise they can learn to use multi modal input presented are the parsec architecture and performance analyses along several dimensions that demonstrate parsecs features parsecs performance is compared to that of traditional grammar based parsing systems
this paper considers the problem of expressing predicate calculus in connectionist networks that are based on energy minimization given a firstorder logic knowledge base and a bound k a symmetric network is constructed like a boltzman machine or a hopfield network that searches for a proof for a given query if a resolution based proof of length no longer than k exists then the global minima of the energy function that is associated with the network represent such proofs the network that is generated is of size cubic in the bound k and linear in the knowledge size there are no restrictions on the type of logic formulas that can be represented the network is inherently fault tolerant and can cope with inconsistency and nonmonotonicity

we present a parallel distributed semantic pds network architecture that addresses the problems of sequencing and ambiguity resolution in natural language understanding a pds network stores phrases and their meanings using multiple pdp networks structured in the form of a semantic net a mechanism called propagation filters is employed to control communication between networks to properly sequence the components of a phrase and to resolve ambiguities simulation results indicate that pds networks and propagation filters can successfully represent high level knowledge can be trained relatively quickly and provide for parallel inferencing at the knowledge level
we have developed a four language automatic language identification system for high quality speech the system uses a neural network based segmentation algorithm to segment speech into seven broad phonetic categories phonetic and prosodic features computed on these categories are then input to a second network that performs the language classification the system was trained and tested on separate sets of speakers of american english japanese mandarin chinese and tamil it currently performs with an accuracy of on the utterances of the test set i
i present a modular network architecture and a learning algorithm based on incremental dynamic programming that allows a single learning agent to learn to solve multiple markovian decision tasks mdts with significant transfer of learning across the tasks i consider a class of mdts called composite tasks formed by temporally concatenating a number of simpler elemental mdts the architecture is trained on a set of composite and elemental mdts the temporal structure of a composite task is assumed to be unknown and the architecture learns to produce a temporal decomposition it is shown that under certain conditions the solution of a composite mdt can be constructed by computationally inexpensive modifications of the solutions of its constituent elemental mdts
this paper examines whether temporal difference methods for training connectionist networks such as suttonss td algorithm cn be successfully applied to complex real world problems a number of important practical issues are identified and discussed from a general theoretical perspective these practical issues are then examined in the context of a case study in which td is applied to learning the game of backgammon from the outcome of self play this is apparently the first application of this algorithm to a complex nontrivial task it is found that with zero knowledge built in the network is able to learn from scratch to play the entire game at a fairly strong intermediate level of performance which is clearly better than conventional commercial programs and which in fact surpasses comparable networks trained on a massive human expert data set the hidden units in these network have apparently discovered useful features a longstanding goal of computer games research furthermore when a set of hand crafted features is added to the input representation the resulting networks reach a near expert level of performance and have achieved good results against world class human play i
harmonet a system employing connectionist networks for music processing is presented after being trained on some dozen bach chorales using error backpropagation the system is capable of producing four part chorales in the style of jsbach given a one part melody our system solves a musical real world problem on a performance level appropriate for musical practice harmonets power is based on a a new coding scheme capturing musically relevant information and b the integration of backpropagation and symbolic algorithms in a hierarchical system combining the advantages of both

a network model with temporal sequencing and state dependent modulatory features is described the model is motivated by neurocognitive data characterizing different states of waking and sleeping computer studies demonstrate how unique states of sequencing can exist within the same network under different aminergic and cholinergic modulatory influences relationships between state dependent modulation memory sequencing and learning are discussed
do you want your neural net algorithm to learn sequences do not limit yourself to conventional gradient descent or approximations thereof instead use your sequence learning algorithm any will do to implement the following method for history compression no matter what your final goals are train a network to predict its next input from the previous ones since only unpredictable inputs convey new information ignore all predictable inputs but let all unexpected inputs plus information about the time step at which they occurred become inputs to a higher level network of the same kind working on a slower self adjusting time scale go on building a hierarchy of such networks this principle reduces the descriptions of event sequences without loss of information thus easing supervised or reinforcement learning tasks alternatively you may use two recurrent networks to collapse a multi level predictor hierarchy into a single recurrent net experiments show that systems based on these principles can require less computation per time step and many fewer training sequences than conventional training algorithms for recurrent nets finally you can modify the above method such that predictability is not defined in a yes or no fashion but in a continuous fashion schmidhuber
there exist large classes of time series such as those with nonlinear moving average components that are not well modeled by feedforward networks or linear models but can be modeled by recurrent networks we show that recurrent neural networks are a type of nonlinear autoregressive moving average narma model practical ability will be shown in the results of a competition sponsored by the puget sound power and light company where the recurrent networks gave the best performance on electric load forecasting i
second order recurrent networks that recognize simple finite state languages over are induced from positive and negative examples using the complete gradient of the recurrent network and sufficient training examples to constrain the definition of the language to be induced solutions are obtained that correctly recognize strings of arbitrary length a method for extracting a finite state automaton corresponding to an optimized network is demonstrated i
simple second order recurrent networks are shown to readily learn small known regular grammars when trained with positive and negative strings examples we show that similar methods are appropriate for learning unknown grmnmars from examples of their strings the training algorithm is an incremental real time recurrent learning rtrl method that computes the complete gradient and updates the weights at the end of each string after or during training a dynamic clustering algorithm extracts the production rules that the neural network has learned the methods are illustrated by extracting rules from unknown deterministic regular grammars for many cases the extracted grammar outperforms the neural net from which it was extracted in conectly classifying unseen strings
we present a framework for programming the hidden unit representations of simple recurrent networks based on the use of hint units additional targets at the output layer we present two ways of analysing a network trained within this framework input patterns act as operators on the information encoded by the context units symmetrically patterns of activation over the context units act as curried functions of the input sequences simulations demonstrate that a network can learn to represent three different functions simultaneously and canonical discriminant analysis is used to investigate how operators and curried functions are represented in the space of hidden unit activations
the two well known learning algorithms of recurrent neural networks are the back propagation rumelhart el al werbos and the forward propagation williams and zipset the main drawback of back propagation is its off line backward path in time for error cumulation this violates the on line requirement in many practical applications although the forward propagation algorithm can be used in an on line manner the annoying drawback is the heavy computation load required to update the high dimensional sensitivity matrix on operations for each time step therefore to develop a fast forward algorithm is a challenging task in this paper wl proposed a forward learning algorithm which is one order faster only on operations for each time step than the sensitivity matrix algorithm the basic idea is that instead of integrating the high dimensional sensitivity dynamic equation we solve forward in time for its greens function to avoid the redundant computations and then update the weights whenever the error is to be corrected a numerical example for classifying state trajectories using a recurrent network is presented it substantiated the faster speed of the proposed algorithm than the williams and zipsers algorithm i
winner take all wta networks in which inhibitory interconnections are used to determine the most highly activated of a pool of units are an important part of many neural network models unfortunately convergence of normal wta networks is extremely sensitive to the magnitudes of their weights which must be hand tuned and which generally only provide the right amount of inhibition across a relatively small range of initial conditions this paper presents dynamicallyadaptive winner take all dawta networks which use a regulatory unit to provide the competitive inhibition to the units in the network the dawta regulatory unit dynamically adjusts its level of activation during competition to provide the right amount of inhibition to differentiate between competitors and drive a single winner this dynamic adaptation allows dawta networks to perform the winner take all function for nearly any network size or initial condition using on connections in addition the dawta regulatory unit can be biased to find the level of inhibition necessary to settle upon the k most highlyactivated units and therefore serve as a k winners take all network
because eye muscles never cocontract and do not deal with external loads one can write an equation that relates motoneuron firing rate to eye position and velocity a very uncommon situation in the cns the semicircular canals transduce head velocity in a linear manner by using a high background discharge rate imparting linearity to the premotor circuits that generate eye movements this has allowed deducing some of the signal processing involved including a neural network that integrates these ideas are often summarized by block diagrams unfortunately they are of little value in describing the behavior of single neurons a finding supported by neural network models
we have investigated the properties of neurons in inferior temporal it cortex in monkeys performing a pattern matching task simple backpropagation networks were trained to discriminate the various stimulus conditions on the basis of the measured neuronal signal we also trained networks to predict the neuronal response waveforms from the spatial patterns of the stimuli the results indicate that it neurons convey temporally encoded information about both current and remembered patterns as well as about their behavioral context decoding of neuronal signals in visual pattern recognition

we have previously described an unsupervised learning procedure that discovers spatially coherent properties of the world by maximizing the information that parameters extracted from different parts of the sensory input convey about some common underlying cause when given random dot stereograms of curved surfaces this procedure learns to extract surface depth because that is the property that is coherent across space it also learns how to interpolate the depth at one location from the depths at nearby locations becker and hinton in this paper we propose two new models which handle surfaces with discontinuities the first model attempts to detect cases of discontinuities and reject them the second model develops a mixture of expert interpolators it learns to detect the locations of discontinuities and to invoke specialized asymmetric interpolators that do not cross the discontinuities i
we have constructed a recurrent network that stabilizes images of a moving object on the retina of a simulated eye the structure of the network was motivated by the organization of the primate visual target tracking system the basic components of a complete target tracking system were simulated including visual processing sensory motor interface and motor control our model is simpler in structure function and performance than the primate system but many of the complexities inherent in a complete system are present recurrent eye tracking network using a distributed representation of image motion visual processing retinotopic eye maps velocity motor interface estimate of retinal velocity motor conxol i figure the overall structure of the visual tracking model target j eye
networks for reconstructing a sparse or noisy function often use an edge field to segment the function into homogeneous regions this approach assumes that these regions do not overlap or have disjoint parts which is often false for example images which contain regions split by an occluding object cant be properly reconstructed using this type of network we have developed a network that overcomes these limitations using support maps to represent the segmentation of a signal in our approach the support of each region in the signal is explicitly represented results from an initial implementation demonstrate that this method can reconstruct images and motion sequences which contain complicated occlusion
network vision systems must make inferences from evidential information across levels of representational abstraction from low level invariants through intermediate scene segments to high level behaviorally relevant object descriptions this paper shows that such networks can be realized as markov random fields mrfs we show first how to construct an mrf functionally equivalent to a hough transform parameter network thus establishing a principled probabilistic basis for visual networks second we show that these mrf parameter networks are more capable and flexible than traditional methods in particular they have a well defined probabilistic interpretation intrinsically incorporate feedback and offer richer representations and decision capabilities
it is shown that both changes in viewing position and illumination conditions can be compensated for prior to recognition using combinations of images taken from different viewing positions and different illumination conditions it is also shown that in agreement with psychophysical findings the computation requires at least a sign bit image as input contours alone are not sufficient i
neurons encoding simple visual features in area v such as orientation direction of motion and color are organized in retinotopic maps however recent physiological experiments have shown that the responses of many neurons in v and other cortical areas are modulated by the direction of gaze we have developed a neural network model of the visual cortex to explore the hypothesis that visual features are encoded in headcentered coordinates at early stages of visual processing new experiments are suggested for testing this hypothesis using electrical stimulations and psychophysical observations i
visual attention is the ability to dynamically restrict processing to a subset of the visual field researchers have long argued that such a mechanism is necessary to efficiently perform many intermediate level visual tasks this paper describes visit a novel neural network model of visual attention the current system models the search for target objects in scenes containing multiple distractors this is a natural task for people it is studied extensively by psychologists and it requires attention the networks behavior closely matches the known psychophysical data on visual search and visual attention visit also matches much of the physiological data on attention and provides a novel view of the functionality of a number of visual areas this paper concentrates on the biological plausibility of the model and its relationship to the primary visual cortex pulvinar superior colliculus and posterior parietal areas
i exhibit a systematic way to derive neural nets for vision problems it involves formulating a vision problem as bayesian inference or decision on a comprehensive model of the visual domain given by a probabilistic grammar i

a combined neural network and rule based approach is suggested as a general framework for pattern recognition this approach enables unsupervised and supervised learning respectively while providing probability estimates for the output classes the probability maps are utilized for higher level analysis such as a feedback for smoothing over the output label maps and the identification of unknown patterns pattern discovery the suggested approach is presented and demonstrated in the texture analysis task a correct classification rate in the percentile is achieved for both unstructured and structured natural texture mosaics the advantages of the probabilistic approach to pattern analysis are demonstrated
visual object recognition involves the identification of images of d objects seen from arbitrary viewpoints we suggest an approach to object recognition in which a view is represented as a collection of points given by their location in the image an object is modeled by a set of d views together with the correspondence between the views we show that any novel view of the object can be expressed as a linear combination of the stored views consequently we build a linear operator that distinguishes between views of a specific object and views of other objects this operator can be implemented using neural network architectures with relatively simple structures
intrator proposed a feature extraction method that is related to recent statistical theory huber friedman and is based on a biologically motivated model of neuronal plasticity bienenstock et al this method has been recently applied to feature extraction in the context of recognizing d objects from single d views intrator and gold here we describe experiments designed to analyze the nature of the extracted features and their relevance to the theory and psychophysics of object recognition
the method of structural risk minimization refers to tuning the capacity of the classifier to the available amount of training data this capacity is influenced by several factors including properties of the input space nature and structure of the classifier and learning algorithm actions based on these three factors are combined here to control the capacity of linear classifiers and improve generalization on the problem of handwritten digit recognition i risk minimization and capacity empirical risk minimization a common way of training a given classifier is to adjust the parameters w in the classification function fx w to minimize the irathing error etrain ie the frequency of errors on a set of p training examples etrain estimates the expected risk based on the empirical data provided by the p available examples the method is thus called empirical risk minimization but the classification function fx w which minimizes the empirical risk does not necessarily minimize the generalization error ie the expected value of the risk over the full distribution of possible inputs and their corresponding outputs such generalization error egene cannot in general be computed but it can be estimated on a separate test set etest other ways of guyon vapnik boser bottou and solla estimating eoee include the leave one ou or moving control method vap for a review see moo capacity and guaranteed risk any family of classification functions fx w can be characterized by its capacity the vapnik chervonenkis dimension or vc dimension vap is such a capacity defined as the maximum number h of training examples which can be learnt without error for all possible binary labelings the vc dimension is in some cases simply given by the number of free parameters of the classifier but in most practical cases it is quite difficult to determine it analytically the vc theory provides bounds let fx w be a set of classification functions of capacity h with probability r for a number of training examples p h simultaneously for all classification functions fx w the generalization error eoene is lower than a guaranteed risk defined by eoaarant etrain ep h etrain rl where eph etri is proportional to e hlnph rlp for small etri and to v for etrain close to one vapvap for a fixed number of training examples p the training error decreases monotonically as the capacity h increases while both guaranteed risk and generalization error go through a minimum before the minimum the problem is overdetermined the capacity is too small for the amount of training data beyond the minimum the problem is underdetermined the key issue is therefore to match the capacity of the classifier to the amount of training data in order to get best generalization performance the method of structural risk minimization srm vapvap provides a way of achieving this goal structural risk minimization let us choose a family of classifiers fxw and define a structure consisting of nested subsets of elements of the family c c c c by defining such a structure we ensure that the capacity h of the subset of classifiers is less than h of subset the method of srm amounts to finding the subset s pt for which the classifier fxw which minimizes the empirical risk within such subset yields the best overall generalization performance two problems arise in implementing srm i how to select opt ii how to find a good structure problem i arises because we have no direct access to in our experiments we will use the minimum of either ett or eaa at to select s pt and show that these two minima are very close a good structure reflects the a priori knowledge of the designer and only few guidelines can be provided from the theory to solve problem ii the designer must find the best compromise between two competing terms et i and reducing h causes to decrease but to increase a good structure should be such that decreasing the vc dimension happens at the expense of the smallest possible increase in training error we now examine several ways in which such a structure can be built structural risk minimization or character recognition principal component analysis optimal brain damage and weight decay consider three apparently different methods of improving generalization performance principal component analysis a preprocessing transformation of input space the optimal brain damage an architectural modification through weight pruning lds and a regularization method weight decay a modification of the learning algorithm vap for the case of a linear classifier these three approaches are shown here to control the capacity of the learning system through the same underlying mechanism a reduction of the effective dimension of weight space based on the curvature properties of the mean squared error mse cost function used for training linear classifier and mse training consider a binary linear classifier fx w wtx where w t is the transpose of w and the function takes two values and indicating to which class x belongs the vc dimension of such classifier is equal to the dimension of input space i or the number of weights h dimw dimx n the empirical risk is given by p yk wtxk p kl where x is the k th example and y is the corresponding desired output the problem of minimizing etrain as a function of w can be approached in different ways dh but it is often replaced by the problem of minimizing a mean square error mse cost function which differs from in that the nonlinear function has been removed curvature properties of the mse cost function the three structures that we investigate rely on curvature properties of the mse cost function consider the dependence of mse on one of the parameters wi training leads to the optimal value w for this parameter one way of reducing the capacity is to set wi to zero for the linear classifier this reduces the vcdimension by one h dimw n the mse increase resulting from setting wi is to lowest order proportional to the curvature of the mse at wt since the decrease in capacity should be achieved at the smallest possible expense in mse increase directions in weight space corresponding to small mse curvature are good candidates for elimination the curvature of the mse is specified by the hessian matrix h of second derivatives of the mse with respect to the weights for a linear classifier the hessian matrix is given by twice the correlation matrix of the training inputs h p xxt the hessian matrix is symmetric and can be diagonalized to get rid of cross terms we assume for simplicity that the first component of vector x is constant and set to so that the corresponding weight introduces the bias value guyon vapnik boser bottou and solla to facilitate decisions about the simultaneous elimination of several directions in weight space the elements of the hessian matrix after diagonalization are the eigenvalues ki the corresponding eigenvectors give the principal directions w of takes a the mse in the rotated axis the increase amse due to setting w i simple form zxmse w the quadratic approximation becomes an exact equality for the linear classifier corresponding to small eigenvalues ki of h are good candiprincipal directions w i dates for elimination principal component analysis one common way of reducing the capacity of a classifier is to reduce the dimension of the input space and thereby reduce the number of necessary free parameters or weights principal component analysis pca is a feature extraction method based on eigenvalue analysis input vectors x of dimension n are approximated by a linear combination of m n vectors forming an ortho normal basis the coefficients of this linear combination form a vector x t of dimension m the optimal basis in the least square sense is given by the rn eigenvectors corresponding to the rn largest eigenvalues of the correlation matrix of the training inputs this matrix is of h a structure is obtained by ranking the classifiers according to m the vc dimension of the classifier is reduced to h dimx m optimal brain damage for a linear classifier pruning can be implemented in two different but equivalent ways i change input coordinates to a principal axis representation prune the components corresponding to small eigenvalues according to pca and then train with the mse cost function ii change coordinates to a principal axis representation train with mse first and then prune the weights to get a weight vector w of dimension rn n procedure i can be understood as a preprocessing whereas procedure ii involves an a posterjori modification of the structure of the classifier network architecture the two procedures become identical if the weight elimination in ii is based on a smallest eigenvalue criterion procedure ii is very reminiscent of optimal brain damage obd a weight pruning procedure applied after training in obd the best candidates for pruning are those weights which minimize the increase amse defined in equation the rn weights that are kept do not necessarily correspond to the largest rn eigenvalues due to the extra factor of w in equation in either implementation the vc dimension is reduced to h dimw dimx m weight decay capacity can also be controlled through an additional term in the cost function to be minimized simultaneously withqe linear classifiers can be ranked according to the norm ilwll zj w of the weight vector a structure is constructed structural risk minimization or character recognition by allowing within the subset only those classifiers which satisfy ilwll aor the positive bounds cr form an increasing sequence c ca cr this sequence can be matched with a monotonically decreasing sequence of positive lagrange multipliers x a such that our training problem stated as the minimization of mse within a specific set s is implemented through the minimization of a new cost function mse iwi a this is equivalent to the weight decay procedure wd in a mechanical analogy the term rllwl a is like the energy of a spring of tension which pulls the weights to zero as it is easier to pull in the directions of small curvature of the mse wd pulls the weights to zero predominantly along the principal directions of the hessian matrix h associated with small eigenvalues in the principal axis representation the minimum wv of the cost function mse wll a is a simple function of the minimum w of the mse in the limit w wxix the weight w is attenuated by a factor xixi weights become negligible for xi and remain unchanged for xi the effect of this attenuation can be compared to that of weight pruning pruning all weights such that i reduces the capacity to h ovai i where ovu i if u and ovu otherwise by analogy we introduce the weight decay capacity i i this expression arises in various theoretical frameworks moomck and is valid only for broad spectra of eigenvalues smoothing higher order units and regularization combining several different structures achieves further performance improvements the combination of exponential smoothing a preprocessing transformation of input space and regularization a modification of the learning algorithm is shown here to improve character recognition the generalization ability is dramatically improved by the further
we developed a neural net architecture for segmenting complex images ie to localize two dimensional geometrical shapes in a scene without prior knowledge of the objects positions and sizes a scale variation is built into the network to deal with varying sizes this algorithm has been applied to video images of railroad cars to find their identification numbers over of the characters were located correctly in a data base of images despite a large variation in lighting conditions and often a poor quality of the characters a part of the network is executed on a processor board containing an analog neural net chip graf et al while the rest is implemented as a software model on a workstation or a digital signal processor
we present a feed forward network architecture for recognizing an unconstrained handwritten multi digit string this is an extension of previous work on recognizing isolated digits in this architecture a single digit recognizer is replicated over the input the output layer of the network is coupled to a viterbi alignment module that chooses the best interpretation of the input training errors are propagated through the viterbi module the novelty in this procedure is that segmentation is done on the feature maps developed in the space displacement neural network sdnn rather than the input pixel space i
we present a neural network algorithm that simultaneously performs segmentation and recognition of input patterns that self organizes to detect input pattern locations and pattern boundaries we demonstrate this neural network architecture on character recognition using the nist database and report on results herein the resulting system simultaneously segments and recognizes touching or overlapping characters broken characters and noisy images with high accuracy

hand printed digits can be modeled as splines that are governed by about control points for each known digit the control points have preferred home locations and deformations of the digit are generated by moving the control points away from their home locations images of digits can bc produced by placing gaussian ink generators uniformly along the spline real images can be recognized by finding the digit model most likely to have generated the data for each digit model we use an elastic matching algorithm to minimize an energy function that includes both the deformation energy of the digit model and the log probability that the model would generate the inked pixels in the image the model with the lowest total energy wins if a uniform noise process is included in the model of image generation some of the inked pixels can be rejected as noise as a digit model is fitting a poorly segmented image the digit models learn by modifying the home locations of the control points i
a method is described for generating plan like reflexive obstacle avoidance behaviour in a mobile robot the experiments reported here use a simulated vehicle with a primitive range sensor avoidance behaviour is encoded as a set of continuous functions of the perceptual input space these functions are stored using cmacs and trained by a variant of bano and suttons adaptive critic algorithm as the vehicle explores its surroundings it adapts its responses to sensory stimuli so as to minimise the negative reinforcement arising from collisions strategies for local navigation are therefore acquired in an explicitly goal driven fashion the resulting trajectories form elegant collisionfree paths through the environment i
whenever an agent learns to control an unknown environment two opposing principles have to be combined namely exploration long term optimization and exploitation short term optimization many real valued connectionist approaches to learning control realize exploration by randomness in action selection this might be disadvantageous when costs are assigned to negative experiences the basic idea presented in this paper is to make an agent explore unknown regions in a more directed manner this is achieved by a so called competence map which is trained to predict the controllers accuracy and is used for guiding exploration based on this a bistable system enables smoothly switching attention between two behaviors exploration and exploitation depending on expected costs and knowledge gain the appropriateness of this method is demonstrated by a simple robot navigation task
a neural network solution is proposed for solving path planning problems faced by mobile robots the proposed network is a two dimensional sheet of neurons forming a distributed representation of the robots workspace lateral interconnections between neurons are cooperative so that the network exhibits oscillatory behaviour these oscillations are used to generate solutions of bellmans dynamic programming equation in the context of path planning simulation experiments imply that these networks locate global optimal paths even in the presence of substantial levels of circuit noise dynamic programming and path planning consider a dof robot moving about in a dimensional world a robots location is denoted by the real vector p the collection of all locations forms a set called the workspace an admissible point in the workspace is any location which the robot may occupy the set of all admissible points is called the free workspace the free workspaces complement represents a collection of obstacles the robot moves through the workspace along a path which is denoted by the parameterized curve pt an admissible path is one which lies wholly in the robots free workspace assume that there is an initial robot position p and a desired final position the robot path planning problem is to find an admissible path with p and p j as endpoints such that some optimality criterion is satisfied the path planning problem may be stated more precisely from an optimal control lemmon theorists viewpoint treat the robot as a dynamic system which is characterized by a state vector p and a control vector u for the highest levels in a control hierarchy it can be assumed that the robots dynamics are modeled by the differential equation u this equation says that the state velocity equals the applied control to define what is meant by optimal a performance functional is introduced ju lipq prl cp utudt where ilxll is the norm of vector x and where the functional cp is unity if p lies in the free workspace and is infinite otherwise this weighting functional is used to insure that the control does not take the robot into obstacles equation ls optimality criterion minimizes the robots control effort while penalizing controls which do not satisfy the terminal constraints with the preceding definitions the optimal path planning problem states that for some final time q find the control ut which minimizes the performance functional ju one very powerful method for tackling this minimization problem is to use dynamic programming bryson according to dynamic programming the optimal control uopt is obtained from the gradient of an optimal return function jop in other words uopt xj the optimal return functional satisfies the hamilton jacobi bellman hjb equation for the dynamic optimization problem given above the hjb equation is easily shown to be oj xjtzj cp cp this is a first order nonlinear partial differential equation pde with terminal boundary condition jtf lipqpi once equation has been solved for the jo then the optimal path is determined by following the gradient of jo solutions to equation must generally be obtained numerically one solution approach numerically integrates a full discretization of equation backwards in time using the terminal condition joq as the starting point the proposed numerical solution is attempting to find characteristic trajectories of the nonlinear first order pde the pde nonlinearities however only insure that these characteristics exist locally ie in an open neighborhood about the terminal condition the resulting numerical solutions are therefore only valid in a local sense this is reflected in the fact that truncation errors introduced by the discretization process will eventually result in numerical solutions violating the underlying principle of optimality embodied by the hjb equation in solving path planning problems local solutions based on the numerical integration of equation are not acceptable due to the local nature of the resulting solutions global solutions are required and these may be obtained by solving an associated variational problem benton assume that the optimal return function at time ty is known on a closed set b the variational solution for equation states that the optimal return at time t ty at a point p in the neighborhood of the boundary set b will be given by jpt min jy tf pyla yes q t oscillatory neural fields for globally optimal path planning where ilpll denotes the l norm of vector p equation is easily generalized to other vector norms and only applies in regions where cp ie the robots free workspace for obstacles jpt jp tf for all t tf in other words the optimal return is unchanged in obstacles oscillatory neural fields the proposed neural network consists of mn neurons arranged as a d sheet called a neural field the neurons are put in a one to one correspondence with the ordered pairs ij where i n and j m the ordered pair ij will sometimes be called the ijth neurons label associated with the i jth neuron is a set of neuron labels denoted by nid the neurons whose labels lie in ni are called the neighbors of the i jth neuron the ijth neuron is characterized by two states the short term activity sta state xij is a scalar representing the neurons activity in response to the currently applied stimulus the long term activity lta state wij is a scalar representing the neurons average activity in response to recently applied stimuli each neuron produces an output fxij which is a unit step function of the sta state ie fix ifx and fx ifx a neuron will be called active or inactive if its output is unity or zero respectively each neuron is also characterized by a set of constants these constants are either externally applied inputs or internal parameters they are the disturbance yij the rate constant xij and the position vector pij the position vector is a d vector mapping the neuron onto the robots workspace the rate constant models the sta states underlying dynamic time constant the rate constant is used to encode whether or not a neuron maps onto an obstacle in the robots workspace the external disturbance is used to initiate the networks search for the optimal path the evolution of the sta and lta states is controlled by the state equations these equations are assumed to change in a synchronous fashion the sta state equation is y dktfxkt where the summation is over all neurons contained within the neighborhood nii of the ijth neuron the function gx is zero if x and is x if x this function is used to prevent the neurons activity level from falling below zero dkt are network parameters controlling the strength of lateral interactions between neurons the lta state equation is w equation means that the lta state is incremented by one every time the i jth neurons output changes specific choices for the interconnection weights result in oscillatory behaviour the specific network under consideration is a cooperative field where d i if k l lemmon ij and dtt a if kl ij without loss of generality it will also be assumed that the external disturbances are bounded between zero and one it is also assumed that the rate constants aij are either zero or unity in the path planning application rate constants will be used to encode whether or not a given neuron represents an obstacle or a point in the free workspace consequently any neuron where aij will be called an obstacle neuron and any neuron where xij will be called a free space neuron under these assumptions it has been shown lemmon a that once a free space neuron turns active it will be oscillating with a period of provided it has at least one free space neuron as a neighbor path planning and neural fields the oscillatory neural field introduced above can be used to generate solutions of the bellman iteration eq with respect to the supremum norm assume that all neuron sta and lta states are zero at time assume that the position vectors form a regular grid of points pii ia ja t where a is a constant controlling the grids size assume that all external disturbances but one are zero in other words for a specific neuron with label ij ytt if kl ij and is zero otherwise also assume a neighborhood structure where nij consist of the i jth neuron and its eight nearest neighbors nij i kj lk with these assumptions it has been shown lemmon a that the lta state for the i jth neuron at time n will be given by gn pit where pit is the length of the shortest path from pti and pii with respect to the supremum norm this fact can be seen quite clearly by examining the lta states dynamics in a small closed neighborhood about the kth neuron first note that the lta state equation simply increments the lta state by one every time the neurons sta state toggles its output since a neuron oscillates after it has been initially activated the lta state will represent the time at which the neuron was first activated this time in turn will simply be the length of the shortest path from the site of the initial distrubance in particular consider the neighborhood set for the kth neuron and lets assume that the kth neuron has not yet been activated if the neighbor has been activated with an lta state of a given value then we see that the kth neuron will be activated on the next cycle and we have wtt max winr this is simply a dual form of the bellman iteration shown in equation in other words over the free space neurons we can conclude that the network is solving the bellman equation with respect to the supremum norm in light of the preceding discussion the use of cooperative neural fields for path planning is straightforward first apply a disturbance at the neuron mapping onto the desired terminal position pt and allow the field to generate sta oscillations when the neuron mapping onto the robots current position is activated stop the oscillatory behaviour the resulting lta state distribution for the i jth neuron equals the negative of the minimum distance with respect to the sup norm from that neuron to the initial disturbance the optimal path is then generated by a sequence of controls which ascends the gradient of the lta state distribution oscillatory neural fields for globally optimal path planning fig sta activity waves fig lta distribution several simulations of the cooperative neural path planner have been implemented the most complex case studied by these simulations assumed an array of by neurons several obstacles of irregular shape and size were randomly distributed over the workspace an initial disturbance was introduced at the desired terminal location and sta oscillations were observed a snapshot of the neuronal outputs is shown in figure this figure clearly shows wavefronts of neuronal activity propagating away from the initial disturbance neuron in the upper right hand corner of figure the activity waves propagate around obstacles without any reflections when the activity waves reach the neuron mapping onto the robots current position the sta oscillations were turned off the lta distribution resulting from this particular simulation run is shown in figure in this figure light regions denote areas of large lta state and dark regions denote areas of small lta state the generation of the optimal path can be computed as the robot is moving towards its goal let the robots current position be the ijth neurons position vector the robot will then generate a control which takes it to the position associated with one of the i jth neurons neighbors in particular the control is chosen so that the robot moves to the neuron whose lta state is largest in the neighborhood set nid in other words the next position vector to be chosen is pki such that its lta state is wti max way because of the lta distribution s optimality property this local control strategy is guaranteed to generate the optimal path with respect to the sup norm connecting the robot to its desired terminal position it should be noted that the selection of the control can also be done with an analog neural network in this case the lta lcmmon states of neurons in the neighborhood set nij are used as inputs to a competitively inhibited neural net the competitive interactions in this network will always select the direction with the largest lta state since neuronal dynamics are analog in nature it is important to consider the impact of noise on the implementation analog systems will generally exhibit noise levels with effective dynamic ranges being at most to bits noise can enter the network in several ways the lta state equation can have a noise term lta noise so that the lta distribution may deviate from the optimal distribution in our experiments we assumed that lta noise was additive and white noise may also enter in the selection of the robots controls selection noise in this case the robots next position is the position vector pki such that wki maxyenswy vy where vry is an iid array of stochastic processes simulation results reported below assume that the noise processes vry are positive and uniformly distributed iid processes the
we present two neural network controller leaming schemes based on feedbackerror learning and modular architecture for recognition and control of multiple manipulated objects in the first scheme a gating network is trained to acquire object specific representations for recognition of a number of objects or sets of objects in the second scheme an estimation network is trained to acquire function specific rather than object specific representations which directly estimate physical parameters both recognition networks are trained to identify manipulated objects using somatic andor visual information after learning appropriate motor commands for manipulation of each object are issued by the control networks
the kbann approach uses neural networks to refine knowledge that can be written in the form of simple propositional rules we extend this idea further by presenting the manncon algorithm by which the mathematical equations governing a pid controller determine the topology and initial weights of a network which is further trained using backpropagation we apply this method to the task of controlling the outflow and temperature of a water tank producing statistically significant gains in accuracy over both a standard neural network approach and a non learning pid controller furthermore using the pid knowledge to initialize the weights of the network produces statistically less variation in testset accuracy when compared to networks initialized with small random numbers
a method for transforming performance evaluation signals distal both in space and time into proximal signals usable by supervised learning algorithms presented in jordan jacobs is examined a simple observation concerning differentiation through models trained with redundant inputs as one of their networks is explains a weakness in the original architecture and suggests a modification an internal world model that encodes action space exploration and crucially cancels input redundancy to the forward model is added learning time on an example task cartpole balancing is thereby reduced about to times i
a large class of motor control tasks requires that on each cycle the controller is told its current state and must choose an action to achieve a specified state dependent goal behaviour this paper argues that the optimization of learning rate the number of experimental control decisions before adequate performance is obtained and robustness is of prime importance if necessary at the expense of computation per control cycle and memory requirement this is motivated by the observation that a robot which requires two thousand learning steps to achieve adequate performance or a robot which occasionally gets stuck while learning will always be undesirable whereas moderate computational expense can be accommodated by increasingly powerful computer hardware it is not unreasonable to assume the existence of inexpensive mfiop controllers within a few years and so even processes with control cycles in the low tens of milliseconds will have millions of machine instructions in which to make their decisions this paper outlines a learning control scheme which aims to make effective use of such computational power i memory based learning memory based learning is an approach applicable to both classification and function learning in which all experiences presented to the learning box are explicitly remembered the memory mere is a set of input output pairs mere xyxyxkyk when a prediction is required of the output of a novel input lrquery the memory is searched to obtain experiences with inputs close to lrquery these local neighbours are used to determine a locally consistent output for the query three memory based techniques nearest neighbout kernel regression and local weighted regression are shown in the accompanying figure moore ut nearest neighbout ypredictrquery yi where kernel regression also known as shepards interpoi minimizes zi zquery lation or local weighted avzi yi mem there erages xpedictzquery is a general
the backpropagation algorithm can be used for both recognition and generation of time trajectories when used as a recognizer it has been shown that the performance of a network can be greatly improved by adding structure to the architecture the same is true in trajectory generation in particular a new architecture corresponding to a reversed tdnn is proposed results show dramatic improvement of performance in the generation of hand written characters a combination of tdnn and reversed tdnn for compact encoding is also suggested i
we introduce and demonstrate a bootstrap method for construction of an inverse function for the robot kinematic mapping using only sample configurationspaceworkspace data unsupervised learning clustering techniques are used on pre image neighborhoods in order to learn to partition the configuration space into subsets over which the kinematic mapping is invertible supervised learning is then used separately on each of the partitions to approximate the inverse function the ill posed inverse kinematics function is thereby regularized and a global inverse kinematics solution for the wristless puma manipulator is developed
accurate saccades require interaction between brainstem circuitry and the cerebellum a model of this interaction is described based on kawatos principle of feedback error learning in the model a part of the brainstem the superior colliculus acts as a simple feedback controller with no knowledge of initial eye position and provides an error signal for the cerebellum to correct for eye muscle nonlinearities this teaches the cerebellum modelled as a cmac to adjust appropriately the gain on the brainstem burst generators internal feedback loop and so alter the size of burst sent to the motoneurons with direction only errors the system rapidly learns to make accurate horizontal eye movements from any starting position and adapts realistically to subsequent simulated eye muscle weakening or displacement of the saccadic target

a neurophysiologicaly based model is presented that controls a simulated kinematic arm during goal directed reaches the network generates a quasi feedforward motor command that is learned using training signals generated by corrective movements for each target the network selects and sets the output of a subset of pattern generators during the movement feedback from proprioceptors turns off the pattern generators the task facing individual pattern generators is to recognize when the arm reaches the target and to turn off a distributed representation of the motor command that resembles population vectors seen in io was produced naturally by these simulations
using the double step target displacement paradigm the mechanisms underlying arm trajectory modification were investigated using short msec inter stimulus intervals the resulting hand motions were initially directed in between the first and second target locations the kinematic features of the modified motions were accounted for by the superposition scheme which involves the vectorial addition of two independent point topoint motion units one for moving the hand toward an internally specified location and a second one for moving between that location and the final target location the similarity between the inferred internally specified locations and previously reported measured end points of the first saccades in double step eye movement studies may suggest similarities between perceived target locations in eye and hand motor control
this work discusses various optimization techniques which were proposed in models for controlling arm movements in particular the minimum muscle tension change model is investigated a dynamic simulator of the monkeys arm including seventeen single and double joint muscles is utilized to generate horizontal hand movements the hand trajectories produced by this algorithm are discussed
current inlxa cardia defibnllators make use of simple classification algorithms to determine patient conditions and subsequently to enable proper therapy the simplicity is primarily due to the constraints on power dissipation and area available for implememation sub threshold implementation of artificial neural networks offer potential classifiers with higher performance than commercially available defibrillators in this paper we explore several classifier architectures and discms micro electromc implementation issues
avascular necrosis avn of the femoral head is a common yet potentially serious disorder which can be detected in its very early stages with magnetic resonance imaging we have developed multi layer perceptron networks trained with conjugate gradient optimization which diagnose avn from single magnetic resonance images of the fernoral head with accuracy on training data and accuracy on test data i
automated monitoring of vigilance in attention intensive tasks such as air trafiic control or sonar operation is highly desirable as the operator monitors the instrument the instrument would monitor the operator insuring against lapses we have taken a first step toward this goal by using feedforward neural networks trained with backpropagation to interpret event related potentials elps and electroencephalogram eeg associated with periods of high and low vigilance the accuracy of our system on an erp data set averaged over minutes was better than the accuracy obtained using linear discriminant analysis practical vigilance monitoring will require prediction over shorter time periods we were able to average the elp over as little as minutes and still get correct prediction of a vigilance measure additionally we achieved similarly good performance using segments of eeg power spectrum as short as sec
in a bayesian framework we give a principled account of how domainspecific prior knowledge such as imperfect analytic domain theories can be optimally incorporated into networks of locally tuned units by choosing a specific architecture and by applying a specific training regimen our method proved successful in overcoming the data deficiency problem in a large scale application to devise a neural control for a hot line rolling mill it achieves in this application significantly higher accuracy than optimally tuned standard algorithms such as sigmoidal backpropagation and outperforms the state of the art solution i

this paper deals with an application of neural networks to satellite remote sensing observations because of the complexity of the application and the large amount of data the problem cannot be solved by using a single method the solution we propose is to build multimodules nn architectures where several nn cooperate together such system suffer from generic problem for whom we propose solutions they allow to reach accurate performances for multi valued function approximations and probability estimations the results are compared with six other methods which have been used for this problem we show that the methodology we have developed is general and can be used for a large variety of applications thiria mejia badran and crpon

we present an a pproach for development of a decoder for any complex binary error correcting code ecc via training fiom examples of decoded received words our decoder is a connectionist architecture we describe two separate solutions a systeln level solution the cascaded networks decoder and the ecc enhanced decoder a solution which simplifies the mapping problem which nmst be solved for decoding although both solutions meet our basic approach constraint for simplicity and compactness only the ecc enhanced decoder meets our second basic constraint of being a generic solution
in this paper a tree based neural network viz mars friedman for the modelling of the yield strength of a steel rolling plate mill is described the inputs to the time series model are temperature strain strain rate and interpass time and the output is the corresponding yield stress it is found that the mars based model reveals which variables functional dependence is nonlinear and significant the results are compared with those obtained by using a kalman filter based online tuning method and other classification methods eg cart c bayesian classification it is found that the mars based method consistently outperforms the other methods i
five experiments were performed using several neural network architectures to identify the location of a wave in the time ordered graphical results from a medical test baseline results from the first experiment found correct identification of the target wave in of cases n other experiments investigated the effect of different architectures and preprocessing the raw data on the results the methods used seem most appropriate for time oriented graphical data which has a clear starting point such as electrophoresis or spectrometry rather than continuous tests such as ecgs and eegs
this paper briefly describes an artificial neural network for prettentive visual processing the network is capable of dctermiuing image motion in a type of stimulus which defeats most popular methods of lnotion detection a subset of second order visual motion stimuli known as drift balanced stimulidbs the processing stages of the network described in this paper are integratable into a model capable of simultaneous motion extraction edge detection and the determination of occlusion i
a routing scheme that uses a neural network has been developed that can aid in establishing point to point communication routes through multistage interconnection networks mins the neural network is a network of the type that was examined by hopfield hopfield and in this work the problem of establishing routes through random mins rmins in a shared memory distributed computing system is addressed the performance of the neural network routing scheme is compared to two more traditional approaches exhaustive search routing and greedy routing the results suggest that a neural network router may be competitive for certain rmins i
we have created new networks to unmix signals which have been mixed either with time delays or via filtering we first show that a subset of the hrault jutten learning rules fulfills a principle of minimum output power we then apply this principle to extensions of the hdrault jutten network which have delays in the feedback path our networks perform well on real speech and music signals that have been mixed using time delays or filtering i
a ccd based processor that we call the nnc is presented the nnc implements a fully connected input output two layer network and can be cascaded to form multilayer networks or used in parallel for additional input or output nodes the device computes x e connectionssee when clocked at mhz network weights can be specified to six bits of accuracy and are stored on chip in programmable digital memories a neural network pattern recognition system using nnc and ccd image feature extractor ife devices is described additionally we report a ccd output circuit that exploits inherent nonlinearities in the charge injection process to realize an adjustable threshold sigmoid in a chip area of x i
a ccd based signal processing ic that computes a fully parallel single quadrant vector matrix multiplication has been designed and fabricated with a tm ccdcmos process the device incorporates an array of charge coupled devices ccd which hold an analog matrix of charge encoding the matrix elements input vectors are digital with bit accuracy
biological retinas extract spatial and temporal features in an attempt to reduce the complexity of performing visual tasks we have built and tested a silicon retina which encodes several useful temporal features found in vertebrate retinas the cells in our silicon retina are selective to direction highly sensitive to positive contrast changes around an ambient light level and tuned to a particular velocity inhibitory connections in the null direction perform the direction selectivity we desire this silicon retina is on a x ram die and consists of a x array of photoreceptors
the goal of perception is to extract invariant properties of the underlying world by computing contrast at edges the retina reduces incident light intensities spanning twelve decades to a twentyfold variation in one stroke it solves the dynamic range problem and extracts relative reflectivity bringing us a step closer to the goal we have built a contrastsensitive silicon retina that models all major synaptic interactions in the outer plexiform layer of the vertebrate retina using current mode cmos circuits namely reciprocal synapses between cones and horizontal cells which produce the antagonistic centersurround receptive field and cone and horizontal cell gap junctions which determine its size the chip has x pixels on a x mm die in pm n well technology and is fully functional i
a board is described that contains the anna neural network chip and a dspc digital signal processor the anna analog neural network arithmetic unit chip performs mixed analogdigital processing the combination of anna with the dsp allows high speed end to end execution of numerous signal processing applications including the preprocessing the neural net calculations and the postprocessing steps the anna board evaluates neural networks to times faster than the dsp alone the board is suitable for implementing large million connections networks with sparse weight matrices three applications have been implemented on the board a convolver network for slant detection of text blocks a handwritten digit recognizer and a neural network for recognition based segmentation i

we use constrained optimization to select operating parameters for two circuits a simple transistor square root circuit and an analog vlsi artificial cochlea this automated method uses computer controlled measurement and test equipment to choose chip parameters which minimize the difference between the actual circuits behavior and a specified goal behavior choosing the proper circuit parameters is important to compensate for manufacturing deviations or adjust circuit performance within a certain range as biologically motivated analog vlsi circuits become increasingly complex implying more parameters setting these parameters by hand will become more cumbersome thus an automated parameter setting method can be of great value fleischer automated parameter setting is an integral part of a goal based engineering design methodology in which circuits are constructed with parameters enabling a wide range of behaviors and are then tuned to the desired behaviors automatically i
a novel segmentation algorithm has been developed utilizing an absolutevalue slnoothness penalty instead of the more common quadratic regularizer this functional ilnposes a piece wise constant constraint on the segmented data since the lninilnized energy is guaranteed to be coilvex there are no problems with local lninima and no complex continuation methods are necessary to find the unique global minimum by interpreting the minimized energy as the generalized power of a nonlinear resistive network a continuous time analog segmentation circuit was constructed
we present experimental data from an analog cmos lsi chip that implements the herault jutten adaptive neural network testing procedures and results in time and frequency domain are described these include weight convergence trajectories extraction of a signal in noise and separation of statistically complex signals such as speech i
many auditory theorists consider the temporal adaptation of the auditory nerve a key aspect of speech coding in the auditory periphery experiments with models of auditory localization and pitch perception also suggest temporal adaptation is an important element of practical auditory processing i have designed fabricated and successfully tested an analog integrated circuit that models many aspects of auditory nerve response including temporal adaptation
we demonstrate a self organizing system based on photorefractive ring oscillators we employ the system in two ways that can both be thought of as feature extractors one acts on a set of images exposed repeatedly to the system strictly as a linear feature extractor and the other serves as a signal demultiplexer for fiber optic communications both systems implement unsupervised competitive learning embedded within the mode interaction dynamics between the modes of a set of ring oscillators after a training period the modes of the rings become associated with the different image features or carrier frequencies within the incoming data stream
learning is posed as a problem of function estimation for which two principles of solution are considered empirical risk minimization and structural risk minimization these two principles are applied to two different statements of the function estimation problem global and local systematic improvements in prediction power are illustrated in application to zip code recognition
the bayesian model comparison framework is reviewed and the bayesian occams razor is explained this framework can be applied to feedforward networks making possible objective comparisons between solutions using alternative network architectures objective choice of magnitude and type of weight decay terms quantified estimates of the error bars on network parameters and on network output the framework also generates a measure of the effective number of parameters determined by the data the relationship of bayesian model comparison to recent work on prediction of generalisation ability guyon el ai moody is discussed bayesian inference and occams razor in science a central task is to develop and compare models to account for the data that are gathered typically two levels of inference are involved in the task of data modelling at the first level of inference we assume that one of the models that we invented is true and we fit that model to the data typically a model includes some free parameters fitting the model to the data involves inferring what values those parameters should probably take given the data this is repeated for each model the second level of inference is the task of model comparison here current address darwin college cambridge cb eu uk mackay we wish to compare the models in the light of the data and assign some sort of preference or ranking to the alternatives for example consider the task of interpolating a noisy data set the data set could be interpolated using a splines model polynomials or feedforward neural networks at the first level of inference we find for each individual model the best fit interpolant a process sometimes known as learning at the second level of inference we want to rank the alternative models and state for our particular data set that for example splines are probably the best interpolation model or if the interpolant is modelled as a polynomial it should probably be a cubic or the best neural network for this data set has eight hidden units model comparison is a difficult task because it is not possible simply to choose the model that fits the data best more complex models can always fit the data better so the maximum likelihood model choice leads us inevitably to implausible overparameterised models which generalise poorly occams razor is the principle that states that unnecessarily complex models should not be preferred to simpler ones bayesian methods automatically and quantitatively embody occams razor gull jeffreys without the

in this paper we investigate an average case model of concept learning and give results that place the popular statistical physics and vc dimension theories of learning curve behavior in a common framework i
the complexity of learning in shallow i dimensional neural networks has been shown elsewhere to be linear in the size of the network however when the network has a huge number of units as cortex has even linear time might be unacceptable furthermore the algorithm that was given to achieve this time was based on a single serial processor and was biologically implausible in this work we consider the more natural parallel model of processing and demonstrate an expected time complexity that is constant ie independent of the size of the network this holds even when inter node communication channels are short and local thus adhering to more biological and vlsi constraints i
we report learning measurements from a system composed of a cascadable learning chip data generators and analyzers for training pattern presentation and an x windows based software interface the neuron learning chip has adaptive synapses and can perform boltzmann and mean field learning using separate noise and gain controls we have used this system to do learning experiments on the parity and replication problem the system set fling time limits the learning speed to about patterns per second roughly independent of system size
this paper applies the theory of probably approximately correct pac learning to multiple output feedforward threshold networks in which the weights conform to certain equivalences it is shown that the sample size for reliable learning can be bounded above by a formula similar to that required for single output networks with no equivalences the best previously obtained bounds are improved for all cases i
batch gradient descent aw rldedwt converes to a minimum of quadratic form with a time constant no better than amaxami n where ami n and amax are the minimum and maximum eigenvalues of the hessian matrix of e with respect to w it was recently shown that adding a momentum term awt rldedwt q cawt improves this to xamaxamin although only in the batch case here we show that secondorder momentum aw rldedwt qcawtqfiawt can lower this no further we then regard gradient descent with momentum as a dynamic system and explore a nonquadratic error surface showing that saturation of the error accounts for a variety of effects observed in simulations and justifies some popular heuristics
in many machine learning applications one has access not only to training data but also to some high level a priori knowledge about the desired behavior of the system for example it is known in advance that the output of a character recognizer should be invariant with respect to small spatial distortions of the input images translations rotations scale changes etcetera we have implemented a scheme that allows a network to learn the derivative of its outputs with respect to distortion operators of our choosing this not only reduces the learning time and the amount of training data but also provides a powerful language for specifying what generalizations we wish the network to perform i
we define the concept of polynomial uniform convergence of relative frequencies to probabilities in the distribution dependent context let x let p be a probability distribution on x and let f c x be a family of events the family xp fi has the property of polynomial uniform convergence if the probability that the maximum difference over f between the relative frequency and the probability of an event exceed a given positive e be at most when the sample on which the frequency is evaluated has size polynomial in n e given a t sample xlxt let cntxlxt be the vapnik chervonenkis dimension of the family xlxt fi f lf e fn and mnt the expectation ecntt we show that xnpn fnni has the property of polynomial uniform convergence iff there exists such that mnt ont applications to distribution dependent pac learning are discussed
we study a particular type of boltzmann machine with a bipartite graph structure called a harm nium our interest is in using such a machine to model a probability distribution on binary input vectors we analyze the class of probability distributions that can be modeled by such machines showing that for each n i this class includes arbitrarily good apprximation to any distribution on the set of all n vectors of binary inputs we then present two learning algorithms for these machines the first learning algorithm is the standard gradient ascent heuristic for computing maximum likelihood estimates for the parameters ie weights and thresholds of the model here we give a closed form for this gradient that is significantly easier to compute than the corresponding gradient for the general boltzmann machine the second learning algorithm is a greedy method that creates the hidden units and computes their weights one at a time this method is a variant of the standard method for projection pursuit density estimation we give experimental results for these learning methods on synthetic data and natural data from the domain of handwritten digits i
we present a distribution free model for incremental learning when concepts vary with time concepts are caused to change by an adversary while an incremental learning algorithm attempts to track the changing concepts by minimizing the error between the current target concept and the hypothesis for a single halfplane and the intersection of two half planes we show that the average mistake rate depends on the maximum rate at which an adversary can modify the concept these theoretical predictions are verified with simulations of several learning algorithms including back propagation
a general relationship is developed between the vc dimension and the statistical lower epsilon capacity which shows that the vc dimension can be lower bounded in order by the statistical lower epsilon capacity of a network trained with random samples this relationship explains quantitatively how generalization takes place after memorization and relates the concept of generalization consistency with the capacity of the optimal classifier over a class of classifiers with the same structure and the capacity of the bayesian classifier furthermore it provides a general methodology to evaluate a lower bound for the vc dimension of feedforward multilayer neural networks this general methodology is applied to two types of networks which are important for hardware implementations two layer n l networks with binary weights integer thresholds for the hidden units and zero threshold for the output unit and a single neuron n networks with binary weigths and a zero threshold specifically we obtain w on here w is the total number owe d ow and dl of weights of the n l networks dl and d represent the vcdimensions for the n and nl networks respectively i
this paper will address an important question in machine learning what kind of network architectures work better on what kind of problems a projection pursuit learning network has a very similar structure to a one hidden layer sigmoidal neural network a general method based on a continuous version of projection pursuit regression is developed to show that projection pursuit regression works better on angular smooth functions than on laplacian smooth functions there exists a ridge function approximation scheme to avoid the curse of dimensionality for approximating functions in
an important issue in neural computation is the dynamic range of weights in the neural networks many experimental results on learning indicate that the weights in the networks can grow prohibitively large with the size of the inputs here we address this issue by studying the tradeoffs between the depth and the size of weights in polynomial size networks of linear threshold elements ltes we show that there is an efficient way of simulating a network of ltes with large weights by a network of ltes with small weights in particular we prove that every depth d polynomial size network of ltes with eponentially large integer weights can be simulated by a depth d polynomial size network of ltes with polynomially bounded integer weights to prove these results we use tools from harmonic analysis of boolean functions our technique is quite general it provides insights to some other problems for example we are able to improve the best known results on the depth of a network of linear threshold elements that computes the comparison sum and product of two n bits numbers and the maximum and the sorting of n n bit numbers i
it has been observed in numerical simulations that a weight decay can improve generalization in a feed forward neural network this paper explains why it is proven that a weight decay has two effects in a linear network first it suppresses any irrelevant components of the weight vector by choosing the smallest vector that solves the learning problem second if the size is chosen right a weight decay can suppress some of the effects of static noise on the targets which improves generalization quite a lot it is then shown how to extend these results to networks with hidden layers and non linear units finally the theory is confirmed by some numerical simulations using the data from nettalk i

we describe a neural network called rulenet that learns explicit symbolic condition action rules in a formal string manipulation domain rulenet discovers functional categories over elements of the domain and at various points during learning extracts rules that operate on these categories the rules are then injected back into rulenet and training continues in a process called iterative projection by incorporating rules in this way rulenet exhibits enhanced learning and generalization performance over alternative neural net approaches by integrating symbolic rule learning and subsymbolic category learning rulenet has capabilities that go beyond a purely symbolic system we show how this architecture can be applied to the problem of case role assignment in natural language processing yielding a novel rule based solution
we propose and empirically evaluate a method for the extraction of expertcomprehensible rules from trained neural networks our method operates in the context of a three step process for learning that uses rule based domain knowledge in combination with neural networks empirical tests using realworlds problems from molecular biology show that the rules our method extracts from trained neural networks closely reproduce the accuracy of the network from which they came are superior to the rules derived by a learning system that directly refines symbolic rules and are expert comprehensible
in this paper we present a neural network architecture that discovers a recursive decomposition of its input space based on a generalization of the modular architecture of jacobs jordan nowlan and hinton the architecture uses competition among networks to recursively split the input space into nested regions and to learn separate associative mappings within each region the learning algorithm is shown to perform gradient ascent in a log likelihood function that captures the architectures hierarchical structure
one way of simpliing neural networks so they generalize better is to add an extra term io the error function that will penalize complexity we propose a new penalty term in which the distribution of weight values is modelled as a mixture of multiple gaussians under this model a set of weights is simple if the weights can be clustered into subsets so that the weights in each cluster have similar values we allow the parameters of the mixture model to adapt at t he same time as the network learns simulations demonstrate that this complexity term is more effective than previous complexity terms
an alternative to the typical technique of selecting training examples independently from a fixed distribution is formulated and analyzed in which the current example is presented repeatedly until the error for that item is reduced to some criterion value then another item is randomly selected the convergence time can be dramatically increased or decreased by this heuristic depending on the task and is very sensitive to the value of
stochastic gradient descent is a general algorithm which includes lms on line backpropagation and adaptive k means clustering as special cases the standard choices of the learning rate both adaptive and fixed functions of time often perform quite poorly in contrast our recently proposed class of search then converge learning rate schedules darken and moody display the theoretically optimalasymptotic convergence rate and a superior ability to escape from poor local minima however the user is responsible for setting a key parameter we propose here a new methodology for creating the first completely automatic adaptive learning rates which achieve the optimal rate of convergence
although the detection of invariant structure in a given set of input patterns is vital to many recognition tasks connectionist learning rules tend to focus on directions of high variance principal components the prediction paradigm is often used to reconcile this dichotomy here we suggest a more direct approach to invariant learning based on an anti hebbian learning rule an unsupervised two layer network implementing this method in a competitive setting learns to extract coherent depth information from random dot stereograms
several parallel analogue algorithms based upon mean field theory mft approximations to an underlying statistical mechanics formulation and requiring an externally prescribed annealing schedule now exist for finding approximate solutions to difficult combinatorial optimisation problems they have been applied to the travelling salesman problem tsp as well as to various issues in computational vision and cluster analysis i show here that any given mft algorithm can be combined in a natural way with notions from the areas of constrained optimisation and adaptive simulated annealing to yield a single homogenous and efficient parallel relaxation technique for which an externally prescribed annealing schedule is no longer required the results of numerical simulations on city and city tsp problems are presented which show that the ensuing algorithms are typically an order of magnitude faster than the mft algorithms alone and which also show on occasion superior solutions as well i
one method proposed for improving the generalization capability of a feedforward network trained with the backpropagation algorithm is to use artificial training vectors which are obtained by adding noise to the original training vectors we discuss the connection of such backpropagation training with noise to kernel density and kernel regression estimation we compare by simulated examples backpropagation backpropagation with noise and kernel regression in mapping estimation and pattern classification contexts
connections between spline approximation approximation with rational functions and feedforward neural networks are studied the potential improvement in the degree of approximation in going from single to two hidden layer networks is examined some results of bitman and soomjak regarding the degree of approximation achievable when knot positions are chosen on the basis of the probability distribution of examples rather than the function values are extended
feedforward networks composed of units which compute a sigmoidal function of a weighted sum of their inputs have been much investigated we tested the approximation and estimation capabilities of networks using functions more complex than sigmoids three classes of functions were tested polynomials rational functions and flexible fourier series unlike sigmoids these classes can fit non monotonic functions they were compared on three problems prediction of boston housing prices the sunspot count and robot arm inverse dynamics the complex units attained clearly superior performance on the robot arm problem which is a highly non monotonic pure approximation problem on the noisy and only mildly nonlinear boston housing and sunspot problems differences among the complex units were revealed polynomials did poorly whereas rationals and flexible fourier series were comparable to sigmoids
this paper is concerned with the problem of learning in networks where some or all of the functions involved are not smooth examples of such networks are those whose neural transfer functions are piecewise linear and those whose error function is defined in terms of the norm up to now networks whose neural transfer functions are piecewise linear have received very little consideration in the literature but the possibility of using an earor function defined in terms of the norm has received some attention in this latter work however the problems that can occur when gradient methods are used for nonsmooth error functions have not been addressed in this paper we draw upon some recent results from the field of nonsmooth optimiofion nso to present an algorithm for the nonsmooth case our motivation for this work arose out of the fact that we have been able to show that in backpropagation an error function based upon the norm overcomes the difficulties which can occur when using the norm
we present an iterative algorithm for nonlinear regression based on construction of sparse polynomials polynomials are built sequentially from lower to higher order selection of new terms is accomplished using a novel look ahead approach that predicts whether a variable contributes to the remaining error the algorithm is based on the tree growing heuristic in lms trees which we have extended to approximation of arbitrary polynonrials of the input features in addition we provide a new theoretical justification for this heuristic approach the algorithm is shown to discover a known polynomial from samples and to make accurate estimates of pixel values in an image processing task
a constructive algorithm is proposed for feed forward neural networks which uses node splitting in the hidden layers to build large networks from smaller ones the small network forms an approximate model of a set of training data and the split creates a larger more powerful network which is initialised with the approximate solution already found the insufficiency of the smaller network in modelling the system which generated the data leads to oscillation in those hidden nodes whose weight vectors cover regions in the input space where more detail is required in the model these nodes are identified and split in two using principal component analysis allowing the new nodes to cover the two main modes of each oscillating vector nodes are selected for splitting using principal component analysis on the oscillating weight vectors or by examining the hessian matrix of second derivatives of the network error with respect to the weights the second derivative method can also be applied to the input layer where it provides a useful indication of the relative importances of parameters for the classification task node splitting in a standard multi layer perceptton is equivalent to introducing a hinge in the decision boundary to allow more detail to be learned initial results were promising but further evaluation indicates that the long range effects of decision boundaries cause the new nodes to slip back to the old node position and nothing is gained this problem does not occur in networks of localised receptive fields such as radial basis functions or ganssian mixtures where the technique appears to work well node splitting a contructive algorithm for feed forward neural networks
automatic determination of proper neural network topology by trimming over sized networks is an important area of study which has previously been addressed using a variety of techniques in this paper we present information measure based skeletonisation imbs a new approach to this problem where superfluous hidden units are removed based on their information measure im this measure borrowed from decision tree induction techniques reflects the degree to which the hyperplane formed by a hidden unit discriminates between training data classes we show the results of applying imbs to three classification tasks and demonstrate that it removes a substantial number of hidden units without significantly affecting network performance
gsplines is an algorithm for building functional models of data it uses genetic search to discover combinations of basis functions which are then used to build a least squares regression model because it produces a population of models which evolve over time rather than a single model it allows analysis not possible with other regression based approaches


the probabilistic neural network pnn algorithm represents the likelihood function of a given class as the sum of identical isotropic gaussians in practice pnn is often an excellent pattern classifier outperforming other classifiers including backpropagation however it is not robust with respect to affine transformations of feature space and this can lead to poor performance on certain data we have derived an extension of pnn called weighted pnn wpnn which compensates for this flaw by allowing anisotropic gaussians ie gaussians whose covariance is not a multiple of the identity matrix the covariance is optimized using a genetic algorithm some interesting features of which are its redundant logarithmic encoding and large population size experimental results validate our claims i
we designed and trained a connectionist network to generate letterforms in a new font given just a few exemplars from that font during learning our network constructed a distributed internal representation of fonts as well as letters despite the fact that each training instance exemplified both a font and a letter it was necessary to have separate but interconnected hidden units for letter and font representations m several alternative architectures were not successful
we compare two strategies for training connectionist as well as nonconnectionist models for statistical pattern recognition the probabilistic strategy is based on the notion that bayesian discrimination ie optimal classification is achieved when the classifier learns the a posteriori class distributions of the random feature vector the differential strategy is based on the notion that the identity of the largest class a posteriori probability of the feature vector is all that is needed to achieve bayesian discrimination each strategy is directly linked to a family of objective functions that can be used in the supervised training procedure we prove that the probabilistic strategy linked with error measure objective functions such as mean squared error and cross entropy typically used to train classifiers necessarily requires larger training sets and more complex classifier architectures than those needed to approximate the bayesian discriminant function in contrast we prove that the differential strategy linked with classificationfigure of merit objective functions cfmo requires the minimum classifier functional complexity and the fewest training examples necessary to approximate the bayesian discriminant function with specified precision measured in probability of error we present our proofs in the context of a game of chance in which an unfair c sided die is tossed repeatedly we show that this rigged game of dice is a paradigm at the root of all statistical pattern recognition tasks and demonstrate how a simple extension of the concept leads us to a general information theoretic model of sample complexity for statistical pattern recognition hampshire and kumar
three methods for improving the performance of gaussian radial basis function rbf networks were tested on the nettalk task in rbf a new example is classified by computing its euclidean distance to a set of centers chosen by unsupervised methods the application of supervised learning to learn a non euclidean distance metric was found to reduce the error rate of rbf networks while supervised learning of each centers variance resulted in inferior performance the best improvement in accuracy was achieved by networks called generalized radial basis function grbf networks in grbf the center locations are determined by supervised learning after training on words rbf classifies of letters correct while grbf scores letters correct on a separate test set from these and other experiments we conclude that supervised learning of center locations can be very important for radial basis function learning i
optimizing the performance of self organizing feature maps like the kohonen map involves the choice of the output space topology we present a topographic product which measures the preservation of neighborhood relations as a criterion to optimize the output space topology of the map with regard to the global dimensionality d a as well as to the dimensions in the individual directions we test the topographic product method not only on synthetic mapping examples but also on speech data in the latter application our method suggests an output space dimensionality of d a in coincidence with recent recognition results on the same data set
we present here an interesting experiment in quick modeling by humans performed independently on small samples in several languages and two continents over the last three years comparisons to decision tree procedures and neural net processing are given from these we conjecture that human reasoning is better represented by the latter but substantially different from both implications for the strong convergence hypothesis between neural networks and machine learning are discussed now expanded to include human reasoning comparisons
two projection based feedforward network learning methods for modelfree regression problems are studied and compared in this paper one is the popular back propagation learning bpl the other is the projection pursuit learning ppl unlike the totally parametric bpl method the ppl non parametrically estimates unknown nonlinear functions sequentially neuron by neuron and layer by layer at each iteration while jointly estimating the interconnection weights in terms of learning efficiency both methods have comparable training speed when based on a gaussnewton optimization algorithm while the ppl is more parsimonious in terms of learning robustness toward noise outliers the bpl is more sensitive to the outliers
existing metrics for the learning performance of feed forward neural networks do not provide a satisfactory basis for comparison because the choice of the training epoch limit can determine the results of the comparison i propose new metrics which have the desirable property of being independent of the training epoch limit the efficiency measures the yield of correct networks in proportion to the training effort expended the optimal epoch limit provides the greatest efficiency the learning performance is modelled statistically and asymptotic performance is estimated implementation details may be found in hamey
xve present a novel classification and regression method tha t coinbines exploratory projection pursuit unsulervised training with projection pursuit regression supervised training to yield a new family of costcomplexity penalty terms some improved generalization properties are demonstrated on real world problems
this paper describes a technique for learning both the number of states and the topology of hidden markov models from examples the induction process starts with the most specific model consistent with the training data and generalizes by successively merging states both the choice of states to merge and the stopping criterion are guided by the bayesian posterior probability we compare our algorithm with the baum welch method of estimating fixed size models and find that it can induce minimal hmms from data in cases where fixed estimation does not converge or requires redundant parameters to converge
artificial neural networks are comprised of an interconnected collection of certain nonlinear devices examples of commonly used devices include linear threshold elements sigmoidal elements and radial basis elements we employ results from harmonic analysis and the theory of rational approximation to obtain almost tight lower bounds on the size ie number of elements of neural networks the class of neural networks to which our techniques can be applied is quite general it includes any feedforward network in which each element can be piecewise approximated by a low degree rational function for example we prove that any depth d ql network of sigmoidal units or linear threshold elements computing the parity function of n variables must have fdn d size for any fixed in addition we prove that this lower bound is almost tight by showing that the parity function can be computed with odn ld sigmoidal units or linear threshold elements in a depth d network these almost tight bounds are the first known complexity results on the size of neural networks with depth more than two our lower bound techniques yield a unified approach to the complexity analysis of various models of neural networks with feedforward structures moreover our results indicate that in the context of computing highly oscillating symmetric boolean func siu roychowdhury and kailath tions networks of continuous output units such as sigmoidal elements do not offer significant reduction in size compared with networks of linear threshold elements of binary outputs i

holographic recurrent networks hrns are recurrent networks which incorporate associative memory techniques for storing sequential structure hrns can be easily and quickly trained using gradient descent techniques to generate sequences of discrete outputs and trajectories through continuous space the performance of hrns is found to be superior to that of ordinary recurrent networks on these sequence generation tasks i
a boosting algorithm converts a learning machine with error rate less than to one with an arbitrarily low error rate however the algorithm discussed here depends on having a large supply of independent training samples we show how to circumvent this problem and generate an ensemble of learning machines whose performance in optical character recognition problems is dramatically improved over that of a single network we report the effect of boosting on four databases all handwritten consisting of digits from segmented zip codes from the united state postal service usps and the following from the national institute of standards and testing nist digits upper case alphas and lower case alphas we use two performance measures the raw error rate no rejects and the reject rate required to achieve a error rate on the patterns not rejected boosting improved performance in some cases by a factor of three
memory based classification algorithms such as radial basis functions or k nearest neighbors typically rely on simple distances euclidean dot product which are not particularly meaningful on pattern vectors more complex better suited distance measures are often expensive and rather ad hoc elastic matching deformable templates we propose a new distance measure which a can be made locally invariant to any set of transformations of the input and b can be computed efficiently we tested the method on large handwritten character databases provided by the post office and the nist using invariances with respect to translation rotation scaling shearing and line thickness the method consistently outperformed all other systems tested on the same databases
an artificial neural network ann is commonly modeled by a threshold circuit a network of interconnected processing units called linear threshold gates the depth of a network represents the number of unit delays or the time for parallel computation the size of a circuit is the number of gates and measures the amount of hardware it was known that traditional logic circuits consisting of only unbounded fan in and or not gates would require at least flognloglog n depth to compute common arithmetic functions such as the product or the quotient of two n bit numbers unless we allow the size and fan in to increase exponentially in n we show in this paper that anns can be much more powerful than traditional logic circuits in particular we prove that that iterated addition can be computed by depth ann and multiplication and division can be computed by depth anns with polynomial size and polynomially bounded integer weights respectively moreover it follows from known lower bound results that these anns are optimal in depth we also indicate that these techniques can be applied to construct polynomial size depth ann for powering and depth ann for multiple product i
although considerable interest has been shown in language inference and automata induction using recurrent neural networks success of these models has mostly been limited to regular languages we have previously demonstrated that neural network pushdown automaton nnpda model is capable of learning deterministic context free languages eg anb n and parenthesis languages from examples however the learning task is computationally intensive in this paper we discus some ways in which a priori knowledge about the task and data could be used for efficient learning we also observe that such knowledge is often an experimental prerequisite for learning nontrivial languages eg a n b ncb mam i
we address the problem of learning an unknown function by putting together several pieces of information hints that we know about the function we introduce a method that generalizes learning from examples to learning from hints a canonical representation of hints is defined and illustrated for new types of hints all the hints are represented to the learning process by examples and examples of the function are treated on equal footing with the rest of the hints during learning examples from different hints are selected for processing according to a given schedule we present two types of schedules fixed schedules that specify the relative emphasis of each hint and adaptive schedules that are based on how well each hint has been learned so fr our learning method is compatible with any descent technique that we may choose to use i
platcs resource allocation network ran platt a b is modified for a reinforcement learning paradigm and to restart existing hidden units rather than adding new units after restarting units continue to learn via back propagation the resulting restart algorithm is tested in a q learning network that learns to solve an inverted pendulum problem solutions are found faster on average with the restart algorithm than without it i
in a multi layered neural network any one of the hidden layers can be viewed as computing a distributed representation of the input several encoder experiments have shown that when the representation space is small it can be fully used but computing with such a representation requires completely dependable nodes in the case where the hidden nodes are noisy and unreliable we find that error correcting schemes emerge simply by using noisy units during training random errors injected during backpropagation result in spreading representations apart average and minimum distances increase with misfire probability as predicted by coding theoretic considerations furthermore the effect of this noise is to protect the machine against permanent node failure thereby potentially extending the useful lifetime of the machine
the relationships between learning development and evolution in nature is taken seriously to suggest a model of the developmental process whereby the genotypes manipulated by the genetic algorithm ga might be expressed to form phenotypic neural networks nnet that then go on to learn ontol is a grammar for generating polynomial nnets for time series prediction genomes correspond to an ordered sequence of ontol productions and define a grammar that is expressed to generate a nnet the nnets weights are then modified by learning and the individuals prediction error is used to determine ga fitness a new gene doubling operator appears critical to the formation of new genetic alternatives in the preliminary but encouraging results presented i
this paper describes rpture a system for revising probabilistic knowledge bases that combines neural and symbolic learning methods raptute uses a modified version of backpropagation to refine the certainty factors of a mycln style rule base and uses ids information gain heuristic to add new rules results on refining two actual expert knowledge bases demonstrate that this combined approach performs better than previous methods i
an incremental higher order non recurrent network combines two properties found to be useful for learning sequential tasks higherorder connections and incremental
a performance comparison of two self organizing networks the kohonen feature map and the recently proposed growing cell structures is made for this purpose several performance criteria for self organizing networks are proposed and motivated the models are tested with three example problems of increasing difficulty the kohonen feature map demonstrates slightly superior results only for the simplest problem for the other more difficult and also more realistic problems the growing cell structures exhibit significantly better performance by every criterion additional advantages of the new model are that all parameters are constant over time and that size as well as structure of the network are determined automatically i
given a set of training examples determining the appropriate number of free parameters is a challenging problem constructive learning algorithms attempt to solve this problem automatically by adding hidden units and therefore free parameters during learning we explore an alternative class of algorithms called metamorphosis algorithms in which the number of units is fixed but the number of free parameters gradually increases during learning the architecture we investigate is composed of rbf units on a lattice which imposes flexible constraints on the parameters of the network virtues of this approach include variable subset selection robust parameter selection multiresolution processing and interpolation of sparse training data
a new boundary hunting radial basis function bh rbf classifier which allocates rbf centers constructively near class boundaries is described this classifier creates complex decision boundaries only in regions where confusions occur and corresponding rbf outputs are similar a predicted square error measure is used to determine how many centers to add and to determine when to stop adding centers two experiments are presented which demonstrate the advantages of the bhrbf classifier one uses artificial data with two classes and two input features where each class contains four clusters but only one cluster is near a decision region boundary the other uses a large seismic database with seven classes and input features in both experiments the bhrbf classifier provides a lower error rate with fewer centers than are required by more conventional rbf gaussian mixture or mlp classifiers
large vc dimension classifiers can learn difficult tasks but are usually impractical because they generalize well only if they are trained with huge quantities of data in this paper we show that even high order polynomial classifiers in high dimensional spaces can be trained with a small amount of training data and yet generalize better than classifiers with a smaller vc dimension this is achieved with a maximum margin algorithm the generalized portrait the technique is applicable to a wide variety of classifiers including percepttons polynomial classifiers sigma pi unit networks and radial basis functions the effective number of parameters is adjusted automatically by the training algorithm to match the complexity of the problem it is shown to equal the number of those training patterns which are closest patterns to the decision boundary supporting patterns bounds on the generalization error and the speed of convergence of the algorithm are given experimental results on handwritten digit recognition demonstrate good generalization compared to other algorithms
we propose a very simple and well principled way of computing the optimal step size in gradient descent algorithms the on line vermon is very efficient computationally and is applicable to large backpropagation networks trained on large data sets the main ingredient is a technique for estimating the principal eigenvalues and eigenvectors of the objective functions second derivative matrix hessian which does not require to even calculate the hessian several other applications of this technique are proposed for speeding up learning or for eliminating useless parameters
we investigate the use of information from all second order derivatives of the error function to perform network pruning ie removing unimportant weights from a trained network in order to improve generalization simplify networks reduce hardware or storage requirements increase the speed of further training and in some cases enable rule extraction our method optimal brain surgeon obs is significantly better than magnitude based methods and optimal brain damage le cun denker and solla which often remove the wrong weights obs permits the pruning of more weights than other methods for the same error on the training set and thus yields better generalization on test data crucial to obs is a recursion relation for calculating the inverse hessian matrix h l from training data and structural information of the net obs permits a a and a reduction in weights over backpropagation with weight decay on three benchmark monks problems thrun et al of obs optimal brain damage and magnitude based methods only obs deletes the correct weights from a trained xor network in every case finally whereas sejnowski and rosenberg used weights in their nettalk network we used obs to prune a network to just weights yielding better generalization

we proposed a model of time warping invariant neural networks twinn to handle the time warped continuous signals although twinn is a simple modification of well known recurrent neural network analysis has shown that twinn completely removes time warping and is able to handle difficult classification problem it is also shown that twinn has certain advantages over the current available sequential processing schemes dynamic programmingdp hidden markov modelhmm time delayed neural networkstdnn and neural network finite automatannfa we also analyzed the time continuity employed in twinn and pointed out that this kind of structure can memorize longer input history compared with neural network finite automata nnfa this may help to understand the well accepted fact that for learning grammatical reference with nnfa one had to start with very short strings in training set the numerical example we used is a trajectory classification problem this problem making a feature of variable sampling rates having internal states continuous dynamics heavily time warped data and deformed phase space trajectories is shown to be difficult to other schemes with twinn this problem has been learned in iterations for benchmark we also trained the exact same problem with tdnn and completely failed as expected i
in a new incremental cascade network architecture has been presented this paper discusses the properties of such cascade networks and investigates their generalization abilities under the particular constraint of small data sets the evaluation is done for cascade networks consisting of local linear maps using the mackeyglass time series prediction task as a benchmark our results indicate that to bring the potential of large networks to bear on the problem of extracting information from small data sets without running the risk of overfitting deeply cascaded network architectures are more favorable than shallow broad architectures that contain the same number of nodes
the bootstrap algorithm is a computational intensive procedure to derive nonparametric confidence intervals of statistical estimators in situations where an analytic solution is intractable it is applied to neural networks to estimate the predictive distribution for unseen inputs the consistency of different bootstrap procedures and their convergence speed is discussed a small scale simulation experiment shows the applicability of the bootstrap to practical problems and its potential use
previously ve have introduced the idea of neural network transfer where learning on a target problem is sped up by using the weights obtained from a network trained for a related source task here we present a new algorithm called discriminability based transfer dbt which uses an information measure to estimate the utility of hyperplanes defined by source weights in the target network and rescales transferred weight magnitudes accordingly several experiments demonstrate that target networks initialized via dbt learn significantly faster than networks initialized randomly i
the algorithm presented performs gradient descent on the weight space of an artificial neural network ann using a finite difference to approximate the gradient the method is novel in that it achieves a computational complexity similar to that of node perturbation on but does not require access to the activity of hidden or intemal neurons this is possible due to a stochastic relation between perturbations at the weights and the neurons of an ann the algorithm is also similar to weight perturbation in that it is optimal in terms of hardware requirements when used for the training of vlsi implementafiom of anns
vector quantization is useful for data compression competitive learning which minimizes reconstruction error is an appropriate algorithm for vector quantization of unlabelled data vector quantization of labelled data for classification has a different objective to minimize the number of misclassificafions and a different algorithm is appropriate we show that a variant of kohonens lvq algorithm can be seen as a multiclass extension of an algorithm which in a restricted class case can be proven to converge to the bayes optimal classification boundary we compare the performance of the lvq algorithm to that of a modified version having a decreasing window and normalized step size on a ten class vowel classification problem
many techniques for model selection in the field of neural networks correspond to well established statistical methods the method of stopped training on the other hand in which an oversized network is trained until the error on a further validation set of examples deteriorates then training is stopped is a true innovation since model selection doesnt require convergence of the training process in this paper we show that this performance can be significantly enhanced by extending the nonconvergent model selection method of stopped training to include dynamic topology modifications dynamic weight pruning and modified complexity penalty term methods in which the weighting of the penalty term is adjusted during the training process
we have designed an architecture to span the gap between biophysics and cognitive science to address and explore issues of how a discrete symbol processing system can arise from the continuum and how complex dynamics like oscillation and synchronization can then be employed in its operation and affect its learning we show how a discrete time recurrent elman network architecture can be constructed from recurrently connected oscillatory associative memory modules described by continuous nonlinear ordinary differential equations the modules can learn connection weights between themselves which will cause the system to evolve under a clocked machine cycle by a sequence of transitions of attractors within the modules much as a digital computer evolves by transitions of its binary flip flop attractors the architecture thus employs the principle of computing with attractors used by macroscopic systems for reliable computation in the presence of noise we have specifically constructed a system which functions as a finite state automaton that recognizes or generates the infinite set of six symbol strings that are defined by a reber grammar it is a symbol processing system but with analog input and oscillatory subsymbolic representations the time steps machine cycles of the system are implemented by rhythmic variation clocking of a bifurcation parameter this holds input and context modules clamped at their attractors while hidden and output modules change state then clamps hidden and output states while context modules are released to load those states as the new context for the next cycle of input superior noise immunity has been demonstrated for systems with dynamic attractors over systems with static attractors and synchronization binding between coupled oscillatory attractors in different modules has been shown to be important for effecting reliable transitions synchronization and grammatical inference in an oscillating elman net i

the inverse kinematics problem for redundant manipulators is ill posed and nonlinear there are two fundamentally different issues which result in the need for some form of regularization the existence of multiple solution branches global ill posedness and the existence of excess degrees of freedom local illposedness for certain classes of manipulators learning methods applied to input output data generated from the forward function can be used to globally regularize the problem by partitioning the domain of the forward mapping into a finite set of regions over which the inverse problem is well posed local regularization can be accomplished by an appropriate parameterization of the redundancy consistently over each region as a result the ill posed problem can be transformed into a finite set of well posed problems each can then be solved separately to construct approximate direct inverse functions

one way to speed up reinforcement learning is to enable learning to happen simultaneously at multiple resolutions in space and time this paper shows how to create a q learning managerial hierarchy in which high level managers learn how to set tasks to their submanagers who in turn learn how to satisfy them sub managers need not initially understand their managers commands they simply learn to maximise their reinforcement in the context of the current command we illustrate the system using a simple maze task as the system learns how to get around satisfying commands at the multiple levels it explores more efficiently than standard flat q learning and builds a more comprehensive map
this paper describes a technique called input reconstruction reliability estimation irre for determining the response reliability of a restricted class of multi layer percepttons mlps the technique uses a networks ability to accurately encode the input pattern in its internal representation as a measure of its reliability the more accurately a network is able to reconstruct the input pattern from its internal representation the more reliable the network is considered to be irre is provides a good estimate of the reliability of mlps trained for autonomous driving results are presented in which the reliability estimates provided by irre are used to select between networks trained for different driving situations
how can artificial neural nets generalize better from fewer examples in order to generalize successfully neural network learning methods typically require large training data sets we introduce a neural network learning method that generalizes rationally from many fewer data points relying instead on prior knowledge encoded in previously learned neural networks for example in robot control learning tasks reported here previously learned networks that model the effects of robot actions are used to guide subsequent learning of robot control functions for each observed training example of the target function eg the robot control policy the learner explains the observed example in terms of its prior knowledge then analyzes this explanation to infer additional information about the shape or slope of the target function this shape knowledge is used to bias generalization when learning the target function results are presented applying this approach to a simulated robot task based on reinforcement learning
recent research on reinforcement learning has focused on algorithms based on the principles of dynamic programming dp one of the most promising areas of application for these algorithms is the control of dynamical systems and some impressive results have been achieved however there are significant gaps between practice and theory in particular there are no convergence proofs for problems with continuous state and action spaces or for systems involving non linear function approximators such as multilayer percepttons this paper presents research applying dp based reinforcement learning theory to linear quadratic regulation lqr an important class of control problems involving continuous state and action spaces and requiring a simple type of non linear function approximator we describe an algorithm based on q learning that is proven to converge to the optimal controller for a large class of lqr problems we also describe a slightly different algorithm that is only locally convergent to the optimal q function demonstrating one of the possible pitfalls of using a non linear function approximator with dp based learning

the primate brain must solve two important problems in grasping movements the first problem concerns the recognition of grasped objects specifically how does the brain integrate visual and motor information on a grasped object the second problem concerns hand shape planning specifically how does the brain design the hand configuration suited to the shape of the object and the manipulation task a neural network model that solves these problems has been developed the operations of the network are divided into a learning phase and an optimization phase in the learning phase internal representations which depend on the grasped objects and the task are acquired by integrating visual and somatosensory information in the optimization phase the most suitable hand shape for grasping an object is determined by using a relaxation computation of the network present address parallel distributed processing research dept sony corporation kitashinagawa shinagawa ku tokyo japan uno fukumura suzuki and kawato

a peg in hole insertion task is used as an example to illustrate the utility of direct associative reinforcement learning methods for learning control under real world conditions of uncertainty and noise task complexity due to the use of an unchainfeted hole and a clearance of less than ram is compounded by the presence of positional uncertainty of magnitude exceeding to times the clearance despite this extreme degree of uncertainty our results indicate that direct reinforcement learning can be used to learn a robust reactive control strategy that results in skillful peg in hole insertions
trajectory extension learning is a new technique for learning control in robots which assumes that there exists some parameter of the desired trajectory that can be smoothly varied from a region of easy solvability of the dynamics to a region of desired behavior which may have more difficult dynamics by gradually varying the parameter practice movements remain near the desired path while a neural network learns to approximate the inverse dynamics for example the average speed of motion might be varied and the inverse dynamics can be bootstrapped from slow movements with simpler dynamics to fast movements this provides an example of the more general concept of a practice strategy in which a sequence of intermediate tasks is used to simplify learning a complex task i show an example of the application of this idea to a real joint direct drive robot arm
within a simple test bed application of feed forward neurocontrol for short term planning of robot trajectories in a dynamic environment is studied the action network is embedded in a sensorymotoric system architecture that contains a separate world model it is continuously fed with short term predicted spatio temporal obstacle trajectories and receives robot state feedback the action net allows for external switching between alternative planning tasks it generates goal directed motor actions subject to the robots kinematic and dynamic constraints such that collisions with moving obstacles are avoided using supervised learning we distribute examples of the optimal planner mapping over a structure level adapted parsimonious higher order network the training database is generated by a dynamic programming algorithm extensive simulations reveal that the local planner mapping is highly nonlinear but can be effectively and sparsely represented by the chosen powerful net model excellent generalization occurs for unseen obstacle configurations we also discuss the limitations of feed forward neurocontrol for growing planning horizons tel fax e mml geraldnerouni bonnde learning spario temporal planning from a dynamic programming teacher
a three step method for function approximation with a fuzzy system is proposed first the membership functions and an initial rule representation are learned second the rules are compressed as much as possible using information theory and finally a computational network is constructed to compute the function value this system is applied to two control examples learning the truck and trailer backer upper control system and learning a cruise control system for a radio controlled model car
the invariance of an objects identity as it transformed over time provides a powerful cue for perceptual learning we present an unsupervised learning procedure which maximizes the mutual information between the representations adopted by a feed forward network at consecutive time steps we demonstrate that the network can learn entirely unsupervised to classify an ensemble of several patterns by observing pattern trajectories even though there are abrupt transitions from one object to another between trajectories the same learning procedure should be widely applicable to a variety of perceptual learning tasks i
neurons in area mt of primate visual cortex encode the velocity of moving objects we present a model of how mt cells aggregate responses from v to form such a velocity representation two different sets of units with local receptive fields receive inputs from motion energy filters one set of units forms estimates of local motion while the second set computes the utility of these estimates outputs from this second set of units gate the outputs from the first set through a gain control mechanism this active process for selecting only a subset of local motion responses to integrate into more global responses distinguishes our model from previous models of velocity estimation the model yields accurate velocity estimates in synthetic images containing multiple moving targets of varying size luminance and spatial frequency profile and deals well with a number of transparency phenomena i
multiple single neuron responses were recorded from a single electrode in v of alert behaving monkeys drifting sinusoidal gratings were presented in the cells overlapping receptive fields and the stimulus was varied along several visual dimensions the degree of dimensional separability was calculated for a large population of neurons and found to be a continuum several cells showed different temporal response dependencies to variation of different stimulus dimensions ie the tuning of the modulated firing was not necessarily the same as that of the mean firing rate we describe a multidimensional receptive field and use simultaneously recorded responses to compute a multi neuron receptive field describing the information processing capabilities of a group of cells using dynamic correlation analysis we propose several computational schemes for multidimensional spatiotemporal tuning for groups of cells the implications for neuronal coding of stimuli are discussed stern aertsen vaadia and hochstein
the classical computational model for stereo vision incorporates a uniqueness inhibition constraint to enforce a one to one feature match thereby sacrificing the ability to handle transparency critics of the model disregard the uniqueness constraint and argue that the smoothness constraint can provide the excitation support required for transparency computation however this modification fails in neighborhoods with sparse features we propose a bayesian approach to stereo vision with priors favoring cohesive over transparent surfaces the disparity and its segmentation into a multi layer depth planes representation are simultaneously computed the smoothness constraint propagates support within each layer providing mutual excitation for non neighboring transparent or partially occluded regions test results for various random dot and other stereograms are presented
in visual processing the ability to deal with missing and noisy information is crucial occlusions and unreliable feature detectors often lead to situations where little or no direct information about features is available however the available information is usually sufficient to highly constrain the outputs we discuss bayesian techniques for extracting class probabilities given partial data the optimal solution involves integrating over the missing dimensions weighted by the local probability densities we show how to obtain closed form approximations to the bayesian solution using gaussian basis function networks the framework extends naturally to the case of noisy features simulations on a complex task d hand gesture recognition validate the theory when both integration and weighting by input densities are used performance decreases gracefully with the number of missing or noisy features performance is substantially degraded if either step is omitted

simplified models of the lateral geniculate nucles lgn and striate cortex illustrate the possibility that feedback to the lgn may be used for robust low level pattern analysis the information fed back to the lgn is rebroadcast to cortex using the lgns full fan out so the cortexlgncortex pathway mediates extensive cortico cortical communication while keeping the number of necessary connections small
human vision systems integrate information nonlocally across long spatial ranges for example a moving stimulus appears smeared when viewed briefly ms yet sharp when viewed for a longer exposure ms burr this suggests that visual systems combine information along a trajectory that matches the motion of the stimulus our self organizing neural network model shows how developmental exposure to moving stimuli can direct the formation of horizontal trajectory specific motion integration pathways that unsmear representations of moving stimuli these results account for burrs data and can potentially also model other phenomena such as visual inertia
in this work we apply a texture classification network to remote sensing image analysis the goal is to extract the characteristics of the area depicted in the input image thus achieving a segmented map of the region we have recently proposed a combined neural network and rule based framework for texture recognition the framework uses unsupervised and supervised learning and provides probability estimates for the output classes we describe the texture classification network and extend it to demonstrate its application to the landsat and aerial image analysis domain i
we have designed a neural network which detects the direction of egomotion from optic flow in the presence of eye movements lappe and rauschecker the performance of the network is consistent with human psychophysical cta and its output neurons show great similarity to triple component cells in area mstd of monkey visual cortex we now show that by using assumptions about the kind of eye movements that the observer is likely to perform our model can generate various other cell types found in mstd as well

the ensemble dynamics of stochastic learning algorithms can be studied using theoretical techniques from statistical physics we develop the equations of motion for the weight space probability densities for stochastic learning algorithms we discuss equilibria in the diffusion approximation and provide expressions for special cases of the lms algorithm the equilibrium densities are not in general thermal gibbs distributions in the objective function being minimized but rather depend upon an effective potential that includes diffusion effects finally we present an exact analytical expression for the time evolution of the density for a learning algorithm with weight updates proportional to the sign of the gradient i
in this paper we discuss the asymptotic properties of the most commonly used variant of the backpropagation algorithm in which network weights are trained by means of a local gradient descent on examples drawn randomly from a fixed training set and the learning rate of the gradient updates is held constant simple backpropagation using stochastic approximation results we show that for this training process approaches a batch training and provide results on the rate of convergence further we show that for small one can approximate simple back propagation by the sum of a batch training process and a gaussian diffusion which is the unique solution to a linear stochastic differential equation using this approximation we indicate the reasons why simple backpropagation is less likely to get stuck in local minima than the batch training process and demonstrate this empirically on a number of examples
in the presence of outliers the existing self organizing rules for principal component analysis pca perform poorly using statistical physics techniques including the gibbs distribution binary decision fields and effective energies we propose self organizing pca rules which are capable of resisting outliers while fulfilling various pca related tasks such as obtaining the first principal component vector the first k principal component vectors and directly finding the subspace spanned by the first k vector principal component vectors without solving for each vector individually comparative experiments have shown that the proposed robust rules improve the performances of the existing pca algorithms significantly when outliers are present

we analyze the query by committee algorithm a method for filtering informative queries from a random stream of inputs we show that if the two member committee algorithm achieves information gain with positive lower bound then the prediction error decreases exponentially with the number of queries we show that in particular this exponential decrease holds for query learning of thresholded smooth functions i
we analyse the effects of analog noise on the synaptic arithmetic during multilayer perceptron training by expanding the cost function to include noise mediated penalty terms predictions are made in the light of these calculations which suggest that fault tolerance generalisation ability and learning trajectory should be improved by such noise injection extensive simulation experiments on two distinct classification problems substantiate the claims the results appear to be perfectly general for all training schemes where weights are adjusted incrementally and have wide ranging implications for all applications particularly those involving inaccurate analog neural vlsi
we present the information theoretic derivation of a learning algorithm that clusters unlabelled data with linear discriminants in contrast to methods that try to preserve information about the input patterns we maximize the information gained from observing the output of robust binary discriminators implemented with sigmoid nodes we derive a local weight adaptation rule via gradient ascent in this objective demonstrate its dynamics on some simple data sets relate our approach to previous work and suggest directions in which it may be extended

we have attempted to use information theoretic quantities for aalyzing neuronal connection structure from spike trains two point mutual information and its maximum value channel capacity between a pir of neurons were found to be useful for sensitive detection of crosscorrelation and for estimation of synaptic strength respectively three point mutual information among three neurons could give their interconnection structure therefore our information theoretic analysis was shown to be a very powerful technique for deducing neuronal connection structure some concrete examples of its application to simulated spike trmns are presented
we use statistical mechanics to study generalization in large committee machines for an architecture with nonoverlapping recepfive fields a replica calculation yields the generalization error in the limit of a large number of hidden units for continuous weights the generalization error falls off asymptotically inversely proportional to c the number of training examples per weight for binary weights we find a discontinuous transition from poor to perfect generalization followed by a wide region of metastability broken replica symmetry is found within this region at low temperatures for a fully connected architecture the generalization error is calculated within the annealed approximation for both binary and continuous weights we find transitions from a symmetric state to one with specialized hidden units accompanied by discontinuous drops in the generalization error i
we present an algorithm for creating a neural network which produces accurate probability estimates as outputs the network implements a gibbs probability distribution model of the training database this model is created by a new transformation relating the joint probabilities of attributes in the database to the weights gibbs potentials of the distributed network model the theory of this transformation is presented together with experimental results one advantage of this approach is the network weights are prescribed without iterative gradient descent used as a classifier the network tied or outperformed published results on a variety of databases

the occurence of chaos in recurrent neural networks is supposed to depend on the architecture and on the synaptic coupling strength it is studied here for a randomly diluted architecture by normalizing the variance of synaptic weights we produce a bifurcation parameter dependent on this variance and on the slope of the transfer function but independent of the connectivity that allows a sustained activity and the occurence of chaos when reaching a critical value even for weak connectivity and small size we find numerical results in accordance with the theoretical ones previously established for fully connected infinite sized networks moreover the route towards chaos is numerically checked to be a quasi periodic one whatever the type of the first bifurcation is hopf bifurcation pitchfork or flip doyon cessac quoy and samuelides
recurrent networks of threshold elements have been studied intensively as associative memories and pattern recognition devices while most research has concentrated on fully connected symmetric networks which relax to stable fixed points asymmetric networks show richer dynamical behavior and can be used as sequence generators or flexible pattern recognition devices in this paper we approach the problem of predicting the complex global behavior of a class of random asymmetric networks in terms of network parameters these networks can show fixed point cyclical or effectively aperiodic behavior depending on parameter values and our approach can be used to set parameters as necessary to obtain a desired complexity of dynamics the approach also provides qualitative insight into why the system behaves as it does and suggests possible applications
we analyze in detail the performance of a hamming network classifying inputs that are distorted versions of one of its m stored memory patterns the activation function of the memory neurons in the original hamming network is replaced by a simple threshold function the resulting threshold hamming network thn correctly classifies the input pattern with probability approaching using only om in m connections in a single iteration the thn drastically reduces the time and space complexity of hamming network classifiers
we present a methodological framework enabling a detailed description of the performance of hopfield like attractor neural networks ann in the first two iterations using the bayesian approach we find that performance is improved when a history based term is included in the neurons dynamics a further enhancement of the networks performance is achieved by judiciously choosing the censored neurons those which become active in a given iteration on the basis of the magnitude of their post synaptic potentials the contribution of biologically plausible censored historydependent dynamics is especially marked in conditions of low firing activity and sparse connectivity two important characteristics of the mammalian cortex in such networks the performance attained is higher than the performance of two independent iterations which represents an upper bound on the performance of history independent networks
a method for creating a non linear encoder decoder for multidimensional data with compact representations is presented the commonly used technique of autoassociation is extended to allow non linear representations and an objective function which penalizes activations of individual hidden units is shown to result in minimum dimensional encodings with respect to allowable error in reconstruction i
neural networks with binary weights are very important from both the theoretical and practical points of view in this paper we investigate the learnability of single binary percepttons and unions of p binary perceptron networks e an or of binary percepttons where each input unit is connected to one and only one perceptron we give a polynomial time algorithm that pac learns these networks under the uniform distribution the algorithm is able to identify both the network connectivity and the weight values necessary to represent the target function these results suggest that under reasonable distributions p perceptron networks may be easier to learn than fully connected networks i
two theorems and a lemma are presented about the use of jackknife estimator and the cross validation method for model selection theorem gives the asymptotic form for the jackknife estimator combined with the model selection criterion this asymptotic form can be used to obtain the fit of a model the model selection criterion we used is the negative of the average predictive likehood the choice of which is based on the idea of the cross validation method lemma provides a formula for further exploration of the asymptotics of the model selection criterion theorem gives an asymptotic form of the model selection criterion for the regression case when the parameters optimization criterion has a penalty term theorem also proves the asymptotic equivalence of moodys model selection criterion moody and the cross validation method when the distance measure between response y and regression function takes the form of a squared difference
learning curves show hoxv a neural network is improved as the number of training examples increases and how it is related to the network complexity the present paper clarifies asymptotic properties and their relation of two learning curves one concerning the predictive loss or generalization loss and the other the training loss the result gives a natural definition of the complexity of a neural network moreover it provides a new criterion of model selection
we compare activation functions in terms of the approximation power of their feedforward nets we consider the case of analog as well as boolean input i
a connection is drawn between rational functions the realization theory of dynamical systems and feedforward neural networks this allows us to parametrize single hidden layer scalar neural networks with almost arbitrary analytic activation functions in terms of strictly proper rational functions hence we can solve the uniqueness of parametrization problem for such networks i
we have trained networks of e ii units with short range connections to simulate simple cellular automata that exhibit complex or chaotic behaviour three levels of learning are possible in decreasing order of difficulty learning the underlying automaton rule learning asymptotic dynamical behaviour and learning to extrapolate the training history the levels of learning achieved with and without weight sharing for different automata provide new insight into their dynamics
the feed forward networks with fixed hidden units fhu networks are compared against the category of remaining feed forward networks with variable hidden units vttu networks two broad classes of tasks on a finite domain x c r n are considered approximation of every function from an open subset of functions on x and representation of every dichotomy of x for the first task it is found that both network categories require the same minimal number of synaptic weights for the second task and x in general position it is shown that vhu networks with threshold logic hidden units can have approximately in times fewer hidden units than any fhu network must have
a number f hybrid multilayer percepgon mipfniddon markov model hmm speech recognition systems have been developed in recent years morgan and bourlard in this paper we present a new mip architecture and training algorith which allows the modeling of context dependent phonetic classes in a hybrid gained at different degrees context dependence in order to obtain a robust estimate of the cmtextdependent probabilities tests with the darpa resource management database have shown substantial advantages of the context dependent mlps over earlier contextindependent mlps and have shown substantial advantages of thig hybrid approach over a pure approach
this study demonstrates a paradigm for modeling speech production based on neural networks using physiological data from speech utterances a neural network learns the forward dynamics relating motor commands to muscles and the ensuing articulator behavior that allows articulator trajectories to be generated from motor commands constrained by phoneme input strings and global performance parameters from these movement trajectories a second neural network generates parcor parameters that are then used to synthesize the speech acoustics i
this paper discusses the parameterization of speech by an analog cochlear model the tradeoff between time and frequency resolution is viewed as the fundamental difference between conventional spectrographic analysis and cochlear signal processing for broadband rapid changing signals the models response exhibits a wavelet like analysis in the scale domain that preserves good temporal resolution the frequency of each spectral component in a broadband signal can be accurately determined from the interpeak intervals in the instantaneous firing rates of auditory fibers such properties of the cochlear model are demonstrated with natural speech and synthetic complex signals i
channel equalization problem is an important problem in high speed communications the sequences of symbols transmitted are distorted by neighboring symbols traditionally the channel equalization problem is considered as a channel inversion operation one problem of this approach is that there is no direct correspondence between error probability and residual error produced by the channel inversion operation in this paper the optimal equalizer design is formulated as a classification problem the optimal classifier can be constructed by bayes decision rule in general it is nonlinear an efficient hybrid linearnonlinear equalizer approach has been proposed to train the equalizer the error probability of new linearnonlinear equalizer has been shown to be better than a linear equalizer in an experimental channel
we would like to incorporate speaker dependent consistencies such as gender in an otherwise speaker independent speech recognition system in this paper we discuss a gender dependent neural network gdnn which can be tuned for each gender while sharing most of the speaker independent parameters we use a classification network to help generate gender dependent phonetic probabilities for a statistical hmm recognition system the gender classification net predicts the gender with high accuracy on a resource management test set however the integration of the gdnn into our hybrid hmm neural network recognizer provided an improvement in the recognition score that is not statistically significant on a resource management test set
matched filtering has been one of the most powerful techniques employed for transient detection here we will show that a dynamic neural network outperforms the conventional approach when the artificial neural network ann is trained with supervised learning schemes there is a need to supply the desired signal for all time although we are only interested in detecting the transient in this paper we also show the effects on the detection agreement of different strategies to construct the desired signal the extension of the bayes decision rule desired signal optimal in static classification performs worse than desired signals constructed by random noise or prediction during the background
connectionist speech recognition systems are often handicapped by an inconsistency between training and testing criteria this problem is addressed by the multi state time delay neural network ms tdnn a hierarchical phoneme and word classifier which uses dtw to modulate its connectivity pattern and which is directly trained on word level targets the consistent use of word accuracy as a criterion during both training and testing leads to very high system performance even wilh limited training data until now the ms tdnn has been applied primarily to small vocabulary recognition and word spotting tasks in this paper we apply the architecture to large vocabulary continuous speech recognition and demonstrate that our ms tdnn outperforms all other systems that have been tested on the cmu conference registration database
untill recently state of the art large vocabulary continuous speech recognition csr has employed hidden markov modeling hmm to model speech sounds in an attempt to improve over hmm we developed a hybrid system that integrates hmm technology with neural networks we present the concept of a segmental neural net snn for phonetic modeling in csr by taking into account all the frames of a phonetic segment simultaneously the snn overcomes the well known conditional independence limitation of hivims in several speaker independent experiments with the darpa resource management corpus the hybrid system showed a consistent improvement in performance over the baseline hivim system
the multi state time delay neural network ms tdnn integrates a nonlinear time alignment procedure dtw and the highaccuracy phoneme spotting capabilities of a tdnn into a connectionist speech recognition system with word level classification and error backpropagation we present an ms tdnn for recognizing continuously spelled letters a task characterized by a small but highly confusable vocabulary our ms tdnn achieves word accuracy on speaker dependentindependent tasks outperforming previously reported results on the same databases we propose training techniques aimed at improving sentence level performance including free alignment across word boundaries word duration modeling and error backpropagation on the sentence rather than the word level architectures integrating submodules specialized on a subset of speakers achieved further improvements
this paper reports on the performance of two methods for recognition based segmentation of strings of on line hand printed capital latin characters the input strings consist of a timeordered sequence of x y coordinates punctuated by pen lifts the methods were designed to work in run on mode where there is no constraint on the spacing between characters while both methods use a neural network recognition engine and a graph algorithmic post processor their approaches to segmentation are quite different the first method which we call inseg for input segmentation uses a combination of heuristics to identify particular penlifts as tentative segmentation points the second method which we call outseg for output segmentation relies on the empirically trained recognition engine for both recognizing characters and identifying relevant segmentation points i
we propose in this paper a statistical model planar hidden markov model phmm describing statistical properties of images the model generalizes the single dimensional hmm used for speech processing to the planar case for this model to be useful an efficient segmentation algorithm similar to the viterbi algorithm for hmm must exist we present conditions in terms of the phmm parameters that are sufficient to guarantee that the planar segmentation problem can be solved in polynomial time and describe an algorithm for that this algorithm aligns optimally the image with the model and therefore is insensitive to elastic distortions of images using this algorithm a joint optimal segmentation and recognition of the image can be performed thus overcoming the wealmess of traditional ocr systems where segmentation is performed independently before the recognition leading to unrecoverable recognition errors the phmm approach was evaluated using a set of isolated hand written digits an overall digit recognition accuracy of was achieved an analysis of the results showed that even in the simple case of recognition of isolated characters the elimination of elastic distortions enhances the performance significantly we expect that the advantage of this approach will be even more significant for tasks such as connected writing recognitionspotting for which there is no known high accuracy method of recognition
we are developing a forecaster for daily extremes of demand for electric power encountered in the service area of a large midwestern utility and using this application as a testbed for approaches to input dimension reduction and decomposition of network training projection pursuit regression representations and the ability of algorithms like sir to quickly find reasonable weighting vectors enable us to confront the vexing architecture selection problem by reducing high dimensional gradient searchs to fitting single input single output siso subnets we introduce dimension reduction algorithms to select features or relevant subsets of a set of many variables based on minimizing an index of level set dispersions closely related to a projection index and to sir and combine them with backfitting to implement a neural network version of projection pursuit the performance achieved by our approach when trained on data and tested on data is comparable to that achieved in our earlier study of backpropagation trained networks i
hidden markov models hmms can be applied to several important problems in molecular biology we introduce a new convergent learning algorithm for hmms that unlike the classical baum welch algorithm is smooth and can be applied on line or in batch mode with or without the usual viterbi most likely path approximation left right hmms with insertion and deletion states are then trained to represent several protein families including immunoglobulins and kinases in all cases the models derived capture all the important statistical properties of the families and can be used efficiently in a number of important tasks such as multiple alignment motif detection and classification and division of biology california institute of technology t and department of psychology stanford university baldi chauvin hunkapiller and mcclure
the planar thallium myocardial perfusion scintigram is a widely used diagnostic technique for detecting and estimating the risk of coronary artery disease neural networks learned to interpret thallium scintigrams as determined by individual expert ratings standard error backpropagation was compared to standard lms and lms combined with one layer of rbf units using the leave one out method generalization was tested on all cases training time was determined automatically from cross validation performance best performance was attained by the rbflms network with three hidden units per view and compares favorably with human experts
we have designed fabricated and tested an analog vlsi chip which computes radial basis functions in parallel we have developed a synapse circuit that approximates a quadratic function we aggregate these circuits to form radial basis functions these radial basis functions are then averaged together using a follower aggregator
an analog cmos vlsi neural processing chip has been designed and fabricated the device employs pulse stream neural state silmalling and is capable of computing some million symaptic connections per second in addition to basic characterisation results the performance of the chip in solving real world problems is also demonstrated
the real time computation of motion from real images using a single chip with integrated sensors is a hard problen we present two analog vlsi schemes that use pulse domain neuromorphic circuits to compute motion pulses of variable width rather than graded potentials represent a natural medimn for evaluating temporal relationships both algorithms measure speed by timing a moving edge in the image our first model is inspired by reichardts algorithm in the fly and yields a non monotonic response rs velocity curve we present data from a chip that implements this model our second algorithm yields a nonotonic response rs velocity curve and is currently being translated into silicon i
we describe an analog vlsi implementation of a multi dimensional gradient estimation and descent technique for minimizing an onchip scalar function f the implementation uses noise injection and multiplicative correlation to estimate derivatives as in anderson kerns one intended application of this technique is setting circuit parameters on chip automatically rather than manually kirk gradient descent optimization may be used to adjust synapse weights for a backpropagation or other on chip learning implementation the approach combines the features of continuous multi dimensional gradient descent and the potential for an annealing style of optimization we present data measured from our analog vlsi implementation
the field of software simulators for neural networks has been expanding very rapidly in the last years but their importance is still being underestimated they must provide increasing levels of assistance for the design simulation and analysis of neural networks with our object oriented framework sesame we intend to show that very high degrees of transparency manageability and flexibility for complex experiments can be obtained sesames basic design philosophy is inspired by the natural way in which researchers explain their computational models experiments are performed with networks of building blocks which can be extended very easily mechanisms have been integrated to facilitate the construction and analysis of very complex architectures among these mechanisms are the automatic configuration of building blocks for an experiment and multiple inheritance at run time
networks with local inhibition are shown to have enhanced computational performance with respect to the classical hopfield like networks in particular the critical capacity of the network is increased as well as its capability to store correlated patterns chaotic dynamic behaviour exponentially long transients of the devices indicates the overloading of the associative memory an implementation based on a programmable logic device is here presented a neurons circuit is implemented whir a xilink device the peculiarity of this solution is the possibility to change parts of the project weights transfer function or the whole architecture with a simple software download of the configuration into the xilink chip
we demonstrate the use of a digital signal processing board to construct hybrid networks consisting of computer model neurons connected to a biological neural network this system operates in real time and the synaptic connections are realistic effective conductances therefore the synapses made from the computer model neuron are integrated correctly by the postsynaptic biological neuron this method provides us with the ability to add additional completely known elements to a biological network and study their effect on network activity moreover by changing the parameters of the model neuron it is possible to assess the role of individual conductances in the activity of the neuron and in the network in which it participates present address xl universit de bordeaux enserb cnrsura crs de la liberation talence cedex france present address lnpc cnrs universit de bordeaux place de dr peyneau arcachon france renaud lemasson lemasson marder and abbott
several research group are inlhmcnl ing alalog integrated circuit models of biological auditory processing the outluts of these circuit nodels have taken several knns including vicico format for monitor display simple scanned out put for oscilloscope display and parallel analog outluts suitable for da ta acquisition systems in this paper we describe an alternative output method for silicon auditory models suitable for direct interfa ce to digital computers present address hi mallowaid mi anatomical neurolhamacology unit mansfield rd oxford ox tii england mamvax oxford ac uk present address nass sivilolti lancr escarch north vinedo avenue pasadena ca masstannercom present address dave gilhspic svallics rcla rd parkway san jose ca davegsynaptics corn silicon auditory processors as computer peripherals

typical methods for gradient descent in neural network learning involve calculation of derivatives based on a detailed knowledge of the network model this requires extensive time consuming calculations for each pattern presentation and high precision that makes it difficult to implement in vlsi we present here a perturbation technique that measures not calculates the gradient since the technique uses the actual network as a measuring device errors in modeling neuron activation and synaptic weights do not cause errors in gradient descent the method is parallel in nature and easy to implement in vlsi we describe the theory of such an algorithm an analysis of its domain of applicability some simulations using it and an outline of a hardware implementation i
basic connectionist principles imply that gramxnars should take the form of systems of parallel soft constraints defining an optimization problem the solutions to which are the well formed structures in the language such harmonic grammars have been successfully applied to a number of problems in the theory of natural languages iiere it is shown that formal languages too can be specified by harmonic grammars rather than by conventional serial re write rule systems i harmonic grammars in collaboration with gdraldine legendre yoshiro miyata and alan prince i have been studying how symbolic computation in human cognition can arise naturally as a higher level virtual machine realized in appropriately designed lower level connectionist networks the basic computational principles of the approach are these a when analyzed at the lower level mental representations are distributed patterns of connectionist activity when analyzed at a higher level these same representations constitute symbolic structures the particular symbolic structure s is characterized as a set of fillerrole bindings iri using a collection of structural roles ri each of which may be occupied by a filler fi a constituent symbolic struc smolensky co ture the corresponding lower level description is an activity vector s ifiri these tensor product representations can be defined recursively fillers which are themselves complex structures are represented by vectors which in turn are recursively defined as tensor product representations smolensky smolensky when analyzed at the lower level mental processes are massively parallel numerical activation spreading when analyzed at a higher level these same processes constitute a form of symbol manipulation in which entire structures possibly involving recursive embedding are manipulated in parallel dolan and smolensky legendre et al a smolensky when the lower level description of the activation spreading processes satisfies certain mathematical properties this process can be analyzed on a higher level as the construction of that symbolic structure including the given input structure which maximizes harmony equivalently minimizes energy the iiarmony can be computed either at the lower level as a particular mathematical function of the numbers conaprising the activation pattern or at the higher level as a function of the symbolic constituents comprising the structure in the simplest cases the core of the tiarmony function can be written at the lower connectionist level simply as the quadratic form h atwa where a is the networks activation vector and w its connection weight matrix at the higher level h cc hc c each hc c is the harmony of having the two symbolic constituents c and c in the same structure the ci are constituents in particular structural roles and may be the same cohen and grossberg golden goldend hinton and sejnowski hinton and sejnowski hopfield hopfield iiopfield legendre et al a smolensky smolensky once itarmony connectionist well formedness is identified with grammaticality linguistic well formedness the following results lc legendre et al a a the explicit form of the harmony function can be computed to be a sum of terms each of which measures the well formedness arising from the coexistence within a single structure of a pair of constituents in their particular structural roles b a descriptive grammar can thus be identified as a set of soft rules each of the form if a linguistic structure simultaneously contains constituent c in structural role r and constituent c in structural role r then add to h the harmony value of the quantity which may be positive or negative a set of such soft rules or constraints or preferences defines a harmonic grammar c the constituents in the soft rules include both those that are given in the input and the hidden constituents that are assigned to the input by the grammar the problem for the parser computational harmonic grammars for formal languages grainmar is to construct that structure containing both input and hidden constituents with the highest overall harmony hs harmonic grainmar iig is a formal developlnent of conceptual ideas linking iiarmony to linguistics which were first proposed in lakoffs cognitive phonology lakoff lakoff and goldsmiths harmonic phonology goldsmith goldsmith in press for an application of hg to natural language syntaxsemantics see legendre et al a legendre et al b legendre et al b legendre et al in press iiarmonic grammar has more recently evolved into a non numerical formalism called optimality theory which has been successfully applied to a range of problems in phonology prince and smolensky prince and smolensky in preparation for a comprehensive discussion of the overall research program see smolensky et al hgs for formal languages one means for assessing the expressive power of harxnonic grammar is to apply it to the specification of formal languages can eg any context free language cfl l be specified by an itg can a set of soft rules of the form b be given so that a string s l iff the maximum harmony tree with s as terminals has say h a crucial limitation of these soft rules is that each may only refer to a pair of constituents in this sense they are only second order it simplifies the exposition to describe as pairs those in which both constituents are the same these actually correspond to first order soft rules which also exist in tig for a cfl a tree is well formed iff all of its local trees arewhere a local tree is just some node and all its children thus the hg rules need only refer to pairs of nodes which fall in a single local tree ie parent child pairs andor sibling pairs the h value of the entire tree is just the sum of all the numbers for each such pair of nodes given by the soft rules defining the iig it is clear that for a general context free grammar cfg pairwise evaluation doesnt suffice consider eg the following cfg fragment go a b c a d e f b e and the ill formed local tree a b e here a is the parent b and e the two children pairwise well formedness checks fail to detect the ill formedness since the first rule says b can be a left child of a the second that e can be a right child of a and the third that b can be a left sibling of e the ill formedness can be detected only by examining all three nodes simultaneously and seeing that this triple is not licensed by any single rule one possible approach would be to extend hg to rules higher than second order involving more than two constituents this corresponds to h functions of degree higher than such h functions go beyond standard connectionist networks with pairwise connectivity requiring networks defined over hypergraphs rather than ordinary graphs there is a natural alternative however that requires no change at all in itg but instead adopts a special kind of grammar for the cfl the basic trick is a modification of an idea taken from generalized phrase structure grammar gazdar et al a theory that adapts cfgs to the study of natural languages it is useful to introduce a new normal form for cfgs harmonic normal form smolensky hnf in hnf all rules of are three types ai b c a a and a ai and there is the further requirement that there can be only one branching rule with a given left hand side the unique branching condition here we use lowercase letters to denote ternfinal symbols and have two sorts of non terminals general symbols like a and subcategorized symbols like aa ai to see that every cfl l does indeed have an hnf grammar it suffices to first take a cfg for l in chomsky normal form and for each necessarily binary branching rule ab c i replace the symbol a on the left hand side with ai using a different value of i for each branching rule with a given left hand side and ii add the rule a ai subcategorizing the general category a which may have several legal branching expansions into the specialized subcategories ai each of which has only one legal branching expansion makes it possible to determine the well formedness of an entire tree simply by examining each parentchild pair separately an entire tree is wellformed iff every parentchild pair is the unique branching condition enables us to evaluate the harmony of a tree simply by adding up a collection of numbers specified by the soft rules of an iig one for each node and one for each link of the tree now any cfl l can be specified by a harmonic grammar first find an hnf grammar ghw for l from it generate a set of soft rules defining a harmonic grammar g via the correspondences ghnf gh ai start symbol a e ct a or ai ail b c ra if a is at any node add to h ra if a is at any node add to h ratq if ai is at any node add to h root if s is at the root add to h if a is a left child of a add to h if b is a left child of ai add to h if c is a right child of ai add to h the soft rules ra ra rai and root are first order and evaluate tree nodes the remaining second order soft rules are legal domination rules evaluating tree links this iig assigns h to any legal parse tree with s at the root and h for any other tree thus s l iff the maximal itarmony completion of s to a tree has h proof h ve evaluate the harmony of any tree by conceptually breaking up its nodes and links into pieces each of which contributes either or to h in legal trees there will be complete cancellation of the positive and negative contributions illegal trees will have uncancelled is leading to a total h the decomposition of nodes and links proceeds as follows replace each undirected link in the tree with a pair of directed links one pointing up to the parent the other down to the child if the link joins a legal parentchild pair the corresponding legal domination rule will contribute to h break this into two contributions of one for each of the directed links we similarly break up the non terminal nodes into sub nodes a non terminal node labelled harmonic grammars for formal languages ai has two children in legal trees and we break such a node into three sub nodes one corresponding to each downward link to a child and one corresponding to the upward link to the parent of ai according to soft rule tai the contribution of this node ai to h is tiffs is distributed as three contributions of one for each sub node similarly a non terminal node labelled a has only one child in a legal tree so we break it into two sub nodes one for the downward link to the only child one for tle upward link to the parelit of a the contribution of dictated by soft rule tga is similarly decmnposed into two contributions of one for each sub node there is no need to break up terminal nodes which in legal trees have only one outgoing link upward to the parent the contribution froin ra is already just we can evaluate the harmony of any tree by examining each node now decomposed into a set of sub nodes and determining the contribution to h made by the node and its outgoing directed links we will not double count link contributions this way half the contribution of each original undirected link is counted at each of the nodes it connects consider first a non terminal node n labelled by ai if it has a legal parent it will have an upward link to the parent that contributes which cancels the contributed by ns corresponding sub node if n has a legal left child the downward link to it will contribute cancelling the contributed by ns corresponding sub node similarly for the right child thus the total contribution of this node will be if it has a legal parent and two legal children for each missing legal child or parent the node contributes an uncancelled so the contribution of this node n in the general case is h the nmnber of ntissing legal children and parents of node n the saine result holds of the non branching non terminals labelled a the only difference is that now the only child that could be ntissing is a legal left child if a happens to be a legal start symbol in root position then the of the sub node corresponding to the upward link to a parent is cancelled not by a legal parent as usual but rather by the of the soft rule rroot the result still holds even in this case if we simply agree to count the root position itself as a legal parent for start symbols and finally holds of a terminal node n labelled a such a node can have no ntissing child but might have a missing legal parent thus the total harmony of a tree is h n hn with hn given by that is h is the minus the total lmmber of missing legal children and parents for all nodes in the tree thus h if each node has a legal parent and all its required legal children otherwise h because the grammar is in harmonic normal forin a parse tree is legal iff every every node has a legal parent and its required smolensky nuxnber of legal children where legal parentchild doxninations are defined only pairwise in terms of the parent and one child blind to any other children that might be present or absent thus we have established the desired result that the maximum llarmony parse of a string s has h iff s l we can also now see how to understand the soft rules of gh and how to generalize beyond context free languages the soft rules say that each node makes a negative contribution equal to its valence while each link makes a positive contribution equal to its valence where the valence of a node or link is just the number of links or nodes it is attached to in a legal tree the negative contributions of the nodes are made any time the node is present these are cancelled by positive contributions from the links only when the link constitutes a legal domination sanctioned by the grammar so in order to apply the sanhe strategy to unrestricted grainmars we will simply set the magnitude of the negative contributions of nodes equal to their valence as determined by the grammar we can illustrate the technique by showing how itnf solves the problem with the simple three rule grammar fragment go introduced early in this section the corresponding hnf grammar fragment ghnf given by the above construction is a b c a a a d e a a f b e f f to avoid extraneous complications froin adding a start node above and terminal nodes below suppose that both a and f are valid start symbols and that b c d e are terminal nodes then the corresponding hg gh assigns to the ill formed tree a b e the harmony since according to ghnf b and e are both missing a legal parent and a is missing two legal children introducing a now necessary subcategorized version of a helps but not enough a a b e and a a b e both have h since in each one leaf node is missing a legal parent e and b respectively and the ai node is ntissing the corresponding legal child but the correct parse of the string b e f fill b e has h this technique can be generalized froin context free to unrestricted type formal languages which are equivalent to turing machines in the languages they generate eg itopcroft and ullman the ith production rule in an unrestricted grammar ri aaan flfltim is replaced by the two rules r ri and ri introducing new non terminal symbols fi the corresponding soft rules in the harmonic grammar are then if the kth parent of fi is a add to h and if is the kth child of fi add to h there is also the rule rri if fi is at any node add himi to h there are also soft rules ra r and rroot defined as in the context free case acknowledgement s i aln grateful to gdraldine legendre yoshiro miyata and alan prince for many helpful discussions the research presented here has been supported in part by nsf grant bs and by the university of colorado at boulder council on research and creative work harmonic grammars for formal languages references cohen m a and grossberg s absolute stability of global pattern formation and parallel memory storage by competitive neural networks ieee transactions on systems man and cybernetics dolan c p and smolensky p tensor product production system a modular architecture and representation connection science gazdar g klein e pullurn g and sag i generalized phrase structure grammar harvard university press cambridge ma golden r m the brain state in a box neural model is a gradient descent algorithm mathematical psychology golden r m a unified framework for connectionist systems biological cybernetics goldsmith j a autosegmental and metrical phonology basil blackwell oxford goldsmith j a in press phonology as an intelligent system in napoli d j and kegl j a editors bridges between psychology and linguistics a swarthmore festschrifi for lila gleitman cainbridge university press cainbridge hinton g e and sejnowski t j analyzing cooperative computation in proceedings of the fifth annual conference of the cognitive science society rochester ny erlbaum associates hinton g e and sejnowski t j learning and relearning in boltzmann machines in rumelhart d e mcclelland j l and the pdp research group editors parallel distributed processing explorations in the microstructure of cognition volume i foundations chapter pages mit pressbradford books canabridge ma hopcroft j e and ullman j d
neural network models have been criticized for their inability to make use of compositional representations in this paper we describe a series of psychological phenomena that demonstrate the role of structured representations in cognition these findings suggest that people compare relational representations via a process of structural alignment this process will have to be captured by any model of cognition symbolic or subsymbolic

we demonstrate in this paper how certain forms of rule based knowledge can be used to prestructure a neural network of normalized basis functions and give a probabilistic interpretation of the network architecture we describe several ways to assure that rule based knowledge is preserved during training and present a method for complexity reduction that tries to minimize the number of rules and the number of conjuncts after training the refined rules are extracted and analyzed
we describe a model of visual word recognition that accounts for several aspects of the temporal processing of sequences of briefly presented words the model utilizes a new representation for written words based on dynamic time warping and multidimensional scaling the visual input passes through cascaded perceptual comparison and detection stages we describe how these dynamical processes can account for several aspects of word recognition including repetition priming and repetition blindness i
we propose a model of the development of geometric reasoning in children that explicitly involves learning the model uses a neural network that is initialized with an understanding of geometry similar to that of second grade children through the presentation of a series of examples the model is shown to develop an understanding of geometry similar to that of fifth grade children who were trained using similar materials
representations for semantic information about words are necessary for many applications of neural networks in natural language processing this paper describes an efficient corpus based method for inducing distributed semantic representations for a large number of words from lexical coccurrence statistics by means of a large scale linear regression the representations are successfully applied to word sense disambiguation using a nearest neighbor method
which processes underly our ability to quickly recognize familiar objects within a complex visual input scene in this paper an implemented neural network model is described that attempts to specify how selective visual attention perceptual organisation and invariance transformations might work together in order to segment select and recognize objects out of complex input scenes containing multiple possibly overlapping objects retinotopically organized feature maps serve as input for two main processing routes the wherepathway dealing with location information and the what pathway computing the shape and attributes of objects a location based attention mechanism operates on an early stage of visual processing selecting a contigous region of the visual field for preferential processing additionally location based attention plays an important role for invariant object recognition controling appropriate normalization processes within the what pathway object recognition is supported through the segmentation of the visual field into distinct entities in order to represent different segmented entities at the same time the model uses an oscillatory binding mechanism connections between the where pathway and the what pathway lead to a flexible cooperation between different functional subsystems producing an overall behavior which is consistent with a variety of psychophysical data goebel i

in the electrosensory system of weakly electric fish descending pathways to a first order sensory nucleus have been shown to influence the gain of its output neurons the underlying neural mechanisms that subserve this descending gain control capability are not yet fully understood we suggest that one possible gain control mechanism could involve the regulation of total membrane conductance of the output neurons in this paper a neural model based on this idea is used to demonstrate how activity levels on descending pathways could control both the gain and baseline excitation of a target neuron i
a model of the hippocampus as a central element in rat navigation is presented simulations show both the behaviour of single cells and the resultant navigation of the rat these are compared with single unit recordings and behavioural data the firing of ca place cells is simulated as the artificial rat moves in an environment this is the input for a neuronal network whose output at each theta cycle is the next direction of travel for the rat cells are characterised by the number of spikes fired and the time of firing with respect to hippocampal rhythm learning occurs in on off synapses that are switched on by simultaneous preand post synaptic activity the simulated rat navigates successfully to goals encountered one or more times during exploration in open fields one minute of random exploration of a lrn environment allows navigation to a newly presented goal from novel starting positions a limited number of obstacles can be successfully avoided background experiments have shown the hippocampus to be crucial to the spatial memory and navigational ability of the rat okeefe nadel single unit recordings in freely moving rats have revealed place cells in fields ca and ca of the hippocampus whose firing is restricted to small portions of the rats environment the corresponding place fields okeefe dostrovsky see fig la in addition cells have been found in the dorsal pre subiculum whose primary behavioural burgess okeefe and recce a i ii ii iiii ii i i i i i i i i time s figure a a typical ca place field max rate over is is spikess b one second of the eeg rhythm is shown in c as the rat runs through a place field a shows the times of firing of the place cell vertical ticks immediately above and below the eeg mark the positive to negative zero crossings of the eeg which we define as or of phase b shows the phase of at which each spike was fired okeefe recce correlate is head direction taube et al both are suggestive of navigation temporal as well as spatial aspects of the electrophysiology of the hippocampal region are significant for a model the hippocampal eeg rhythm is best characterised as a sinusoid of frequency hz and occurs whenever the rat is making displacement movements recently place cell firing has been found to have a systematic phase relationship to the local eeg okeefe recce see and fig lb finally the rhythm has been found to modulate long term potentiation of synapses in the hippocampus pavlides et al
we present a theory of cortico hippocmnpal interaction in discrinination learning the hippocmnpal region is presumed to form new stinulus representations which facilitate learning by enhancing the discrininability of predictive stimuli and conpressing stimulus stimulus redundancies the cortical and cerebellar regions which are the sites of long term metnory nay acquire these new representations but me not asstuned to be capable of forming new representations themselves instantiated as a connectionist model this theory accounts for a wide range of trial level classical conditioning phenmnena in normal intact and hippocampal lesioned animals it also nakes several novel predictions which renain to be investigated empirically the theory implies that the hippocmnpal region is involved in even the simplest learning tasks although hippocampal lesioned animals may be able to use other strategies to learn these tasks the theory predicts that they will show consistently different patterns of transfer and generalization when the task demands change
so far there has been no general method for relating extracellular electrophysiological measured activity of neurons in the associative cortex to underlying network or cognitive states we propose to model such data using a multivariate poisson hidden markov model we demonstrate the application of this approach for temporal segmentation of the firing patterns and for characterization of the cortical responses to external stimuli using such a statistical model we can significantly discriminate two behavioral modes of the monkey and characterize them by the different firing patterns as well as by the level of coherency of their multi unit firing activity our study utilized measurements carried out on behaving rhesus monkeys by m abeles e vaadia and h bergman of the hadassa medical school of the hebrew university
an information theoretic optimization principle infomax has previously been used for unsupervised learning of statistical regularities in an input ensemble the principle states that the inputoutput mapping implemented by a processing stage should be chosen so as to maximize the average mutual information between input and output patterns subject to constraints and in the presence of processing noise in the present work i show how infomax when applied to a class of nonlinear input output mappings can under certain conditions generate optimal filters that have additional useful properties output activity for each input pattern tends to be concentrated among a relatively small number of nodes the filters are sensitive to higher order statistical structure beyond pairwise correlations if the input features are localized the filters receptive fields tend to be localized as well multiresolution sets of filters with subsampling at low spatial frequencies related to pyramid coding and wavelet representations emerge as favored solutions for certain types of input ensembles i
the vestibulo ocular reflex vor is a compensatory eye movement that stabilizes images on the retina during head turns its magnitude or gain can be modified by visual experience during head movements possible learning mechanisms for this adaptation have been explored in a model of the oculomotor system based on anatomical and physiological constraints the local correlational learning rules in our model reproduce the adaptation and behavior of the vor under certain parameter conditions from these conditions predictions for the time course of adaptation at the learning sites are made
we present a local learning rule in which hebbian learning is conditional on an incorrect prediction of a reinforcement signal we propose a biological interpretation of such a framework and display its utility through examples in which the reinforcement signal is cast as the delivery of a neuromodulator to its target three examples are presented which illustrate how this framework can be applied to the development of the oculomotor system
a switching between apparently coherent oscillatory and stochastic episodes of activity has been observed in responses from cat and monkey visual cortex we describe the dynamics of these phenomena in two parallel approaches a phenomenological and a rather microscopic one on the one hand we analyze neuronal responses in terms of a hidden state model hsm the parameters of this model are extracted directly from experimental spike trains they characterize the underlying dynamics as well as the coupling of individual neurons to the network this phenomenological model thus provides a new framework for the experimental analysis of network dynamics the application of this method to multi unit activities from the visual cortex of the cat substantiates the existence of oscillatory and stochastic states and quantifies the switching behaviour in the assembly dynamics on the other hand we start from the single spiking neuron and derive a master equation for the time evolution of the assembly state which we represent by a phase density this phase density dynamics pdd exhibits costability of two attractors a limit cycle and a fixed point when synaptic interaction is nonlinear external fluctuations can switch the bistable system from one state to the other finally we show that the two approaches are mutually consistent and therefore both explain the detailed time structure in the data pawelzik bauer deppisch and geisel i
a new computational model that addresses the formation of both topography and ocular dominance is presented this is motivated by experimental evidence that these phenomena may be subserved by the same mechanisms an important aspect of this model is that ocular dominance segregation can occur when input activity is both distributed and positively correlated between the eyes this allows investigation of the dependence of the pattern of ocular dominance stripes on the degree of correlation between the eyes it is found that increasing correlation leads to narrower stripes experiments are suggested to test whether such behaviour occurs in the natural system
we interpret the time interval data obtained from periodically stimulated sensory neurons in terms of two simple dynamical systems driven by noise with an embedded weak periodic function called the signal a bistable system defined by two potential wells separated by a barrier and a fitzhugh nagnmo system the implementation is by analog simulation elcotronic circuits which mimic the dynamics for a given signal frequency our simulators have only two adjustable parameters the signal and noise intensities we show that experimental data obtained from the periodically stimulated mechanoreceptor in the crayfish tailfan can be accurately approximated by these simulations finally we discuss stochastic resonance in the two models
the formation of propagating spiral waves is studied in a randomly connected neural network composed of integrate and fire neurons with recovery period and cxcitatory connections using computer simulations network activity is initiated by periodic stimulation at a single point the results suggest that spiral waves can arise in such a network via a sub critical hopf bifurcation
this paper examines and extends the work of linsker on self organising feature detectors linsker concentrates on the visual processing system but infers that the weak assumptions made will allow the model to be used in the processing of other sensory information this claim is examined here with special attention paid to the auditory system where there is much lower connectivity and therefore more statistical variability on line training is utilised to obtain an idea of training times these are then compared to the time available to pre natal mammals for the formation of feature sensitive cells i
this paper presents a neural network able to control saccadic movements the input to the network is a specification of a stimulation site on the collicular motor map the output is the time course of the eye position in the orbit horizontal and vertical angles the units in the network exhibit a one to one correspondance with neurons in the intermediate layer of the superior colliculus collicular motor map in the brainstem and with oculomotor neurons simulations carried out with this network demonstrate its ability to reproduce in a straightforward fashion many experimental observations
it is known from biological data that the response patterns of interneurons in the olfactory macroglomerulus mgc of insects are of central importance for the coding of the olfactory signal we propose an analytically tractable model of the mgc which allows us to relate the distribution of response patterns to the architecture of the network
information theory is used to derive a simple formula for the amount of information conveyed by the firing rate of a neuron about any experimentally measured variable or combination of variables eg running speed head direction location of the animal etc the derivation treats the cell as a communication channel whose input is the measured variable and whose output is the cells spike train applying the formula we find systematic differences in the information content of hippocampal place cells in different experimental conditions
an autoencoder network uses a set of recognition weights to convert an input vector into a code vector it then uses a set of generative weights to convert the code vector into an approximate reconstruction of the input vector we derive an objective function for training autoencoders based on the minimum description length mdl principle the aim is to minimize the information required to describe both the code vector and the reconstruction error we show that this information is minimized by choosing code vectors stochastically according to a boltzmann distribution where the generafive weights define the energy of each possible code vector given the input vector unfortunately if the code vectors use distributed representations it is exponentially expensive to compute this boltzmann distribution because it involves all possible code vectors we show that the recognition weights of an autoencoder can be used to compute an approximation to the boltzmann distribution and that this approximation gives an upper bound on the description length even when this bound is poor it can be used as a lyapunov function for learning both the generafive and the recognition weights we demonstrate that this approach can be used to learn factorial codes

although recurrent neural nets have been moderately successful in learning to emulate finite state machines fsms the continuous internal state dynamics of a neural net are not well matched to the discrete behavior of an fsm we describe an architecture called olc that allows discrete states to evolve in a net as learning progresses oolcr consists of a standard recurrent neural net trained by gradient descent and an adaptive clustering technique that quantizes the state space dolce is based on the assumption that a finite set of discrete internal states is required for the task and that the actual network state belongs to this set but has been corrupted by noise due to inaccuracy in the weights dolce learns to recover the discrete state with maximum a posterjori probability from the noisy state simulations show that ooc leads to a significant improvement in generalization performance over earlier neural net approaches to fsm induction
this paper presents a formulation for unsupervised learning of clusters reflecting multiple causal structure in binary data unlike the standard mixture model a multiple cause model accounts for observed data by combining assertions from many hidden causes each of which can pertain to varying degree to any subset of the observable dimensions a crucial issue is the mixing function for combining beliefs from different cluster centers in order to generate data reconstructions whose errors are minimized both during recognition and learning we demonstrate a weakness inherent to the popular weighted sum followed by sigmoid squashing and offer an alternative form of the nonlinearity results are presented demonstrating the algorithms ability successfully to discover coherent multiple causal representations of noisy test data and in images of printed characters i
we present a new algorithm for eliminating excess parameters and improving network generalization after supervised training the method principal cmnponents pruning pcp is based on principal component analysis of the node activations of successive layers of the network it is simple cheap to implement and effective it requires no network retraining and does not involve calculating the full hessian of the cost function only the weight and the node activity correlation matrices for each layer of nodes are required we demonstrate the efficacy of the method on a regression problem using polynomial basis functions and on an economic time series prediction problem using a two layer feedforward network i

we analyze a simple hill climbing algorithm imhc that was previously shown to outperform a genetic algorithm ga on a simple royal load function we then analyze an idealized genetic algorithm iga that is significantly faster than imhc and that gives a lower bound for ga speed we identify the features of the iga that give rise to this speedup and discuss how these features can be incorporated into a real ga
selecting a good model of a set of input points by cross validation is a computationally intensive process especially if the number of possible models or the number of training points is high techniques such as gradient descent are helpful in searching through the space of models but problems such as local minima and more importantly lack of a distance metric between various models reduce the applicability of these search methods hoeffding races is a technique for finding a good model for the data by quickly discarding bad models and concentrating the computational effort at differentiating between the better ones this paper focuses on the special case of leave one out cross validation applied to memorybased learning algorithms but we also argue that it is applicable to any class of model selection problems
we show how an ehnan network architecture constructed from recurrently connected oscillatory associative memory network modules can employ selective attentional control of synchronization to direct the flow of communication and computation within the architecture to solve a grammatical inference problem previously we have shown how the discrete time ehnan network algorithm can be implemented in a network completely described by continuous ordinary differential equations the time steps machine cycles of the system are implemented by rhythmic variation clocking of a bifurcation parameter in this architecture oscillation amplitude codes the information content or activity of a module unit whereas phase and frequency are used to softwire the network only synchronized modules communicate by exchanging amplitude information the activity of non resonating modules contributes incoherent crosstalk noise attentional control is modeled as a special subset of the hidden modules with ouputs which affect the resonant frequencies of other hidden modules they control synchrony among the other modules and direct the flow of computation attention to effect transitions between two subgraphs of a thirteen state automaton which the system emulates to generate a reber grammar the internal crosstalk noise is used to drive the required random transitions of the automaton baird troyer and eeckman
learning to recognize or predict sequences using long term context has many applications however practical and theoretical problems are found in training recurrent neural networks to perform tasks in which inputoutput dependencies span long intervals starting from a mathematical analysis of the problem we consider and compare alternative algorithms and architectures on tasks for which the span of the inputoutput dependencies can be controlled results on the new algorithms show performance qualitatively superior to that obtained with backpropagation

this paper introduces gnarl an evolutionary program which induces recurrent neural networks that are structurally unconstrained in contrast to constructive and destructive algorithms gnarl employs a population of networks and uses a fitness functions unsupervised feedback to guide search through network space annealing is used in generating both gaussian weight changes and structural modifications applying gnarl to a complex search and collection task demonstrates that the system is capable of inducing networks with complex internal dynamics
with a point matching distance measure which is invariant under translation rotation and permutation we learn d point set objects by clustering noisy point set images unlike traditional clustering methods which use distance measures that operate on feature vectors a representation common to most problem domains this object based clustering technique employs a distance measure specific to a type of object within a problem domain formulating the clustering problem as two nested objective functions we derive optimization dynamics similar to the expectation maximization algorithm used in mixture models i
data clustering amounts to a combinatorial optimization problem to reduce the complexity of a data representation and to increase its precision central and pairwise data clustering are studied in the maximum entropy framework for central clustering we derive a set of reestimation equations and a minimization procedure which yields an optimal number of clusters their centers and their cluster probabilities a meanfield approximation for pairwise clustering is used to estimate assignment probabilities a selfconsistent solution to multidimensional scaling and pairwise clustering is derived which yields an optimal embedding and clustering of data points in a d dimensional euclidian space
one of the advantages of supervised learning is that the final error metric is available during training for classifiers the algorithm can directly reduce the number of misclassifications on the training set unfortunately when modeling human learning or constructing classifiers for autonomous robots supervisory labels are often not available or too expensive in this paper we show that we can substitute for the labels by making use of structure between the pattern distributions to different sensory modalities we show that minimizing the disagreement between the outputs of networks processing patterns from these different modalities is a sensible approximation to minimizing the number of misclassifications in each modality and leads to similar results using the peterson barney vowel dataset we show that the algorithm performs well in finding appropriate placement for the codebook vectors particularly when the confuseable classes are different for the two modalities
real world learning tasks may involve high dimensional data sets with arbitrary patterns of missing data in this paper we present a framework based on maximum likelihood density estimation for learning from such data sets we use mixture models for the density estimates and make two distinct appeals to the expectationmaximization em principle dempster et al in deriving a learning algorithm em is used both for the estimation of mixture components and for coping with missing data the resulting algorithm is applicable to a wide range of supervised as well as unsupervised learning problems results kom a classification benchmark the iris data set are presented
we analyze how data with uncertain or missing input features can be incorporated into the training of a neural network the general solution requires a weighted integration over the unknown or uncertain input although computationally cheaper closed form solutions can be found for certain gaussian basis function gbf networks we also discuss cases in which heuristical solutions such as substituting the mean of an unknown input can be harmful i
we describe a number of learning rules that can be used to train supervised parallel feature extraction systems the learning rules are derived using gradient ascent of a quality function we consider a number of quality functions that are rational functions of higher order moments of the extracted feature values we show that one system learns the principle components of the correlation matrix principal component analysis systems are usually not optimal feature extractors for classification therefore we design quality functions which produce feature vectors that support unsupervised classification the properties of the different systems are compared with the help of different artificially designed datasets and a database consisting of all munsell color spectra i
the singular value decomposition svd is an important tool for linear algebra and can be used to invert or approximate matrices although many authors use svd synonymously with eigenvector decomposition or principal components transform it is important to realize that these other methods apply only to symmetric matrices while the svd can be applied to arbitrary nonsquare matrices this property is important for applications to signal transmission and control i propose two new algorithms for iterative computation of the svd given only sample inputs and outputs from a matrix although there currently exist many algorithms for eigenvector decomposition sanger for example these are the first true samplebased svd algorithms
we present a fast algorithm for non linear dimension reduction the algorithm builds a local linear model of the data by merging pca with clustering based on a new distortion measure experiments with speech and image data indicate that the local linear algorithm produces encodings with lower distortion than those built by five layer auto associative networks the local linear algorithm is also more than an order of magnitude faster to train i
an approach is presented to learning high dimensional functions in the case where the learning algorithm can affect the generation of new data a local modeling algorithm locally weighted regression is used to represent the learned function architectural parameters of the approach such as distance metrics are also localized and become a function of the query point instead of being global statistical tests are given for when a local model is good enough and sampling should be moved to a new area our methods explicitly deal with the case where prediction accuracy requirements exist during exploration by gradually shifting a center of exploration and controlling the speed of the shift with local prediction accuracy a goal directed exploration of state space takes place along the fringes of the current data support until the task goal is achieved we illustrate this approach with simulation results and results from a real robot learning a complex juggling task
by their very nature memory based algorithms such as knn or parzen windows require a computationally expensive search of a large database of prototypes in this paper we optimize the searching process for tangent distance simard lecun and denker to improve speed performance the closest prototypes are found by recursively searching included subsets of the database using distances of increasing complexity this is done by using a hierarchy of tangent distances increasing the number of tangent vectors from to its maximum and multiresolution using wavelets at each stage a confidence level of the classification is computed if the confidence is high enough the computation of more complex distances is avoided the resulting algorithm applied to character recognition is close to three orders of magnitude faster than computing the full tangent distance on every prototypes i
we propose a learning algorithm for a variable memory length markov process human communication whether given as text handwriting or speech has multi characteristic time scales on short scales it is characterized mostly by the dynamics that generate the process whereas on large scales more syntactic and semantic information is carried for that reason the conventionally used fixed memory markov models cannot capture effectively the complexity of such structures on the other hand using long memory models uniformly is not practical even for as short memory as four the algorithm we propose is based on minimizing the statistical prediction error by extending the memory or state length adaptively until the total prediction error is sufficiently small we demonstrate the algorithm by learning the structure of natural english text and applying the learned model to the correction of corrupted text using less than states the models performance is far superior to that of fixed memory models with similar number of states we also show how the algorithm can be applied to intergenic e coli dna base prediction with results comparable to hmm based methods i
four versions of a k nearest neighbor algorithm with locally adaptive k are introduced and compared to the basic k nearest neighbor algorithm knn locally adaptive knn algorithms choose the value of k that should be used to classify a query by consulting the results of cross validation computations in the local neighborhood of the query local knn methods are shown to perform similar to knn in experiments with twelve commonly used data sets encouraging results in three constructed tasks show that local methods can significantly outperform knn in specific applications local methods can be recommended for on line learning and for applications where different regions of the input space are covered by patterns solving different sub tasks
in this paper it is shown that the conventional back propagation bpp algorithm for neural network regression is robust to leverages data with corrupted but not to outliers data with y corrupted a robust model is to model the error as a mixture of normal distribution the influence function for this mixture model is calculated and the condition for the model to be robust to outliers is given em algorithm is used to estimate the parameter the usefulness of model selection criteria is also discussed illustrative simulations are performed i
the conventional bayesian justification of backprop is that it finds the map weight vector as this paper shows to find the map i o function instead one must add a correction term to backprop that term biases one towards i o functions with small description lengths and in particular favors some kinds of feature selection pruning and weight sharing

in drug activity prediction as in handwritten character recognition the features extracted to describe a training example depend on the pose location orientation etc of the example in handwritten character recognition one of the best techniques for addressing this problem is the tangent distance method of simard lecun and denker jain et al a b introduce a new technique dynamic reposing that also addresses this problem dynamic reposing iteratively learns a neural network and then reposes the examples in an effort to maximize the predicted output values new models are trained and new poses computed until models and poses converge this paper compares dynamic reposing to the tangent distance method on the task of predicting the biological activity of musk compounds in a fold cross validation a comparison of dynamic reposing and tangent distance for drug activity prediction dynamic reposing attains correct compared to for the tangent distance method for a neural network with standard poses and for the nearest neighbor method
we propose a method for improving the performance of any network designed to predict the next value of a time series we advocate analyzing the deviations of the networks predictions from the data in the training set this can be carried out by a secondary network trained on the time series of these residuals the combined system of the two networks is viewed as the new predictor we demonstrate the simplicity and success of this method by applying it to the sunspots data the small corrections of the secondary network can be regarded as resulting from a taylor expansion of a complex network which includes the combined system we find that the complex network is more difficult to train and performs worse than the two step procedure of the combined system
the back propagation algorithm has been modified to work without any multiplications and to tolerate computations with a low resolution which makes it more attractive for a hardware implementation numbers are represented in floating point format with i bit mantissa and bits in the exponent for the states and i bit mantissa and bit exponent for the gradients while the xveights are bit fixed point numbers in this way all the computations can be executed with shift and add operations large netxvorks with over weights were trained and demonstrated the same performance as networks computed with full precision an estimate of a circuit implementation shows that a large network can be placed on a single chip reaching more than billion weight updates per second a speedup is also obtained on any machine where a multiplication is slower than a shift operation i
bumptrees are geometric data structures introduced by omohundro to provide efficient access to a collection of functions on a euclidean space of interest we describe a modified bumptree structure that has been employed as a neural network classifier and compare its performance on several classification tasks against that of radial basis function networks and the standard mutli layer perceptron
performance of many nonparametric methods critically depends on the strategy for positioning knots along the regression surface constrained topological mapping algorithm is a novel method that achieves adaptive knot placement by using a neural network based on kohonens self organizing maps we present a modification to the original algorithm that provides knot placement according to the estimated second derivative of the regression surface i
we present a new incremental radial basis function network suitable for classification and regression problems center positions are continuously updated through soft competitive learning the width of the radial basis functions is derived from the distance to topological neighbors during the training the observed error is accumulated locally and used to determine where to insert the next unit this leads in case of classification problems to the placement of units near class borders rather than near frequency peaks as is done by most existing methods the resulting networks need few training epochs and seem to generalize very well this is demonstrated by examples i
we extend optimal brain surgeon obs a second order method for pruning networks to allow for general error measures and explore a reduced computational and storage implementation via a dominant eigenspace decomposition simulations on nonlinear noisy pattern classification problems reveal that ob does lead to improved generalization and performs favorably in comparison with optimal brain damage obd we find that the required retraining steps in obd may lead to inferior generalization a result that can be interpreted as due to injecting noise back into the system a common technique is to stop training of a large network at the minimum validation error we found that the test error could be reduced even further by means of ob but not obd pruning our results justify the t o approximation used in ob and indicate why retraining in a highly pruned network may lead to inferior performance hassibi stork wolff and watanabe
in the present paper we propose an entropy method to transform the internal representation the entropy function is defined with respect to the state of hidden unit that is internal representation the internal representation can be transformed by changing the parameter c for the entropy function thus the transformation is referred to as transformation the internal representation can be transformed according to given problems by transforming the internal representation into the minimum entropy representation we can obtain kernel networks smaller networks with explicit interpretation on the other hand by changing appropriately the parameter a we can obtain intermediate internal representations for the improved generalization we applied the entropy method to an autoencoder and we succeeded in obtaining kernel networks with small internal entropy in addition we applied the method to the frequency identification problem and we could obtain derived networks whose generalization performance was significantly superior to the performance by standard back propagation i
we present an algorithm for the training of feedforward and recurrent neural networks it detects internal representation conflicts and uses these conflicts in a constructive manner to add new neurons to the network the advantages are twofold starting with a small network neurons are only allocated when required by detecting and resolving internal conflicts at an early stage learning time is reduced empirical results on two real world problems substantiate the faster learning speed when applied to the training of a recurrent network on a well researched sequence recognition task the reber grammar training times are significantly less than previously reported
i propose a learning algorithm for learning hierarchical models for object recognition the model architecture is a compositional hierarchy that represents part whole relationships parts are described in the local context of substructures of the object the focus of this report is learning hierarchical models from data ie inducing the structure of model prototypes from observed exemplars of an object at each node in the hierarchy a probability distribution governing its parameters must be learned the connections between nodes reflects the structure of the object the formulation of substructures is encouraged such that their parts become conditionally independent the resulting model can be interpreted as a bayesian belief network and also is in many respects similar to the stochastic visual grammar described by mjolsness
this paper proposes a practical optimization method for layered neural networks by which the optimal model and parameter can be found simultaneously we modify the conventional information criterion into a differentiable function of parameters and then minimize it while controlling it back to the ordinary form effectivehess of this inethod is discussed theoretically and experimentally i
we study the problexn of when to stop learning a class of feedforward networks networks with linear outputs neuron and fixed input weights when they are trained with a gradient descent algorithm on a finite number of examples under general regularity conditions it is shown that there are in general three distinct phases in the generalization performance in the learning process and in particular the network has better generalization performance when learning is stopped at a certain time before the global minimum of the empirical error is reached a notion of effective size of a machine is defined and used to explain the trade off between the complexity of the machine and the training error in the learning process the study leads naturally to a network size selection criterion which turns out to be a generalization of akaikes infornmtion criterion for the learning process it is shown tha t stopping learning before the global minimum of the empirical error has the effect of network size selection

we study the complexity problem in artificial feedforward neural networks designed to approximate real valued functions of several real variables ie we estimate the number of neurons in a network required to ensure a given degree of approximation to every function in a given function class we indicate how to construct networks with the indicated number of neurons evaluating standard activation functions our general theorem shows that the smoother the activation function the better the rate of approximation i
training classifiers on large databases is computationally demanding it is desirable to develop efficient procedures for a reliable prediction of a classifiers suitability for implementing a given task so that resources can be assigned to the most promising candidates or freed for exploring new classifier candidates we propose such a practical and principled predictive method practical because it avoids the costly procedure of training poor classifiers on the whole training set and principled because of its theoretical foundation the effectiveness of the proposed procedure is demonstrated for both singleand multi layer networks i
we study feed forward nets with arbitrarily many layers using the standard sigmoid tanh x aside from technicalities our theorems are complete knowledge of the output of a neural net for arbitrary inputs uniquely specifies the architecture weights and thresholds and there are only finitely many critical points on the error surface for a generic training problem neural nets were originally introduced as highly simplified models of the nervous system today they are widely used in technology and studied theoretically by scientists from several disciplines however they remain little understood mathematically a feed forward neural net consists of i a finite sequence of positive integers do d dl a family of real numbers w defined for t l j d and l kde a family of real numbers defined for il ijd the sequence do d d is called the architecture of the neural net while the wk are called weights and the thresholds neural nets are used to compute non linear maps from i to m by the following construction ve begin by fixing a nonlinear function x of one variable analogy with the nervous system suggests that we take x asymptotic to constants as x tends to q x a standard choice which we adopt throughout this paper is r alternate address dept of mathematics princeton university princeton nj t fefferman and markel e for tanhx given an input ttoo oo we define real numbers x l j dt by the following induction on t tj if g then xj if the x are known vith g fixed g l then we set xj wjix oj for jsd lkdt here x z t dt are interpreted as the outputs of dt neurons in the th layer of the net the output map of the net is defined as the map i tl tdo zf z in practical applications one tries to pick the neural net ddtd w so that the output map i approximates a given map about which we have only imperfect information the main result of this paper is that under generic conditions perfect knowledge of the output map i uniquely specifies the architectre the weights and the thresholds of a neural net up to obvious symmetries more precisely the obvious symmetries are as follows let be permutations with e de de and let e l j dr be a collection of t ls assume that yt identity and ej whenever or l then one checks easily that the neural nets dodd wj oj and s h ave do dl t the same output map if we set e t t t t t jk zjwytjvtlkek and oj ejotj i this reflects the facts that the neurons in layer are interchangeable l and that the function orx is odd the nets and will be called isornorphc if they are related by note in particular that isomorphic neural nets have the same architecture our main theorem asserts that under generic conditions any two neural nets with the same output map are isomorphic we discuss the generic conditions which we impose on neural nets we have to avoid obvious counterexamples such as suppose all the weights w are zero then the output map i is constant the architecture and thresholds of the neural net are clearly not uniquely determined by i fix j j with e l and j j dto suppose we have e and to to to xo therefore the wj k wj k for all k then gives xj recovering a feed forward net from its output tl fol so o lo only through the sum jj output depends on j and wjj the output map does not uniquely determine the weights our hypotheses are more than adequate to exclude these counterexamples specifically we assume that and ffl ffi for j j wk and forjjtheratio l wjkwj k is not equal to any fraction of the form pq with p q integers and q d evidently these conditions hold for generic neural nets the precise statement of our main theorem is as follows if two neural nets satisfy and have the same output then the nets are isomorphic it would be interesting to replace by minimal hypotheses and to study functions rrc other than tanh x we now sketch the proof of our main result sacrificing accuracy for simplicity after a trivial reduction we may assume do d thus the outputs of the nodes c t are functions of one variable and the output map of the neural net is t at the key idea is to continue the xst analytically to complex values oft note and to read off the structure of the net from the set of singularities of the xj that rrc tanh x is meromorphic with poles at the points of an arithmetic progression m ri m z this leads to two crucial observations e and when the poles of zjt form an arithmetic progression when g every pole of any z t is an accumulation point of poles of any xj t t rrwt j which is merely in fact is immediate from the formula z the special case do of we obtain rn ri rn g z iij cv to see fix e j and assume for simpliciw that r t has a simple pole at to while x t k is analytic in a neighborhood of to then lt t to from ft and we obtain with f analytic in a neighborhood of to ls xjt to gt with gt wjft z e wjxk t oj analytic in a neighborhood of to thus in a neighborhood of to the poles of x t are the solutions f of the equation e to fefferman and markel there are infinitely many solutions of accumulating at to hence to is an accumulation point of poles of at which completes the proof of hi view of it is natural to make the following definitions the natural domain of a neural net is the largest open subset of the complex plane to which the output map t zt can be analytically continued for t we define the th singular set singc by setting singo complement of the natural domain in c and singe the set of all accumulation points of singt these definitions are made entirely in terms of the output map without reference to the structure of the given neural net on the other hand the sets singr contain nearly complete information on the architecture weights and thresholds of the net this will allow us to read off the structure of a neural net from the analytic continuation of its output map to see how the sets singf reflect the structure of the net we reason as follows from and we expect that for l singl t is the union over j dt of the set of poles of zjt together with their accumulation points which we ignore here and for t l singt is empty immediately then we can read off the depth l of the neural net it is simply the smallest e for which singt is empty z we proceed bv induction on ve need to solve for d when and show that singl is the union of arithmetic progressions nj j therefore from sing we can read off d and the ii we will return to this point later in the
we show how randomly scrambling the output classes of various fractions of the training data may be used to improve predictive accuracy of a classification algorithm we present a method for calculating the noise sensitivity signature of a learning algorithm which is based on scrambling the output classes this signature can be used to indicate a good match between the complexity of the classifier and the complexity of the data use of noise sensitivity signatures is distinctly different from other schemes to avoid overtraining such as cross validation which uses only part of the training data or various penalty functions which are not data adaptive noise sensitivity signature methods use all of the training data and are manifestly data adaptive and non parametric they are well suited for situations with limited training data i
we have recently shown that the widely known lms algorithm is an h a optimal estimator the h a criterion has been introduced initially in the control theory literature as a means to ensure robust performance in the face of model uncertainties and lack of statistical information on the exogenous signals we extend here our analysis to the nonlinear setting often encountered in neural networks and show that the backpropagation algorithm is locally h a optimal this fact provides a theoretical justification of the widely observed excellent robustness properties of the lms and backpropagation algorithms we further discuss some implications of these results i
in this paper the efficiency of recurrent neural network implementations of m state finite state machines will be explored specifically it will be shown that the node complexity for the unrestricted case can be bounded above by o x it will also be shown that the node complexity is o xm log m when the weights and thresholds are restricted to the set and o m when the fan in is restricted to two matching lower bounds will be provided for each of these upper bounds assuming that the state of the fsm can be encoded in a subset of the nodes of size log mi i
for two layer networks with n sigmoidal hidden units the generalization error is shown to be bounded by o cd n where d and n are the input dimension and the number of training samples respectively e represents the expectation on random number i of hidden units x n the probability pri k k n is dctermined by a prior distribution of weights which corresponds to a gibbs distribtttion of a regularizer this relationship makes it possible to characterize explicitly how a regularization term affects biasvariance of networks the bound can be obtained analytically for a large c lass of commonly used priors it can also be applied to estimate the expected network complexity er in practice the result provides a quantitative explanation on how large networks can generalize well
we show that a randomly selected n tuple of points of r with probability is such that any multi layer perceptron with the first hidden layer composed of hi threshold logic units can impleof ment exactly zi then such a perceptron must have all units of the first hidden layer fully connected to inputs this implies the maximal capacities in the sense of cover of n input patterns per hidden unit and input patterns per synaptic weight of such networks both capacities are achieved by networks with single hidden layer and are the same as for a single neuron comparing these results with recent estimates of vc dimension we find that in contrast to the single neuron case for sufficiently large n and hi the vc dimension exceeds covers capacity i
the fundamental backpropagation bp algorithm for training artificial neural networks is cast as a deterministic nonmonotone perturbed gradient method under certain natural assumptions such as the series of learning rates diverging while the series of their squares converging it is established that every accumulation point of the online bp iterates is a stationary point of the bp error function the results presented cover serial and parallel online bp modified bp with a momentum term and bp with weight decay

the problem of learning from examples in multilayer networks is studied within the framework of statistical mechanics using the replica formalism we calculate the average generalization error of a fully connected committee machine in the limit of a large number of hidden units if the number of training examples is proportional to the number of inputs in the network the generalization error as a function of the training set size approaches a finite value if the number of training examples is proportional to the number of weights in the network we find first order phase transitions with a discontinuous drop in the generalization error for both binary and continuous weights
neurons learning under an unsupervised hebbian learning rule can perform a nonlinear generalization of principal component analysis this relationship between nonlinear pca and nonlinear neurons is reviewed the stable fixed points of the neuron learning dynamics correspond to the maxima of the statist ic optimized under nonlinear pca however in order to predict what the neuron learns knowledge of the basins of attractions of the neuron dynamics is required here the correspondence between nonlinear pca and neural networks breaks down this is shown for a simple model methods of statistical mechanics can be used to find the optima of the objective function of non linear pca this determines what the neurons can learn in order to find how the solutions are partitioned amoung the neurons however one must solve the dynamics
we describe the use of smoothing spline analysis of variance ssanova in the penalized log likelihood context for learning estimating the probability p of a outcome given a training set with attribute vectors and outcomes p is of the form pt elt elt where if t is a vector of attributes f is learned as a sum of smooth functions of one attribute plus a sum of smooth functions of two attributes etc the smoothing parameters governing f are obtained by an iterative unbiased risk or iterative gcv method confidence intervals for these estimates are available
solvable models of nonlinear lemning machines me proposed and learning in artificial neural networks is studied based on the theory of ordinary differential equations a learning algorithm is constructed by which the optimal parameter can be found without any recursive procedure the solvable models enable us to analyze the reason why experimental results by the error backpropagation often contradict the statistical learning theory

the satisfiability of random cnf formulae with precisely k variables per clause k sat is a popular testbed for the performance of search algorithms formulae have m clauses from n variables randomly negated keeping the ratio a mn fixed for k this model has been proven to have a sharp threshold at a between formulae which are almost aways satisfiable and formulae which are almost never satisfiable as n oc computer experiments for k and carried out in collaboration with b selman of att bell labs show similar threshold behavior for each value of k finite size scaling a theory of the critical point phenomena used in statistical physics is shown to characterize the size dependence near the threshold annealed and replica based mean field theories give a good account of the results permanent address ibm tj watson research center yorktown heights ny usa kirkwatsonibmcom portions of this work were done while visiting the salk institute with support from the mcdonnell pew foundation kirkpatrick gyorgyi tishby and troyansky large scale computation without a length scale it is increasingly possible to model the natural world on a computer condensed matter physics has strategies to manage the complexities of such calculations usually depending on a characteristic length for example molecules or atoms with finite ranged ineracfions can be broken down into weakly interacing smaller parts we may also use symmetry to identify natural modes of the system as a whole even in the most difficult case continuous phase transitions correlated over a wide range of scales the renormalization group provides a way of collapsing the problem down to its relevant parts by providing a generator of behavior on all scales in terms of the critical point itself but length scales are not much help in organizing another sort of large calculation examples include large rule based expert systems that model the particulars of complex industrial processes digital equipment for example has used a network of three or more expert systems originally called rxcon to check computer orders for completeness and internal consistency to schedule production and shipping and to aid a salesman to anticipate customers needs this very detailed set of tasks in required programmers and rules to deal with parts in the ten years described by barker it grew x employing programmers and nearly rules to deal with part numbers x in ten years is only moderate growth and it would be valuable to understand how technical social and business factors have constrained it many important commercial and scientific problems without length scales are ready for attack by computer modelling or automatic classification and lie within a few decades of xcons size retail industries routinely track distinct items kept in stock banks credit card companies and specialized information providers are building models of what americans have bought and might want to buy next in biology human metabolism is currently described in terms of substances coupled through reactions and the data is doubling yearly similarly amino acid sequences are known for proteins a deeper understanding of the computational cost of these problems of order is needed to see which are practical and how they can be simplified we study an idealization of xcon style resolution search and find obvious collective effects which may be at the heart of its computational complexity threshold phenomena and random k sat properties of randomly generated combinatorial structures often exhibit sharp threshold phenomena analogous to the phase transitions studied in condensed matter physics recently thresholds have been observed in randomly generated boolean formulae mitchell et al consider the k satisfiability problem k sat an instance of k sat is a boolean formula in conjunctive normal form cnf ie a conjunction logical and of disjunctions or clauses logical ors where each disjunction contains exactly k literals a literal is a boolean variable or with equal probability its negation the task is to determine whether there is an assignment to the variables such that all clauses evaluate to true here we will use n to denote the number of variables and m for the number of clauses in a formula the statistical mechanics of k satisfaction for randomly generated sat instances it has been shown analytically that for large n when the ratio rt mn is less than the instances are almost all satisfiable whereas for ratios larger than almost all instances are unsatisfiable chvtal and reed goerdt for k a rigorous analysis has proven to be elusive experimental evidence however strongly suggests a threshold with a m for sat mitchell et al crawford and auton larrabee one of the main reasons for studying randomly generated cnf formulae is for their use in the empirical evaluation of combinatorial search algorithms cnf formulae are good candidates for the evaluation of such algorithms because determining their satisfiability is an np complete problem this also holds for larger values of k for k or the satisfiability problem can be solved efficiently aspvall et al despite the worst case complexity simple heuristic methods can usually determine the satisfiability of random formulae however computationally challenging test instances are found by generating formulae at or near the threshold mitchell et al cheeseman has made a similar observation of increased computational cost for heuristic search at a boundary between two distinct phases or behaviors of a combinatorial model we will provide a precise characterization of the n dependence of the threshold phenomena for k sat with k ranging from to we will employ finite size scaling a method from statistical physics in which direct observation of the width of the threshold or critical region of a transition is used to characterize the universal behavior of quantities across the entire critical region extending the analysis to combinatorial problems in which n characterizes the size of the model observed for discussion of the applicability of finite size scaling to systems without a metric see kirkpatrick and selman thresholds for sat sat sat sat and sat o i o o j mn o fig fraction of unsatisfiable formulae for and sat kirkpatrick gyrgyi tishby and troyansky experimental data we have generated extensive data on the satisfiability of randomly generated kcnf formulae with k ranging from to fig i shows the fraction of random k sat formulae that is unsatisfiable as a function of the ratio for example the left most curve in fig i shows the fraction of formulae that is unsatisfiable for random cnf formulae with variables over a range of values of each data point was generated using randomly generated formulae giving accuracy we used a highly optimized implementation of the davis putnam procedure crawford and auton the procedure works best on formulae with smaller k data was obtained for k on samples with n for k with n and for k with n all at comparable computing cost fig for n ranging from to shows a threshold for each value of k except for the case k the curves cross at a single point and sharpen up with increasing n for k the intersections between the curves for the largest values of n seem to be converging to a single point as well although the curves for smaller n deviate the point where of the formulae are unsatisfiable is thought to be where the computationally hardest problems are found mitchell e al cheeseman e al the point lies consistently to the right of the scale invariant point the point where the curves cross each other and shifts with n there is a simple explanation for the rapid shift of the thresholds to the right with increasing k the probability that a given clause is satisfied by a random input configuration is if we treat the clauses as independent the probability that all clauses are satisfied is m v we define the entropy per input as n times the log of the expected number of satisfying configurations n s alog ototann and the vanishing of the entropy gives an estimate of the threshold identical to the upper bound derived by several workers see franco and citations in chvhtal aam log n this is called an annealed estimate for ac because it ignores the interactions between clauses just as annealed theories of materials see mdzard average over many details of the disorder we have marked aa with an arrow for each k in the figures and tabulate it in table results of finite size scaling analysis from fig it is clear that the threshold sharpens up for larger values of n both the threshold shift and the increasing slope in the curves of fig can be accounted for by finite size scaling see stauffer and aharony or kirkpatrick and swendsen we plot the fraction of samples unsatisfied against the dimensionless rescaled variable y no values for rc and y must be derived from the experimental data first a is determined as the crossing point of the curves for large n in fig then y is determined to make the slopes match up through the critical region in fig for k we find that these two parameters capture both the threshold shift and the steepening of the curves using a and y we see that f the fraction the statistical mechanics of k satisfaction o o scaled crossover funcbon all sat models k n oe k n soo annealed limit o x y fig rescaled sat data using tc u fig rescaled data for and sat approach annealed limit of unsatisfiable formulae is given by fn fy where the invariant function f is that graphed in fig a description of the threshold shift follows immediately if we define y by fy then ets etc tyn from fig we find that cs n crawford and auton fit their data on the point as a function of n by arbitrarily assuming that the leading correction will be on they obtain eta in however the two expressions differ by only a few percent as n ranges from to c we also obtained good results in rescaling the data for the other values of k in table i we give the critical parameters obtained from this analysis the error bars are subjective and show the range of each parameter over which the best fits were obtained note that u appears to be tending to and aann becomes an increasingly good approximation to etc as k increases the success of finite size scaling with different powers u is strong evidence for criticality ie diverging correlations even in the absence of any length finally we found that all the crossovers were similar in shape in fact combining the various rescaled curves in figure shows that the curves for k all coincide in the vicinity of the point and tend to a limiting form which can be obtained by extending the annealed arguments of the previous section if we define then the probability that a formula remains unsatisfied for all n configurations is fy y the curve for k is similar in form but shifted to the right from the other ones kirkpatrick gyrgyi tishby and troyansky o a n n ol oz c o q table critical parameters for random k sat outline of statistical mechanics analysis space permits only a sketch of our analysis of this model since the n inputs are binary we may represent them as a vector x of ising spins x xi l i n each random formula can be written as a sum of its m clauses cj m where jl k cj iijjtx i where the vector jjl has only one non zero element q i at the input which it selects evaluates to the number of clauses left unsatisfied by a particular configuration it is natural to take the value of to be the energy the partition function z tre tr h ecj where is the inverse of a fictitious temperature factors into contributions from each clause the annealed approximation mentioned above consists simply of taking the trace over each subproduct individually neglecting their interactions in this construction we expect both energy and entropy to be extensive quantities that is proportional to n fig shows that this is indeed the case for srt the lines in fig are the annealed predictions srt k alefarm expressions for the energy can also be obtained from the annealed theory and used to compare the specific heat observed in numerical experiments with the simple limit in which the clauses do not interact this gives evidence supporting the identification of the unsatisfied phase as a spin glass finally a plausible phase diagram for the spin glass like unsatisfied phase is obtained by solving for st at finite temperatures to perform the averaging over the random clauses correctly requires introducing replicas see mdzard which are identical copies of the random formula and defining q the overlap between the expectation values of the spins in any two replicas as the new order parameter the results appear to be capable of accounting the statistical mechanics of k satisfaction for the difference between experiment and the annealed predictions at finite k for example an uncontrolled approximation in which we consider just two replicas gives the values of as in table and accounts rather closely for the average overlap found experimentally between pairs of lowest energy states as shown in fig the replica theory gives q as the solution of k q k qk l qln q q for q as a function of a this gives the lines in fig we defined a in table as the point of inflection or the maximum in the slope of fig entropy as function of a for k g and fig q calculated from g replica theory vs experimental ground state overlaps arrows pointing up are oann arrows pointing down are ag conclusions we have shown how finite size scaling methods from statistical physics can be used to model the threshold in randomly generated k sat problems given the good fit of our scaling analysis we conjecture that this method can also give useful models of phase transitions in other combinatorial problems with a control parameter several authors have attempted to relate np hardness or np completeness to the characteristics of phase transitions in models of disordered systems fu and anderson see fu have proposed spin glasses magnets with spin interactions of random sign as having inherent exponential complexity huberman and colleagues see clearwater were first to focus on the diverging correlation length seen at continuous phase transitions as the root of computational complexity in fact both effects can play important roles but are not sufficient and may not even be necessary there are np complete problems eg travelling salesman or max clique which lack a phase boundary at which hard problems cluster percolation thresholds are phase transitions yet the cost of exploring the largest cluster never exceeds n steps exponential search cost in k sat comes from the random signs of the inputs which require that the space be searched repeatedly note that a satisfying kirkpatrick gyrgyi tishby and troyansky input configuration in sat can be determined or its non existence proven in polynomial time because it can be reduced to a percolation problem on a random directed graph aspvall the spin glass hamiltonians studied by fu and anderson have a form close to our sat formulae but the questions studied are different finding an input configuration which falsities the minimum number of clauses is like finding the ground state in a spin glass phase and is np hard when c even for k therefore if both diverging correlations diverging in size if no lengths are defined and random sign or spin glass effects are present we expect a local search like davis putnam to be exponentially difficult on average but these characteristics do not imply np completeness references aspvail b plass mf and tarjan re a linear time algorithm for testing the truth of certain quantified boolean formulae inform process let vol barker v e and oconnor d commun assoc for computing machinery cheeseman p kanefsky b and taylor wm where the really hard problems are proceedings ijcai clearwater sh huberman ba hogg t cooperative solution of constraint satisfaction problems science vol crawford jm and auton ld experimental results on the crossover point in satisfiability problems proc of aaai chvgtal v and reed b mick gets some the odds are on his side proc of stoc u y the uses and abuses of statistical mechanics in computational complexity in lectures in the sciences of complexity ed d stein pp addison wesley franco j and paull m probabilistic analysis of the davis putnam procedure for solving the satisfiability problem discrete applied math vol goerdt a a threshold for unsatisfiability proc th int syrup on the math foundations of comp sc prague czechoslovakia kirkpatrick s and swendsen rh statistical mechanics and disordered systems cacm vol kirkpatrick s and selman b submitted for publication larrabee t and tsuji y evidence for a satisfiability threshold for random cnf formulas proc of the aaai spring symposium on ai and np hard problems palto alto ca mdzard m parisi g virasoro ma spin glass theory and beyond singapore world scientific mitchell d selman b and levesque hj hard and easy distributions of sat problems proc of aaai stauffer d and aharony a

we prove that except possibly for small exceptional sets discretetime analog neural nets are globally observable ie all their corrupted pseudo orbits on computer simulations actually reflect the true dynamical behavior of the network locally finite discrete boolean neural networks are observable without exception
what is the correct theoretical description of neuronal activity the analysis of the dynamics of a globally connected network of spiking neurons the spike response model shows that a description by mean firing rates is possible only if active neurons fire incoherently if firing occurs coherently or with spatio temporal correlations the spike structure of the neural code becomes relevant alternatively neurons can be gathered into local or distributed ensembles or assemblies a description based on the mean ensemble activity is in principle possible but the interaction between different assemblies becomes highly nonlinear a description with spikes should therefore be preferred
most theoretical investigations of large recurrent networks focus on the properties of the macroscopic order parameters such as population averaged activities or average overlaps with memories however the statistics of the fluctuations in the local activities may be an important testing ground for comparison between models and observed cortical dynamics we evaluated the neuronal correlation functions in a stochastic network comprising of excitatory and inhibitory populations we show that when the network is in a stationary state the cross correlations are relatively weak ie their amplitude relative to that of the auto correlations are of order of ln n being the size of the interacting population this holds except in the neighborhoods of bifurcations to nonstationary states as a bifurcation point is approached the amplitude of the cross correlations grows and becomes of order and the decay timeconstant diverges this behavior is analogous to the phenomenon of critical slowing down in systems at thermal equilibrium near a critical point near a hopf bifurcation the cross correlations exhibit damped oscillations ginzburg and sompolinsky
stochastic optimization algorithms typically use learning rate schedules that behave asymptotically as t ot the ensemble dynamics leen and moody for such algorithms provides an easy path to results on mean squared weight error and asymptotic normality we apply this approach to stochastic gradient algorithms with momentum we show that at late times learning is governed by an effective learning rate e o where is the momentum parameter we describe the behavior of the asymptotic weight error and give conditions on e that insure optimal convergence speed finally we use the results to develop an adaptive form of momentum that achieves optimal convergence speed independent of o i
in meilijson and ruppin we presented a methodological framework describing the two iteration performance of hopfieldlike attractor neural networks with history dependent bayesian dynamics we now extend this analysis in a number of directions input patterns applied to small subsets of neurons general connectivity architectures and more efficient use of history we show that the optimal signal activation function has a slanted sigmoidal shape and provide an intuitive account of activation functions with a non monotone shape this function endows the model with some properties characteristic of cortical neurons firing i
motivated by mathematical modeling analog implementation and distributed simulation of neural networks we present a definition of asynchronous dynamics of general ct dynamical systems defined by ordinary differential equations based on notions of local times and communication times we provide some preliminary results on globally asymptotical convergence of asynchronous dynamics for contractive and monotone ct dynamical systems when applying the results to neural networks we obtain some conditions that ensure additive type neural networks to be asynchronizable
several recurrent networks have been proposed as representations for the task of formal language learning after training a recurrent network recognize a formal language or predict the next symbol of a sequence the next logical step is to understand the information processing carried out by the network some researchers have begun to extracting finite state machines from the internal state trajectories of their recurrent networks this paper describes how sensitivity to initial conditions and discrete measurements can trick these extraction methods to return illusory finite state descriptions
biological neurons have a variety of intrinsic properties because of the large number of voltage dependent currents that control their activity neuromodulatory substances modify both the balance of conductances that determine intrinsic properties and the strength of synapses these mechanisms alter circuit dynamics and suggest that functional circuits exist only in the modulatory environment in which they operate
intradendritic electrophysiological recordings reveal a bewildering repertoire of complex electrical spikes and plateaus that are difficult to reconcile with conventional notions of neuronal function in this paper we argue that such dendritic events are just an exuberant expression of a more important mechanism a proportional current amplifier whose primary task is to offset electrotonic losses using the example of functionally important synaptic inputs to the superficial layers of an anatomically and electrophysiologically reconstructed layer pyramidal neuron we derive and simulate the properties of conductances that linearize and amplify distal synaptic input current in a graded manner the amplification depends on a potassium conductance in the apical tuft and calcium conductances in the apical trunk to whom all correspondence should be addressed bernander koch and douglas
based on precise anatomical data of the bees olfactory system we propose an investigation of the possible mechanisms of modulation and control between the two levels of olfactory information processing the antennal lobe glomeruli and the mushroom bodies we use simplified neurons but realistic architecture as a first conclusion we postulate that the feature extraction performed by the antennal lobe glomeruli and interneurons necessitates central input from the mushroom bodies for fine tuning the central input thus facilitates the evolution from fuzzy olfactory images in the glomerular layer towards more focussed images upon odor presentation
using a quasi realistic model of the feedback inhibition of motoneurons mns by renshaw cells we show that weak inhibition is sufficient to maximally desynchronize mns with negligible effects on total mn activity mn synchrony can produce a hz peak in the force power spectrum which may cause instability in feedback loops
maps of orientation preference and ocular dominance were recorded optically from the cortices of infant macaque monkeys ranging in age from to weeks in agreement with previous observations we found that basic features of orientation and ocular dominance maps as well as correlations between them are present and robust by weeks of age we did observe changes in the strength of ocular dominance signals as well as in the spacing of ocular dominance bands both of which increased steadily between and weeks of age the latter finding suggests that the adult spacing of ocular dominance bands depends on cortical growth in neonatal animals since we found no corresponding increase in the spacing of orientation preferences however there is a possibility that the orientation preferences of some cells change as the cortcal surface expands since correlations between the patterns of orientation selectivity and ocular dominance are present at an age when the visual system is still immature it seems more likely that their development may be an innate process and may not require extensive visual experience obermayer kiorpes and blasdel i
in order to best understand a visual system one should attempt to characterize the natural images it processes we gather images from the woods and find that these scenes possess an ensemble scale invariance further they are highly non gaussian and this nongaussian character cannot be removed through local linear filtering we find that including a simple gain control nonlinearity in the filtering process makes the filter output quite gaussian meaning information is maximized at fixed channel variance finally we use the measured power spectrum to place an upper bound on the information conveyed about natural scenes by an array of receptors i
the fovea of a mammal retina was simulated with its detailed biological properties to study the local preprocessing of images the direct visual pathway photoreceptors bipolar and ganglion cells and the horizontal units as well as the d amacrine cells were simulated the computer program simulated the analog non spiking transmission between photoreceptor and bipolar cells and between bipolar and ganglion cells as well as the gap junctions between horizontal cells and the release of dopamine by d amacrine cells and its diffusion in the extra cellular space a x photoreceptors retina containing units was carried out this retina displayed contour extraction with a mach effect and adaptation to brightness the simulation showed that the dopaminergic amacrine cells were necessary to ensure adaptation to local brightness
a gradient descent algorithm for parameter estimation which is similar to those used for continuous time recurrent neural networks was derived for hodgkin huxley type neuron models using membrane potential trajectories as targets the parameters maximal conductances thresholds and slopes of activation curves time constants were successfully estimated the algorithm was applied to modeling slow non spike oscillation of an identified neuron in the lobster stomatogastric ganglion a model with three ionic currents was trained with experimental data it revealed a novel role of a current for slow oscillation below mv i
we provide a computational description of the function of the mauthner system this is the bralnstem circuit which initiates faststart escapes in telcost fish in response to sounds our simulations using backpropagation in a realistically constrained feedforward network have generated hypotheses which are directly interpretable in terms of the activity of the auditory nerve fibers the principle cells of the system and their associated inhibitory neurons
in an effort to understand saccadic eye movements and their relation to visual attention and other forms of eye movements we in collaboration with a number of other laboratories are carrying out a large scale effort to design and build a complete primate oculomotor system using analog cmos vlsi technology using this technology a low power compact multi chip system has been built which works in real time using real world visual inputs we describe in this paper the performance of an early version of such a system including a d array of photoreceptors mimicking the retina a circuit computing the mean location of activity representing the superior colliculus a saccadic burst generator and a one degree of freedom rotational platform which models the dynamic properties of the primate oculomotor plant i
signal processing and classification algorithms often have limited applicability resulting from an inaccurate model of the signals underlying structure we present here an efficient bayesian algorithm for modeling a signal composed of the superposition of brief poisson distributed functions this methodology is applied to the specific problem of modeling and classifying extracellular neural waveforms which are composed of a superposition of an unknown number of action potentials aps previous approaches have had limited success due largely to the problems of determining the spike shapes deciding how many are shapes distinct and decomposing overlapping aps a bayesian solution to each of these problems is obtained by inferring a probabilistic model of the waveform this approach quantifies the uncertainty of the form and number of the inferred ap shapes and is used to obtain an efficient method for decomposing complex overlaps this algorithm can extract many times more information than previous methods and facilitates the extracellular investigation of neuronal classes and of interactions within neuronal circuits bayesian modeling and classification of neural signals

we do not have a good understanding of how theoretical principles of learning are realized in neural systems to address this problem we built a computational model of development in the owls sound localization system the structure of the model is drawn from known experimental data while the learning principles come from recent work in the field of brain style computation the model accounts for numerous properties of the owls sound localization system makes specific and testable predictions for future experiments and provides a theory of the developmental process i
masino and knudsen showed some remarkable results which suggest that head motion in the barn owl is controlled by distinct circuits coding for the horizontal and vertical components of movement this implies the existence of a set of orthogonal internal coordinates that are related to meaningful coordinates of the external world no coherent computational theory has yet been proposed to explain this finding i have proposed a simple model which provides a framework for a theory of low level motor learning i show that the theory predicts the observed microstimulation results in the barn owl the model rests on the concept of optimal unsupervised motor learning which provides a set of criteria that predict optimal internal representations i describe two iterative neural network algorithms which find the optimal solution and demonstrate possible mechanisms for the development of internal representations in animals i
i detail the design and construction of an analog vlsi model of the neural system responsible for swimming behaviors of the leech why the leech the biological network is small and relatively well understood and the silicon model can therefore span three levels of organization in the leech nervous system neuron ganglion system it represents one of the first comprehensive models of leech swimming operating in real time the circuit employs biophysically motivated analog neurons networked to form multiple biologically inspired silicon ganglia these ganglia are coupled using known interganglionic connections thus the model retains the flavor of its biological counterpart and though simplified the output of the silicon circuit is similar to the output of the leech swim central pattern generator the model operates on the same timeand spatial scale as the leech nervous system and will provide an excellent platform with which to explore real time adaptive locomotion in the leech and other simple invertebrate nervous systems

transition point dynamic programming tpdp is a memorybased reinforcement learning direct dynamic programming approach to adaptive optimal control that can reduce the learning time and memory usage required for the control of continuous stochastic dynamic systems tpdp does so by determining an ideal set of transition points tps which specify only the control action changes necessary for optimal control tpdp converges to an ideal tp set by using a variation of q learning to assess the merits of adding swapping and removing tps from states throughout the state space when applied to a race track problem tpdp learned the optimal control policy much sooner than conventional q learning and was able to do so using less memory
recently ott grebogi and yorke ogy found an effective method to control chaotic systems to unstable fixed points by using only small control forces however ogys method is based on and limited to a linear theory and requires considerable knowledge of the dynamics of the system to be controlled in this paper we use two radial basis function networks one as a model of an unknown plant and the other as the controller the controller is trained with a recurrent learning algorithm to minimize a novel objective function such that the controller can locate an unstable fixed point and drive the system into the fixed point with no a priori knowledge of the system dynamics our results indicate that the neural controller offers many advantages over ogys technique


this paper describes the q routing algorithm for packet routing in which a reinforcement learning module is embedded into each node of a switching network only local communication is used by each node to keep accurate statistics on which routing decisions lead to minimal delivery times in simple experiments involving a node irregularly connected network q routing proves superior to a nonadaptive algorithm based on precomputed shortest paths and is able to route efficiently even when critical aspects of the simulation such as the network load are allowed to vary dynamically the paper concludes with a discussion of the tradeoff between discovering shortcuts and maintaining stable policies
consider the problem of learning inputoutput mappings through exploration eg learning the kinematics or dynamics of a robotic manipulator if actions are expensive and computation is cheap then we should explore by selecting a trajectory through the input space which gives us the most amount of information in the fewest number of steps i discuss how results from the field of optimal experiment design may be used to guide such exploration and demonstrate its use on a simple kinematics problem i
we describe the relationship between certain reinforcement learning rlmethods based on dynamic programming dpand a class of unorthodox monte carlo methods for solving systems of linear equations proposed in the s these methods recast the solution of the linear system as the expected vlue of a statistic suitably defined over sample paths of a markov chain the significance of our observations lies in arguments curriss that these monte carlo methods scale better with respect to state space size than do standard iterative techniques for solving systems of linear equations this analysis also establishes convergence rate estimates because methods used in rl systems for approximating the evaluation function of a fixed control policy also approximate solutions to systems of linear equations the connection to these monte carlo methods establishes that algorithms very similar to td algorithms sutton are asymptotically more efficient in a precise sense than other methods for ewluating policies further all dp based rl methods have some of the properties of these monte carlo algorithms which suggests that although rl is often perceived to be slow for sufficiently large problems it may in fact be more efficient than other known classes of methods capable of producing the same results barto and duff
reinforcement learning methods based on approximating dynamic programming dp are receiving increased attention due to their utility in forming reactive control policies for systems embedded in dynamic environments environments are usually modeled as controlled markov processes but when the environment model is not known a priori adaptive methods are necessary adaptive control methods are often classified as being direct or indirect direct methods directly adapt the control policy from experience whereas indirect methods adapt a model of the controlled process and compute control policies based on the latest model our focus is on indirect adaptive dp based methods in this paper we present a convergence result for indirect adaptive asynchronous value iteration algorithms for the case in which a look up table is used to store the value function our result implies convergence of several existing reinforcement learning algorithms such as adaptive real time dynamic programming artdp barto bradtke singh and prioritized sweeping moore atkeson although the emphasis of researchers studying dp bascd reinforcement learning has been on direct adaptive methods such as q learning watkins and methods using td algorithms sutton it is not clear that these direct methods are preferable in practice to indirect methods such as those analyzed in this paper gullapalli and barto
increasing attention has recently been paid to algorithms based on dynamic programming dp due to the suitability of dp for learning problems involving control in stochastic environments where the system being controlled is only incompletely known however a unifying theoretical account of these methods has been missing in this paper we relate dp based learning algorithms to the powerful techniques of stochastic approximation via a new convergence theorem enabling us to establish a class of convergent algorithms to which both tda and q learning belong i

we describe an extension to the mixture of experts architecture for modelling and controlling dynamical systems which exhibit nultiple modes of behavior this extension is based on a markov process model and suggests a recurrent network for gating a set of linear or non linear controllers the new architecture is demonstrated to be capable of learning effective control strategies for jump linear and non linear plants with multiple modes of behavior i
we propose a trajectory planning and control theory for continuous movements such as connected cursive handwriting and continuous natural speech its hardware is based on our previously proposed forward inverse relaxation neural network wada kawato computationally its optimization principle is the minimum torquechange criterion regarding the representation level hard constraints satisfied by a trajectory are represented as a set of via points extracted from a handwritten character accordingly we propose a via point estimation algorithm that estimates via points by repeating the trajectory formation of a character and the via point extraction from the character in experiments good quantitative agreement is found between human handwriting data and the trajectories generated by the theory finally we propose a recognition schema based on the movement generation we show a result in which the recognition schema is applied to the handwritten character recognition and can be extended to the phoneme timing estimation of natural speech
this paper describes an algorithm for verification of signatures written on a pen input tablet the algorithm is based on a novel artificial neural network called a siamese neural network this network consists of two identical sub networks joined at their outputs during training the two sub networks extract features from two signatures while the joining neuron measures the distance between the two feature vectors verification consists of comparing an extracted feature vector with a stored feature vector for the signer signatures closer to this stored representation than a chosen threshold are accepted all other signatures are rejected as forgeries i
this paper describes the use of a convolutional neural network to perform address block location on machine printed mail pieces locating the address block is a difficult object recognition problem because there is often a large amount of extraneous printing on a mail piece and because address blocks vary dramatically in size and shape we used a convolutional locator network with four outputs each trained to find a different corner of the address block a simple set of rules was used to generate abl candidates from the network output the system performs very well when allowed five guesses the network will tightly bound the address delivery information in of the cases
we have developed an artificial neural network based gaze tracking system which can be customized to individual users unlike other gaze trackers which normally require the user to wear cumbersome headgear or to use a chin rest to ensure head immobility our system is entirely non intrusive currently the best intrusive gaze tracking systems are accurate to approximately degrees in our experiments we have been able to achieve an accuracy of degrees while allowing head mobility in this paper we present an empirical analysis of the performance of a large number of artificial neural network architectures for this task
human genes are not continuous but rather consist of short coding regions exons interspersed with highly variable non coding regions introns we apply hmms to the problem of modeling exons introns and detecting splice sites in the human genome our most interesting result so far is the detection of particular oscillatory patterns with a minimal period of roughly nucleotides that seem to be characteristic of exon regions and may have significant biological implications and division of biology california institute of technology rand department of psychology stanford university baldi brunak chauvin engelbrecht and krogh oxon intron exon splice site acceptor site splice site donor site consensus sequences i i i i i ntcag g cccccccc ag gtagt figure structure of eukaryotic genes not to scale introns are typically much longer than exons
changes in lighting conditions strongly effect the performance and reliability of computer vision systems we report face recognition results under drastically changing lighting conditions for a computer vision system which concurrently uses a contrast sensitive silicon retina and a conventional gain controlled ccd camera for both input devices the face recognition system employs an elastic matching algorithm with wavelet based features to classify unknown faces to assess the effect of analog on chip preprocessing by the silicon retina the ccd images have been digitally preprocessed with a bandpass filter to adjust the power spectrum the silicon retina with its ability to adjust sensitivity increases the recognition rate up to percent these comparative experiments demonstrate that preprocessing with an analog vlsi silicon retina generates image data enriched with object constant features
this paper introduces a new recognition based segmentation approach to recognizing on line cursive handwriting from a database of english words the original input stream of a y pen coordinates is encoded as a sequence of uniform stroke descriptions that are processed by six feed forward neural networks each designed to recognize letters of different sizes words are then recognized by performing best first search over the space of all possible segmentations results demonstrate that the method is effective at both writer dependent recognition to error rate and writer independent recognition to error rate i
we developed a system for finding address blocks on mail pieces that can process four images per second besides locating the address block our system also determines the writing style handwritten or machine printed and moreover it measures the skew angle of the text lines and cleans noisy images a layout analysis of all the elements present in the image is performed in order to distinguish drawings and dirt from text and to separate text of advertisement from that of the destination address a speed of more than four images per second is obtained on a modular hardware platform containing a board with two of the netk neural net chips a sparc processor board and a board with digital signal processors the system has been tested with more than images its performance depends on the quality of the images and lies between correct location in vej noisy images to over in cleaner images

airline companies usually schedule their flights and crews well in advance to optimize their crew pools activities many events such as flight delays or the absence of a member require the crew pool rescheduling team to change the initial schedule rescheduling in this paper we show that the neural network comparison paradigm applied to the backgammon game by tesauro tesatlro and sejnowski can also be applied to the rescheduling problem of an aircrew pool indeed both problems correspond to choosing the best solution from a set of possible ones without ranking them called here best choice problem the paper explains from a mathematical point of view the architecture and the learning strategy of the backpropagation neural network used for the best choice problem we also show how the learning phase of the network can be accelerated finally we apply the neural network model to some real rescheduling problems for the belgian airline sabena i

the game of go has a high branching factor that defeats the tree search approach used in computer chess and long range spatiotemporal interactions that make position evaluation extremely difficult development of conventional go programs is hampered by their knowledge intensive nature we demonstrate a viable alternative by training networks to evaluate go positions via temporal difference td learning our approach is based on network architectures that reflect the spatial organization of both input and reinforcement signals on the go board and training protocols that provide exposure to competent though unlabelled play these techniques yield far better performance than undifferentiated networks trained by selfplay alone a network with less than weights learned within games of x go a position evaluation function that enables a primitive one ply search to defeat a commercial go program at a low playing level

online cursive handwriting recognition is currently one of the most intriguing challenges in pattern recognition this study presents a novel approach to this problem which is composed of two complementary phases the first is dynamic encoding of the writing trajectory into a compact sequence of discrete motor control symbols in this compact representation we largely remove the redundancy of the script while preserving most of its intelligible components in the second phase these control sequences are used to train adaptive probabilistic acyclic automata paa for the important ingredients of the writing trajectories eg letters we present a new and efficient learning algorithm for such stochastic automata and demonstrate its utility for spotting and segmentation of cursive scripts our experiments show that over of the letters are correctly spotted and identified prior to any higher level language model moreover both the training and recognition algorithms are very efficient compared to other modeling methods and the models are on line adaptable to other writers and styles
this paper describes the mmk a massively parallel simd computer which is easy to program high in performance low in cost and effective for implementing highly parallel neural network architectures the mmk has bit serial processing elements each of which has bits of memory and all of which are interconnected by a switching network the entire system resides on a single pc at compatible card it is programmed from the host computer using a c language class library which abstracts the parallel processor in terms of fast arithmetic operators for vectors of variable precision integers i
a neurocomputer was implemented using radial basis functions and a combination of analog and digital vlsi cimuits the hybrid system uses custom analog circuits for the input layer and a digital signal processiag board for the hidden and output layers the system combines the advantages of both analog and digital cimuits featuring low power consumption while minimizing overall system error the analog circuits have been fabricated and tested the system has been built and several applications have been executed on the system one application provides significantly better results for a remote sensing problem than have been previously obtained using onvelltional methods
we present experimental results on supervised learning of dynamical features in an analog vlsi neural network chip the recurrent network containing six continuous time analog neurons and free parameters connection strengths and thresholds is trained to generate time varying outputs approximating given periodic signals presented to the network the chip implements a stochastic perturbative algorithm which observes the error gradient along random directions in the parameter space for error descent learning in addition to the integrated learning functions and the generation of pseudo random perturbations the chip provides for teacher forcing and long term storage of the volatile parameters the network learns a i khz circular trajectory in sec the chip occupies mm x mm in a pm cmos process and dissipates mw i
recent physiological research has shown that synchronization of oscillatory responses in striate cortex may code for relationships between visual features of objects a vlsi circuit has been designed to provide rapid phase locking synchronization of multiple oscillators to allow for further exploration of this neural mechanism by exploiting the intrinsic random transistor mismatch of devices operated in subthreshold large groups of phase locked oscillators can be readily partitioned into smaller phase locked groups a multiple target tracker for binary images is described utilizing this phase locking architecture a vlsi chip has been fabricated and tested to verify the architecture the chip employs pulse amplitude modulation pam to encode the output at the periphery of the system i
this paper describes a low power analogue vlsi neural network called wattle wattle is a three layer perceptron with multiplying dac synapses and on chip switched capacitor neurons fabricated in um cmos the on chip neurons facillitate variable gain per neuron and lower energyconnection than for previous designs the intended application of this chip is intra cardiac electrogram classification as part of an implantable pacemakerdefibrillator system measurements of the chip indicate that pj per connection is achievable as part of an integrated system wattle has been successfully trained in loop on parity and iceg morphology classification problems i
we use mean field theory methods from statistical mechanics to derive the softmax nonlinearity from the discontinuous winnertake all wta mapping we give two simple ways of implementing softmax as a multiterminal network element one of these has a number of important network theoretic properties it is a reciprocal passive incrementally passive nonlinear resistive multiterminal element with a content function having the form of informationtheoretic entropy these properties should enable one to use this element in nonlinear itc networks with such other reciprocal elements as resistive fuses and constraint boxes to implement very high speed analog optimization algorithms using a minimum of hardware

we built a high speed digital mean field boltzmann chip and sbus board for general problems in constraint satisfaction and learning each chip has neural processors and weight update processors supporting an arbitrary topology of up to functional neurons on chip learning is at a theoretical maximum rate of x s connection updatessec recall is patternssec for typical conditions the chips high speed is due to parallel computation of inner products limited but adequate precision for weights and activations bits fast clock mhz and several design insights digital boltzmann vlsi for constraint satisfaction and learning i
we present a neural network simulation which we implemented on the massively parallel connection machine in contrast to previous work this simulator is based on biologically realistic neurons with nontrivial single cell dynamics high connectivity with a structure modelled in agreement with biological data and preservation of the temporal dynamics of spike interactions we simulate neural networks of neurons coupled by about synapses per neuron and estimate the performance for much larger systems communication between neurons is identified as the computationally most demanding task and we present a novel method to overcome this bottleneck the simulator has already been used to study the primary visual system of the cat i
the most commonly used neural network models are not well suited to direct digital implementations because each node needs to perform a large number of operations between floating point values fortunately the ability to learn from examples and to generalize is not restricted to networks of this type indeed networks where each node implements a simple boolean function boolean networks can be designed in such a way as to exhibit similar properties two algorithms that generate boolean networks from examples are presented the results show that these algorithms generalize very well in a class of problems that accept compact boolean network descriptions the techniques described are general and can be applied to tasks that are not known to have that characteristic two examples of applications are presented image reconstruction and hand written character recognition i
we will present the implementation of intelligent electronic circuits realized for the first time using a new functional device called neuron mos transistor neumos or vmos in short simulating the behavior of biological neurons at a single transistor level search for the most resembling data in the memory cell array for instance can be automatically carried out on hardware without any software manipulation soft hardware which we named can arbitrarily change its logic function in real time by external control signals without any hardware modification implementation of a neural network equipped with an on chip self learning capability is also described through the studies of vmos intelligent circuit implementation we noticed an interesting similarity in the architectures of vmos logic circuitry and biological systems
a fast event driven software simulator has been developed for simulating large networks of spiking neurons and synapses the primitive network elements are designed to exhibit biologically realistic behaviors such as spiking refractoriness adaptation axonal delays summation of post synaptic current pulses and tonic current inputs the efficient event driven representation allows large networks to be simulated in a fraction of the time that would be required for a full compartmental model simulation corresponding analog cmos vlsi circuit primitives have been designed and characterized so that large scale circuits may be simulated prior to fabrication
we introduce a new approach for on line recognition of handwritten words written in unconstrained mixed style the preprocessor performs a word level normalization by fitting a model of the word structure using the em algorithm words are then coded into low resolution annotated images where each pixel contains information about trajectory direction and curvature the recognizer is a convolution network which can be spatially replicated from the network output a hidden markov model produces word scores the entire system is globally trained to minimize word level errors i
we present a method for learning tracking and recognizing human hand gestures recorded by a conventional ccd camera without any special gloves or other sensors a view based representation is used to model aspects of the hand relevant to the trained gestures and is found using an unsupervised clustering technique we use normalized correlation networks with dynamic time warping in the temporal domain as a distance function for unsupervised clustering views are computed separably for space and time dimensions the distributed response of the combination of these units characterizes the input data with a low dimensional representation a supervised classification stage uses labeled outputs of the spatio temporal units as training data our system can correctly classify gestures in real time with a low cost image processing accelerator
we propose a computational model for how the cortex discriminates shape and depth from texture the model consists of four stages extraction of local spatial frequency frequency characterization detection of texture compression by normalization and integration of the normalized frequency over space the model accounts for a number of psychophysical observations including experiments based on novel random textures these textures are generated from white noise and manipulated in fourier domain in order to produce specific frequency spectra simulations with a range of stimuli including real images show qualitative and quantitative agreement with human perception
the feature correspondence problem is a classic hurdle in visual object recognition concerned with determining the correct mapping between the features measured from the image and the features expected by the model in this paper we show that determining good correspondences requires information about the joint probability density over the image features we propose likelihood based correspondence matching as a general principle for selecting optimal correspondences the approach is applicable to non rigid models allows nonlinear perspective transformations and can optimally deal with occlusions and missing features experiments with rigid and non rigid d hand gesture recognition support the theory the likelihood based techniques show almost no decrease in classification performance when compared to performance with perfect correspondence knowledge i
the goal of this work was to investigate the role of primate mt neurons in solving the structure from motion sfm problem three types of receptive field rf surrounds found in area mt neurons k tanaka et a allman et a correspond as our analysis suggests to the th i st and nd order fuzzy space differential operators the large surroundcenter radius ratio allows both differentiation of smooth velocity fields and discontinuity detection at boundaries of objects the model is in agreement with recent psychophysical data on surface interpolation involvement in sfm we suggest that area mt partially segregates information about object shape from information about spatial relations necessary for navigation and manipulation
we address the problem of optical flow reconstruction and in particular the problem of resolving ambiguities near edges they occur due to i the aperture problem and ii the occlusion problem where pixels on both sides of an intensity edge are assigned the same velocity estimates and confidence however these measurements are correct for just one side of the edge the non occluded one our approach is to introduce an uncertmnty field with respect to the estimates and confidence measures we note that the confidence measures are large at intensity edges and larger at the convex sides of the edges ie inside corners than at the concave side we resolve the ambiguities through local interactions via coupled markov random fields mrf the result is the detection of motion for regions of images with large global convexity i
we present a mean field theory method for locating twodimensional objects that have undergone rigid transformations the resulting algorithm is a form of coarse to fine correlation matching we first consider problems of matching synthetic point data and derive a point matching objective function a tractable line segment matching objective function is derived by considering each line segment as a dense collection of points and approximating it by a sum of gaussians the algorithm is tested on real images from which line segments are extracted and matched i

recent work by becker and hinton becker and hinton shows a promising mechanism based on maximizing mutual information assuming spatial coherence by which a system can selforganize itself to learn visual abilities such as binocular stereo we introduce a more general criterion based on bayesian probability theory and thereby demonstrate a connection to bayesian theories of visual perception and to other organization principles for early vision atick and redlich methods for implementation using variants of stochastic learning are described and for the special case of linear filtering we derive an analytic expression for the output
short term memory is indispensable for the processing of time varying information with artificial neural networks in this paper a model for linear memories is presented and ways to include memories in connectionist topologies are discussed a comparison is drawn among different memory types with indication of what is the salient characteristic of each memory model
spotting tasks require detection of target patterns from a background of richly varied non target inputs the performance measure of interest for these tasks called the figure of merit fom is the detection rate for target patterns when the false alarm rate is in an acceptable range a new approach to training spotters is presented which computes the fom gradient for each input pattern and then directly maximizes the fom using backpropagation this eliminates the need for thresholds during training it also uses network resources to model bayesian a posteriori probability functions accurately only for patterns which have a significant effect on the detection accuracy over the false alarm rate of interest fom training increased detection accuracy by percentage points for a hybrid radial basis function rbf hidden markov model hmm wordspotter on the credit card speech corpus
we have developed visual preprocessing algorithms for extracting phonologically relevant features from the grayscale video image of a speaker to provide speaker independent inputs for an automatic lipreading speechreading system visual features such as mouth openclosed tongue visiblenot visible teeth visiblenotvisible and several shape descriptors of the mouth and its motion are all rapidly computable in a manner quite insensitive to lighting conditions we formed a hybrid speechreading system consisting of two time delay neural networks video and acoustic and integrated their responses by means of independent opinion pooling the bayesian optimal method given conditional independence which seems to hold for our data this hybrid system had an error rate lower than that of the acoustic subsystem alone on a five utterance speaker independent task indicating that video can be used to improve speech recognition wolff prasad stork and hennecke
a new classifier is presented for text independent speaker recognition the new classifier is called the modified neural tree network mntn the ntn is a hierarchical classifier that combines the properties of decision trees and feed forward neural networks the mntn differs from the standard ntn in that a new learning rule based on discriminant learning is used which minimizes the classification error as opposed to a norm of the approximation error the mntn also uses leaf probability measures in addition to the class labels the mntn is evaluated for several speaker identification experiments and is compared to multilayer percepttons mlps decision trees and vector quantization vq classifiers the vq classifier and mntn demonstrate comparable performance and perform significantly better than the other classifiers for this task additionally the mntn provides a logarithmic saving in retrieval time over that of the vq classifier the mntn and vq classifiers are also compared for several speaker verification experiments where the mntn is found to outperform the vq classifier
progress has been made in comlutational implementation of speech production based on physiological data an inverse dynamics model of the speech articulators musculo skeletal system which is tile maiping from articulator trajectories to electromyographic emg signals was modeled using tile acquired forward dynamics model and temporal smoothness of emg activation and range constraints this inverse dynamics model allows the use of a faster speech motor control scheme which can be applied to phoneme tospeech synthesis via musclo skeletal system dynamics or to future use in speech recognition tile forward acoustic model which is the mapping from articulator trajectories to tile acoustic parameters was improved by adding velocity and voicing information inputs to distinguish acoustic parameter differences caused by chaes in source characteristics
hybrid connectionisthmm systems model time both using a markov chain and through properties of a connectionist network in this paper we discuss the nature of the time dependence currently employed in our systems using recurrent networks rns and feed forward multi layer perceptrons mlps in particular we introduce local recurrences into a mlp to produce an enhanced input representation this is in the form of an adaptive gamma filter and incorporates an automatic approach for learning temporal dependencies we have experimented on a speakerindependent phone recognition task using the timit database results using the gamma filtered input representation have shown improvement over the baseline mlp system improvements have also been obtained through merging the baseline and gamma filter models
previously we had developed the concept of a segmental neural net snn for phonetic modeling in continuous speech recognition csr this kind of neural network technology advanced the state of the art of large vocabulary csr which employs hidden markov models hmm for the arpa word resource management corpus more recently we started porting the neural net system to a larger more challenging corpus the arpa word wall street journal wsj corpus during the porting we explored the following research directions to refine the system i training context dependent models with a regularization method ii training snn with projection pursuit and ii combining different models into a hybrid system when tested on both a development set and an independent test set the resulting neural net system alone yielded a performance at the level of the hmm system and the hybrid snnhmm system achieved a consistent word error reduction over the hmm system this paper describes our hybrid system with emphasis on the optimization methods employed
although the visual and auditory systems share the same basic tasks of informing an organism about its environment most connectionist work on hearing to date has been devoted to the very different problem of speech recognition we believe that the most fundamental task of the auditory system is the analysis of acoustic signals into components corresponding to individual sound sources which bregman has called auditory scene analysis computational and connectionist work on auditory scene analysis is reviewed and the outline of a general model that includes these approaches is described
we consider the problem of how the cns learns to control dynamics of a mechanical system by using a paradigm where a subjects hand interacts with a virtual mechanical environment we show that learning control is via composition of a model of the imposed dynamics some properties of the computational elements with which the cns composes this model are inferred through the generalization capabilities of the subject outside the training data
this study explores the extent to which a network that learns the temporal relationships within and between the component features of western tonal music can account for music theoretic and psychological phenomena such as the tonal hierarchy and rhythmic expectancies predicted and generated sequences were recorded as the representation of a note waltz melody was learnt by a predictive recurrent network the network learned transitions and relations between and within pitch and timing components accent and duration values interacted in the development of rhythmic and metric structures and with training the network developed chordal expectancies in response to the activation of individual tones analysis of the hidden unit representation revealed that musical sequences are represented as transitions between states in hidden unit space
imagine you have designed a neural network that successfully learns a complex classification task what are the relevant input features the classifier relies on and how are these features combined to produce the classification decisions there are applications where a deeper insight into the structure of an adaptive system and thus into the underlying classification problem may well be as important as the systems performance characteristics eg in economics or medicine gds is a backpropagation based training scheme that produces networks transformable into an equivalent and concise set of if then rules this is achieved by imposing penalty terms on the network parameters that adapt the network to the expressive power of this class of rules thus during training we simultaneously minimize classification and transformation error some real world tasks demonstrate the viability of our approach i
a variant of the encoder architecture where units at the input and output layers represent nodes on a graph is applied to the task of mapping locations to sets of neighboring locations the degree to which the resuiting internal ie hidden unit representations reflect global properties of the environment depends upon several parameters of the learning procedure architectural bottlenecks noise and incremental learning of landmarks are shown to be important factors in maintaining topographic relationships at a global scale i
models of analog retrieval require a computationally cheap method of estimating similarity between a probe and the candidates in a large pool of memory items the vector dot product operation would be ideal for this purpose if it were possible to encode complex structures as vector representations in such a way that the superficial similarity of vector representations reflected underlying structural similarity this paper describes how such an encoding is provided by holographic reduced representations hrrs which are a method for encoding nested relational structures as fixed width distributed representations the conditions under which structural similarity is reflected in the dot product rankings of hrrs are discussed
the non linear complexities of neural networks make network solutions difficult to understand sangers contribution analysis is here extended to the analysis of networks automatically generated by the cascadecorrelation learning algorithm because such networks have cross connections that supersede hidden layers standard analyses of hidden unit activation patterns are insufficient a contribution is defined as the product of an output weight and the associated activation on the sending unit whether that sending unit is an input or a hidden unit multiplied by the sign of the output target for the current input pattern intercorrelations among contributions as gleaned from the matrix of contributions x input patterns can be subjected to principal components analysis pca to extract the main features of variation in the contributions such an analysis is applied to three problems continuous xor arithmetic comparison and distinguishing between two interlocking spirals in all three cases this technique yields useful insights into network solutions that are consistent across several networks
in this paper we propose an extension to the raam by pollack this extension the labeling raam lraam can encode labeled graphs with cycles by representing pointers explicitly data encoded in an lraam can be accessed by pointer as well as by content direct access by content can be achieved by transforming the encoder network of the lraam into an analog hopfield network with hidden units different access procedures can be defined depending on the access key sufficient conditions on the asymptotical stability of the associated hopfield network are briefly introduced
we apply active exemplar selection plutowski white to predicting a chaotic time series given a fixed set of examples the method chooses a concise subset for training fitting these exemplars results in the entire set being fit as well as desired the algorithm incorporates a method for regulating network complexity automatically adding exemplars and hidden units as needed fitting examples generated from the mackey glass equation with fractal dimension to an rmse of required about exemplars and to hidden units the method requires an order of magnitude fewer floating point operations than training on the entire set of examples is significantly cheaper than two contending exemplar selection techniques and suggests a simpler active selection technique that performs comparably i
a new neural network the binary diamond is presented and its use as a classifier is demonstrated and evaluated the network is of the feed forward type it learns from examples in the one shot mode and recruits new neurons as needed it was tested on the problem of pixel classification and performed well possible applications of the network in associative memories are outlined
in this paper we will consider the problem of classifying electroencephalogram eeg signals of normal subjects and subjects suffering from psychiatric disorder eg obsessive compulsive disorder schizophrenia using a class of artificial neural networks viz multi layer perceptton it is shown that the multilayer perceptton is capable of classifying unseen test eeg signals to a high degree of accuracy
almost all models of orientation and direction selectivity in visual cortex are based on feedforward connection schemes where geniculate input provides all excitation to both pyramidal and inhibitory neurons the latter neurons then suppress the response of the former for non optimal stimuli however anatomical studies show that up to of the excitatory synaptic input onto any cortical cell is provided by other cortical cells the massive excitatory feedback nature of cortical circuits is embedded in the canonical microcircuit of douglas martin we here investigate analytically and through biologically realistic simulations the functioning of a detailed model of this circuitry operating in a hysterelic mode in the model weak geniculate input is dramatically amplified by intracortical excitation while inhibition has a dual role i to prevent the early geniculate induced excitation in the null direction and ii to restrain excitation and ensure that the neurons fire only when the stimulus is in their receptive field among the humbert suarez christof koch rodney douglas insights gained are the possibility that hysteresis underlies visual cortical function paralleling proposals for short term memory and strong limitations on linearity tests that use gratings properties of visual cortical neurons are compared in detail to this model and to a classical model of direction selectivity that does not include excitatory cortico cortical connections the model explain a number of puzzling features of direction selective simple cells including the small somatic input conductance changes that have been measured experimentally during stimulation in the null direction the model also allows us to understand why the velocity response curve of area neurons is different from that of their lgn afferents and the origin of expansive and compressive nonlinearities in the contrast response curve of striate cortical neurons
we propose a computational framework for understanding and modeling human consciousness this framework integrates many existing theoretical perspectives yet is sufficiently concrete to allow simulation experiments we do not attempt to explain qualia subjective experience but instead ask what differences exist within the cognitive information processing system when a person is conscious of mentally represented information versus when that information is unconscious the central idea we explore is that the contents of consciousness correspond to temporally persistent states in a network of computational modules three simulations are described illustrating that the behavior of persistent states in the models corresponds roughly to the behavior of conscious states people experience when performing similar tasks our simulations show that periodic settling to persistent ie conscious states improves performance by cleaning up inaccuracies and noise forcing decisions and helping keep the system on track toward a solution
biological sensorimotor systems are not static maps that transform input sensory information into output motor behavior evidence from many lines of research suggests that their representations are plastic experience dependent entities while this plasticity is essential for flexible behavior it presents the nervous system with dimcult organizational challenges if the sensorimotor system adapts itself to perform well under one set of circumstances will it then perform poorly when placed in an environment with different demands negative transfer will a later experience dependent change undo the benefits of previous learning catastrophic interference we explore the first question in a separate paper in this volume shadmehr et al here we present psychophysical and computational results that explore the question of catastrophic interference in the context of a dynamic motor learning task under some conditions subjects show evidence of catastrophic interference under other conditions however subjects appear to be immune to its effects these results suggest that motor learning can undergo a process of consolidation modular neural networks are well suited for the demands of learning multiple inputoutput mappings by incorporating the notion of fastand slow changing connections into a modular architecture we were able to account for the psychophysical results tom brashers krug reza shadmehr emanuel todorov i

current understanding of the effects of damage on neural networks is rudimentary even though such understanding could lead to important insights concerning neurological and psychiatric disorders motivated by this consideration we present a simple analytical framework for estimating the functional damage resulting from focal structural lesions to a neural network the effects of focal lesions of varying area shape and number on the retrieval capacities of a spatially organized associative memory although our analytical results are based on some approximations they correspond well with simulation results this study sheds light on some important features characterizing the clinical manifestations of multi infarct dementia including the strong association between the number of infarcts and the prevalence of dementia after stroke and the multiplicative interaction that has been postulated to occur between alzheimers disease and multi infarct dementia dr reggia is also with the department of neurology and the institute of advanced computer studies at the university of maryland eytan ruppin james a reggia
based on computational principles with as yet no direct experimental validation it has been proposed that the central nervous system cns uses an internal model to simulate the dynamic behavior of the motor system in planning control and learning sutton and barto ito kawato et al jordan and rumelhart miall et al we present experimental results and simulations based on a novel approach that investigates the temporal propagation of errors in the sensorimotor integration process our results provide direct support for the existence of an internal model i
a model of short term memory for serially ordered lists of verbal stimuli is proposed as an implementation of the articulatory loop thought to mediate this type of memory baddeley the model predicts the presence of a repeatable time varying context signal coding the timing of items presentation in addition to a store of phonological information and a process of serial rehearsal items are associated with context nodes and phonemes by hebbian connections showing both short and long term plasticity items are activated by phonemic input during presentation and reactivated by context and phonemic feedback during output serial selection of items occurs via a winner take all interaction amongst items with the winner subsequently receiving decaying inhibition an approximate analysis of error probabilities due to gaussian noise during output is presented the model provides an explanatory account of the probability of error as a function of serial position list length word length phonemic similarity temporal grouping item and list familiarity and is proposed as the starting point for a model of rehearsal and vocabulary acquisition
a new model for chemosensory reception is presented it models reactions between odor molecules and receptor proteins and the activation of second messenger by receptor proteins the mathematical formulation of the reaction kinetics is transformed into an artificial neural network ann the resulting feed forward network provides a powerful means for parameter fitting by applying learning algorithms the weights of the network corresponding to chemical parameters can be trained by presenting experimental data we demonstrate the simulation capabilities of the model with experimental data from honey bee chemosensory neurons it can be shown that our model is sufficient to rebuild the observed data and that simpler models are not able to do this task
the spatial distribution and time course of electrical signals in neurons have important theoretical and practical consequences because it is difficult to infer how neuronal form affects electrical signaling we have developed a quantitative yet intuitive approach to the analysis of electrotonus this approach transforms the architecture of the cell from anatomical to electrotonic space using the logarithm of voltage attenuation as the distance metric we describe the theory behind this approach and illustrate its use
a model of the hippocampus is presented which forms rapid self organized representations of input arriving via the perforant path performs recall of previous associations in region ca and performs comparison of this recall with afferent input in region ca this comparison drives feedback regulation of cholinergic modulation to set appropriate dynamics for learning of new representations in region ca and ca the network responds to novel patterns with increased cholinergic modulation allowing storage of new self organized representations but responds to familiar patterns with a decrease in acetylcholine allowing recall based on previous representations this requires selectivity of the cholinergic suppression of synaptic transmission in stratum radiatum of regions ca and ca which has been demonstrated experimentally
a biological neuron can be viewed as a device that maps a multidimensional temporal event signal dendritic postsynaptic activations into a unidimensional temporal event signal action potentials we have designed a network the spatio temporal event mapping stem architecture which can learn to perform this mapping for arbitrary biophysical models of neurons such a network appropriately trained called a stem cell can be used in place of a conventional compartmental model in simulations where only the transfer function is important such as network simulations the stem cell offers advantages over compartmental models in terms of computational efficiency analytical tractabililty and as a framework for vlsi implementations of biological neurons

songbirds learn to imitate a tutor song through auditory and motor learning we have developed a theoretical framework for song learning that accounts for response properties of neurons that have been observed in many of the nuclei that are involved in song learning specifically we suggest that the anteriorforebrain pathway which is not needed for song production in the adult but is essential for song acquisition provides synaptic perturbations and adaptive evaluations for syllable vocalization learning a computer model based on reinforcement learning was constructed that could replicate a real zebra finch song with accuracy based on a spectrographic measure the second generation of the birdsong model replicated the tutor song with accuracy
a neural network model for the self organization of ocular dominance and lateral connections from binocular input is presented the self organizing process results in a network where afferent weights of each neuron organize into smooth hill shaped receptive fields primarily on one of the retinas neurons with common eye preference form connected intertwined patches and lateral connections primarily link regions of the same eye preference similar self organization of cortical structures has been observed experimentally in strabismic kittens the model shows how patterned lateral connections in the cortex may develop based on correlated activity and explains why lateral connection patterns follow receptive field properties such as ocular dominance

the auditory system of the barn owl contains several spatial maps in young barn owls raised with optical prisms over their eyes these auditory maps are shifted to stay in register with the visual map suggesting that the visual input imposes a frame of reference on the auditory maps however the optic tectum the first site of convergence of visual with auditory information is not the site of plasticity for the shift of the auditory maps the plasticity occurs instead in the inferior colliculus which contains an auditory map and projects into the optic tectum we explored a model of the owl remapping in which a global reinforcement signal whose delivery is controlled by visual foveation a hebb learning rule gated by reinforcement learned to appropriately adjust auditory maps in addition reinforcement learning preferentially adjusted the weights in the inferior colliculus as in the owl brain even though the weights were allowed to change throughout the auditory system this observation raises the possibility that the site of learning does not have to be genetically specified but could be determined by how the learning procedure interacts with the network architecture alexandre pouget cedric deffayet terrence j sejnowski optic tectum alhistrlatum inferior colllculus external nucleus forebrain field l icx fbr infeor cotculus nl ucleus icc cochlea visual system figure schematic view of the auditory pathways in the barn owl i
the macaque lateral geniculate nucleus lgn exhibits an intricate lamination pattern which changes midway through the nucleus at a point coincident with small gaps due to the blind spot in the retina we present a three dimensional model of morphogenesis in which local cell interactions cause a wave of development of neuronal receptive fields to propagate through the nucleus and establish two distinct lamination patterns we examine the interactions between the wave and the localized singularities due to the gaps and find that the gaps induce the change in lamination pattern we explore critical factors which determine general lgn organization
accumulating data from neurophysiology and neuropsychology have suggested two information processing roles for prefrontal cortex pfc short term active memory and inhibition we present a new behavioral task and a computational model which were developed in parallel the task was developed to probe both of these prefrontal functions simultaneously and produces a rich set of behavioral data that act as constraints on the model the model is implemented in continuous time thus providing a natural framework in which to study the temporal dynamics of processing in the task we show how the model can be used to examine the behavioral consequences of neuromodulation in pfc specifically we use the model to make novel and testable predictions regarding the behavioral performance of schizophrenics who are hypothesized to suffer from reduced dopaminergic tone in this brain area i
we implement and study a computational model of stevens theory of the pathogenesis of schizophrenia this theory hypothesizes that the onset of schizophrenia is associated with reactive synaptic regeneration occurring in brain regions receiving degenerating temporal lobe projections concentrating on one such area the frontal cortex we model a frontal module as an associative memory neural network whose input synapses represent incoming temporal projections we analyze how in the face of weakened external input projections compensatory strengthening of internal synaptic connections and increased noise levels can maintain memory capacities which are generally preserved in schizophrenia however these compensatory changes adversely lead to spontaneous biased retrieval of stored memories which corresponds to the occurrence of schizophrenic delusions and hallucinations without any apparent external trigger and for their tendency to concentrate on just few central themes our results explain why these symptoms tend to wane as schizophrenia progresses and why delayed therapeutical intervention leads to a much slower response eytan ruppin james a reggia david horn
the parietal cortex is thought to represent the egocentric positions of objects in particular coordinate systems we propose an alternative approach to spatial perception of objects in the parietal cortex from the perspective of sensorimotor transformations the responses of single parietal neurons can be modeled as a gaussian function of retinal position multiplied by a sigmoid function of eye position which form a set of basis functions we show here how these basis functions can be used to generate receptive fields in either retinotopic or head centered coordinates by simple linear transformations this raises the possibility that the parietal cortex does not attempt to compute the positions of objects in a particular frame of reference but instead computes a general purpose representation of the retinal location and eye position from which any transformation can be synthesized by direct projection this representation predicts that hemineglect a neurological syndrome produced by parietal lesions should not be confined to egocentric coordinates but should be observed in multiple frames of reference in single patients a prediction supported by several experiments alexandre pouget terrence j sejnowski i
many cells in the dorsal part of the medial superior temporal mst area of visual cortex respond selectively to spiral flow patterns specific combinations of expansioncontraction and rotation motions previous investigators have suggested that these cells may represent self motion spiral patterns can also be generated by the relative motion of the observer and a particular object an mst cell may then account for some portion of the complex flow field and the set of active cells could encode the entire flow in this manner mst effectively segments moving objects such a grouping operation is essential in interpreting scenes containing several independent moving objects and observer motion we describe a model based on the hypothesis that the selective tuning of mst cells reflects the grouping of object components undergoing coherent motion inputs to the model were generated from sequences of ray traced images that simulated realistic motion situations combining observer motion eye movements and independent object motion the input representation was modeled after response properties of neurons in area mt which provides the primary input to area mst after applying an unsupervised learning algorithm the units became tuned to patterns signaling coherent motion the results match many of the known properties of mst cells and are consistent with recent studies indicating that these cells process d object motion information richard zemel terrence j sejnowski i

wolfgang maass institute for theoretical computer science technische universitaet graz a graz austria e mail maassigitu grazacat abstract we investigate the computational power of a formal model for networks of spiking neurons both for the assumption of an unlimited timing precision and for the case of a limited timing precision we also prove upper and lower bounds for the number of examples that are needed to train such networks i
we derive global h a optimal training algorithms for neural networks these algorithms guarantee the smallest possible prediction error energy over all possible disturbances of fixed energy and are therefore robust with respect to model uncertainties and lack of statistical information on the exogenous signals the ensuing estimators are infinite dimensional in the sense that updating the weight vector estimate requires knowledge of all previous weight esimates a certain finite dimensional approximation to these estimators is the backpropagation algorithm this explains the local h a optimality of backpropagation that has been previously demonstrated i
an novel class of locally excitatory globally inhibitory oscillator networks is proposed the model of each oscillator corresponds to a standard relaxation oscillator with two time scales the network exhibits a mechanism of selective gating whereby an oscillator jumping up to its active phase rapidly recruits the oscillators stimulated by the same pattern while preventing others from jumping up we show analytically that with the selective gating mechanism the network rapidly achieves both synchronization within blocks of oscillators that are stimulated by connected regions and desynchronization between different blocks computer simulations demonstrate the networks promising ability for segmenting multiple input patterns in real time this model lays a physical foundation for the oscillatory correlation theory of feature binding and may provide an effective computational framework for scene segmentation and figureground segregation
we present a new method for obtaining the response function and its average g from which most of the properties of learning and generalization in linear perceptrons can be derived we first rederive the known results for the thermodynamic limit of infinite perceptron size n and show explicitly that is self averaging in this limit we then discuss extensions of our method to more general learning scenarios with anisotropic teacher space priors input distributions and weight decay terms finally we use our method to calculate the finite n corrections of order in to g and discuss the corresponding finite size effects on generalization and learning dynamics an important spin off is the observation that results obtained in the thermodynamic limit are often directly relevant to systems of fairly modest real world sizes
we discuss a model of consistent learning with an additional restriction on the probability distribution of training samples the target concept and hypothesis class we show that the model provides a significant improvement on the upper bounds of sample complexity ie the minimal number of random training samples allowing a selection of the hypothesis with a predefined accuracy and confidence further we show that the model has the potential for providing a finite sample complexity even in the case of infinite vc dimension as well as for a sample complexity below vc dimension this is achieved by linking sample complexity to an average number of implementable dichotomies of a training sample rather than the maximal size of a shattered sample ie vc dimension i

learning of continuous valued functions using neural network ensembles committees can give improved accuracy reliable estimation of the generalization error and active learning the ambiguity is defined as the variation of the output of ensemble members averaged over unlabeled data so it quantifies the disagreement among the networks it is discussed how to use the ambiguity in combination with cross validation to give a reliable estimate of the ensemble generalization error and how this type of ensemble cross validation can sometimes improve performance it is shown how to estimate the optimal weights of the ensemble members using unlabeled data by a generalization of query by committee it is finally shown how the ambiguity can be used to select new training data to be labeled in an active learning scheme
random errors and insufficiencies in databases limit the performance of any classifier trained from and applied to the database in this paper we propose a method to estimate the limiting performance of classifiers imposed by the database we demonstrate this technique on the task of predicting failure in telecommunication paths
a neural network learning paradigm based on information theory is proposed as a way to perform in an unsupervisd fashion rodundancy reduction among the elements of the output layer without loss of information from the sensory input the model developed performs nonlinear decorrelation up to higher orders of the cumulant tensors and results in probabilistically independent components of the output layer this means that we dont need to assme gaussian distribution neither at the input nor at the output the theory presented is related to the unsupervised leaming theory of barlow which proposes redundancy reduction as the goal of cognition when nonlinear units are used nonlinear principal component analysis is obtained in this case nonlinear manifolds can be reduced to minimum dimension manifolds if such units are used the network performs a generalized principal component analysis in the sense that non gaussian distributions can be linearly decorrelated and higher orders of the correlation tensors are also taken into account the basic structure of the architecture involves a general transformation that is volume conserving and therefore the entropy yielding a map without loss of information minimization of the mutual information among the output neurons eliminates the redundancy between the outputs and results in statistical decorrelation of the extracted features this is known as factorial learning gustavo deco wilfried brauer
using a statistical mechanical formalism we calculate the evidence generalisation error and consistency measure for a linear perceptton trained and tested on a set of examples generated by a non linear teacher the teacher is said to be unrealisable because the student can never model it without error our model allows us to interpolate between the known case of a linear teacher and an unrealisable nonlinear teacher a comparison of the hyperparameters which maximise the evidence with those that optimise the performance measures reveals that in the non linear case the evidence procedure is a misleading guide to optimising performance finally we explore the extent to which the evidence procedure is unreliable and find that despite being sub optimal in some circumstances it might be a useful method for fixing the hyperparameters i
this paper presents a rigorous characterization of how a general nonlinear learning machine generalizes during the training process when it is trained on a random sample using a gradient descent algorithm based on reduction of training error it is shown in particular that best generalization performance occurs in general before the global minimum of the training error is achieved the different roles played by the complexity of the machine class and the complexity of the specific machine in the class during learning are also precisely demarcated
we present here an analysis of the stochastic neurodynamics of a neural network composed of three state neurons described by a master equation an outer product representation of the master equation is employed in this representation an extension of the analysis from two to three state neurons is easily performed we apply this formalism with approximation schemes to a simple three state network and compare the results with monte carlo simulations i
we present a statistical method that pac learns the class of stochastic perceptrons with arbitrary monotonic activation function and weights wi when the probability distribution that generates the input examples is member of a family that we call k blocking distributions such distributions represent an important step beyond the case where each input variable is statistically independent since the k blocking family contains all the markov distributions of order k by stochastic perceptron we mean a perceptron which upon presentation of input vector x outputs i with probability fi wixi because the same algorithm works for any monotonic nondecreasing or nonincreasing activation function f on boolean domain it handles the well studied cases of sigmods and the usual radial basis functions i
in supervised learning learning from queries rather than from random examples can improve generalization performance significantly we study the performance of query learning for problems where the student cannot learn the teacher perfectly which occur frequently in practice as a prototypical scenario of this kind we consider a linear perceptron student learning a binary perceptron teacher two kinds of queries for maximum information gain ie minimum entropy are investigated minimum student space entropy msse queries which are appropriate if the teacher space is unknown and minimum teacher space entropy mtse queries which can be used if the teacher space is assumed to be known but a student of a simpler form has deliberately been chosen we find that for msse queries the structure of the student space determines the efficacy of query learning whereas mtse queries lead to a higher generalization error than random examples due to a lack of feedback about the progress of the student in the way queries are selected i
we consider the effect of combining several least squares estimators on the expected performance of a regression problem computing the exact bias and variance curves as a function of the sample size we are able to quantitatively compare the effect of the combination on the bias and variance separately and thus on the expected error which is the sum of the two our exact calculations demonstrate that the combination of estimators is particularly useful in the case where the data set is small and noisy and the function to be learned is unrealizable for large data sets the single estimator produces superior results finally we show that by splitting the data set into several independent parts and training each estimator on a different subset the performance can in some cases be significantly improved key words bias variance least squares combination
the performance of on line algorithms for learning dichotomies is studied in on line learning the number of examples p is equivalent to the learning time since each example is presented only once the learning curve or generalization error as a function of p depends on the schedule at which the learning rate is lowered for a target that is a perceptton rule the learning curve of the perceptton algorithm can decrease as fast as p i if the schedule is optimized if the target is not realizable by a perceptton the perceptton algorithm does not generally converge to the solution with lowest generalization error for the case of unrealizability due to a simple output noise we propose a new on line algorithm for a perceptron yielding a learning curve that can approach the optimal generalization error as fast as p l we then generalize the perceptron algorithm to any class of thresholded smooth functions learning a target from that class for well behaved input distributions if this algorithm converges to the optimal solution its learning curve can decrease as fast
this paper discusses the use of artificial neural networks for dynamic modelling of time series we argue that multistep prediction is more appropriate to capture the dynamics of the underlying dynamical system because it constrains the iterated model we show how this method can be implemented by a recurrent ann trained with trajectory learning we also show how to select the trajectory length to train the iterated predictor for the case of chaotic time series experimental results corroborate the proposed method
we propose a novel rigorous approach for the analysis of linskers unsupervised hebbian learning network the behavior of this model is determined by the underlying nonlinear dynamics which are parameterized by a set of parameters originating from the hebbian rule and the arbor density of the synapses these parameters determine the presence or absence of a specific receptive field also referred to as a connection pattern as a saturated fixed point attractor of the model in this paper we perform a qualitative analysis of the underlying nonlinear dynamics over the parameter space determine the effects of the system parameters on the emergence of various receptive fields and predict precisely within which parameter regime the network will have the potential to develop a specially designated connection pattern in particular this approach exposes for the first time the crucial role played by the synaptic density functions and provides a complete precise picture of the parameter space that defines the relationships among the different receptive fields our theoretical predictions are confirmed by numerical simulations jianfeng feng h pan v p roychowdhury i
we estimate the number of training samples required to ensure that the performance of a neural network on its training data matches that obtained when fresh data is applied to the network existing estimates are higher by orders of magnitude than practice indicates this work seeks to narrow the gap between theory and practice by transforming the problem into determining the distribution of the supremum of a random field in the space of weight vectors which in turn is attacked by application of a recent technique called the poisson clumping heuristic i
we study the asymptotic properties of the sequence of iterates of weight vector estimates obtained by training a multilayer feedforward neural network with a basic gradient descent method using a fixed learning constant and no batch processing in the onedimensional case an exact analysis establishes the existence of a limiting distribution that is not gaussian in general for the general case and small learning constant a linearization approximation permits the application of results from the theory of random matrices to again establish the existence of a limiting distribution we study the first few moments of this distribution to compare and contrast the results of our analysis with those of techniques of stochastic approximation i
increasing attention has been paid to reinforcement learning algorithms in recent years partly due to successes in the theoretical analysis of their behavior in markov environments if the markov assumption is removed however neither generally the algorithms nor the analyses continue to be usable we propose and analyze a new learning algorithm to solve a certain class of non markov decision problems our algorithm applies to problems in which the environment is markov but the learner has restricted access to state information the algorithm involves a monte carlo policy evaluation combined with a policy improvement method that is similar to that of markov decision problems and is guaranteed to converge to a local maximum the algorithm operates in the space of stochastic policies a space which can yield a policy that performs considerably better than any deterministic policy although the space of stochastic policies is continuous even for a discrete action space our algorithm is computationally tractable tommi jaakkola satinder p singh michael i jordan

it is widely accepted that the use of more compact representations than lookup tables is crucial to scaling reinforcement learning rl algorithms to real world problems unfortunately almost all of the theory of reinforcement learning assumes lookup table representations in this paper we address the pressing issue of combining function approximation and rl and present a function approximator based on a simple extension to state aggregation a commonly used form of compact representation namely soft state aggregation a theory of convergence for rl with arbitrary but fixed soft state aggregation a novel intuitive understanding of the effect of state aggregation on online rl and a new heuristic adaptive state aggregation algorithm that finds improved compact representations by exploiting the non discrete nature of soft state aggregation preliminary empirical results are also presented i
a straightforward approach to the curse of dimensionality in reinforcement learning and dynamic programming is to replace the lookup table with a generalizing function approximator such as a neural net although this has been successful in the domain of backgammon there is no guarantee of convergence in this paper we show that the combination of dynamic programming and function approximation is not robust and in even very benign cases may produce an entirely wrong policy we then introduce grow support a new algorithm which is safe from divergence yet can still reap the benefits of successful generalization i

reinforcement learning addresses the problem of learning to select actions in order to maximize ones performance in unknown environments to scale reinforcement learning to complex real world tasks such as typically studied in ai one must ultimately be able to discover the structure in the world in order to abstract away the myriad of details and to operate in more tractable problem spaces this paper presents the skills algorithm skills discovers skills which are partially defined action policies that arise in the context of multiple related tasks skills collapse whole action sequences into single operators they are learned by minimizing the compactness of action policies using a description length argument on their representation empirical results in simple grid navigation tasks illustrate the successful discovery of structure in reinforcement learning
semi maxkov decision problems axe continuous time generalizations of discrete time markov decision problems a number of reinforcement leaxning lgorithms have been developed recently for the solution of markov decision problems based on the ideas of asynchronous dynamic programming and stochastic approximation among these are td q leaxning and real time dynamic programming after reviewing semi maxkov decision problems and bellmans optimality equation in that context we propose lgorithms similax to those named above adapted to the solution of semi markov decision problems we demonstrate these algorithms by applying them to the problem of determining the optimal control for a simple queueing system we conclude with a discussion of circumstances under which these algorithms may be usefully applied i
we prove the convergence of an actorcritic algorithm that is equivalent to q learning by construction its equivalence is achieved by encoding q values within the policy and value function of the actor and critic the resultant actorcritic algorithm is novel in two ways it updates the critic only when the most probable action is executed from any given state and it rewards the actor using criteria that depend on the relative probability of the action that was executed
the basic paradigm for learning in neural networks is learning from examples where a training set of input output examples is used to teach the network the target function learning from hints is a generalization of learning from examples where additional information about the target function can be incorporated in the same learning process such information can come from common sense rules or special expertise in financial market applications where the training data is very noisy the use of such hints can have a decisive advantage we demonstrate the use of hints in foreign exchange trading of the us dollar versus the british pound the german mark the japanese yen and the swiss franc over a period of months we explain the general method of learning from hints and how it can be applied to other markets the learning model for this method is not restricted to neural networks i
this paper discusses the linearly weighted combination of estimators in which the weighting functions are dependent on the input we show that the weighting functions can be derived either by evaluating the input dependent variance of each estimator or by estimating how likely it is that a given estimator has seen data in the region of the input space close to the input pattern the latter solution is closely related to the mixture of experts approach and we show how learning rules for the mixture of experts can be derived from the theory about learning with missing features the presented approaches are modular since the weighting functions can easily be modified no retraining if more estimators are added furthermore it is easy to incorporate estimators which were not derived from data such as expert systems or algorithms i
we introduce a recurrent architecture having a modular structure and we formulate a training procedure based on the em algorithm the resulting model has similarities to hidden markov models but supports recurrent networks processing style and allows to exploit the supervised learning paradigm while using maximum likelihood estimation i
we propose a statistical mechanical framework for the modeling of discrete time series maximum likelihood estimation is done via boltzmann learning in one dimensional networks with tied weights we call these networks boltzmann chains and show that they contain hidden markov models hmms as a special case our framework also motivates new architectures that address particular shortcomings of hmms we look at two such architectures parallel chains that model feature sets with disparate time scales and looped networks that model long term dependencies between hidden states for these networks we show how to implement the boltzmann learning rule exactly in polynomial time without resort to simulated or mean field annealing the necessary computations are done by exact decimation procedures from statistical mechanics
if data collection is costly there is much to be gained by actively selecting particularly informative data points in a sequential way in a bayesian decision theoretic framework we develop a query selection criterion which explicitly takes into account the intended use of the model predictions by markov chain monte carlo methods the necessary quantities can be approximated to a desired precision as the number of data points grows the model complexity is modified by a bayesian model selection strategy the properties of two versions of the criterion are demonstrated in numerical experiments

visualizing and structuring pairwise dissimilarity data are difficult combinatorial optimization problems known as multidimensional scaling or pairwise data clustering algorithms for embedding dissimilarity data set in a euclidjan space for clustering these data and for actively selecting data to support the clustering process are discussed in the maximum entropy framework active data selection provides a strategy to discover structure in a data set efficiently with partially unknown data
a new learning algorithm is derived which performs online stochastic gradient ascent in the mutual information between outputs and inputs of a network in the absence of a priori knowledge about the signal and noise components of the input propagation of information depends on calibrating network non linearities to the detailed higher order moments of the input density functions by incidentally minimising mutual information between outputs as well as maximising their individual entropies the network factorises the input into independent components as an example application we have achieved near perfect separation of ten digitally mixed speech signals our simulations lead us to believe that our network performs better at blind separation than the heraultjutten network reflecting the fact that it is derived rigorously from the mutual information objective anthony j bell terrence j sejnowski
differentiation between the nodes of a competitive learning network is conventionally achieved through competition on the basis of neural activity simple inhibitory mechanisms are limited to sparse representations while decorrelation and factorization schemes that support distributed representations are computationally unattractive by letting neural plasticity mediate the competitive interaction instead we obtain diffuse nonadaptive alternatives for fully distributed representations we use this technique to simplify and improve our binary information gain optimization algorithm for feature extraction schraudolph and sejnowski the same approach could be used to improve other learning algorithms
existing recurrent net learning algorithms are inadequate we introduce the conceptual framework of viewing recurrent training as matching vector fields of dynamical systems in phase space phasespace reconstruction techniques make the hidden states explicit reducing temporal learning to a feed forward problem in short we propose viewing iterated prediction lf as the best way of training recurrent networks on deterministic signals using this framework we can train multiple trajectories insure their stability and design arbitrary dynamical systems

dynamic cell structures dcs represent a family of artificial neural architectures suited both for unsupervised and supervised learning they belong to the recently martinetz introduced class of topology representing networks trn which build perfectly topology preserving feature maps dcs employ a modified khonen learning rule in conjunction with competitive hebbian learning the kohonen type learning rule serves to adjust the synaptic weight vectors while hebbian learning establishes a dynamic lateral connection structure between the units reflecting the topology of the feature manifold in case of supervised learning ie function approximation each neural unit implements a radial basis function and an additional layer of linear output units adjusts according to a delta rule dcs is the first rbf based approximation scheme attempting to concurrently learn and utilize a perfectly topology preserving map for improved performance simulations on a selection of cmu benchmarks indicate that the dcs idea applied to the growing cell structure algorithm fritzke leads to an efficient and elegant algorithm that can beat conventional models on similar tasks
although artificial neural networks have been applied in a variety of real world scenarios with remarkable success they have often been criticized for exhibiting a low degree of human comprehensibility techniques that compile compact sets of symbolic rules out of artificial neural networks offer a promising perspective to overcome this obvious deficiency of neural network representations this paper presents an approach to the extraction of if then rules from artificial neural networks its key mechanism is validity interval analysis which is a generic tool for extracting symbolic knowledge by propagating rule like knowledge through backpropagation style neural networks empirical studies in a robot arm domain illustrate the appropriateness of the proposed method for extracting rules from networks with real valued and distributed representations
we have determined the capacity and information efficiency of an associative net configured in a brain like way with partial connectivity and noisy input cues recall theory was used to calculate the capacity when pattern recall is achieved using a winners takeall strategy transforming the dendritic sum according to input activity and unit usage can greatly increase the capacity of the associative net under these conditions for moderately sparse patterns maximum information efficiency is achieved with very low connectivity levels this corresponds to the level of connectivity commonly seen in the brain and invites speculation that the brain is connected in the most information efficient way i
radial basis function rbf networks also known as networks of locally tuned processing units see are well known for their ease of use most algorithms used to train these types of networks however require a fixed architecture in which the number of units in the hidden layer must be determined before training starts the rce training algorithm introduced by reilly cooper and elbaurn see and its probabilistic extension the p rce algorithm take advantage of a growing structure in which hidden units are only introduced when necessary the nature of these algorithms allows training to reach stability much faster than is the case for gradient descent based methods unfortunately p rce networks do not adjust the standard deviation of their prototypes individually using only one global value for this parameter this paper introduces the dynamic decay adjustment dda algorithm which utilizes the constructive nature of the p rce algorithm together with independent adaptation of each prototypes decay factor in addition this radial adjustment is class dependent and distinguishes between different neighbours it is shown that networks trained with the presented algorithm perform substantially better than common rbf networks michael r berthold jay diamond i
we present a new algorithm for finding low complexity networks with high generalization capability the algorithm searches for large connected regions of so called fiat minima of the error function in the weight space environment of a fiat minimum the error remains approximately constant using an mdl based argument fiat minima can be shown to correspond to low expected overfitting although our algorithm requires the computation of second order derivatives it has backprops order of complexity experiments with feedforward and recurrent nets are described in an application to stock market prediction the method outperforms conventional backprop weight decay and optimal brain surgeon i
product units provide a method of automatically learning the higher order input combinations required for efficient learning in neural networks however we show that problems are encountered when using backpropagation to train networks containing these units this paper examines these problems and proposes some atypical heuristics to improve learning using these heuristics a constructive method is introduced which solves well researched problems with significantly less neurons than previously reported secondly product units are implemented as candidate units in the cascade correlation fahlman lebiere system this resulted in smaller networks which trained faster than when using sigmoidal or gaussian units i
we present a deterministic annealing variant of the em algorithm for maximum likelihood parameter estimation problems in our approach the em process is reformulated as the problem of minimizing the thermodynamic free energy by using the principle of macimum entropy and statistical mechanics analogy unlike simulated annealing approaches this minimization is deterministically performed moreover the derived algorithm unlike the conventional em algorithm can obtain better estimates free of the initial parameter values
this paper studies the problem of diffusion in markovian models such as hidden markov models hmms and how it makes very difficult the task of learning of long term dependencies in sequences using results from markov chain theory we show that the problem of diffusion is reduced if the transition probabilities approach or under this condition standard hmms have very limited modeling capabilities but inputoutput hmms can still perform interesting computations
we introduce a novel algorithm for actorial learning motivated by segmentation problems in computational vision in which the underlying factors correspond to clusters of highly correlated input features the algorithm derives from a new kind of competitive clustering model in which the cluster generators compete to explain each feature of the data set and cooperate to explain each input example rather than competing for examples and cooperating on features as in traditional clustering algorithms a natural extension of the algorithm recovers hierarchical models of data generated from multiple unknown categories each with a different multiple causal structure several simulations demonstrate the power of this approach i
this paper presents an alternating minimization am algorithm used in the training of radial basis function and linear regressor networks the algorithm is a modification of a small step interior point method used in solving primal linear programs the algorithm has a convergence rate of ov dl iterations where n is a measure of the network size and l is a measure of the resulting solutions accuracy two results are presented that specify how aggressively the two steps of the am may be pursued to ensure convergence of each step of the alternating minimization i
a self organizing neural network for sequence classification called sardnet is described and analyzed experimentally sardnet extends the kohonen feature map architecture with activation retention and decay in order to create unique distributed response patterns for different sequences sardnet yields extremely dense yet descriptive representations of sequential input in very few training iterations the network has proven successful on mapping arbitrary sequences of binary and real numbers as well as phonemic representations of english words potential applications include isolated spoken word recognition and cognitive science models of sequence processing i
this paper studies the convergence properties of the well known k means clustering algorithm the k means algorithm can be described either as a gradient descent algorithm or by slightly extending the mathematics of the em algorithm to this hard threshold case we show that the k means algorithm actually minimizes the quantization error using the very fast newton algorithm i
we develop a principled strategy to sample a function optimally for function approximation tasks within a bayesian framework using ideas from optimal experiment design we introduce an objective function incorporating both bias and variance to measure the degree of approximation and the potential utility of the data points towards optimizing this objective we show how the general strategy can be used to derive precise algorithms to select data for two cases learning unit step functions and polynomial functions in particular we investigate whether such active algorithms can learn the target with fewer examples we obtain theoretical and empirical results to suggest that this is the case
understanding knowledge representations in neural nets has been a difficult problem principal components analysis pca of contributions products of sending activations and connection weights has yielded valuable insights into knowledge representations but much of this work has focused on the correlation matrix of contributions the present work shows that analyzing the variance covariance matrix of contributions yields more valid insights by taking account of weights
casting neural network weights in symbolic terms is crucial for interpreting and explaining the behavior of a network additionally in some domains a symbolic description may lead to more robust generalization we present a principled approach to symbolic rule extraction based on the notion of weight templates parameterized regions of weight space corresponding to specific symbolic expressions with an appropriate choice of representation we show how template parameters may be efficiently identified and instantiated to yield the optimal match to a units actual weights depending on the requirements of the application domain our method can accommodate arbitrary disjunctions and copjunctions with ok complexity simple n of m expressions with ok complexity or a more general class of recursive n of m expressions with ok a complexity where k is the number of inputs to a unit our method of rule extraction offers several benefits over alternative approaches in the literature and simulation results on a variety of problems demonstrate its effectiveness
many real world learning problems are best characterized by an interaction of multiple independent causes or factors discovering such causal structure from the data is the focus of this paper based on zemel and i iintons cooperative vector quantizer cvq architecture an unsupervised learning algorithm is derived from the expectation maximization em framework due to the combinatorial nature of the data generation process the exact e step is computationally intractable two alternative methods for computing the e step are proposed gibbs sampling and mean field approximation and some promising empirical results are presented
an incremental network model is introduced which is able to learn the important topological relations in a given set of input vectors by means of a simple hebb like learning rule in contrast to previous approaches like the neural gas method of martinetz and schulten this model has no parameters which change over time and is able to continue learning adding units and connections until a performance criterion has been met applications of the model include vector quantization clustering and interpolation i
we propose an alternative model for mixtures of experts which uses a different parametric form for the gating network the modified model is trained by the em algorithm in comparison with earlier models trained by either em or gradient ascent there is no need to select a learning stepsize we report simulation experiments which show that the new architecture yields faster convergence we also apply the new model to two problem domains piecewise nonlinear function approximation and the combination of multiple previously trained classifiers i
most of the common techniques for estimating conditional probability densities are inappropriate for applications involving periodic variables in this paper we introduce three novel techniques for tackling such problems and investigate their performance using synthetic data we then apply these techniques to the problem of extracting the distribution of wind vector directions from radar scatterometer data gathered by a remote sensing satellite i
we introduce and study methods of inserting synaptic noise into dynamically driven recurrent neural networks and show that applying a controlled amount of noise during training may improve convergence and generalization in addition we analyze the effects of each noise parameter additive vs multiplicative cumulative vs non cumulative per time step vs per string and predict that best overall performance can be achieved by injecting additive noise at each time step extensive simulations on learning the dual parity grammar from temporal strings substantiate these predictions
hinton proposed that generalization in artificial neural nets should improve if nets learn to represent the domains underlying regularities abu mustafas hints work shows that the outputs of a backprop net can be used as inputs through which domainspecific information can be given to the net we extend these ideas by showing that a backprop net learning many related tasks at the same time can use these tasks as inductive bias for each other and thus learn better we identify five mechanisms by which multitask backprop improves generalization and give empirical evidence that multitask backprop generalizes better in real domains i
we present a graph based method for rapid accurate search through prototypes for transformation invariant pattern classification our method has in theory the same recognition accuracy as other recent methods based on tangent distance simard et al since it uses the same categorization rule nevertheless ours is significantly faster during classification because far fewer tangent distances need be computed crucial to the success of our system are a novel graph architecture in which transformation constraints and geometric relationships among prototypes are encoded during learning and an improved graph search criterion used during classification these architectural insights are applicable to a wide range of problem domains here we demonstrate that on a handwriting recognition task a basic implementation of our system requires less than half the computation of the euclidean sorting method i

in this paper we derive classifiers which are winner take all wta approximations to a bayes classifier with gaussian mixtures for class conditional densities the derived classifiers include clustering based algorithms like lvq and k means we propose a constrained rank gaussian mixtures model and derive a wta algorithm for it our experiments with two speech classification tasks indicate that the constrained rank model and the wta approximations improve the performance over the unconstrained models i
we present efficient algorithms for dealing with the problem of missing inputs incomplete feature vectors during training and recall our approach is based on the approximation of the input data distribution using parzen windows for recall we obtain closed form solutions for arbitrary feedforward networks for training we show how the backpropagation step for an incomplete pattern can be approximated by a weighted averaged backpropagation step the complexity of the solutions for training and recall is independent of the number of missing features we verify our theoretical results using one classification and one regression problem i
many different discrete time recurrent neural network architectures have been proposed however there has been virtually no effort to compare these archtectures experimentally in this paper we review and categorize many of these architectures and compare how they perform on various classes of simple problems including grammatical inference and nonlinear system identification i

prior constraints are imposed upon a learning problem in the form of distance measures prototypical d point sets and graphs are learned by clustering with point matching and graph matching distance measures the point matching distance measure is approx invariant under affine transformationstranslation rotation scale and shear and permutations it operates between noisy images with missing and spurious points the graph matching distance measure operates on weighted graphs and is invariant under permutations learning is formulated as an optimization problem large objectives so formulated million variables are efficiently minimized using a combination of optimization techniques algebraic transformations iterative projective scaling clocked objectives and deterministic annealing i
this paper explores the application of temporal difference td learning sutton to forecasting the behavior of dynamical systems with realvalued outputs as opposed to game like situations the performance of td learning in comparison to standard supervised learning depends on the amount of noise present in the data in this paper we use a deterministic chaotic time series from a low noise laser for the task of direct five step ahead predictions our experiments show that standard supervised learning is better than td learning the td algorithm can be viewed as linking adjacent predictions a similar effect can be obtained by sharing the internal representation in the network we thus compare two architectures for both paradigms the first architecture separate hidden units consists of individual networks for each of the five direct multi step prediction tasks the second shared hidden units has a single larger hidden layer that finds a representation from which all five predictions for the next five steps are generated for this data set we do not find any significant difference between the two architectures httpwww cscoloradoeduandreashomehtm this paper is avlab wi figus in colors as ftpftpcscoloradoedupu time seriesmypaperskazlasweigendnipspsz peter kazlas andreas s weigend
an analogue vlsi neural network has been designed and tested to perform cardiac morphology classification tasks analogue techniques were chosen to meet the strict power and area requirements of an implantable cardioverter defibrillator icd system the robustness of the neural network architecture reduces the impact of noise drift and offsets inherent in analogue approaches the network is a multi layer perceptron with on chip digital weight storage a bucket brigade input to feed the intracardiac electrogram iceg to the network and has a winner take all circuit at the output the network was trained in loop and included a commercial icd in the signal processing path the system has successfully distinguished arrhythmia for different patients with better than true positive and true negative detections for dangerous rhythms which cannot be detected by present icds the chip was implemented in urn cmos and consumes less than nw maximum average power in an area of x ram i
we present a silicon model of an axon which shows promise as a building block for pulse based neural computations involving correlations of pulses across both space and time the circuit shares a number of features with its biological counterpart including an excitation threshold a brief refractory period after pulse completion pulse amplitude restoration and pulse width restoration we provide a simple explanation of circuit operation and present data from a chip fabricated in a standard rn cmos process through the mos implementation service mosis we emphasize the necessity of the restoration of the width of the pulse in time for stable propagation in axons i

we describe an analog vlsi implementation of the art algorithm carpenter a prototype chip has been fabricated in a standard low cost gm double metal single poly cmos process it has a die area of lcrn and is mounted in a pins pga package the chip realizes a modified version of the original art architecture such modification has been shown to preserve all computational properties of the original algorithm serrano a while being more appropriate for vlsi realizations the chip implements an art network with f nodes and f nodes it can therefore cluster binary pixels input patterns into up to different categories modular expansibility of the system is possible by assembling an nxm array of chips without any extra interfacing circuitry resulting in an f layer with xn nodes and an f layer with xm nodes pattern classification is performed in less than gs which means an equivalent computing power of x connections and connection updates per second although internally the chip is analog in nature it interfaces to the outside world through digital signals thus having a true asynchrounous digital behavior experimental chip test results are available which have been obtained through test equipments for digital chips
a novel two terminal device consisting of a thin j layer ofp a sih sandwiched between vanadium and chromium electrodes exhibits a non volatile analogue memory action this device stores synaptic weights in an ann chip replacing the capacitor previously used for dynamic weight storage two different synapse designs are discussed and results are presented i

we present an analog vlsi chip for parallel analog vector quantization the mosis tzm double poly cmos tiny chip contains an array of x charge based distance estimation cells implementing a mean absolute difference mad metric operating on a input analog vector field and analog template vectors the distance cell including dynamic template storage measures x xm additionally the chip features a winner take all wta output circuit of linear complexity with global positive feedback for fast and decisive settling of a single winner output experimental results on the complete x vq system demonstrate correct operation with db analog input dynamic range and tzsec cycle time at mw power dissipation
the localization and orientation to various novel or interesting events in the environment is a critical sensorimotor ability in all animals predator or prey in mammals the superior colliculus sc plays a major role in this behavior the deeper layers exhibiting topographically mapped responses to visual auditory and somatosensory stimuli sensory information arriving from different modalities should then be represented in the same coordinate frame auditory cues in particular are thought to be computed in head based coordinates which must then be transformed to retinal coordinates in this paper an analog vlsi implementation for auditory localization in the azimuthal plane is described which extends the architecture proposed for the barn owl to a primate eye movement system where further transformation is required this transformation is intended to model the projection in primates from auditory cortical areas to the deeper layers of the primate superior colliculus this system is interfaced with an analog vlsi based saccadic eye movement system also being constructed in our laboratory
we consider the problem of decoding block coded data using a physical dynamical system we sketch out a decompression algorithm for fractal block codes and then show how to implement a recurrent neural network using physically simple but highly nonlinear analog circuit models of neurons and synapses the nonlinear system has many fixed points but we have at our disposal a procedure to choose the parameters in such a way that only one solution the desired solution is stable as a partial proof of the concept we present experimental data from a small system a neuron analog cmos chip fabricated in a m analog p well process this chip operates in the subthreshold regime and for each choice of parameters converges to a unique stable state each state exhibits a qualitatively fractal shape
we have continued our study of a parallel perturbative learning method alspector et al and implications for its implementation in analog vlsi our new results indicate that in most cases a single parallel perturbation per pattern presentation of the function parameters weights in a neural network is theoretically the best course this is not true however for certain problems and may not generally be true when faced with issues of implementation such as limited precision in these cases multiple parallel perturbations may be best as indicated in our previous results
this paper describes a way of neural hardware implementation with the analog digital mixed mode neural chip the full custom neural vlsi of universally reconstructible artificial neural networkuran is used to implement korean speech recognition system a multi layer perceptron with linear neurons is trained successfully under the limited accuracy in computations the network with a large frame input layer is tested to recognize spoken korean words at a forward retrieval multichip hardware module is suggested with eight chips or more for the extended performance and capacity ii song han hwang soo lee ki chul kim
we describe single transistor silicon synapses that compute learn and provide non volatile memory retention the single transistor synapses simultaneously perform long term weight storage compute the product of the input and the weight value and update the weight value according to a hebbian or a backpropagation learning rule memory is accomplished via charge storage on polysilicon floating gates providing long term retention xvithout refresh the synapses efficiently use the physics of silicon to perform weight updates the weight value is increased using tunneling and the weight value decreases using hot electron injection the small size and low power operation of single transistor synapses allows the development of dense synaptic arrays we describe the design fabrication characterization and modeling of an array of single transistor synapses when the steady state source current is used as the representation of the weight value both the incrementing and decrementing functions are proportional to a power of the source current the synaptic array was fabricated in the standard pro double poly analog process available from mosis i
deciding the appropriate representation to use for modeling human auditory processing is a critical issue in auditory science while engineers have successfully performed many single speaker tasks with lpc and spectrogram methods more difficult problems will need a richer representation this paper describes a powerful auditory representation known as the correlogram and shows how this non linear representation can be converted back into sound with no loss of perceptually important information the correlogram is interesting because it is a neurophysiologically plausible representation of sound this paper shows improved methods for spectrogram inversion conventional pattern playback inversion of a cochlear model and inversion of the correlogram representation
in this paper we consider speech coding as a problem of speech modelling in particular prediction of parameterised speech over short time segments is performed using the hierarchical mixture of experts hme jordan jacobs the hme gives two advantages over traditional non linear function approximators such as the multi layer perceptton mlp a statistical understanding of the operation of the predictor and provision of information about the performance of the predictor in the form of likelihood information and local error bars these two issues are examined on both toy and real world problems of regression and time series prediction in the speech coding context we extend the principle of combining local predictions via the hme to a vector quantization scheme in which fixed local codebooks are combined on line for each observation
glove talkli is a system which translates hand gestures to speech through an adaptive interface hand gestures are mapped continuously to control parameters of a parallel formant speech synthesizer the mapping allows the hand to act as an artificial vocal tract that produces speech in real time this gives an unlimited vocabulary in addition to direct control of fundamental frequency and volume currently the best version of glove talkli uses several input devices including a cyberglove a contactglove a space tracker and a foot pedal a parallel formant speech synthesizer and neural networks the gesture to speech task is divided into vowel and consonant production by using a gating network to weight the outputs of a vowel and a consonant neural network the gating network and the consonant network are trained with examples from the user the vowel network implements a fixed user defined relationship between hand position and vowel sound and does not require any training examples from the user volume fundamental frequency and stop consonants are produced with a fixed mapping from the input devices one subject has trained to speak intelligibly with glove talkli he speaks slowly with speech quality similar to a text to speech synthesizer but with far more natural sounding pitch variations s sidney fels geoffrey hinton
this paper presents ongoing work on a speaker independent visual speech recognition system the work presented here builds on previous research efforts in this area and explores the potential use of simple hidden markov models for limited vocabulary speaker independent visual speech recognition the task at hand is recognition of the first four english digits a task with possible applications in car phone dialing the images were modeled as mixtures of independent gaussian distributions and the temporal dependencies were captured with standard left to right hidden markov models the results indicate that simple hidden markov models may be used to successfully recognize relatively unprocessed image sequences the system achieved performance levels equivalent to untrained humans when asked to recognize the first four english digits
in this paper we incorporate the hierarchical mixtures of experts hme method of probability estimation developed by jordan into an hmmbased continuous speech recognition system the resulting system can be thought of as a continuous density hmm system but instead of using gaussian mixtures the hme system employs a large set of hierarchically organized but relatively small neural networks to perform the probability density estimation the hierarchical structure is reminiscent of a decision tree except for two important differences each expert or neural net performs a soft decision rather than a hard decision and unlike ordinary decision trees the parameters of all the neural nets in the hme are automatically trainable using the em algorithm we report results on the arpa word and word wall street journal corpus using hme models
the paper presents a rapid speaker normalization technique based on neural network spectral mapping the neural network is used as a front end of a continuous speech recognition system speakerdependent hmm based to normalize the input acoustic data from a new speaker the spectral difference between speakers can be reduced using a limited amount of new acoustic data phonetically rich sentences recognition error of phone units from the acoustic phonetic continuous speech corpus apasci is decreased with an adaptability ratio of we used local basis networks of elliptical gaussian kernels with recursive allocation of units and on line optimization of parameters gran model for this application the model included a linear term the results compare favorably with multivariate linear mapping based on constrained orthonormal transformations
speech recognizers provide good performance for most users but the error rate often increases dramatically for a small percentage of talkers who are different from those talkers used for training one expensive solution to this problem is to gather more training data in an attempt to sample these outlier users a second solution explored in this paper is to artificially enlarge the number of training talkers by transforming the speech of existing training talkers this approach is similar to enlarging the training set for ocr digit recognition by warping the training digit images but is more difficult because continuous speech has a much larger number of dimensions eg linguistic phonetic style temporal spectral that differ across talkers we explored the use of simple linear spectral warping to enlarge a talker training data base used for word spotting the average detection rate overall was increased by percentage points from to for male speakers and percentage points from to for female speakers this increase is small but similar to that obtained by doubling the amount of training data
we present a unifying view of discrete time operator models used in the context of finite word length linear signal processing comparisons are made between the recently presented gamma operator model and the delta and rho operator models for performing nonlinear system identification and prediction using neural networks a new model based on an adaptive bilinear transformation which generalizes all of the above models is presented
we describe a framework for learning saccadic eye movements using a photometric representation of target points in natural scenes the representation takes the form of a high dimensional vector comprised of the responses of spatial filters at different orientations and scales we first demonstrate the use of this response vector in the task of locating previously foveated points in a scene and subsequently use this property in a multisaccade strategy to derive an adaptive motor map for delivering accurate saccades
we describe a system that can track a hand in a sequence of video frames and recognize hand gestures in a user independent manner the system locates the hand in each video frame and determines if the hand is open or closed the tracking system is able to track the hand to within t pixels of its correct location in of the flames from a test set containing video sequences from different individuals captured in different room environments the gesture recognition network correctly determines if the hand being tracked is open or closed in of the frames in this test set the system has been designed to operate in real time with existing hardware i
we describe a framework for real time tracking of facial expressions that uses neurally inspired correlation and interpolation methods a distributed view based representation is used to characterize facial state and is computed using a replicated correlation network the ensemble response of the set of view correlation scores is input to a network based interpolation method which maps perceptual state to motor control states for a simulated d face model activation levels of the motor state correspond to muscle activations in an anatomically derived model by integrating fast and robust d processing with d models we obtain a system that is able to quickly track and interpret complex facial motions in real time
perceptual learning is defined as fast improvement in performance and retention of the learned ability over a period of time in a set of psychophysical experiments we demonstrated that perceptual learning occurs for the discrimination of direction in stochastic motion stimuli here we model this learning using two approaches a clustering model that learns to accommodate the motion noise and an averaging model that learns to ignore the noise simulations of the models show performance similar to the psychophysical results i
this paper outlines a dynamic theory of development and adaptation in neural networks with feedback connections given input ensemble the connections change in strength according to an associative learning rule and approach a stable state where the neuronal outputs are decorrelated we apply this theory to primary visual cortex and examine the implications of the dynamical decorrelation of the activities of orientation selective cells by the intracortical connections the theory gives a unified and quantitative explanation of the psychophysical experiments on orientation contrast and orientation adaptation using only one parameter we achieve good agreements between the theoretical predictions and the experimental data i

this paper presents a new method for image compression by neural networks first we show that we can use neural networks in a pyramidal framework yielding the so called pca pyramids then we present an image compression method based on the pca pyramid which is similar to the laplace pyramid and wavelet transform some experimental results with real images are reported finally we present a method to combine the quantization step with the learning of the pca pyramid
this paper presents an unsupervised learning scheme for categorizing d objects from their d projected images the scheme exploits an auto associative networks ability to encode each view of a single object into a representation that indicates its view direction we propose two models that employ different classification mechanisms the first model selects an auto associative network whose recovered view best matches the input view and the second model is based on a modular architecture whose additional network classifies the views by splitting the input space nonlinearly we demonstrate the effectiveness of the proposed classification models through simulations using d wire frame objects
a fundamental open problem in computer vision determining pose and correspondence between two sets of points in space is solved with a novel robust and easily implementable algorithm the technique works on noisy point sets that may be of unequal sizes and may differ by non rigid transformations a d variation calculates the pose between point sets related by an affine transformation translation rotation scale and shear a d to d variation calculates translation and rotation an objective describing the problem is derived from mean field theory the objective is minimized with clocked em like dynamics experiments with both handwritten and synthetic data provide empirical evidence for the method i

the problem of interpolating between specified images in an image sequence is a simple but important task in model based vision we describe an approach based on the abstract task of manifold learning and present results on both synthetic and real image sequences this problem arose in the development of a combined lip reading and speech recognition system
the efficiency of image search can be greatly improved by using a coarse to fine search strategy with a multi resolution image representation however if the resolution is so low that the objects have few distinguishing features search becomes difficult we show that the performance of search at such low resolutions can be improved by using context information ie objects visible at low resolution which are not the objects of interest but are associated with them the networks can be given explicit context information as inputs or they can learn to detect the context objects in which case the user does not have to be aware of their existence we also use integrated feature pyramids which represent high frequency information at low resolutions the use of multiresolution search techniques allows us to combine information about the appearance of the objects on many scales in an efficient way a natural fom of exemplar selection also arises from these techniques we illustrate these ideas by training hierarchical systems of neural networks to find clusters of buildings in aerial photographs of farmland

simard lecun denker showed that the performance of nearest neighbor classification schemes for handwritten character recognition can be improved by incorporating invariance to specific transformations in the underlying distance metric the so called tangent distance the resulting classifier however can be prohibitively slow and memory intensive due to the large amount of prototypes that need to be stored and used in the distance comparisons in this paper we develop rich models for representing large subsets of the prototypes these models are either used singly per class or as basic building blocks in conjunction with the k means clustering algorithm this work was performed while trevor hastie was a member of the statistics and data analysis research group att bell laboratories murray hill nj trevor hastie patrice simard eduard siickinger i
this paper presents results from the first use of neural networks for the real time feedback control of high temperature plasmas in a tokamak fusion experiment the tokamak is currently the principal experimental device for research into the magnetic confinement approach to controlled fusion in the tokamak hydrogen plasmas at temperatures of up to million k are confined by strong magnetic fields accurate control of the position and shape of the plasma boundary requires real time feedback control of the magnetic field structure on a time scale of a few tens of microseconds software simulations have demonstrated that a neural network approach can give significantly better performance than the linear technique currently used on most tokamak experiments the practical application of the neural network approach requires high speed hardware for which a fully parallel implementation of the multilayer perceptron using a hybrid of digital and analogue technology has been developed c bishop p haynes m smith t todd d trotman c windsor
we construct a mixture of locally linear generative models of a collection of pixel based images of digits and use them for recognition different models of a given digit are used to capture different styles of writing and new images are classified by evaluating their log likelihoods under each model we use an em based algorithm in which the m step is computationally straightforward principal components analysis pca incorporating tangent plane information about expected local deformations only requires adding tangent vectors into the sample covariance matrices for the pca and it demonstrably improves performance
the theory of optimal unsupervised motor learning shows how a network can discover a reduced order controller for an unknown nonlinear system by representing only the most significant modes here i extend the theory to apply to command sequences so that the most significant components discovered by the network correspond to motion primitives combinations of these primitives can be used to produce a wide variety of different movements i demonstrate applications to human handwriting decomposition and synthesis as well as to the analysis of electrophysiological experiments on movements resulting from stimulation of the frog spinal cord i
in this study an integrated neural network control architecture for nonlinear dynamic systems is presented most of the recent emphasis in the neural network control field has no error feedback as the control input which rises the lack of adaptation problem the integrated architecture in this paper combines feed forward control and error feedback adaptive control using neural networks the paper reveals the different internal functionality of these two kinds of neural network controllers for certain input styles eg state feedback and error feedback with error feedback neural network controllers learn the slopes or the gains with respect to the error feedback producing an error driven adaptive contrbl systems the results demonstrate that the two kinds of control scheme can be combined to realize their individual advantages testing with disturbances added to the plant shows good tracking and adaptation with the integrated neural control architecture
each year people spend a huge amount of time typing the text people type typically contains a tremendous amount of redundancy due to predictable word usage patterns and the texts structure this paper describes a neural network system call autotypist that monitors a persons typing and predicts what will be entered next autotypist displays the most likely subsequent word to the typist who can accept it with a single keystroke instead of typing it in its entirety the multi layer perceptron at the heart of autotypist adapts its predictions of likely subsequent text to the users word usage pattern and to the characteristics of the text currently being typed increases in typing speed of when typing english prose and when typing c code have been demonstrated using the system suggesting a potential time savings of more than hours per user per year in addition to increasing typing speed autotypist reduces the number of keystrokes a user must type by a similar amount for english for computer programs this keystroke savings has the potential to significantly reduce the frequency and severity of repeated stress injuries caused by typing which are the most common injury suffered in todays office environment
to compress text files a neural predictor network p is used to approximate the conditional probability distribution of possible next characters given n previous characters ps outputs are fed into standard coding algorithms that generate short codes for characters with high predicted probability and long codes for highly unpredictable characters tested on short german newspaper articles our method outperforms widely used lempel ziv algorithms used in unix functions such as compress and gzip jiirgen schmidhuber stefan heil
experiments demonstrated that sigmoid multilayer perceptron mlp networks provide slightly better risk prediction than conventional logistic regression when used to predict the risk of death stroke and renal failure on patients who underwent coronary artery bypass operations at the lahey clinic mlp networks with no hidden layer and networks with one hidden layer were trained using stochastic gradient descent with early stopping mlp networks and logistic regression used the same input features and were evaluated using bootstrap sampling with replications roc areas for predicting mortality using preoperative input features were for logistic regression and for mlp networks regularization provided by early stopping was an important component of improved performance a simplified approach to generating confidence intervals for mlp risk predictions using an auxiliary confidence mlp was developed the confidence mlp is trained to reproduce confidence intervals that were generated during training using the outputs of mlp networks trained with different bootstrap samples
the tnm staging system has been used since the early s to predict breast cancer patient outcome in an attempt to increase prognostic accuracy many putative prognostic factors have been identified because the tnm stage model can not accommodate these new factors the proliferation of factors in breast cancer has lead to clinical confusion what is required is a new computerized prognostic system that can test putative prognostic factors and integrate the predictive factors with the tnm variables in order to increase prognostic accuracy using the area under the curve of the receiver operating characteristic we compare the accuracy of the following predictive models in terms of five year breast cancer specific survival ptnm staging system principal component analysis classification and regression trees logistic regression cascade correlation neural network conjugate gradient descent neural probabilistic neural network and backpropagation neural network several statistical models are significantly more ac harry b burke david b rosen philip h goodman curate than the tnm staging system logistic regression and the backpropagation neural network are the most accurate prediction models for predicting five year breast cancer specific survival
this paper presents neurochess a program which learns to play chess from the final outcome of games neurochess learns chess board evaluation functions represented by artificial neural networks it integrates inductive neural network learning temporal differencing and a variant of explanation based learning performance results illustrate some of the strengths and weaknesses of this approach
diagnosis of human disease or machine fault is a missing data problem since many variables are initially unknown additional information needs to be obtained the joint probability distribution of the data can be used to solve this problem we model this with mixture models whose parameters are estimated by the em algorithm this gives the benefit that missing data in the database itself can also be handled correctly the request for new information to refine the diagnosis is performed using the maximum utility principle since the system is based on learning it is domain independent and less labor intensive than expert systems or probabilistic networks an example using a heart disease database is presented
in remote sensing applications ground truth data is often used as the basis for training pattern recognition algorithms to generate thematic maps or to detect objects of interest in practical situations experts may visually examine the images and provide a subjective noisy estimate of the truth calibrating the reliability and bias of expert labellers is a non trivial problem in this paper we discuss some of our recent work on this topic in the context of detecting small volcanoes in magellan sar images of venus empirical results using the expectation maximization procedure suggest that accounting for subjective noise can be quite significant in terms of quantifying both human and algorithm detection performance i
in this paper we present npen a connectionist system for writer independent large vocabulary on line cursive handwriting recognition this system combines a robust input representation which preserves the dynamic writing information with a neural network architecture a so called multi state time delay neural network ms tdnn which integrates recognition and segmentation in a single framework our preprocessing transforms the original coordinate sequence into a still temporal sequence of feature vectors which combine strictly local features like curvature or writing direction with a bitmap like representation of the coordinates proximity the ms tdnn architecture is well suited for handling temporal sequences as provided by this input representation our system is tested both on writer dependent and writer independent tasks with vocabulary sizes ranging from up to words for example on a word vocabulary we achieve word recognition rates up to writer dependent and writer independent without using any language models stefan manke michael finke alex waibel
for machines to perform classification tasks such as speech and character recognition appropriately handling deformed patterns is a key to achieving high performance the authors presents a new type of classification system an adaptive input field neural network aifnn which includes a simple pre trained neural network and an elastic input field attached to an input layer by using an iterative method aifnn can determine an optimal afiine translation for an elastic input field to compensate for the original deformations the convergence of the aifnn algorithm is shown aifnn is applied for handwritten numerals recognition consequently of originally misclassified patterns are correctly categorized and total performance is improved without modifying the neural network i
multi class classification problems can be efficiently solved by partitioning the original problem into sub problems involving only two classes for each pair of classes a potentially small neural network is trained using only the data of these two classes we show how to combine the outputs of the two class neural networks in order to obtain posterior probabilities for the class decisions the resulting probabilistic pairwise classifier is part of a handwriting recognition system which is currently applied to check reading we present results on real world data bases and show that from a practical point of view these results compare favorably to other neural network approaches
experiments were performed to reveal some of the computational properties of the human motor memory system we show that as humans practice reaching movements while interacting with a novel mechanical environment they learn an internal model of the inverse dynamics of that environment subjects show recall of this model at testing sessions hours after the initial practice the representation of the internal model in memory is such that there is interference when there is an attempt to learn a new inverse dynamics map immediately after an anticorrelated mapping was learned we suggest that this interference is an indication that the same computational elements used to encode the first inverse dynamics map are being used to learn the second mapping we predict that this leads to a forgetting of the initially learned skill i
one of the fundamental properties that both neural networks and the central nervous system share is the ability to learn and generalize from examples while this property has been studied extensively in the neural network literature it has not been thoroughly explored in human perceptual and motor learning we have chosen a coordinate transformation system the visuomotor map which transforms visual coordinates into motor coordinates to study the generalization effects of learning new input output pairs using a paradigm of computer controlled altered visual feedback we have studied the generalization of the visuomotor map subsequent to both local and context dependent remappings a local remapping of one or two input output pairs induced a significant global yet decaying change in the visuomotor map suggesting a representation for the map composed of units with large functional receptive fields our study of context dependent remappings indicated that a single point in visual space can be mapped to two different finger locations depending on a context variable the starting point of the movement furthermore as the context is varied there is a gradual shift between the two remappings consistent with two visuomotor modules being learned and gated smoothly with the context i
the additive clustering adcl usmodel shepard arabie treats the similarity of two stimuli as a weighted additive measure of their common features inspired by recent work in unsupervised learning with multiple cause models we propose a new statistically well motivated algorithm for discovering the structure of natural stimulus classes using the adclus model which promises substantial gains in conceptual simplicity practical efficiency and solution quality over earlier efforts we also present preliminary results with artificial data and two classic similarity data sets
we have recently developed a theory of spatial representations in which the position of an object is not encoded in a particular frame of reference but instead involves neurons computing basis functions of their sensory inputs this type of representation is able to perform nonlinear sensorimotor transformations and is consistent with the response properties of parietal neurons we now ask whether the same theory could account for the behavior of human patients with parietal lesions these lesions induce a deficit known as hemineglect that is characterized by a lack of reaction to stimuli located in the hemispace contralateral to the lesion a simulated lesion in a basis function representation was found to replicate three of the most important aspects of hemineglect i the models failed to cross the leftmost lines in line cancellation experiments ii the deficit affected multiple frames of reference and iii it could be object centered these results strongly support the basis function hypothesis for spatial representations and provide a computational theory of hemineglect at the single cell level i

a significant limitation of neural networks is that the representations they learn are usually incomprehensible to humans we present a novel algorithm trepan for extracting comprehensible symbolic representations from trained neural networks our algorithm uses queries to induce a decision tree that approximates the concept represented by a given network our experiments demonstrate that trepan is able to produce decision trees that maintain a high level of fidelity to their respective networks while being comprehensible and accurate unlike previous work in this area our algorithm is general in its applicability and scales well to large networks and problems with high dimensional input spaces

in consideration of attention as a means for goal directed behavior in non stationary environments we argue that the dynamics of attention should satisfy two opposing demands long term maintenance and quick transition these two characteristics are contradictory within the linear domain we propose the near saddlenode bifurcation behavior of a sigmoidal unit with self connection as a candidate of dynamical mechanism that satisfies both of these demands we further show in simulations of the bug eat food tasks that the near saddle node bifurcation behavior of recurrent networks can emerge as a functional property for survival in nonstationary environments
the choice of an input representation for a neural network can have a profound impact on its accuracy in classifying novel instances however neural networks are typically computationally expensive to train making it difficult to test large numbers of alternative representations this paper introduces fast quality measures for neural network representations allowing one to quickly and accurately estimate which of a collection of possible representations for a problem is the best we show that our measures for ranking representations are more accurate than a previously published measure based on experiments with three difficult real world pattern recognition problems i
an essential feature of intelligent sensory processing is the ability to focus on the part of the signal of interest against a background of distracting signals and to be able to direct this focus at will in this paper the problem of auditory scene segmentation is considered and a model of the early stages of the process is proposed the behaviour of the model is shown to be in agreement with a number of well known psychophysical results the principal contribution of this model lies in demonstrating how streaming might result from interactions between the tonotopic patterns of activity of input signals and traces of previous activity which feedback and influence the way in which subsequent signals are processed

we have analyzed the relationship between correlated spike count and the peak in the cross correlation of spike trains for pairs of simultaneously recorded neurons from a previous study of area mt in the macaque monkey zohary et al we conclude that common input responsible for creating peaks on the order of ten milliseconds wide in the spike train cross correlograms ccgs is also responsible for creating the correlation in spike count observed at the two second time scale of the trial we argue that both common excitation and inhibition may play significant roles in establishing this correlation
while it is generally agreed that neurons transmit information about their synaptic inputs through spike trains the code by which this information is transmitted is not well understood an upper bound on the information encoded is obtained by hypothesizing that the precise timing of each spike conveys information here we develop a general approach to quantifying the information carried by spike trains under this hypothesis and apply it to the leaky integrate and fire if model of neuronal dynamics we formulate the problem in terms of the probability distribution pt of interspike intervals isis assuming that spikes are detected with arbitrary but finite temporal resolution in the absence of added noise all the variability in the isis could encode information and the information rate is simply the entropy of the isi distribution ht ptlogpt times the spike rate ht thus provides an exact expression for the information rate the methods developed here can be used to determine experimentally the information carried by spike trains even when the lower bound of the information rate provided by the stimulus reconstruction method is not tight in a preliminary series of experiments we have used these methods to estimate information rates of hippocampal neurons in slice in response to somatic current injection these pilot experiments suggest information rates as high as bitsspike information rate of spike trains cortical neurons use spike trains to communicate with other neurons the output of each neuron is a stochastic function of its input from the other neurons it is of interest to know how much each neuron is telling other neurons about its inputs how much information does the spike train provide about a signal consider noise nt added to a signal st to produce some total input yt st nt this is then passed through a possibly stochastic functional to produce the output spike train yt zt we assume that all the information contained in the spike train can be represented by the list of spike times that is there is no extra information contained in properties such as spike height or width note however that many characteristics of the spike train such as the mean or instantaneous rate c stevens a zador can be derived from this representation if such a derivative property turns out to be the relevant one then this formulation can be specialized appropriately we will be interested then in the mutual information ist zt between the input signal ensemble st and the output spike train ensemble zt this is defined in terms of the entropy hs of the signal the entropy hz of the spike train and their joint entropy hs z isz hs hzhsz note that the mutual information is symmetric is z iz since the joint entropy hs z hz s note also that if the signal st and the spike train zt are completely independent then the mutual information is since the joint entropy is just the sum of the individual entropie hsz hs hz this is completely in line with our intuition since in this case the spike train can provide no information about the signal information estimation through stimulus reconstruction bialek and colleagues bialek et al have used the reconstruction method to obtain a strict lower bound on the mutual information in an experimental setting this method is based on an expression mathematically equivalent to eq involving the conditional entropy hsiz of the signal given the spike train sz sslz hshestslz where hestsiz is an upper bound on the conditional entropy obtained from a reconstruction ses t t of the signal the entropy is estimated from the second order statistics of the reconstruction error et st ses t t from the maximum entropy property of the gaussian this is an upper bound intuitively the first equation says that the information gained about the spike train by observing the stimulus is just the initial uncertainty of the signal in the absence of knowledge of the spike train minus the uncertainty that remains about the signal once the spike train is known and the second equation says that this second uncertainty must be greater for any particular estimate than for the optimal estimate information estimation through spike train reliability we have adopted a different approach based an equivalent expression for the mutual information s z z zls a the first term hz is the entropy of the spike train while the second hzis is the conditional entropy of the spike train given the signal intuitively this like the inverse repeatability of the spike train given repeated applications of the same signal eq has the advantage that if the spike train is a deterministic function of the input it permits exact calculation of the mutual information this follows from an important difference between the conditional entropy term here and in eq whereas hsiz has both a deterministic and a stochastic component hzis has only a stochastic component thus in the absence of added noise the discrete entropy hzi and eq reduces to i z hz if isis are independent then the hz can be simply expressed in terms of the entropy of the discrete isi distribution pt h t pti log pt i information through a spiking neuron as hz nilt where n is the number of spikes in z here vt is the probability that the spike occurred in the interval at to i at the assumption of finite timing precision at keeps the potential information finite the advantage of considering the isi distribution pt rather than the full spike train distribution pz is that the former is univariate while the latter is multivariate estimating the former requires much less data under what conditions are isis independent correlations between isis can arise either through the stimulus or the spike generation mechanism itself below we shall guarantee that correlations do not arise from the spike generator by considering the forgetful integrate and fire if model in which all information about the previous spike is eliminated by the next spike if we further limit ourselves to temporally uncorrelated stimuli ie stimuli drawn from a white noise ensemble then we can be sure that isis are independent and eq can be applied in the presence of noise hzit must also be evaluated to give is t hthtis htis is the conditional entropy of the isi given the signal htis ptjsitlogptjsit jl sit where ptjlsit is the probability of obtaining an isi of tj in response to a particular stimulus sit in the presence of noise nt the conditional entropy can be thought of as a quantification of the reliability of the spike generating mechanism it is the average trial to trial variability of the spike train generated in response to repeated applications of the same stimulus maximum spike train entropy in what follows it will be useful to compare the information rate for the if neuron with the limiting case of an exponential isi distribution which has the maximum entropy for any point process of the given rate papoulis this provides an upper bound on the information rate possible for any spike train given the spike rate and the temporal precision let ft e r be an exponential distribution with a mean spike rate assuming a temporal precision of at the entropyspike is ht log and the entropytime for a rate is ht og t for example if hz and at sec this gives bitssecond spikesecond bitsspike that is if we discretize a i hz spike train into i msec bins it is not possible for it to transmit more than bitssecond if we reduce the bin size two fold the rate increases by log i bitspike to bitsspike while if we double it we lose one bits to get bits note that at a different firing rate eg hz halving the bin size still increases the entropyspike by i bitspike but because the spike rate is twice as high this becomes a bitsecond increase in the information rate the if model now we consider the functional describing the forgetful leaky if model of spike generation suppose we add some noise nt to a signal st yt nt st and threshold the sum to produce a spike train zt st nt specifically suppose the voltage vt of the neuron obeys t vtr yt where r is the membrane time constant both sq and nt have a white gaussian distributions and yt has mean f and variance er if the voltage reaches the threshold at some time t the neuron emits a spike at that time and resets to the initial condition v c stevens a zador in the language of neurobiology this model can be thought of tuckwell as the limiting case of a neuron with a leaky if spike generating mechanism receiving many excitatory and inhibitory synaptic inputs note that since the input yt is white there are no correlations in the spike train induced by the signal and since the neuron resets after each spike there are no correlations induced by the spikegenerating mechanism thus isis are independent and eq can be applied we will estimate the mutual information is z between the ensemble of input signals s and the ensemble of outputs z since in this model isis are independent by construction we need only evaluate ht and htis for this we must determine pt the distribution of isis and ptlsi the conditional distribution of isis for an ensemble of signals sit note that pt corresponds to the first passage time distribution of the ornstein uhlenbeck process tuckwell the neuron model we are considering has two regimes determined by the relation of the asymptotic membrane potential in the absence of threshold ttr and the threshold in the suprathreshold regimer threshold crossings occur even if the signal variance is zero er in the subthreshold regime ttr threshold crossings occur only if a however in the limit that et v ie the mean firing rate is low compared with the integration time constant this can only occur in the subthreshold regime the isi distribution is exponential and its coefficient of variation cv is unity cf softky and koch in this low rate regime the firing is deterministically potsson by this we mean to distinguish it from the more usual usage of potsson neuron the stochastic situation in which the instantaneous firing rate parameter the probability of firing over some interval depends on the stimulus ie cr st in the present case the exponential isi distribution arises from a deterministic mechanism at the border between these regimes when the threshold is just equal to the asymptotic potential ttr we have an explicit and exact solution for the entire isi distribution sugiyama et al trr art aexptr arert it this is the special case where in the absence of fluctuations a the membrane potential hovers just subthreshold its neurophysiological interpretation is that the excitatory inputs just balance the inhibitory inputs so that the neuron hovers just on the verge of firing information rates for noisy and noiseless signals here we compare the information rate for a if neuron at the balance point ttr with the maximum entropy spike train for simplicity and brevity we consider only the zero noise case ie nt fig a shows the information per spike as a function of the firing rate calculated from eq which was varied by changing the signal variance er we assume that spikes can be resolved with a temporal resolution of msec ie that the isi distribution has bins msec wide the dashed line shows the theoretical upper bound given by the exponential distribution this limit can be approached by a neuron operating far below threshold in the potsson limit for both the if model and the upper bound the information per spike is a monotonically decreasing function of the spike rate the model almost achieves the upper bound when the mean isi is just equal to the membrane time constant in the model the information saturates at very low firing rates but for the exponential distribution the information increases without bound at high firing rates the information goes to zero when the firing rate is too fast for individual isis to be resolved at the temporal resolution fig lb shows that the information rate information per second when the neuron is at the balance point goes through a information through a spiking neuron maximum as the firing rate increases the maximum occurs at a lower firing rate than for the exponential distribution dashed line bounding information rates by stimulus reconstruction by construction eq gives an exact expression for the information rate in this model we can therefore compare the lower bound provided by the stimulus reconstruction method eq bialek et al that is we can assess how tight a lower bound it provides fig shows the lower bound provided by the reconstruction solid line and the reliability dashed line methods as a function of the firing rate the firing rate was increased by increasing the mean of the input stimulus yt and noise was set to at low firing rates the two estimates are nearly identical but at high firing rates the reconstruction method substantially underestimates the information rate the amount of the underestimate depends on the model parameters and decreases as noise is added to the stimulus the tightness of the bound is therefore an empirical question while bialek and colleagues show that under the conditions of their experiments the underestimate is less than a factor of two it is clear that the potential for underestimate under different conditions or in different systems is greater discussion while it is generally agreed that spike trains encode information about a neurons inputs it is not clear how that information is encoded one idea is that it is the mean firing rate alone that encodes the signal and that variability about this mean is effectively noise an alternative view is that it is the variability itself that encodes the signal ie that the information is encoded in the precise times at which spikes occur in this view the information can be expressed in terms of the interspike interval isi distribution of the spike train this encoding scheme yields much higher information rates than one in which only the mean rate over some interval longer than the typical isi is considered here we have quantified the information content of spike trains under the latter hypothesis for a simple neuronal model we consider a model in which by construction the isis are independent so that the information rate in bitssec can be computed directly from the information per spike in bitsspike and the spike rate in spikessec the information per spike in turn depends on the temporal precision with which spikes can be resolved if precision were infinite then the information content would be infinite as well since any message could for example be encoded in the decimal expansion of the precise arrival time of a single spike the reliability of the spike transduction mechanism and the entropy of the isi distribution itself for low firing rates when the neuron is in the subthreshold limit the isi distribution is close to the theoretically maximal exponential distribution much of the recent interest in information theoretic analyses of the neural code can attributed to the seminal work of bialek and colleagues bialek et al rieke et al who measured the information rate for sensory neurons in a number of systems the present results are in broad agreement with those of deweese who considered the information rate of a linear filtered threshold crossing x lftc model deweese developed a functional expansion in which the first term describes the limit in which spike times not isis are independent and the second term is a correction for correlations the lftc model differs from the present if model mainly in that it does not reset after each spike consequently the natural in the lftc model gaussian signal and noise are convolved with a hnear filter the times at which the resulting waveform crosses some threshold are called spikes c stevens a zador representation of the spike train in the lftc model is as a sequence tt of firing times while in the if model the natural representation is as a sequence ti tn of isis the choice is one of convenience since the two representations are equivalent the two models are complementary in the lftc model results can be obtained for colored signals and noise while such conditions are awkward in the if model in the if model by contrast a class of highly correlated spike trains can be conveniently considered that are awkward in the lftc model that is the indendent isi condition required in the if model is less restrictive than the independent spike condition of the lftc model spikes are independent iff isis are indepenndent and the isi distribution pt is exponential in particular at high firing rates the isi distribution can be far from exponential and therefore the spikes far from independent even when the isis themselves are independent because we have assumed that the input st is white its entropy is infinite and the mutual information can grow without bound as the temporal precision with which spikes are resolved improves nevertheless the spike train is transmitting only a minute fraction of the total available information the signal thereby saturates the capacity of the spike train while it is not at all clear whether this is how real neurons actually behave it is not implausible a typical cortical neuron receives as many as synaptic inputs and if the information rate of each input is the same as the target then the information rate impinging upon the target is q fold greater neglecting synaptic unreliability which could decrease this substantially than its capacity in a preliminary series of experiments we have used the reliability method to estimate the information rate of hippocampal neuronal spike trains in slice in response to somatic current injection stevens and zador unpublished under these conditions isis appear to be independent so the method developed here can be applied in these pilot experiments an information rates as high as bitsspike was observed references bialek w rieke f de ruyter van steveninck r and warland d reading a neural code science deweese m optimization principles for the neural code in hasselmo m editor advances in neural information processing systems vol mit press cambridge ma papoulis a probability random variables and stochastic processes nd edition mcgraw hill rieke f warland d de ruyter van steveninck r and bialek w neural coding mit press softky w and koch c the highly irregular firing of cortical cells is inconsistent with temporal integration of random epsps j neuroscience sugiyama h moore g and perkel d solutions for a stochastic model of neuronal spike production mathematical biosciences tuckwell h
topographic maps in primary areas of mammalian cerebral cortex reorganise as a result of behavioural training the nature of this reorganisation seems consistent with the behaviour of competitive neural networks as has been demonstrated in the past by computer simulation we model tactile training on the hand representation in primate somatosensory cortex using the neural field theory of amari and his colleagues expressions for changes in both receptive field size and magnification factor are derived which are consistent with owl monkey experiments and make a prediction which goes beyond them
the vestibulo ocular reflex vor stabilizes images on the retina during rapid head motions the gain of the vor the ratio of eye to head rotation velocity is typically around when the eyes are focused on a distant target however to stabilize images accurately the vor gain must vary with context eye position eye vergerice and head translation we first describe a kinematic model of the vor which relies solely on sensory information available from the semicircular canals head rotation the otoliths head translation and neural correlates of eye position and vergerice angle we then propose a dynamical model and compare it to the eye velocity responses measured in monkeys the dynamical model reproduces the observed amplitude and time course of the modulation of the vor and suggests one way to combine the required neural signals within the cerebellum and the brain stem it also makes predictions for the responses of neurons to multiple inputs head rotation and translation eye position etc in the oculomotor system
an extended version of the dual constraint model of motor endplate morphogenesis is presented that includes activity dependent and independent competition it is supported by a wide range of recent neurophysiological evidence that indicates a strong relationship between synaptic efficacy and survival the computational model is justified at the molecular level and its predictions match the developmental and regenerative behaviour of real synapses i
in the poisson neuron model the output is a rate modulated poisson process snyder and miller the time varying rate parameter rt is an instantaneous function g of the stimulus rt gst in a poisson neuron then rt gives the instantaneous firing rate the instantaneous probability of firing at any instant t and the output is a stochastic function of the input in part because of its great simplicity this model is widely used usually with the addition of a refractory period especially in in vivo single unit electrophysiological studies where st is usually taken to be the value of some sensory stimulus in the integrate and fire neuron model by contrast the output is a filtered and thresholded function of the input the input is passed through a low pass filter determined by the membrane time constant v and integrated until the membrane potential vt reaches threshold at which point vt is reset to its initial value by contrast with the poisson model in the integrate and fire model the ouput is a deterministic function of the input although the integrate and fire model is a caricature of real neural dynamics it captures many of the qualitative features and is often used as a starting point for conceptualizing the biophysical behavior of single neurons here we show how a slightly modified poisson model can be derived from the integrate and fire model with noisy inputs yt st nt in the modified model the transfer function g is a sigmoid err whose shape is determined by the noise variance a understanding the equivalence between the dominant in vivo and in vitro simple neuron models may help forge links between the two levels cf stevens a zador i
a computational model of song learning in the song sparrow melospiza melodia learns to categorize the different syllables of a song sparrow song and uses this categorization to train itself to reproduce song the model fills a crucial gap in the computational explanation of birdsong learning by exploring the organization of perception in songbirds it shows how competitive learning may lead to the organization of a specific nucleus in the bird brain replicates the song production results of a previous model doya and sejnowski and demonstrates how perceptual learning can guide production through reinforcement learning
we analyse the geometry of eye rotations and in particular saccarles using basic lie group theory and differential geometry various parameterizations of rotations are related through a unifying mathematical treatment and transformations between co ordinate systems are computed using the campbell bakerhausdorff formula next we describe listings law by means of the lie algebra so this enables us to demonstrate a direct connection to dontiers law by showing that eye orientations are restricted to the quotient space ss the latter is equivalent to the sphere which is exactly the space of gaze directions our analysis provides a mathematical framework for studying the oculomotor system and could also be extended to investigate the geometry of multi joint arm movements i
binaural coincidence detection is essential for the localization of external sounds and requires auditory signal processing with high temporal precision we present an integrate and fire model of spike processing in the auditory pathway of the barn owl it is shown that a temporal precision in the microsecond range can be achieved with neuronal time constants which are at least one magnitude longer an important feature of our model is an unsupervised hebbian learning rule which leads to a temporal fine tuning of the neuronal connections email kempterwgerstlvh physiktu muenchende temporal coding in the submillisecond range model of barn owl auditory pathway
selective suppression of transmission at feedback synapses during learning is proposed as a mechanism for combining associative feedback with self organization of feedforward synapses experimental data demonstrates cholinergic suppression of synaptic transmission in layer i feedback synapses and a lack of suppression in layer iv feedforward synapses a network with this feature uses local rules to learn mappings which are not linearly separable during learning sensory stimuli and desired response are simultaneously presented as input feedforward connections form self organized representations of input while suppressed feedback connections leam the transpose of feedforward connectivity during recall suppression is removed sensory input activates the self organized representation and activity generates the learned response
we present a hypothesis about how the cerebellum could participate in regulating movement in the presence of significant feedback delays without resorting to a forward model of the motor plant we show how a simplified cerebellar model can learn to control endpoint positioning of a nonlinear spring mass system with realistic delays in both afferent and efferent pathways the models operation involves prediction but instead of predicting sensory input it directly regulates movement by reacting in an anticipatory fashion to input patterns that include delayed sensory feedback
because of the distance between the skull and brain and their different resistivities electroencephalographic eeg data collected from any point on the human scalp includes activity generated within a large brain area this spatial smearing of eeg data by volume conduction does not involve significant time delays however suggesting that the independent component analysis ica algorithm of bell and sejnowski is suitable for performing blind source separation on eeg data the ica algorithm separates the problem of source identification from that of source localization first results of applying the ica algorithm to eeg and event related potential erp data collected during a sustained auditory detection task show ica training is insensitive to different random seeds ica may be used to segregate obvious artifactual eeg components line and muscle noise eye movements from other sources ica is capable of isolating overlapping eeg phenomena including alpha and theta bursts and spatially separable erp components to separate ica channels nonstationarities in eeg and behavioral state can be tracked using ica via changes in the amount of residual correlation between ica filtered output channels s makeig a j bell t p jung t j sejnowski i

despite the phylogenic and structural differences the visual systems of different species whether vertebrate or invertebrate share certain functional properties the center surround opponent receptive field csrf mechanism represents one such example here analogous csrfs are shown to be formed in an artificial neural network which learns to localize contours edges of the luminance difference furthermore when the input pattern is corrupted by a background noise the csrfs of the hidden units becomes shallower and broader with decrease of the signal to noise ratio snr the same kind of snr dependent plasticity is present in the csrf of real visual neurons in bipolar cells of the carp retina as is shown here experimentally as well as in large monopolar cells of the fly compound eye as was described by others also analogous snrdependent plasticity is shown to be present in the biphasic flash responses bpfr of these artificial and biological visual systems thus the spatial csrf and temporal bpfr filtering properties with which a wide variety of creatures see the world appear to be optimized for detectability of changes in space and time i
in this paper the problem of learning appropriate domain specific bias is addressed it is shown that this can be achieved by learning many related tasks from the same domain and a theorem is given bounding the number tasks that must be learnt a corollary of the theorem is that if the tasks are known to possess a common internal representation or preprocessing then the number of examples required per task for good generahsation when learning n tasks simultaneously scales like oa where oa is a bound on the minimum number of examples requred to learn a single task and oa b is a bound on the number of examples required to learn each task independently an experiment providing strong qualitative support for the theoretical results is reported i
a statistical theory for overtraining is proposed the analysis treats realizable stochastic neural networks trained with kullbackleibler loss in the asymptotic case it is shown that the asymptotic gain in the generalization error is small if we perform early stopping even if we have access to the optimal stopping time considering cross validation stopping we answer the question in what ratio the examples should be divided into training and testing sets in order to obtain the optimum performance in the non asymptotic region cross validated early stopping always decreases the generalization error our large scale simulations done on a cm are in nice agreement with our analytical findings i

we study the characteristics of learning with ensembles solving exactly the simple model of an ensemble of linear students we find surprisingly rich behaviour for learning in large ensembles it is advantageous to use under regularized students which actually over fit the training data globally optimal performance can be obtained by choosing the training set sizes of the students appropriately for smaller ensembles optimization of the ensemble weights can yield significant improvements in ensemble generalization performance in paxticulax if the individual students are subject to noise in the training process choosing students with a wide range of regularization parameters makes this improvement robust against changes in the unknown level of noise in the training data i
this paper shows that neural networks which use continuous activation functions have vc dimension at least as large as the square of the number of weights w this result settles a long standing open question namely whether the well known ow log w bound known for hard threshold nets also held for more general sigmoidal nets implications for the number of samples needed for valid generalization are discussed i
recurrent perceptron classifiers generalize the classical perceptron model they take into account those correlations and dependences among input coordinates which arise from linear digital filtering this paper provides tight bounds on sample complexity associated to the fitting of such models to experimental data
it has remained unknown whether one can in principle carry out reliable digital computations with networks of biologically realistic models for neurons this article presents rigorous constructions for simulating in real time arbitrary given boolean circuits and finite automata with arbitrarily high reliability by networks of noisy spiking neurons in addition we show that with the help of shunting inhibition even networks of very unreliable spiking neurons can simulate in real time any mcculloch pitts neuron or threshold gate and therefore any multilayer perceptron or threshold circuit in a reliable manner these constructions provide a possible explanation for the fact that biological neural systems can carry out quite complex computations witlfin msec it turns out that the assumption that these constructions require about the shape of the epsps and the behaviour of the noise are surprisingly weak i
in this paper we examine a perceptron learning task the task is realizable since it is provided by another perceptron with identical architecture both perceptrons have nonlinear sigmoid output functions the gain of the output function determines the level of nonlinearity of the learning task it is observed that a high level of nonlinearity leads to overfitting we give an explanation for this rather surprising observation and develop a method to avoid the overfitting this method has two possible interpretations one is learning with noise the other cross validated early stopping i learning rules from examples the property which makes feedforward neural nets interesting for many practical applications is their ability to approximate functions which are given only by examples feed forward networks with at least one hidden layer of nonlinear units are able to approximate each continuous function on a n dimensional hypercube arbitrarily well while the existence of neural function approximators is already established there is still a lack of knowledge about their practical realizations also major problems which complicate a good realization like overfitting need a better understanding in this work we study overfitting in a one layer perceptron model the model allows a good theoretical description while it exhibits already a qualitatively similar behavior as the multilayer perceptron a one layer perceptron has n input units and one output unit between input and output it has one layer of adjustable weights wi i n the output z is a possibly nonlinear function of the weighted sum of inputs xi ie n zgh with h v a realizable learning task which exhibits overfitting the quality of the function approximation is measured by the difference between the correct output z and the nets output z averaged over all possible inputs in the supervised learning scheme one trains the network using a set of examples x u p for which the correct output is known it is the learning task to minimize a certain cost function which measures the difference between the correct output z u and the nets output zu averaged over all examples using the mean squared error as a suitable measure for the difference between the outputs we can define the training error et and the generalization error eg as p i ec the development of both errors as a function of the number p of trained examples is given by the learning curves training is conventionally done by gradient descend for theoretical purposes it is very useful to study learning tasks which are provided by a second network the so called teacher network this concept allows a more transparent definition of the difficulty of the learning task also the monitoring of the training process becomes clearer since it is always possible to compare the student network and the teacher network directly suitable quantities for such a comparison are in the perceptron case the following order parameters r iiwi wi wi q iiwii wi i i both have a very transparent interpretation r is the normalized overlap between the weight vectors of teacher and student and q is the norm of the students weight vector these order parameters can also be used in multilayer learning but their number increases with the number of all possible permutations between the hidden units of teacher and student the learning task here we concentrate on the case in which a student perceptton has to learn a mapping provided by another perceptton we choose identical networks for teacher and student both have the same sigmoid output function ie gh gh tanhh identical network architectures of teacher and student are realizable tasks in principle the student is able to learn the task provided by the teacher exactly unreaiizabie tasks can not be learnt exactly there remains always a finite error if we use uniformally distributed random inputs x and weights w the weighted sum h in can be assumed as gaussian distributed then we can express the generalization error by the order parameters tanh tztanh qrz qv r z with the gaussian measure dz oo vr exp from equation we can see how the student learns the gain of the teachers output function it adjusts the norm q of its weights the gain plays an important role since it allows to tune the function tanhh between a linear function and a highly nonlinear function now we want to determine the learning curves of this task s bos emergence of overfitting explicit expression for the weights below the storage capacity of the perceptton ie a the minimum of the training error et is zero a zero training error implies that every example has been learnt exactly thus the weights with minimal norm that fulfill this condition are given by the pseudoinverse see hertz et al i note that the weights are completely independent of the output function gh gh they are the same as in the simplest realizable case linear perceptron learns linear perceptron statistical mechanics the calculation of the order parameters can be done by a method from statistical mechanics which applies the commonly used replica method for details about the replica approach see hertz et al the solution of the continuous perceptron problem can be found in bss et al since the results of the statistical mechanics calculations are exact only in the thermodynamic limit ie n oo the variable c is the more natural measure it is defined as the fraction of the number of patterns p over the system size n ie c pin in the thermodynamic limit n and p are infinite but c is still finite normally reasonable system sizes such as n are already well described by this theory usually one concentrates on the zero temperature limit because this implies that the training error et accepts its absolute minimum for every number of presented examples p the corresponding order parameters for the case linear perceptron learns linear student are q v vfs r vfs the zero temperature limit can also be called exhaustive training since the student net is trained until the absolute minimum of et is reached for small c and high gains ie levels of nonlinearity exhaustive training leads to overfitting that means the generalization error ega is not as it should monotonously decreasing with a it is one reason for overfitting that the training follows too strongly the examples the critical gain c which determines whether the generalization error egc is increasing or decreasing function for small values of a can be determined by a linear approximation for small a both order parameters are small and the students tanh function in can be approximated by a linear function this simplifies the equation to the following expression eae ea h with hdz tanhzz since the function h has an upper bound ie vr the critical gain is reached if c h the numerical solution gives if is higher the slope of eaa is positive for small a in the following considerations we will use always the gain as an example since this is an intermediate level of nonlinearity a realizable learning task which exhibits overfitting i i i i looo pn figure learning curves ea for the problem tanh perceptron learns tanhperceptron for different values of the gain even in this realizable case exhaustive training can lead to overfitting if the gain is high enough how to understand the emergence of overfitting here the evaluation of the generalization error in dependence of the order parameters r and q is helpful fig shows the function egr q for r between and and q between and the exhaustive training in realizable cases follows always the line qr r independent of the actual output function that means training is guided only by the training error and not by the generalization error if the gain is higher than the line eg ea starts with a lower slope than qr r which results in overfitting how to avoid overfitting from fig we can guess already that q increases too fast compared to r maybe the ratio between q and r is better during the training process so we have to develop a description for the training process first training process we found already that the order parameters for finite temperatures t of the statistical mechanics approach are a good description of the training process in an unrealizable learning task bss so we use the finite temperature order parameters also in this task these are again taken from the task linear perceptton learns linear perceptton v aa a raa a a qaa a a laa a with the temperature dependent variable a q q x s bos q o i i local min abs mifi local mih i i i i i i i i i i figure contour plot of earq defined by the generalization error as a function of the two order parameters starting from the minimum eg at r q the contour lines for eg are given dotted lines the dashed line corresponds to eg the solid lines are parametric curves of the order parameters r q for certain training strategies the straight line illustrates exhaustive training the lower ones the optimal training which will be explained in fig here the gain the zero temperature limit corresponds to a we will show now that the decrease of the temperature dependent parameter a from oo to describes the evolution of the order parameters during the training process in the training process the natural parameter is the number of parallel training steps t in each parallel training step all patterns are presented once and all weights are updated fig shows the evolution of the order parameters as parametric curves r q the exhaustive learning curve is defined by a i with the parameter c solid line for each c the training ends on this curve the dotted lines illustrate the training process a runs from infinity to simulations of the training process have shown that this theoretical curve is a good description at least after some training steps we will now use this description of the training process for the definition of an optimized training strategy optimal temperature the optimized training strategy chooses not a or the corresponding temperature t but the value of a ie temperature which minimizes the generalization error eo in the lower solid curve indicating the parametric curve r q the value of a is chosen for every a which minimizes eo the function eaa has two minima between a and the solid line indicates always the absolute minimum the parametric curves corresponding to the local minima are given by the double dashed and dash dotted lines note that the optimized value a is always related to an optimized temperature through equation but the parameter a is also related to the number of training steps t a realizable learning task which exhibits overfitting q o i local min abs min local min simulation i i i i i li figure training process the order parameters as parametric curves r q with the parameters a and a the straight solid line corresponds to exhaustive learning ie a i marks at a the dotted lines describe the training process for fixed c iterative training reduces the parameter a from c to examples for a are given the lower solid line is an optimized learning curve to achieve this curve the value of a is chosen which minimizes eg absolutely between c and the error ec has two minima the double dashed and dash dotted lines indicate the second local minimum of ec compare with fig to see which is the absolute and which the local minimum of eo a naive early stopping procedure ends always in the minimum with the smaller q since it is the first minimum during the training process see simulation indicated with errorbars early stopping fig and fig together indicate that an earlier stopping of the training process can avoid the overfitting but in order to determine the stopping point one has to know the actual generalization error during the training cross validation tries to provide an approximation for the real generalization error the cross validation error ecv is defined like et see on a set of examples which are not used during the training here we calculate the optimum using the real generalization error given by r and q to determine the optimal point for early stopping it is a lower bound for training with finite cross validation sets some preliminary tests have shown that already small cross validation sets approximate the real e quite well training is stopped when eg increases the resulting curve is given by the errorbars in fig the errorbars indicate the standard deviation of a simulation with n averaged over trials in fig the same results are shown as learning curves egc there one can see clearly that the early stopping strategy avoids the overfitting summary and outlook in this paper we have shown that overfitting can also emerge in realizable learning tasks the calculation of a critical gain and the contour lines in fig imply that s bis eg exh local min abs min local min simulation pn figure learning curves corresponding to the parametric curves in fig the upper solid line shows again exhaustive training the optimized finite temperature curve is the lower solid line from a exhaustive and optimal training lead to identical results see marks the simulation for early stopping errorbars finds the first minimum of ec the reason for the overfitting is the nonlinearity of the problem the network adjusts slowly to the nonlinearity of the task we have developed a method to avoid the overfitting it can be interpreted in two ways training at a finite temperature reduces overfitting it can be realized if one trains with noisy examples in the other interpretation one learns without noise but stops the training earlier the early stopping is guided by cross validation it was observed that early stopping is not completely simple since it can lead to a local minimum of the generalization error one should be aware of this possibility before one applies early stopping since multilayer perceptrons are built of nonlinear perceptrons the same effects are important for multilayer learning a study with large scale simulations miiller et al has shown that overfitting occurs also in realizable multilayer learning tasks acknowledgments i would like to thank s amari and m opper for stimulating discussions and m herrmann for hints concerning the presentation references s bss avoiding overfitting by finite temperature learning and crossvalidation international conference on artificial neural networks vol p s bss w kinzel m opper generalization ability of percepttons with continuous outputs phys rev e j hertz a krogh r g palmer
a stability criterion for dynamic parameter adaptation is given in the case of the learning rate of backpropagation a class of stable algorithms is presented and studied including a convergence proof
a new nearest neighbor method is described for estimating the bayes risk of a multiclass pattern classification problem from sample data eg a classified training set although it is assumed that the classification problem can be accurately described by sufficiently smooth class conditional distributions neither these distributions nor the corresponding prior probabilities of the classes are required thus this method can be applied to practical problems where the underlying probabilities are not known this method is illustrated using two different pattern recognition problems
in this paper recursive estimation algorithms for dynamic modular networks are developed the models are based on gaussian rbf networks and the gating network is considered in two stages at first it is simply a time varying scalar and in the second it is based on the state as in the mixture of local experts scheme the resulting algorithm uses kalman filter estimation for the model estimation and the gating probability estimation both hard and soft competition based estimation schemes are developed where in the former the most probable network is adapted and in the latter all networks are adapted by appropriate weighting of the data i
linear threshold elements are the basic building blocks of artificial neural networks a linear threshold element computes a function that is a sign of a weighted sum of the input variables the weights are arbitrary integers actually they can be very big integers exponential in the number of the input variables however in practice it is difficult to implement big weights in the present literature a distinction is made between the two extreme cases linear threshold functions with polynomial size weights as opposed to those with exponential size weights the main contribution of this paper is to fill up the gap by further refining that separation namely we prove that the class of linear threshold functions with polynomial size weights can be divided into subclasses according to the degree of the polynomial in fact we prove a more general result that there exists a minimal weight linear threshold function for any arbitrary number of inputs and any weight size to prove those results we have developed a novel technique for constructing linear threshold functions with minimal weights
we describe the use of modern analytical techniques in solving the dynamics of symmetric and nonsymmetric recurrent neural networks near saturation these explicitly take into account the correlations between the post synaptic potentials and thereby allow for a reliable prediction of transients i
the fourier transform of boolean functions has come to play an important role in proving many important learnability results we aim to demonstrate that the fourier transform techniques are also a useful and practical algorithm in addition to being a powerful theoretical tool we describe the more prominent changes we have introduced to the algorithm ones that were crucial and without which the performance of the algorithm would severely deteriorate one of the benefits we present is the confidence level for each prediction which measures the likelihood the prediction is correct i
we propose a way of using boolean circuits to perform real valued computation in a way that naturally extends their boolean functionahty the functionahty of multiple fan in threshold gates in this model is shown to mimic that of a hardware implementation of continuous neural networks a vapnik chervonenkis dimension and sample size analysis for the systems is performed giving best known sample sizes for a real valued neural network experimental results confirm the conclusion that the sample sizes required for the networks are significantly smaller than for sigmoidal networks
the process of machine learning can be considered in two stages model selection and parameter estimation in this paper a technique is presented for constructing dynamical systems with desired qualitative properties the approach is based on the fact that an rt dimensional nonlinear dynamical system can be decomposed into one gradient and rt hamiltonian systems thus the model selection stage consists of choosing the gradient and hamiltonian portions appropriately so that a certain behavior is obtainable to estimate the parameters a stably convergent learning rule is presented this algorithm has been proven to converge to the desired system trajectory for all initial conditions and system inputs this technique can be used to design neural network models which are guaranteed to solve the trajectory learning problem i
recent experiments show that the neural codes at work in a wide range of creatures share some common features at first sight these observations seem unrelated however we show that these features arise naturally in a linear filtered threshold crossing lftc model when we set the threshold to maximize the transmitted information this maximization process requires neural adaptation to not only the dc signal level as in conventional light and dark adaptation but also to the statistical structure of the signal and noise distributions we also present a new approach for calculating the mutual information between a neurons output spike train and any aspect of its input signal which does not require reconstruction of the input signal this formulation is valid provided the correlations in the spike train are small and we provide a procedure for checking this assumption this paper is based on joint work deweese preliminary results from the lftc model appeared in a previous proceedings deweese and the conclusions we reached at that time have been reaffirmed by further analysis of the model i
we present a statistical method that exactly learns the class of constant depth perceptron networks with weights taken from f l q and arbitrary thresholds when the distribution that generates the input examples is member of the family of product distributions these networks also known as nonoverlapping perceptron networks or read once formulas over a weighted threshold basis are loop free neural nets in which each node has only one outgoing weighl with arbitrary high probability the learner is able to exactly identify the connectivity or skeleton of the target perceptron network by using a new statistical test which exploits the strong unimodality property of sums of independent random variables
we propose an active learning method with hidden unit reduction which is devised specially for multilayer perceptrons mlp first we review our active learning method and point out that many fisher information based methods applied to mlp have a critical problem the information matrix may be singular to solve this problem we derive the singularity condition of an information mat fix and propose an active learning technique that is applicable to mlp its effectiveness is verified through experiments i

we analyze and compare the well known gradient descent algorithm and a new algorithm called the exponentiated gradient algorithm for training a single neuron with an arbitrary transfer function both algorithms are easily generalized to larger neural networks and the generalization of gradient descent is the standard back propagation algorithm in this paper we prove worstcase loss bounds for both algorithms in the single neuron case since local minima make it difficult to prove worst case bounds for gradient based algorithms we must use a loss function that prevents the formation of spurious local minima we define such a matching loss function for any strictly increasing differentiable transfer function and prove worst case loss bound for any such transfer function and its corresponding matching loss for example the matching loss for the identity function is the square loss and the matching loss for the logistic sigmoid is the entropic loss the different structure of the bounds for the two algorithms indicates that the new algorithm out performs gradient descent when the inputs contain a large number of irrelevant components dp helmbold j kivinen m k warmuth
we show that for a single neuron with the logistic function as the transfer function the number of local minima of the error function based on the square loss can grow exponentially in the dimension
an adaptive back propagation algorithm is studied and compared with gradient descent standard back propagation for on line learning in two layer neural networks with an arbitrary number of hidden units within a statistical mechanics framework both numerical studies and a rigorous analysis show that the adaptive back propagation method results in faster training by breaking the symmetry between hidden units more efficiently and by providing faster convergence to optimal generalization than gradient descent
topographic mappings occur frequently in the brain a popular approach to understanding the structure of such mappings is to map points representing input features in a space of a few dimensions to points in a dimensional space using some selforganizing algorithm we argue that a more general approach may be useful where similarities between features are not constrained to be geometric distances and the objective function for topographic matching is chosen exphcitly rather than being specified implicitly by the self organizing algorithm we investigate analytically an example of this more general approach applied to the structure of interdigitated mappings such as the pattern of ocular dominance columns in primary visual cortex
the dynamics of complex neural networks modelling the selforganization process in cortical maps must include the aspects of long and short term memory the behaviour of the network is such characterized by an equation of neural activity as a fast phenomenon and an equation of synaptic modification as a slow part of the neural system we present a quadratic type lyapunov function for the flow of a competitive neural system with fast and slow dynamic variables we also show the consequences of the stability analysis on the neural net parameters i
we examine the issue of evaluation of model specific parameters in a modified vc formalism two examples are analyzed the dimensional homogeneous perceptron and the dimensional higher order neuron both models are solved theoretically and their leaming curves are compared against true learning curves it is shown that the formalism has the potential to generate a variety of leaming curves including ones displaying phase transitions
we present a bayesian framework for inferring the parameters of a mixture of experts model based on ensemble learning by variational free energy minimisation the bayesian approach avoids the over fitting and noise level under estimation problems of traditional maximum likelihood inference we demonstrate these methods on artificial problems and sunspot time series prediction
in this paper we consider probabilities of different asymptotics of convergent unlearning algorithm for the hopfield type neural network plakhov semenov treating the case of unbiased random patterns we show also that failed unlearning results in total memory breakdown
a theory of early stopping as applied to linear models is presented the backpropagation learning algorithm is modeled as gradient descent in continuous time given a training set and a validation set all weight vectors found by early stopping must lie on a certain quadric surface usually an ellipsoid given a training set and a candidate early stopping weight vector all validation sets have least squares weights lying on a certain plane this latter fact can be exploited to estimate the probability of stopping at any given point along the trajectory from the initial weight vector to the leastsquares weights derived from the training set and to estimate the probability that training goes on indefinitely the prospects for extending this theory to nonlinear models are discussed i
for a given recurrent neural network a discrete time model may have asymptotic dynamics different from the one of a related continuous time model in this paper we consider a discrete time model that discretizes the continuous time leaky integrator model and study its parallel and sequential dynamics for symmetric networks we provide sufficient and necessary in many cases conditions for the discretized model to have the same cycle free dynamics of the corresponding continuous time model in symmetric networks i
we introduce and analyze a mixture model for supervised learning of probabilistic transducers we devise an online learning algorithm that efficiently infers the structure and estimates the parameters of each model in the mixture theoretical analysis and comparative simulations indicate that the learning algorithm tracks the best model from an arbitrarily large possibly infinite pool of models we also present an application of the model for inducing a noun phrase recognizer
in this paper we introduce remap an approach for the training and estimation of posterior probabilities using a recursive algorithm that is reminiscent of the em based forward backward liporace algorithm for the estimation of sequence likelihoods although very general the method is developed in the context of a statistical model for transition based speech recognition using artificial neural networks ann to generate probabilities for hidden markov models hmms in the new approach we use local conditional posterior probabilities of transitions to estimate global posterior probabilities of word sequences although we still use anns to estimate posterior probabilities the network is trained with targets that are themselves estimates of local posterior probabilities an initial experimental result shows a significant decrease in error rate in comparison to a baseline system i
in this paper we propose recurrent neurm networks with feedback into the input units for handling two types of data analysis problems on the one hand this scheme can be used for static data when some of the input variables are missing on the other hand it can also be used for sequential data when some of the input variables are missing or are available at different frequencies unlike in the case of probabilistic models eg gaussian of the missing variables the network does not attempt to model the distribution of the missing variables given the observed variables instead it is a more discriminant approach that fills in the missing variables for the sole purpose of minimizing a learning criterion eg to minimize an output error i
family discovery is the task of learning the dimension and structure of a parameterized family of stochastic models it is especially appropriate when the training examples are partitioned into episodes of samples drawn from a single parameter value we present three family discovery algorithms based on surface learning and show that they significantly improve performance over two alternatives on a parameterized classification task i
nearest neighbor classification expects the class conditional probabilities to be locally constant and suffers from bias in high dimensions we propose a locally adaptive form of nearest neighbor classification to try to finesse this curse of dimensionality we use a local linear discriminant analysis to estimate an effective metric for computing neighborhoods we determine the local decision boundaries from centroid information and then shrink neighborhoods in directions orthogonal to these local decision boundaries and elongate them parallel to the boundaries thereafter any neighborhood based classifier can be employed using the modified neighborhoods we also propose a method for global dimension reduction that combines local dimension information we indicate how these techniques can be extended to the regression problem

we propose a new learning method generalized learning vector quantization glvq in which reference vectors are updated based on the steepest descent method in order to minimize the cost function the cost function is deternfined so that the obtained learning rule satisfies the convergence condition we prove that kohonens rule as used in lvq does not satisfy the convergence condition and thus degrades recognition ability experimental results for printed chinese character recognition reveal that glvq is superior to lvq in recognition ability
we investigate the effectiveness of stochastic hillclimbing as a baseline for evaluating the performance of genetic algorithms gas as combinatorial function optimizers in particular we address two problems to which gas have been applied in the literature kozas multiplexer problem and the jobshop problem we demonstrate that simple stochastic hillclimbing methods are able to achieve results comparable or superior to those obtained by the gas designed to address these two problems we further illustrate in the case of the jobshop problem how insights obtained in the formulation of a stochastic hillclimbing algorithm can lead to improvements in the encoding used by a ga
statistically independent features can be extracted by finding a factorial representation of a signal distribution principal component analysis pca accomplishes this for linear correlated and gaussian distributed signals independent component analysis ica formalized by comon extracts features in the case of linear statistical dependent but not necessarily gaussian distributed signals nonlinear component analysis finally should find a factorial representation for nonlinear statistical dependent distributed signals this paper proposes for this task a novel feed forward information conserving nonlinear map the explicit symplectic transformations it also solves the problem of non gaussian output distributions by considering single coordinate higher order statistics i
a bayesian kullback learning scheme called ying yang machine is proposed based on the two complement but equivalent bayesian representations for joint density and their kullback divergence not only the scheme unifies existing major supervised and unsupervised learnings including the classical maximum likelihood or least square learning the maximum information preservation the em em algorithm and information geometry the recent popular helmholtz machine as well as other learning methods with new variants and new results but also the scheme provides a number of new learning models
natural and artificial neural circuits must be capable of traversing specific state space trajectories a natural approach to this problem is to learn the relevant trajectories from examples unfortunately gradient descent learning of complex trajectories in amorphous networks is unsuccessful we suggest a possible approach where trajectories are realized by combining simple oscillators in various modular ways we contrast two regimes of fast and slow oscillations in all cases we show that banks of oscillators with bounded frequencies have universal approximation properties open questions are also discussed briefly i
we derive a smoothing regularizer for recurrent network models by requiring robustness in prediction performance to perturbations of the training data the regularizer can be viewed as a generalization of the first order tikhonov stabilizer to dynamic models the closed form expression of the regularizer covers both time lagged and simultaneous recurrent nets with feedforward nets and onelayer linear nets as special cases we have successfully tested this regularizer in a number of case studies and found that it performs better than standard quadratic weight decay i
there is currently considerable interest in developing general nonlinear density models based on latent or hidden variables such models have the ability to discover the presence of a relatively small number of underlying causes which acting in combination give rise to the apparent complexity of the observed data set unfortunately to train such models generally requires large computational effort in this paper we introduce a novel latent variable algorithm which retains the general non linear capabilities of previous models but which uses a training procedure based on the em algorithm we demonstrate the performance of the model on a toy problem and on data from flow diagnostics for a multi phase oil pipeline
we present a framework for learning in hidden markov models with distributed state representations within this framework we derive a learning algorithm based on the expectation maximization em procedure for maximum likelihood estimation analogous to the standard baum welch update rules the m step of our algorithm is exact and can be solved analytically however due to the combinatorial nature of the hidden state representation the exact e step is intractable a simple and tractable mean field approximation is derived empirical results on a set of problems suggest that both the mean field approximation and gibbs sampling are viable alternatives to the computationally expensive exact algorithm i
a new boosting algorithm of freund and schapire is used to improve the performance of decision trees which are constructed using the information ratio criterion of quinlans c algorithm this boosting algorithm iteratively constructs a series of decision trees each decision tree being trained and pruned on examples that have been filtered by previously trained trees examples that have been incorrectly classified by the previous trees in the ensemble are resampled with higher probability to give a new probability distribution for the next tree in the ensemble to train on results from optical character recognition ocr and knowledge discovery and data mining problems show that in comparison to single trees or to trees trained independently or to trees trained on subsets of the feature space the boosting ensemble is much better
we develop a refined mean field approximation for inference and learning in probabilistic neural networks our mean field theory unlike most does not assume that the units behave as independent degrees of freedom instead it exploits in a principled way the existence of large substructures that are computationally tractable to illustrate the advantages of this framework we show how to incorporate weak higher order interactions into a first order hidden markov model treating the corrections but not the first order structure within mean field theory
we have already shown that extracting long term dependencies from sequential data is difficult both for deterministic dynamical systems such as recurrent networks and probabilistic models such as hidden markov models hmms or inputoutput hidden markov models iohmms in practice to avoid this problem researchers have used domain specific a priori knowledge to give meaning to the hidden or state variables representing past context in this paper we propose to use a more general type of a priori knowledge namely that the temporal dependencies are structured hierarchically this implies that long term dependencies are represented by variables with a long time scale this principle is applied to a recurrent network which includes delays and multiple time scales experiments confirm the advantages of such structures a similar approach is proposed for hmms and iohmms
we study bayesian networks for continuous variables using nonlinear conditional density estimators we demonstrate that useful structures can be extracted from a data set in a self organized way and we present sampling techniques for belief update based on markov blanket conditional density models i
conventional binary classification trees such as cart either split the data using axis aligned hyperplanes or they perform a computationally expensive search in the continuous space of hyperplanes with unrestricted orientations we show that the limitations of the former can be overcome without resorting to the latter for every pair of training data points there is one hyperplane that is orthogonal to the line joining the data points and bisects this line such hyperplanes are plausible candidates for splits in a comparison on a suite of datasets we found that this method of generating candidate splits outperformed the standard methods particularly when the training sets were small i
the bayesian analysis of neural networks is difficult because a simple prior over weights implies a complex prior distribution over functions in this paper we investigate the use of gaussian process priors over functions which permit the predictive bayesian analysis for fixed values of hyperparameters to be carried out exactly using matrix operations two methods using optimization and averaging via hybrid monte carlo over hyperparameters have been tested on a number of challenging problems and have produced excellent results i

sigmoid type belief networks a class of probabilistic neural networks provide a natural framework for compactly representing probabilistic information in a variety of unsupervised and supervised learning problems often the parameters used in these networks need to be learned from examples unfortunately estimating the parameters via exact probabilistic calculations ie the em algorithm is intractable even for networks with fairly small numbers of hidden units we propose to avoid the infeasibility of the e step by bounding likelihoods instead of computing them exactly we introduce extended and complementary representations for these networks and show that the estimation of the network parameters can be made fast reduced to quadratic optimization by performing the estimation in either of the alternative domains the complementary networks can be used for continuous density estimation as well i
neural network ensembles have been shown to be very accurate classification techniques previous work has shown that an effective ensemble should consist of networks that are not only highly correct but ones that make their errors on different parts of the input space as well most existing techniques however only indirectly address the problem of creating such a set of networks in this paper we present a technique called addemup that uses genetic algorithms to directly search for an accurate and diverse set of trained networks addemup works by first creating an initial population then uses genetic operators to continually create new networks keeping the set of networks that are as accurate as possible while disagreeing with each other as much as possible experiments on three dna problems show that addemup is able to generate a set of trained networks that is more accurate than several existing approaches experiments also show that addemup is able to effectively incorporate prior knowledge if available to improve the quality of its ensemble i
we compare two regularization methods which can be used to improve the generalization capabilities of gaussian mixture density estimates the first method uses a bayesian prior on the parameter space we derive em expectation maximization update rules which maximize the a posterior parameter probability in the second approach we apply ensemble averaging to density estimation this includes breimans bagging which recently has been found to produce impressive results for classification networks i
following shrager and johnson we study growth of logical function complexity in a network swept by two overlapping waves one of pruning and the other of hebbian reinforcement of connections results indicate a significant spatial gradient in the appearance of both linearly separable and non linearly separable functions of the two inputs of the network the nls cells are much sparser and their slope of appearance is sensitive to parameters in a highly non linear way
recently several researchers have reported encouraging experimental results when using gaussian or bump like activation functions in multilayer percepttons networks of this type usually require fewer hidden layers and units and often learn much faster than typical sigmoidal networks to explain these results we consider a hyper ridge network which is a simple perceptron with no hidden units and a rid e activation function if g we are interested in partitioningp points in d dimensions into two classes then in the limit as d approaches infinity the capacity of a hyper ridge and a perceptton is identical however we show that for p d which is the usual case in practice the ratio of hyper ridge to perceptron dichotomies approaches pd
backpropagation learning algorithms typically collapse the networks structure into a single vector of weight parameters to be optimized we suggest that their performance may be improved by utilizing the structural information instead of discarding it and introduce a framework for tempering each weight accordingly in the tempering model activation and error signals are treated as approximately independent random variables the characteristic scale of weight changes is then matched to that of the residuals allowing structural properties such as a nodes fan in and fan out to affect the local learning rate and backpropagated error the model also permits calculation of an upper bound on the global learning rate for batch updates which in turn leads to different update rules for bias rs non bias weights this approach yields hitherto unparalleled performance on the family relations benchmark a deep multi layer network for both batch learning with momentum and the delta bar delta algorithm convergence at the optimal learning rate is sped up by more than an order of magnitude
we propose a hierarchical scheme for rapid learning of context dependent skills that is based on the recently introduced parameterized selforganizing map psom the underlying idea is to first invest some learning effort to specialize the system into a rapid learner for a more restricted range of contexts the specialization is carried out by a prior investment learning stage during which the system acquires a set of basis mappings or skills for a set of prototypical contexts adaptation of a skill to a new context can then be achieved by interpolating in the space of the basis mappings and thus can be extremely rapid we demonstrate the potential of this approach for the task of a d visuomotor map for a puma robot and two cameras this includes the forward and backward robot kinematics in d end effector coordinates the dd retina coordinates and also the d joint angles after the investment phase the transformation can be learned for a new camera set up with a single observation
it has recently been shown that gradient descent learning algorithms for recurrent neural networks can perform poorly on tasks that involve long term dependencies in this paper we explore this problem for a class of architectures called narx networks which have powerful representational capabilities previous work reported that gradient descent learning is more effective in narx networks than in recurrent networks with hidden states we show that although narx networks do not circumvent the problem of long term dependencies they can greatly improve performance on such problems we present some experimental results that show that narx networks can often retain information for two to three times as long as conventional recurrent networks i
we present two additions to the hierarchical mixture of experts hme architecture by applying a likelihood splitting criteria to each expert in the hme we grow the tree adaptively during training secondly by considering only the most probable path through the tree we may prune branches away either temporarily or permanently if they become redundant we demonstrate results for the growing and path pruning algorithms which show significant speed ups and more efficient use of parameters over the standard fixed structure in discriminating between two interlocking spirals and classifying bit parity patterns
a new learning algorithm is developed for the design of statistical classifiers minimizing the rate of misclassification the method which is based on ideas from information theory and analogies to statistical physics assigns data to classes in probability the distributions are chosen to minimize the expected classification error while simultaneously enforcing the classifiers structure and a level of randomness measured by shannons entropy achievement of the classifier structure is quantified by an associated cost the constrained optimization problem is equivalent to the minimization of a helmholtz free energy and the resulting optimization method is a basic extension of the deterministic annealing algorithm that explicitly enforces structural constraints on assignments while reducing the entropy and expected cost with temperature in the limit of low temperature the error rate is minimized directly and a hard classifier with the requisite structure is obtained this learning algorithm can be used to design a variety of classifier structures the approach is compared with standard methods for radial basis function design and is demonstrated to substantially outperform other design methods on several benchmark examples while often retaining design complexity comparable to or only moderately greater than that of strict descent based methods d miller a rao k rose a gersho i
a practical method for bayesian training of feed forward neural networks using sophisticated monte carlo methods is presented and evaluated in reasonably small amounts of computer time this approach outperforms other state of the art methods on datalimited tasks from real world domains i
we introduce a constructive incremental learning system for regression problems that models data by means of locally linear experts in contrast to other approaches the experts are trained independently and do not compete for data during learning only when a prediction for a query is required do the experts cooperate by blending their individual predictions each expert is trained by minimizing a penalized local cross validation error using second order methods in this way an expert is able to find a local distance metric by adjusting the size and shape of the receptive field in which its predictions are valid and also to detect relevant input features by adjusting its bias on the importance of individual input dimensions we derive asymptotic results for our method in a variety of simulations the properties of the algorithm are demonstrated with respect to interference learning speed prediction accuracy feature detection and task oriented incremental learning
this paper relates the computational power of fahlmans recurrent cascade correlation rcc architecture to that of finite state automata fsa while some recurrent networks are fsa equivalent rcc is not the paper presents a theoretical analysis of the rcc architecture in the form of a proof describing a large class of fsa which cannot be realized by rcc
we report on our development of a high performance system for neural network and other signal processing applications we have designed and implemented a vector microprocessor and packaged it as an attached processor for a conventional workstation we present performance comparisons with commercial workstations on neural network backpropagation training the spert ii system demonstrates significant speedups over extensively handoptimization code running on the workstations i
a new technique termed softassign is applied for the first time to two classic combinatorial optimization problems the traveling salesman problem and graph partitioning softassign which has emerged from the recurrent neural networkstatistical physics framework enforces two way assignment constraints without the use of penalty terms in the energy functions the softassign can also be generalized from two way winner take all constraints to multiple membership constraints which are required for graph partitioning the softassign technique is compared to the softmax potts glass within the statistical physics framework softmax and a penalty term has been a widely used method for enforcing the two way constraints common within many combinatorial optimization problems the benchmarks present evidence that softassign has clear advantages in accuracy speed parallelizability and algorithmic simplicity over softmax and a penalty term in optimization problems with two way constraints i
we investigate the optimization of neural networks governed by general objective functions practical formulations of such objectives are notoriously difficult to solve a common problem is the poor local extrema that result by any of the applied methods in this paper a novel framework is introduced for the solution of largescale optimization problems it assumes little about the objective function and can be applied to general nonlinear non convex functions objectives in thousand of variables are thus efficiently minimized by a combination of techniques deterministic annealing multiscale optimization attention mechanisms and trust region optimization methods
this paper investigates learning in a lifelong context lifelong learning addresses situations in which a learner faces a whole stream of learning tasks such scenarios provide the opportunity to transfer knowledge across multiple learning tasks in order to generalize more accurately from less training data in this paper several different approaches to lifelong learning are described and applied in an object recognition domain it is shown that across the board lifelong learning approaches generalize consistently more accurately from less training data by their ability to transfer knowledge across learning tasks
many classification problems have the property that the only costly part of obtaining examples is the class label this paper suggests a simple method for using distribution information contained in unlabeled examples to augment labeled examples in a supervised training framework empirical tests show that the technique described in this paper can significantly improve the accuracy of a supervised learner when the learner is well below its asymptotic accuracy level
we introduce a new algorithm designed to learn sparse percepttons over input representations which include high order features our algorithm which is based on a hypothesis boosting method is able to pac learn a relatively natural class of target concepts moreover the algorithm appears to work well in practice on a set of three problem domains the algorithm produces classifiers that utilize small numbers of features yet exhibit good generalization performance perhaps most importantly our algorithm generates concept descriptions that are easy for humans to understand i
the wake sleep algorithm hinton dayan frey and neal is a relatively efficient method of fitting a multilayer stochastic generative model to high dimensional data in addition to the top down connections in the generafive model it makes use of bottom up connections for approximating the probability distribution over the hidden units given the data and it trains these bottom up connections using a simple delta rule we use a variety of synthetic and real data sets to compare the performance of the wake sleep algorithm with monte carlo and mean field methods for fitting the same generative model and also compare it with other models that are less powerful but easier to fit
analog electronic cochlear models need exponentially scaled filters cmos compatible lateral bipolar transistors clbts can create exponentially scaled currents when biased using a resistive line with a voltage difference between both ends of the line since these clbts are independent of the cmos threshold voltage current sources implemented with clbts are much better matched than current sources created with mos transistors operated in weak inversion measurements from integrated test chips are shown to verify the improved matching
both vertebrate and invertebrate retinas are highly eicient in extracting contrast independent of the background intensity over five or more decades this eiciency has been rendered possible by the adaptation of the dc operating point to the background intensity while maintaining high gain transient responses the centersurround properties of the retina allows the system to extract information at the edges in the image this silicon retina models the adaptation properties of the receptors and the antagonistic centersurround properties of the laminar cells of the invertebrate retina and the outer plexiform layer of the vertebrate retina we also illustrate the spatio temporal responses of the silicon retina on moving bars the chip has x pixels on a xmm die and it is fabricated in rn n well technology
a unique architecture of winner search hardware has been developed using a novel neuron like high functionality device called neuron mos transistor or vmos in short as a key circuit element the circuits developed in this work can find the location of the maximum or minimum signal among a number of input data on the continuous time basis thus enabling real time winner tracking as well as fully parallel sorting of multiple input data we have developed two circuit schemes one is an ensemble of selfloop selecting vmos ring oscillators finding the winner as an oscillating node the other is an ensemble of ymos variable threshold inverters receiving a common ramp voltage for competitive excitation where data sorting is conducted through consecutive winner search actions test circuits were fabricated by a double polysilicon cmos process and their operation has been experimentally verified
we present an integrated analog processor for real time wavelet decomposition and reconstruction of continuous temporal signals covering the audio frequency range the processor performs complex harmonic modulation and gaussian lowpass filtering in parallel channels each clocked at a different rate producing a multiresolution mapping on a logarithmic frequency scale our implementation uses mixed mode analog and digital circuits oversampling techniques and switched capacitor filters to achieve a wide linear dynamic range while maintaining compact circuit size and low power consumption we include experimental results on the processor and characterize its components separately from measurements on a single channel test chip
we are developing special purpose low power analog to digital converters for speech and music applications that feature analog circuit models of biological audition to process the audio signal before conversion this paper describes our most recent converter design and a working system that uses several copies of the chip to compute multiple representations of sound from an analog input this multi representation system demonstrates the plausibility of inexpensively implementing an auditory scene analysis approach to sound processing
a one dimensional model of primate smooth pursuit mechanism has been implemented in tm cmos vlsi the model consolidates robinsons negative feedback model with wyatt and polas positive feedback scheme to produce a smooth pursuit system which zeros the velocity of a target on the retina furthermore the system uses the current eye motion as a predictor for future target motion analysis stability and biological correspondence of the system are discussed for implementation at the focal plane a local correlation based visual motion detection technique is used velocity measurements ranging over orders of magnitude with variation provides the input to the smooth pursuit system the system performed successful velocity tracking for high contrast scenes circuit design and performance of the complete smooth pursuit system is presented
in systems that process sensory data there is frequently a model matching stage where class hypotheses are combined to recognize a complex entity we introduce a new model of parallelism the single function multiple data sfmd model appropriate to this stage sfmd functionality can be added with small hardware expense to certain existing simd architectures and as an incremental addition to the programming model adding sfmd to an simd machine will not only allow faster model matching but also increase its flexibility as a general purpose machine and its scope in performing the initial stages of sensory processing i
we describe two parallel analog vlsi architectures that integrate optical flow data obtained from arrays of elementary velocity sensors to estimate heading direction and time to contact for heading direction computation we performed simulations to evaluate the most important qualitative properties of the optical flow field and determine the best functional operators for the implementation of the architecture for time to contact we exploited the divergence theorem to integrate data from all velocity sensors present in the architecture and average out possible errors i
a technique for segmenting sounds using processing based on mammalian early auditory processing is presented the technique is based on features in sound which neuron spike recording suggests are detected in the cochlear nucleus the sound signal is bandpassed and each signal processed to enhance onsets and offsets the onset and offset signals are compressed then clustered both in time and across frequency channels using a network of integrateand fire neurons onsets and offsets are signalled by spikes and the timing of these spikes used to segment the sound i background traditional speech interpretation techniques based on fourier transforms spectrum rccoding and a hidden markov model or neural network interpretation stage have limitations both in continuous speech and in interpreting speech in the presence of noise and this has led to interest in front ends modelling biological auditory systems for speech interpretation systems ainsworth and meyer cosi cole et al auditory modelling systems use similar early auditory processing to that used in biological systems mammalian auditory processing uses two ears and the incoming signal is filtered first by the pinna external car and the anditory canal before it causes the tympanic membrane eardrum to vibrate this vibration is then passed on through the bones of the middle ear to the oval window on the cochlea inside the cochlea the pressure wave causes a pattern of vibration to occur on the basilar membrane this appears to be an active process using both the inner and outer hair cells of the organ of corti the movement is detected by the inner hair cells and turned into neural impulses by the neurons of the spiral ganglion these pass down the auditory nerve and arrive at various parts of the cochlear nucleus from there nerve fibres innervate other areas the lateral and medial nuclei of the superior olive ls smith and the inferior colliculus for example see pickles virtually all modern sound or speech interpretation systems use some form of bandpass filtering following the biology as far as the cochlea most use fourier transforms to perform a calculation of the energy in each band over some time period usually between and ms this is not what the cochlea does auditory modelling front ends differ in the extent and length to which they follow animal early auditory processing but the term generally implies at least that wideband filters are used and that high temporal resolution is maintained in the initial stages this means the use of filtering techniques rather than fourier transforms in the bandpass stage such filtering systems have been implemented by patterson and holdsworth patterson and holdsworth slaney and placed directly in silicon lazzaro and mead lazzaro et al liu et al fragniere and van schaik some auditory models have moved beyond cochlear filtering the inner hair cell has been modelled by either simple rectification smith or has been based on the work of meddis for example patterson and holdsworth cosi brown lazzaro has experimented with a silicon version of lickliders autocorrelation processing licklider lazzaro and mead others such as wu ct al blackwood et al ainsworth and meyer brown berthommier smith have considered the early brainstem nuclei and their possible contribution based on the neurophysiology of the different cell types pickles blackburn and sachs kim et al auditory model based systems have yet to find their way into mainstream speech recognition systems cosi the work presented here uses auditory modelling up to onset cells in the cochlear nucleus it adds a temporal neural network to clean up the segmentation produced this part has been filed as a patent smith though the system has some biological plausibility the aim is an effective data driven segmentation technique implement able in silicon techniques used digitized sound was applied to an auditory front end patterson and holdsworth which bandpassed the sound into channels each with bandwidth f hz where fc is the centre frequency in khz of the band moore and glasberg these were rectified modelling the effect of the inner hair cells the signals produced bear some resemblance to that in the auditory nerve the real system has far more channels and each nerve channel carries spike coded information the coding here models the signal in a population of neighboring auditory nerve fibres the onset offset filter the signal present in the auditory nerve is stronger near the onset of a tone than later pickles this effect is much more pronounced in certain cell types of the cochlear nucleus these fire strongly just after the onset of a sound in the band to which they are sensitive and are then silent this emphasis on onsets was modelled by convolving the signal in each band with a filter which computes two averages a more recent one and a less recent one and subtracts the less recent one from the more recent one one biologically possible justification for this is to consider that a neuron is receiving the same driving input twice one excitatorily and the other inhibitorily the excitatory input has a shorter time constant than the inhibitory input both exponentially weighted averages and averages formed using a ganssian filter have been tried smith but the former place too much emphasis on the most recent part of the signal making the latter more effective onset based sound segmentation the filter output for input signal sx is j t ot k r ft xkftxkrsxdx where fxy vexp yx k and r determine the rise and fall times of the pulses of sotmd that the system is sensitive to we used k r so that the sd of the gaussians are ms and ms the convolving filter has a positive peak at crosses at ms and is then negative with these values the system is sensitive to energy rises and falls which occur in the envelopes of everyday sounds a positive onset offset signal implies that the bandpassed signal is increasing in intensity and a negative onset offset signal implies that it is decreasing in intensity the convolution used is a sound analog of the difference of gaussians operator used to extract blackwhite and whiteblack edges in monochrome images marr and hildreth in smith we performed smmd segmentation directly on this signal compressing the onset offset signal the onset offset signal was divided into two positive going signals an onset signal consisting of the positive going part and an offset signal consisting of the inverted negative going part both were compressed logarithmically where logx was taken as for x this increases the dynamical rmge of the system and models comprcssivc biological effects the compressed onset signal models the output of a population of onset cells this technique for producing an onset signal is related to that of wu et al cosi the integrate and fire neural network to segment the sound using the onset and offset signals they need to be integrated across frequency bands and across time this temporal and tonotopic clustering was achieved using a network of integrate and fire units an integrate and fire unit accumulates its weighted input over time the activity of the mit a is initially and alters according to da va where it is the input to the neuron and the dissipation describes the leakiness of the integration when a reaches a threshold the unit fires ie emits a pulse and a is reset to after firing there is a period of insensitivity to input called the refractory period such neurons are discussed in eg mirolla and strogatz one integrate and fire neuron was used per channeh this neuron received input either from a single chamel or from a set of adjacent channels all with equal positive weighting the output of each neuron was fed back to a set of adjacent neurons again with a fixed positive weight one time step here ms later because of the leaky nature of the accumulation of activity excitatory input to the neuron arriving when its activation is neax threshold has a laxger effect on the next firing time than excitatory input arriving when activation is lower thus if similar input is applied to a set of neurons in adjacent channels the effect of the inter neuron connections is that when the first one fires its neighbors fire almost immediately this allows a network of such neurons to cluster the onset or offset signals producing a sharp burst of spikes across a number of channels providing unambiguous onsets or offsets the external and internal weights of the network were adjusted so that onset or offset input alone allowed neurons to fire while internal input alone was not enough ls smith to cause firing the refractory period used was set to ms for the onset system and ms for the offset system for the onset system the effect was to produce sharp onset firing responses across adjacent channels in response to a sudden increase in energy in some channels thus grouping onsets both tonotopically and temporally this is appropriate for onsets as these are generally brief and clearly marked the output of this stage we call the onset map offsets tend to be more gradual this is due to physical effects for example a percussive sound will start suddenly as the vibrating element starts to move but die away slowly as the vibration ceases see gaver for a discussion even when the vibration does stop suddenly the sound will die away more slowly due to echoes thus we cannot reliably mark the offset of a sound instead we reduce the refractory period of the offset neurons and produce a train of pulses marking the duration of the offset in this chaimel we call the output of this stage the offset map results as the technique is entirely data driven it can be applied to sound from any source it has been applied to both speech and musical sounds figure i shows the effect of applying the techniques discussed to a short piece of speech fig lc shows that the neural network integrates the onset timings across the channels allowing these onsets to be used for segmentation the simplest technique is to divide up the continuous speech at each onset howeverto ensure that the occasional onset in a single channel does not confuse the system and that onsets which occur near to each other do not result in very short segments we demanded that a segmentation boundary have at least onsets inside a period of ms and the minimum segment length was set to ms the utterance neural information processing systems has phonetic representation njurlanfarmcjbnprosasaxj ststams and is segmented into the following segments in ju l r la a nf arm el if an pro os as aij is itst am is the same text spoken more slowly over s rather than s has phonetic representation njuralanfirmeanprosasuj sstams segmenting using this technique gives the following segments in ju l lul r a all an if trm el if an in pr ro os as uj is it st am is although some phonemes are broken between segments the system provides effective segmentation and is relatively insensitive to speech rate the system is also effective at finding speech inside certain types of noise such as motor bike noise as can be seen in fig le and f the system has been used to segment sound from single musical instruments where these have clear breaks between notes this is straightforward in smith correct segmentation was achieved directly from the onset offset signal but was not achieved for slurred sounds in which the notes change smoothly as is visible in figure c the onsets here are clear using the network and the segmentation produced is nearonset based sound segmentation figure a donset and offset maps from author saying neural information processing systems rapidly a envelope of original sound b onset map from channels from hz khz onset filter parameters as in text one neuron per channel with no interconnection neuron refractory period is ms c as b but network has input applied to adjacent channels and internal feedback to channels d offset map produced similarly with refractory period ms e envelope of say thats a nice bike with motorbike noise in background lines mark utterance f g onset offset maps for e perfect best results were obtained here when the input to the network is not spread across channels conclusions and further work an effective data driven segmentation technique based on onset feature detection and using integrate and fire neurons has been demonstrated the systen is relatively immune to broadband noise segmentation is not an end in itself the effectiveness of any technique will depend on the eventual application ls smith figure a slurred flute sound with vertical lines showing boundary between notes b onsets found using a single neuron per channel and no interconnection c as b but with internal feedback from each channel to adjacent channels d offsets found with refractory period ms the segmentation is currently not sing the information on which bands the onsets occm in we propose to extend this work by combining the segmentation described here with work streanfing bands sharing same frequency amplitude modulation the aim of this is to extract sound segments from some subset of the bands allowing segmentation and streaming to run concurrently acknowledgements many thanks are due to the members of the centre for cognitive and computational neurosciencc at the university of stirling references ainsworth w meyer g speech analysis by means of a physiologically based model of the cochlear nerve and cochlcar nucleus in visual representations of speech signals cooke m beet s eds berthommier f modelling neual esponses of the intermediate auditory system in mathematics applied to biology and medicine demongeot j capasso v wucrtz publishing canada blackburn cc sachs mb classification of unit types in the anteroventral cochlear nucleus pst histograms and regularity analysis j neurophysiology onset based sound segmentation blackwood n meyer g aimsworth w a model of the processing of voiced plosives in the auditory nerve and cochlear nucleus proceedings inst of acoustics brown g computational auditory scene analysis tr cs department of computing science university of sheffield england cole r et al the challenge of spoken language systems research directions of the s ieee trans speech and audio processing cosi p on the use of auditory models in speech technology in intelligent perceptual models lncs springer verlag fragniere e van schaik a linear predictive coding of the speech signal using an analog cochlear model mantra internal report mantra center for neuro mimetic systems epfl lausanne switzerland gaver ww what in the world do we hear an ecological approach to auditory event perception ecological psychology kim do sirianni jg chang so responses of dcn pvcn neurons and auditory nerve fibres in manesthetized decerebrate cats to am and pure tones analysis with autocorrelationpower spectrum hearing research lazzaro j mead c silicon modelling of pitch perception prvc natl acad sciences usa lazzaro j wawrzynek j mahowald m sivilotti m gillespie d silicon auditory processors as computer peripherals ieee trans on neural networks may licklider jcr a duplex theory of pitch perception experentia liu w andreou ag goldstein mh analog cochlear model for multiresolution speech analysis advances in neural information processing systems hanson sj cowan jd lee giles c eds morgan kaufmann marr d hildreth e theory of edge detection proc royal society of london b mcddis r simulation of auditory neural transduction further studies j acoust soc am moore bcj glasbcrg br suggested formulae for calculating auditory filter bandwidths and excitation patterns j acoust soc america mirollo re strogatz sh synchronization of pulse coupled biological oscillators siam j appl math patterson r holdsworth j an
an application of laterally interconnected self organizing maps lissom to handwritten digit recognition is presented the lateral connections learn the correlations of activity between units on the map the resulting excitatory connections focus the activity into local patches and the inhibitory connections decorrelate redundant activity on the map the map thus forms internal representations that are easy to recognize with eg a perceptron network the recognition rate on a subset of nist database is higher with lissom than with a regular self organizing map som as the front end and higher than recognition of raw input bitmaps directly these results form a promising starting point for building pattern recognition systems with a lissom map as a front end i
this paper describes the training of a recurrent neural network as the letter posterior probability estimator for a hidden markov model off line handwriting recognition system the network estimates posterior distributions for each of a series of frames representing sections of a handwritten word the supervised training algorithm backpropagation through time requires target outputs to be provided for each frame three methods for deriving these targets are presented a novel method based upon the forwardbackward algorithm is found to result in the recognizer with the lowest error rate i
a method for incorporating context dependent phone classes in a connectionist hmm hybrid speech recognition system is introduced a modular approach is adopted where single layer networks discriminate between different context classes given the phone class and the acoustic data the context networks are combined with a context independent ci network to generate context dependent cd phone probability estimates experiments show an average reduction in word error rate of and from the ci system on arpa word and sqale word tasks respectively due to improved modelling the decoding speed of the cd system is more than twice as fast as the ci system
a new on line learning algorithm which minimizes a statistical dependency among outputs is derived for blind separation of mixed signals the dependency is measured by the average mutual information mi of the outputs the source signals and the mixing matrix are unknown except for the number of the sources the gram charlier expansion instead of the edgeworth expansion is used in evaluating the mi the natural gradient approach is used to minimize the mi a novel activation function is proposed for the on line learning algorithm which has an equivariant property and is easily implemented on a neural network like model the validity of the new learning algorithm are verified by computer simulations i
a hybrid and contextual radial basis function networkhidden markov model off line handwritten word recognition system is presented the task assigned to the radial basis function networks is the estimation of emission probabilities associated to markov states the model is contextual because the estimation of emission probabilities takes into account the left context of the current image segment as represented by its predecessor in the sequence the new system does not outperform the previous system without context but acts differently
completely parallel object recognition is np complete achieving a recognizer with feasible complexity requires a compromise between parallel and sequential processing where a system selectively focuses on parts of a given image one after another successive fixations are generated to sample the image and these samples are processed and abstracted to generate a temporal context in which results are integrated over time a computational model based on a partially recurrent feedforward network is proposed and made credible by testing on the rem world problem of recognition of handwritten digits with encouraging results i
this paper describes the kodak imagelink tm ocr alphanumeric handprint module there are two neural network algorithms at its cc the first network is trained to find individual characters in an alphammeaic field while the secxmd oe performs the classification both networks we uained on gabor projections of the original pixel images which reintired in high reccln ition rates and greater noiso immunity compared to its purely mme cotrate art shustxvich and thrasher this version of the system hos a sienificant applicatic specie postprocessing module the system has beta implemented in specialized parallel hardware which allows it to run at charlsecboard it has been irttsiod at the driver and vehicle l agency dvla in the united kingdom and its overall stwss rate exls character level without rejects which tranglates into field rate ff approimtely of the fields are rejected the system achieves character and field success rate
we define a gamma multi layer perceptton mlp as an mlp with the usual synaptic weights replaced by gamma filters as proposed by de vries and principe de vries and principe and associated gain terms throughout all layers we derive gradient descent update equations and apply the model to the recognition of speech phonemes we find that both the inclusion of gamma filters in all layers and the inclusion of synaptic gains improves the performance of the gamma mlp we compare the gamma mlp with tdnn back tsoi fir mlp and back tsoi iir mlp architectures and a local approximation scheme we find that the gamma mlp results in an substantial reduction in error rates i
matching feature point sets lies at the core of many approaches to object recognition we present a framework for non rigid matching that begins with a skeleton module affine point matching and then integrates multiple features to improve correspondence and develops an object representation based on spatial regions to model local transformations the algorithm for feature matching iteratively updates the transformation parameters and the correspondence solution each in turn the aftme mapping is solved in closed form which permits its use for data of any dimension the correspondence is set via a method for two way constraint satisfaction called softassign which has recently emerged from the neural networkstatistical physics realm the complexity of the non rigid matching algorithm with multiple features is the same as that of the affine point matching algorithm results for synthetic and real world data are provided for point sets in d and d and for d data with multiple types of features and parts i
intermediate and higher vision processes require selection of a subset of the available sensory information before further processing usually this selection is implemented in the form of a spatially circumscribed region of the visual field the so called focus of attention which scans the visual scene dependent on the input and on the attentional state of the subject we here present a model for the control of the focus of attention in primates based on a saliency map this mechanism is not only expected to model the functionality of biological vision but also to be essential for the understanding of complex scenes in machine vision
when a sensory system constructs a model of the environment from its input it might need to verify the models accuracy one method of verification is multivariate time series prediction a good model could predict the near future activity of its inputs much as a good scientific theory predicts future data such a predicting model would require copious top down connections to compare the predictions with the input that feedback could improve the models performance in two ways by biasing internal activity toward expected patterns and by generating specific error signals if the predictions fail a proof of concept model an event driven computationally efficient layered network incorporating cortical features like all excitatory synapses and local inhibition was constructed to make near future predictions of a simple moving stimulus after unsupervised learning the network contained units not only tuned to obvious features of the stimulus like contour orientation and motion but also to contour discontinuity end stopping and illusory contours
visual occlusion events constitute a major source of depth information this paper presents a self organizing neural network that learns to detect represent and predict the visibility and invisibility relationships that arise during occlusion events after a period of exposure to motion sequences containing occlusion and disocclusion events the network develops two parallel opponent channels or chains of lateral excitatory connections for every resolvable motion trajectory one channel the on chain or visible chain is activated when a moving stimulus is visible the other channel the off chain or invisible chain carries a persistent amodal representation that predicts the motion of a formerly visible stimulus that becomes invisible due to occlusion the learning rule uses disinhibition from the on chain to trigger learning in the off chain the on and off chain neurons can learn separate associations with object depth ordering the results are closely related to the recent discovery assad maunsell of neurons in macaque monkey posterior parietal cortex that respond selectively to inferred motion of invisible stimuli
the facial action coding system facs devised by ekman and friesen provides an objective means for measuring the facial muscle contractions involved in a facial expression in this paper we approach automated facial expression analysis by detecting and classifying facial actions we generated a database of over image sequences of subjects performing over distinct facial actions or action combinations we compare three different approaches to classifying the facial actions in these images holistic spatial analysis based on principal components of graylevel images explicit measurement of local image features such as wrinkles and template matching with motion flow fields on a dataset containing six individual actions and subjects these methods had and performances respectively for generalization to novel subjects when combined performance improved to
visual cognition depends critically on the ability to make rapid eye movements known as saccades that orient the fovea over targets of interest in a visual scene saccades are known to be ballistic the pattern of muscle activation for foveafing a prespecified target location is computed prior to the movement and visual feedback is precluded despite these distinctive properties there has been no general model of the saccadic targeting strategy employed by the human visual system during visual search in natural scenes this paper proposes a model for saccadic targeting that uses iconic scene representations derived from oriented spatial filters at multiple scales visual search proceeds in a coarse to fine fashion with the largest scale filter responses being compared first the model was empirically tested by comparing its performance with actual eye movement data from human subjects in a natural visual search task preliminary results indicate substantial agreement between eye movements predicted by the model and those recorded from human subjects
a model of human motion perception is presented the model contains two stages of direction selective units the first stage contains broadly tuned units while the second stage contains units that are narrowly tuned the model accounts for the motion aftereffect through adapting units at the first stage and inhibitory interactions at the second stage the model explains how two populations of dots moving in slightly different directions are perceived as a single population moving in the direction of the vector sum and how two populations moving in strongly different directions are perceived as transparent motion the model also explains why the motion aftereffect in both cases appears as non transparent motion i
a neural network model of d lightness perception is presented which builds upon the facade theory boundary contour systemfeature contour system of grossberg and colleagues early ratio encoding by retinal ganglion neurons as well as psychophysical results on constancy across different backgrounds background constancy are used to provide functional constraints to the theory and suggest a contrast negation hypothesis which states that ratio measures between coplanar regions are given more weight in the determination of lightness of the respective regions simulations of the model address data on lightness perception including the coplanar ratio hypothesis the benary cross and whites illusion
no finite sample is sufficient to determine the density and therefore the entropy of a signal directly some assumption about either the functional form of the density or about its smoothness is necessary both amount to a prior over the space of possible density functions by far the most common approach is to assume that the density has a parametric form by contrast we derive a differential learning rule called emma that optimizes entropy by way of kernel density estimation entropy and its derivative can then be calculated by sampling from this density estimate the resulting parameter update rule is surprisingly simple and efficient we will show how emma can be used to detect and correct corruption in magnetic resonance images mri this application is beyond the scope of existing parametric entropy models
we have developed a foveated gesture recognition system that runs in an unconstrained office environment with an active camera using vision routines previously implemented for an interactive environment we determine the spatial location of salient body parts of a user and guide an active camera to obtain images of gestures or expressions a hidden state reinforcement learning paradigm is used to implement visual attention the attention module selects targets to loveate based on the goal of successful recognition and uses a new multiple model q learning formulation given a set of target and distractor gestures our system can learn where to foveate to maximally discriminate a particular gesture
a neurally inspired visual object recognition system is described called seemore whose goal is to identify common objects from a large known set independent of d viewiag angle distance and non rigid distortion seemores database consists of objects that are rigid shovel non rigid telephone cord articulated book statistical shrubbery and complex photographs of scenes recognition results were obtained using a set of color and shape feature channels within a simple feedforward network architecture in response to a test set of novel test views of each object presented individually in color video images seemore identified the object correctly of the time chance is using a nearest neighbor classifier similar levels of performance were obtained for the subset of non rigid objects generalization behavior reveals emergence of striking natural category structure not explicit in the input feature dimensions i
we present a neural network based face detection system a retinally connected neural network examines small windows of an image and decides whether each window contains a face the system arbitrates between multiple networks to improve performance over a single network we use a bootstrap algorithm for training which adds false detections into the training set as training progresses this eliminates the difficult task of manually selecting non face training examples which must be chosen to span the entire space of non face images comparisons with another state of the art face detection system are presented our system has better performance in terms of detection and false positive rates
central to the performance improvement of a committee relative to individual networks is the error correlation between networks in the committee we investigated methods of achieving error independence between the networks by training the networks with different resampling sets from the original training set the methods were tested on the sinwave artificial task and the real world problems of hepatoma liver cancer and breast cancer diagnoses
infants manipulative exploratory behavior within the environment is a vehicle of cognitive stimulationmccall during this time infants practice and perfect sensorimotor patterns that become behavioral modules which will be seriated and imbedded in more complex actions this paper explores the development of such primitive learning systems using an embodied light weight hand which will be used for a humanoid being developed at the mit artificial intelligence laboratorybrooks and stein primitive grasping procedures are learned from sensory inputs using a connectionist reinforcement algorithm while two submodules preprocess sensory data to recognize the hardness of objects and detect shear using competitive learning and back propagation algorithm strategies respectively this system is not only consistent and quick during the initial learning stage but also adaptable to new situations after training is completed
learning how to adjust to an opponents position is critical to the success of having intelligent agents collaborating towards the achievement of specific tasks in unfriendly environments this paper describes our work on a memory based technique for to choose an action based on a continuous valued state attribute indicating the position of an opponent we investigate the question of how an agent performs in nondeterministic variations of the training situations our experiments indicate that when the random variations fall within some bound of the initial training the agent performs better with some initial training rather than from a tabula rasa
we report on the development of the modular neural system smegli for the visual guidance of robot pick and place actions several neural networks are integrated to a single system that visually recognizes human hand pointing gestures from stereo pairs of color video images the output of the hand recognition stage is processed by a set of color sensitive neural networks to determine the cartesian location of the target object that is referenced by the pointing gesture finally this information is used to guide a robot to grab the target object and put it at another location that can be specified by a second pointing gesture the accuracy of the current system allows to identify the location of the referenced target object to an accuracy of i cm in a workspace area of x cm in our current environment this is sufficient to pick and place arbitrarily positioned target objects within the workspace the system consists of neural networks that perform the tasks of image segmentation estimation of hand location estimation of d pointing direction object recognition and necessary coordinate transforms drawing heavily on the use of learning algorithms the functions of all network modules were created from data examples only
state of the art speech processors in cochlear implants perform channel selection using a spectral maxima strategy this strategy can lead to confusions when high frequency features are needed to discriminate between sounds we present in this paper a novel channel selection strategy based upon pattern recognition which allows smart channel selections to be made the proposed strategy is implemented using multi layer perceptrons trained on a multispeaker labelled speech database the input to the network are the energy coefficients of n energy channels the output of the system are the indices of the m selected channels we compare the performance of our proposed system to that of spectral maxima strategy and show that our strategy can produce significantly better results i
most current methods for prediction of protein secondary structure use a small window of the protein sequence to predict the structure of the central amino acid we describe a new method for prediction of the non local structure called sheet which consists of two or more strands that are connected by hydrogen bonds since strands are often widely separated in the protein chain a network with two windows is introduced after training on a set of proteins the network predicts the sheets well but there are many false positives by using a global energy function the sheet prediction is combined with a local prediction of the three secondary structures c helix fi strand and coil the energy function is minimized using simulated annealing to give a final prediction i
we present results on the use of neural network based autoassociators which act as novelty or anomaly detectors to detect imminent motor failures the autoassociator is trained to reconstruct spectra obtained from the healthy motor in laboratory tests we have demonstrated that the trained autoassociator has a small reconstruction error on measurements recorded from healthy motors but a larger error on those recorded from a motor with a fault we have designed and built a motor monitoring system using an autoassociator for anomaly detection and are in the process of testing the system at three industrial and commercial sites
we report here that changes in the normalized electroencephalographic eeg cross spectrum can be used in conjunction with feedforward neural networks to monitor changes in alertness of operators continuously and in near real time previously we have shown that eeg spectral amplitudes covary with changes in alertness as indexed by changes in behavioral error rate on an auditory detection task here we report for the first time that increases in the frequency of detection errors in this task are also accompanied by patterns of increased and decreased spectral coherence in several frequency bands and eeg channel pairs relationships between eeg coherence and performance vary between subjects but within subjects their topographic and spectral profiles appear stable from session to session changes in alertness also covary with changes in correlations among eeg waveforms recorded at different scalp sites and neural networks can also estimate alertness from correlation changes in spontaneous and unobtrusivelyrecorded eeg signals i

in this paper we propose a memory based q learning algorithm called predictive q routing pq routing for adaptive traffic control we attempt to address two problems encountered in q routing boyan littman namely the inability to fine tune routing policies under low network load and the inability to learn new optimal policies under decreasing load conditions unlike other memory based reinforcement learning algorithms in which memory is used to keep past experiences to increase learning speed pq routing keeps the best experiences learned and reuses them by predicting the traffic trend the effectiveness of pq routing has been verified under various network topologies and traffic conditions simulation results show that pq routing is superior to q routing in terms of both learning speed and adaptability
in recent years the interest of investors has shifted to computerized asset allocation portfolio management to exploit the growing dynamics of the capital markets in this paper asset allocation is formalized as a markovjan decision problem which can be optimized by applying dlnamic programming or reinforcement learning based algorithms using an artificial exchange rate the asset allocation strategy optimized with reinforcement learning q learning is shown to be equivalent to a policy computed by dynamic programming the approach is then tested on the task to invest liquid capital in the german stock market here neural networks are used as value function approximators the resulting asset allocation strategy is superior to a heuristic benchmark policy this is a further example which demonstrates the applicability of neural network based reinforcement learning to a problem setting with a high dimensional state space

this paper discusses the use of multilayer feedforward neural networks for predicting a stocks excess return based on its exposure to various technical and fundamental factors to demonstrate the effectiveness of the approach a hedged portfolio which consists of equally capitalized long and short positions is constructed and its historical returns are benchmarked against t bill returns and the scp index i
this paper describes a neural network based controller for allocating capacity in a telecommunications network this system was proposed in order to overcome a real time response constraint two basic architectures are evaluated a feedforward network heuristic and a feedforward network recurrent network these architectures are compared against a linear programming lp optimiser as a benchmark this lp optimiser was also used as a teacher to label the data samples for the feedforward neural network training algorithm it is found that the systems are able to provide a traffic throughput of and respectively of the throughput obtained by the linear progranuning solution once trained the neural network based solutions are found in a fraction of the time required by the lp optimiser
current environmental monitoring systems assume particles to be spherical and do not attempt to classify them a laser based system developed at the university of hertfordshire aims at classifying airborne particles through the generation of two dimensional scattering profiles the performances of template matching and two types of neural network hypernet and semi linear units are compared for image classification the neural network approach is shown to be capable of comparable recognition performance while offering a number of advantages over template matching i
this paper discusses how a robot can learn goal directed navigation tasks using local sensory inputs the emphasis is that such learning tasks could be formulated as an embedding problem of dynamical systems desired trajectories in a task space should be embedded into an adequate sensory based internal state space so that an unique mapping from the internal state space to the motor command could be established the paper shows that a recurrent neural network suffices in self organizing such an adequate internal state space from the temporal sensory input in our experiments using a real robot with a laser range sensor the robot navigated robustly by achieving dynamical coherence with the environment it was also shown that such coherence becomes structurally stable as the global attractor is self organized in the coupling of the internal and the environmental dynamics
this paper describes a policy iteration algorithm for optimizing the performance of a harmonic function based controller with respect to a user defined index value functions are represented as potential distributions over the problem domain being control policies represented as gradient fields over the same domain all intermediate policies are intrinsically safe ie collisions are not promoted during the adaptation process the algorithm has efficient implementation in parallel simd architectures one potential application travel distance minimization illustrates its usefulness i
compliant control is a standard method for performing fine manipulation tasks like grasping and assembly but it requires estimation of the state of contact soc between the robot arm and the objects involved here we present a method to learn a model of the movement from measured data the method requires little or no prior knowledge and the resulting model explicitly estimates the soc the current soc is viewed as the hidden state variable of a discrete hmm the control dependent transition probabilities between states are modeled as parametrized functions of the measurement we show that their parameters can be estimated from measurements at the same time as the parameters of the movement in each soc the learning algorithm is a variant of the em procedure the e step is computed exactly solving the m step exactly is not possible in general here gradient ascent is used to produce an increase in likelihood i
a neural network based approach is presented for controlling two distinct types of nonlinear systems the first corresponds to nonlinear systems with parametric uncertainties where the parameters occur nonlinearly the second corresponds to systems for which stabilizing control structures cannot be determined the proposed neural controllers are shown to result in closed loop system stability under certain conditions
this paper describes the application of reinforcement learning rl to the difficult real world problem of elevator dispatching the elevator domain poses a combination of challenges not seen in most rl research to date elevator systems operate in continuous state spaces and in continuous time as discrete event dynamic systems their states are not fully observable and they are nonstationary due to changing passenger arrival rates in addition we use a team of rl agents each of which is responsible for controlling one elevator car the team receives a global reinforcement signal which appears noisy to each agent due to the effects of the actions of the other agents the random nature of the arrivals and the incomplete observation of the state in spite of these complications we show results that in simulation surpass the best of the heuristic elevator control algorithms of which we are aware these results demonstrate the power of rl on a very large scale stochastic dynamic optimization problem of practical utility i
job shop scheduling is an important task for manufacturing industries we are interested in the particular task of scheduling payload processing for nasas space shuttle program this paper summarizes our previous work on formulating this task for solution by the reinforcement learning algorithm tda a shortcoming of this previous work was its reliance on hand engineered input features this paper shows how to extend the time delay neural network tdnn architecture to apply it to irregular length schedules experimental tests show that this tdnn tda network can match the performance of our previous hand engineered system the tests also show that both neural network approaches significantly outperform the best previous non learning solution to this problem in terms of the quality of the resulting schedules and the number of search steps required to construct them i
in this paper we examine the practical use of hardware neural networks in an autonomous mobile robot we have developed a hardware neural system based around a custom vlsi chip epsilon ii designed specifically for embedded hardware neural applications we present here a demonstration application of an autonomous mobile robot that highlights the flexibility of this system this robot gains basic mobility competence in very few training epochs using an instinct rule training methodology i

we consider the solution to large stochastic control problems by means of methods that rely on compact representations and a variant of the value iteration algorithm to compute approximate costto go functions while such methods are known to be unstable in general we identify a new class of problems for which convergence as well as graceful error bounds are guaranteed this class involves linear parameterizations of the cost to go function together with an assumption that the dynamic programming operator is a contraction with respect to the euclidean norm when applied to functions in the parameterized class we provide a special case where this assumption is satisfied which relies on the locality of transitions in a state space other cases will be discussed in a full length version of this paper i
we describe the reinforcement learning problem motivate algorithms which seek an approximation to the q function and present new convergence results for two such algorithms i
performing policy iteration in dynamic programming should only require knowledge of relative rather than absolute measures of the utility of actions werbos what baird calls the advantages of actions at states nevertheless most existing methods in dynamic programming including bairds compute some form of absolute utility function for smooth problems advantages satisfy two differential consistency conditions including the requirement that they be free of curl and we show that enforcing these can lead to appropriate policy improvement solely in terms of advantages
in this paper we introduce new algorithms for optimizing noisy plants in which each experiment is very expensive the algorithms build a global non linear model of the expected output at the same time as using bayesian linear regression analysis of locally weighted polynomial models the local model answers queries about confidence noise gradient and hessians and use them to make automated decisions similar to those made by a practitioner of response surface methodology the global and local models are combined naturally as a locally weighted regression we examine the question of whether the global model can really help optimization and we extend it to the case of time varying functions we compare the new algorithms with a highly tuned higher order stochastic optimization algorithm on randomly generated functions and a simulated manufacturing task we note significant improvements in total regret time to converge and final solution quality i
a continuous time continuous state version of the temporal difference td algorithm is derived in order to facilitate the application of reinforcement learning to real world control tasks and neurobiological modeling an optimal nonlinear feedback control law was also derived using the derivatives of the value function the performance of the algorithms was tested in a task of swinging up a pendulum with limited torque both the critic that specifies the paths to the upright position and the actor that works as a nonlinear feedback controller were successfully implemented by radial basis function rbf networks i
we present a new algorithm for associative reinforcement learning the algorithm is based upon the idea of matching a networks output probability with a probability distribution derived from the environment s reward signal this probability matching algorithm is shown to perform faster and be less susceptible to local minima than previously existing algorithms we use probability matching to train mixture of experts networks an architecture for which other reinforcement learning rules fail to converge reliably on even simple problems this architecture is particularly well suited for our algorithm as it can compute arbitrarily complex functions yet calculation of the output probability is simple i
the following investigates the use of single neuron learning algorithms to improve the performance of text retrieval systems that accept natural language queries a retrieval process is explained that transforms the natural language query into the query syntax of a real retrieval system the initial query is expanded using statistical and leaxning techniques and is then used for document ranking and binary classification the results of experiments suggest that kivinen and warmuths exponentiated gradient descent learning algorithm works significantly better than previous approaches i
although td gammon is one of the major successes in machine learning it has not led to similar impressive breakthroughs in temporal difference learning for other applications or even other games we were able to replicate some of the success of td gammon developing a competitive evaluation function on a parameter feed forward neural network without using back propagation reinforcement or temporal difference learning methods instead we apply simple hill climbing in a relative fitness environment these results and further analysis suggest that the surprising success of tesauros program had more to do with the co evolutionary structure of the learning task and the dynamics of the backgammon game itself
we present a connectionist method for representing images that explicitly addresses their hierarchical nature it blends data from neuroscience about whole object viewpoint sensitive cells in inferotemporal cortex s and attentional basis field modulation in v with ideas about hierarchical descriptions based on microfeatures l the resulting model makes critical use of bottom up and top down pathways for analysis and synthesis we illustrate the model with a simple example of representing information about faces hierarchical models images of objects constitute an important paradigm case of a representational hierarchy in which wholes such as faces consist of parts such as eyes noses and mouths the representation and manipulation of part whole hierarchical information in fixed hardware is a heavy millstone around connectionist necks and has consequently been the inspiration for many interesting proposals such as pollacks raam we turned to the primate visual system for clues anterior inferotemporal cortex it appears to construct representations of visually presented objects mouths and faces are both objects and so require fully elaborated representations presumably at the level of anterior it probably using different or possibly partially overlapping sets of cells the natural way to represent the part whole relationship between mouths and faces is to have a neuronal hierarchy with connections bottom up from the mouth units to the face units so that information about the mouth can be used to help recognize or analyze the image of a face and connections top down from the face units to the mouth units expressing the generarive or synthetic knowledge that if there is a face in a scene then there is usually a mouth too there is little we thank larry abbott geoff hinton bruno olshausen tomaso poggio alex pouget emilio salinas and pawan sinha for discussions and comments m riesenhuber and p dayan empirical support for or against such a neuronal hierarchy but it seems extremely unlikely on the grounds that arranging for one with the correct set of levels for all classes of objects seems to be impossible there is recent evidence that activities of cells in intermediate areas in the visual processing hierarchy such as v are influenced by the locus of visual attention this suggests an alternative strategy for representing part whole information in which there is an interaction subject to attentional control between top down generative and bottom up recognition processing in one version of our example activating units in it that represent a particular face leads through the top down generafive model to a pattern of activity in lower areas that is closely related to the pattern of activity that would be seen when the entire face is viewed this activation in the lower areas in turn provides bottom up input to the recognition system in the bottom up direction the attentional signal controls which aspects of that activation are actually processed for example specifying that only the activity reflecting the lower part of the face should be recognized in this case the mouth units in it can then recognize this restricted pattern of activity as being a particular sort of mouth therefore we have provided a way by which the visual system can represent the part whole relationship between faces and mouths this describes just one of many possibilities for instance attentional control could be mainly active during the top down phase instead then it would create in v or indeed in intermediate areas just the activity corresponding to the lower portion of the face in the first place also the focus of attention need not be so ineluctably spatial the overall scheme is based on an hierarchical top down synthesis and bottom up analysis model for visual processing as in the helmholtz machine note that hierarchy here refers to a processing hierarchy rather than the part whole hierarchy discussed above with a synthetic model forming the effective map object attentional eye position image shown in cartoon form in figure where image stands in for the probabilities over the activities of units at various levels in the system that would be caused by seeing the aspect of the object selected by placing the focus and scale of attention appropriately we use this generafive model during synthesis in the way described above to traverse the hierarchical description of any particular image we use the statistical inverse of the synthetic model as the way of analyzing images to determine what objects they depict this inversion process is clearly also sensitive to the attentional eye position it actually determines not only the nature of the object in the scene but also the way that it is depicted ie its instantiation parameters as reflected in the attentional eye position in particular the bottom up analysis model exists in the connections leading to the d viewpoint selective image cells in it reported by logothetis et al s which form population codes for all the represented images mouths noses etc the top down synthesis model exists in the connections leading in the reverse direction in generalizations of our scheme it may of course not be necessary to generate an image all the way down in v the map specifies a top down computational task very like the bottom up one addressed using a multiplicatively controlled synaptic matrix in the shifter model neural models for part whole hierarchies layer o m attentional e e e p figure cartoon of the model in the top down generative direction the model generates images of faces eyes mouths or noses based on an attentional eye position and a selection of a single top layer unit the bottom up recognition direction is the inverse of this map the response of the neurons in the middle layer is modulated sigmoidally as illustrated by the graphs shown inside the circles representing the neurons in the middle layer by the attentional eye position see section for more details of olshausen et al our solution emerges from the control the attentional eye position exerts at various levels of processing most relevantly modulating activity in v equivalent modulation in the parietal cortex based on actual rather than attentional eye position i has been characterized by pouget sejnowski and salinas abbott in terms of basis fields they showed that these basis fields can be used to solve the same tasks as the shifter model but with neuronal rather than synaptic multiplicative modulation in fact eye position modulation almost certainly occurs at many levels in the system possibly including v our scheme clearly requires that the modulating attentional eye position must be able to become detached from the spatial eye position connor et al collected evidence for part of this hypothesis although the coordinate systems of the modulation is not entirely clear from their data bottom up and top down mappings are learned taking the eye position modulation into account in the experiments below we used a version of the wake sleep algorithm for its conceptual and computational simplicity this requires learning the bottom up model from generated imagery during sleep and learning the topdown model from assigned explanations during observation of real input during wake in the current version for simplicity the eye position is set correctly during recognition but we are also interested in exploring automatic ways of doing this results we have developed a simple model that illustrates the feasibility of the scheme presented above in the context of recognizing and generating cartoon drawings of a face and its parts recognition involves taking an image of a face or a part thereof the mouth nose or one of the eyes at an arbitrary position on the retina m riesenhuber and p dayan a b face nse mouth eye figure a recognition the left column of each pair shows the stimuli the right shows the resulting activations in the top layer ordered as face mouth nose and eye the stimuli are faces at random positions in the retina recognition is performed by setting the attentional eye position in the image and setting the attentional scale which creates a window of attention around the attended to position shown by a circle of corresponding size and position b generation each panel shows the output of the generative pathway for a randomly chosen attentional eye position on activating each of the top layer units in turn the focus of attention is marked by a circle whose size reflects the attentional scale the name of the object whose neuronal representation in the top layer was activated is shown above each panel and setting the appropriate top level unit to and the remaining units to zero generation involves imaging either a whole face or of one of its parts selected by the active unit in the top layer at an arbitrary position on the retina the model figure consists of three layers the lowest layer is a x retina in the recognition direction the retina feeds into a layer of hidden units these project to the top layer which has four neurons in the generative direction the connectivity is reversed the network is fully connected in both directions the activity of each neuron based on input from the preceding for recognition or the following layer for generation is a linear function weight matrices w r v r in the recognition and v g w g in the generative direction the attentional eye position influences activity through multiplicative modulation of the neuronal responses in the hidden layer the linear response ri wrpi or ri vgoi of each neuron i in the middle layer based on the bottom up or top down connections is multiplied rero i euc es where are the tuning curves in each dimension by i i u s cbxus of the attentional eye position e e x eu es coding the xand ycoordinates and the scale of the focus of attention respectively thus for the activity mi of hidden neuron i we have mi wrpi i in the recognition pathway and mi vaoi i in the generative pathway the tuning curves of the i are chosen to be sigmoid with random centers ci and random directions di e e a d e sc in other implementations we have also used gaussian tuning functions in fact the only requirement regarding the shape of the tuning functions is that through a superposition of them one can construct functions that show a peaked dependence on the attentional eye position in the recognition direction the attentional eye position also has an influence on the activity in the input layer by defining a window of attention which we implemented using a gaussian window centered at the attentional eye position with its size given by the attentional scale this is to allow the system to learn models of parts based on experience with images of whole faces to train the model we employ a variant of the unsupervised wake sleep algorithm in this algorithm the generative pathway is trained during a wake phase in which neural models for part whole hierarchies stimuli in the input layer the retina in our case cause activation of the neurons in the network through the recognition pathway providing an error signal to train the generarive pathway using the delta rule conversely in the sleep phase random activation of a top layer unit in conjunction with a randomly chosen attentional eye position leads via the generarive connections to the generation of activation in the middle layer and consequently an image in the input layer that is then used to adapt the recognition weights again using the delta rule although the delta rule in wake sleep is fine for the recognition direction it leads to a poor generarive model in our simple case generation is much more difficult than recognition as an interim solution we therefore train the generarive weights using back propagation which uses the activity in the top layer created by the recognition pathway as the input and the retinal activation pattern as the target signal hence learning is still unsupervised except that appropriate attentional eye positions are always set during recognition we have also experimented with a system in which the weights w r and w g are preset and only the weights between layers and are trained for this model training could be done with the standard wake sleep algorithm ie using the local delta rule for both sets of weights figure a shows several examples of the performance of the recognition pathway for the different stimuli after iterations the network is able to recognize the stimuli accurately at different positions in the visual field figure b shows several examples of the output of the generarive model illustrating its capacity to produce images of faces or their parts at arbitrary locations by imaging a whole face and then focusing the attention on eg an area around its center which activates the nose unit through the recognition pathway the relationship that eg a nose is part of a face can be established in a straightforward way discussion representing hierarchical structure is a key problem for connectionism visual images offer a canonical example for which it seems possible to elucidate some of the underlying neural mechanisms the theory is based on d view object selective cells in anterior it and attentional eye position modulation of the firing of cells in v these work in the context of analysis by synthesis or recognition and generative models such that the part whole hierarchy of an object such as a face which contains eyes which contain pupils etc can be traversed in the generative direction by choosing to view the object through a different effective eye position and in the recognition direction by allowing the real and the attentional eye positions to be decoupled to activate the requisite d view selective cells the scheme is related to pollacks recurslye auto associative memory raam system tm raam provides a way of representing tree structured information for instance to learn an object whose structure is a b d a standard threelayer auto associative net would be taught ab leading to a pattern of hidden unit activations c then it would learn cd leading to and finally c leading to which would itself be the representation of the whole object the compression operation ab c and its expansion inverse are required as explicit methods for manipulating tree structure our scheme for representing hierarchical information is similar to raam using the notion of an attentional eye position to perform its compression and expansion m riesenhuber and p dayan operations however whereas raam normally constructs its own codes for intermediate levels of the trees that it is fed here images of faces are as real and as available as those for instance of their associated mouths this not only changes the learning task but also renders sensible a notion of direct recognition without repeated raamification of the parts various aspects of our scheme require comment the way that eye position affects recognition the coding of different instances of objects the use of top down information during bottom up recognition variants of the scheme for objects that are too big or too geometrically challenging to fit in one go into a single image and hierarchical objects other than images we are also working on a more probabilistically correct version taking advantage of the statistical soundness of the helmholtz machine eye position information is ubiquitous in visual processing areas including the lgn and vl as well as the parietal cortex and v further it can be revealed as having a dramatic effect on perception as in ramachandran et als tm study on intermittent exotropes this is a form of squint in which the two eyes are normally aligned but in which the exotropic eye can deviate voluntarily or involuntarily by as much as the study showed that even if an image is burnt on the retina in this eye as an afterimage and so is fixed in retinal coordinates at least one component of the percept moves as the eye moves this argues that information about eye position dramatically effects visual processing in a manner that is consistent with the model presented here of shifts based on modulation this is also required by bridgeman et as theory of perceptual stability across fixations that essentially builds up an impression of a scene in exactly the form of mapping in general there will be many instances for an object eg many different faces in this general case the top level would implement a distributed code for the identity and instantiation parameters of the objects we are currently investigating methods of implementing this form of representation into the model a key feature of the model is the interaction of the synthesis and analysis pathways when traversing the part whole hierarchies this interaction between the two pathways can also aid the system when performing image analysis by integrating information across the hierarchy just as in raam the extra feature required when traversing a hierarchy is short term memory for raam the memory stores information about the various separate sub trees that have already been decoded or encoded for our system the memory is required during generative traversal to force whole activity on lower layers to persist even after the activity on upper layers has ceased to free these upper units to recognize a part memory during recognition traversal is necessary in marginal cases to accumulate information across separate parts as well as the whole this solution to hierarchical representation inevitably gives up the computational simplicity of the naive neuronal hierarchical scheme described in the
in order to process incoming sounds efficiently it is advantageous for the auditory system to be adapted to the statistical structure of natural auditory scenes as a first step in investigating the relation between the system and its inputs we study low order statistical properties in several sound ensembles using a filter bank analysis focusing on the amplitude and phase in different frequency bands we find simple parametric descriptions for their distribution and power spectrum that are valid for very different types of sounds in particular the amplitude distribution has an exponential tail and its power spectrum exhibits a modified power law behavior which is manifested by self similarity and long range temporal correlations furthermore the statistics for different bands within a given ensemble are virtually identical suggesting translation invariance along the cochlear axis these results show that natural sounds are highly redundant and have possible implications to the neural code used by the auditory system i
we employed a white noise velocity signal to study the dynamics of the response of single neurons in the cortical area mt to visual motion responses were quantified using reverse correlation optimal linear reconstruction filters and reconstruction signal to noise ratio snr the snr and lower bound estimates of information rate were lower than we expected ninety percent of the information was transmitted below hz and the highest lower bound on bit rate was bitss a simulated opponent motion energy subunit with poisson spike statistics was able to out perform the mt neurons the temporal integration window measured from the reverse correlation half width ranged from ms the window was narrower when a stimulus moved faster but did not change when temporal frequency was held constant i
in poggio and edelman proposed a view based model of object recognition that accounts for several psychophysical properties of certain recognition tasks the model predicted the existence of view tuned and view invariant units that were later found by logothetis et al logothetis et al in it cortex of monkeys trained with views of specific paperclip objects the model however does not specify the inputs to the view tuned units and their internal organization in this paper we propose a model of these view tuned units that is consistent with physiological data from single cell responses
binocular rivalry is the alternating percept that can result when the two eyes see different scenes recent psychophysical evidence supports an account for one component of binocular rivalry similar to that for other bistable percepts we test the hypothesis that alternation can be generated by competition between topdown cortical explanations for the inputs rather than by direct competition between the inputs recent neurophysiological evidence shows that some binocular neurons are modulated with the changing percept others are not even if they are selective between the stimuli presented to the eyes we extend our model to a hierarchy to address these effects
we train recurrent networks to control chemotaxis in a computer model of the nematode c elegant the model presented is based closely on the body mechanics behavioral analyses neuroanatomy and neurophysiology of g elegans each imposing constraints relevant for information processing simulated worms moving autonomously in simulated chemical environments display a variety of chemotaxis strategies similar to those of biological worms
the encoding of random time varying stimuli in single spike trains of electrosensory neurons in the weakly electric fish eigenmannia was investigated using methods of statistical signal processing at the first stage of the electrosensory system spike trains were found to encode faithfully the detailed time course of random stimuli while at the second stage neurons responded specifically to features in the temporal waveform of the stimulus therefore stimulus information is processed at the second stage of the electrosensory system by extracting temporal features from the faithfully preserved image of the environment sampled at the first stage
we introduce a neurobiologically plausible model of contour integration from visual inputs of individual oriented edges the model is composed of interacting excitatory neurons and inhibitory interneurons receives visual inputs via oriented receptive fields rfs like those in v the rf centers are distributed in space at each location a finite number of cells tuned to orientations spanning compose a model hypercolumn cortical interactions modify neural activities produced by visual inputs selectively amplifying activities for edge elements belonging to smooth input contours elements within one contour produce synchronized neural activities we show analytically and empirically that contour enhancement and neural synchrony increase with contour length smoothness and closure as observed experimentally this model gives testable predictions and in addition introduces a feedback mechanism allowing higher visual centers to enhance suppress and segment contours
this paper develops arguments for a family of temporal log linear models to represent spatio temporal correlations among the spiking events in a group of neurons the models can represent not just pairwise correlations but also correlations of higher order methods are discussed for inferring the existence or absence of correlations and estimating their strength a frequentist and a bayesian approach to correlation detection are compared the frequentist method is based on g statistic with estimates obtained via the max ent principle in the bayesian approach a markov chain monte carlo model composition mc algorithm is applied to search over connectivity structures and laplaces method is used to approximate their posterior probability performance of the methods was tested on synthetic data the methods were applied to experimental data obtained by the fourth author by means of measurements carried out on behaving rhesus monkeys at the hadassah medical school of the hebrew university as conjectured neural connectivity structures need not be neither hierarchical nor decomposable learning quasi synchronization patterns among spiking neurons
biophysical modeling studies have previously shown that cortical pyramidal cells driven by strong nmda type synaptic currents andor containing dendritic voltage dependent ca or na channels respond more strongly when synapses are activated in several spatially clustered groups of optimal size in comparison to the same number of synapses activated diffusely about the dendritic arbor the nonlinear intradendritic interactions giving rise to this cluster sensitivity property are akin to a layer of virtual nonlinear hidden units in the dendrites with implications for the cellular basis of learning and memory and for certain classes of nonlinear sensory processing in the present study we show that a single neuron with access only to excitatory inputs from unoriented onand off center cells in the lgn exhibits the principal nonlinear response properties of a complex cell in primary visual cortex namely orientation tuning coupled with translation invariance and contrast insensitivity we conjecture that this type of intradendritic processing could explain how complex cell responses can persist in the absence of oriented simple cell input b w mel d l ruderman and k a archie i
recently sillito and coworkers nature pp demonstrated that stimulation beyond the classical receptive field crf can not only modulate but radically change a neurons response to oriented stimuli they revealed that patch suppressed cells when stimulated with contrasting orientations inside and outside their crf can strongly respond to stimuli oriented orthogonal to their nominal preferred orientation here we analyze the emergence of such complex response patterns in a simple model of primary visual cortex we show that the observed sensitivity for orientation contrast can be explained by a delicate interplay between local isotropic interactions and patchy long range connectivity between distant iso orientation domains in particular we demonstrate that the observed properties might arise without specific connections between sites with cross oriented crfs i
coarse codes are widely used throughout the brain to encode sensory and motor variables methods designed to interpret these codes such as population vector analysis are either inefficient ie the variance of the estimate is much larger than the smallest possible variance or biologically implausible like maximum likelihood moreover these methods attempt to compute a scalar or vector estimate of the encoded variable neurons are faced with a similar estimation problem they must read out the responses of the presynaptic neurons but by contrast they typically encode the variable with a further population code rather than as a scalar we show how a non linear recurrent network can be used to perform these estimation in an optimal way while keeping the estimate in a coarse code format this work suggests that lateral connections in the cortex may be involved in cleaning up uncorrelated noise among neurons representing similar variables i
a linear architectural model of cortical simple cells is presented the model evidences how mutual inhibition occurring through synaptic coupling functions asymmetrically distributed in space can be a possible basis for a wide variety of spario temporal simple cell response properties including direction selectivity and velocity tuning while spatial asymmetries are included explicitly in the structure of the inhibitory interconnections temporal asymmetries originate from the specific mutual inhibition scheme considered extensive simulations supporting the model are reported
neuromodulation can change not only the mean firing rate of a neuron but also its pattern of firing therefore a reliable neural coding scheme whether a rate coding or a spike time based coding must be robust in a dynamic neuromodulatory environment the common observation that cholinergic modulation leads to a reduction in spike frequency adaptation implies a modification of spike timing which would make a neural code based on precise spike timing difficult to maintain in this paper the effects of cholinergic modulation were studied to test the hypothesis that precise spike timing can serve as a reliable neural code using the whole cell patch clamp technique in rat neocortical slice preparation and compartmental modeling techniques we show that cholinergic modulation surprisingly preserved spike timing in response to a fluctuating inputs that resembles in vivo conditions this result suggests that in vivo spike timing may be much more resistant to changes in neuromodulator concentrations than previous physiological studies have implied a c tang am bartels and t j sejnowski

the parameter space of neural networks has a riemannian metric structure the natural riemannian gradient should be used instead of the conventional gradient since the former denotes the true steepest descent direction of a loss function in the riemannian space the behavior of the stochastic gradient learning algorithm is much more effective if the natural gradient is used the present paper studies the information geometrical structure of perceptrons and other networks and prove that the on line learning method based on the natural gradient is asymptotically as efficient as the optimal batch algorithm adaptive modification of the learning constant is proposed and analyzed in terms of the riemannian measure and is shown to be efficient the natural gradient is finally applied to blind separation of mixtured independent signal sources i
this paper shows that if a large neural network is used for a pattern classification problem and the learning algorithm finds a network with small weights that has small squared error on the training patterns then the generalization performance depends on the size of the weights rather than the number of weights more specifically consider an t layer feed forward network of sigmoid units in which the sum of the magnitudes of the weights associated with each unit is bounded by a the misclassification probability converges to an error estimate that is closely related to squared error on the training set at rate ocattlvlognm ignoring log factors where m is the number of training patterns n is the input dimension and c is a constant this may explain the generalization performance of neural networks particularly when the number of training examples is considerably smaller than the number of weights it also supports heuristics such as weight decay and early stopping that attempt to keep the weights small during training i
a new method to calculate the full training process of a neural network is introduced no sophisticated methods like the replica trick are used the results are directly related to the actual number of training steps some results are presented here like the maximal learning rate an exact description of early stopping and the necessary number of training steps further problems can be addressed with this approach i
we study the number of hidden layers required by a multilayer neural network with threshold units to compute a function f from td to in dimension d gibson characterized the functions computable with just one hidden layer under the assumption that there is no multiple intersection point and that f is only defined on a compact set we consider the restriction of f to the neighborhood of a multiple intersection point or of infinity and give necessary and sufficient conditions for it to be locally computable with one hidden layer we show that adding these conditions to gibsons assumptions is not sufficient to ensure global computability with one hidden layer by exhibiting a new non local configuration the critical cycle which implies that f is not computable with one hidden layer i
a new regression technique based on vapniks concept of support vectors is introduced we compare support vector regression svr with a committee regression technique bagging based on regression trees and ridge regression done in feature space on the basis of these experiments it is expected that svr will have advantages in high dimensionality space because svr optimization does not depend on the dimensionality of the input space

the convergence properties of the gradient descent algorithm in the case of the linear perceptton may be obtained from the response function we derive a general expression for the response function and apply it to the case of data with simple input correlations it is found that correlations severely may slow down learning this explains the success of pca as a method for reducing training time motivated by this finding we furthermore propose to transform the input data by removing the mean across input variables as well as examples to decrease correlations numerical findings for a medical classification problem are in fine agreement with the theoretical results
we propose a new method to compute prediction intervals especially for small data sets the width of a prediction interval does not only depend on the variance of the target distribution but also on the accuracy of our estimator of the mean of the target ie on the width of the confidence interval the confidence interval follows from the variation in an ensemble of neural networks each of them trained and stopped on bootstrap replicates of the original data set a second improvement is the use of the residuals on validation patterns instead of on training patterns for estimation of the variance of the target distribution as illustrated on a synthetic example our method is better than existing methods with regard to extrapolation and interpolation in data regimes with a limited amount of data and yields prediction intervals which actual confidence levels are closer to the desired confidence levels statistical intervals in this paper we will consider feedforward neural networks for regression tasks estimating an underlying mathematical function between input and output variables based on a finite number of data points possibly corrupted by noise we are given a set of plata pairs t which are assumed to be generated according to t f where denotes noise with zero mean straightforwardly trained on such a regression task the output of a network og given a new input vector can be rwcp real world computing partnership snn foundation for neural networks practical confidence and prediction intervals interpreted as an estimate of the regression f ie of the mean of the target distribution given input sometimes this is all we are interested in a reliable estimate of the regression f in many applications however it is important to quantify the accuracy of our statements for regression problems we can distinguish two different aspects the accuracy of our estimate of the true regression and the accuracy of our estimate with respect to the observed output confidence intervals deal with the first aspect ie consider the distribution of the quantity f o prediction intervals with the latter ie treat the quantity t o we see from t o f o i that a prediction interval necessarily encloses the corresponding confidence interval in a method somewhat similar to ours is introduced to estimate both the mean and the variance of the target probability distribution it is based on the assumption that there is a sufficiently large data set ie that their is no risk of overfitting and that the neural network finds the correct regression in practical applications with limited data sets such assumptions are too strict in this paper we will propose a new method which estimates the inaccuracy of the estimator through bootstrap resampling and corrects for the tendency to overfit by considering the residuals on validation patterns rather than those on training patterns bootstrapping and early stopping bootstrapping is based on the idea that the available data set is nothing but a particular realization of some unknown probability distribution instead of sampling over the true probability distribution which is obviously impossible one defines an empirical distribution with so called naive bootstrapping the empirical distribution is a sum of delta peaks on the available data points each with probability content pdat a a bootstrap sample is a collection of pdata patterns drawn with replacement from this empirical probability distribution this bootstrap sample is nothing but our training set and all patterns that do not occur in the training set are by definition part of the validation set for large paata the probability that a pattern becomes part of the validation set is paata pat e when training a neural network on a particular bootstrap sample the weights are adjusted in order to minimize the error on the training data training is stopped when the error on the validation data starts to increase this so called early stopping procedure is a popular strategy to prevent overfitting in neural networks and can be viewed as an alternative to regularization techniques such as weight decay in this context bootstrapping is just a procedure to generate subdivisions in training and validation set similar to k fold cross validation or subsampling on each of the rtrun bootstrap replicates we train and stop a single neural network the output of network i on input vector a is written oi o as the estimate of our ensemble of networks for the regression f we take the average output x i rln o run il this is a so called bagged estimator in it is shown that a proper balancing of the network outputs can yield even better results heskes confidence intervals confidence intervals provide a way to quantify our confidence in the estimate mg of the regression fg ie we have to consider the probability distribution pfm that the true regression is f given our estimate m our line of reasoning goes as follows see also we assume that our ensemble of neural networks yields a more or less unbiased estimate for fg ie the distribution pfm is centered around mg the truth is that neural networks are biased estimators for example neural networks trained on a finite number of examples will always have a tendency as almost any other model to oversmooth a sharp peak in the data this introduces a bias which to arrive at asymptotically correct confidence intervals should be taken into account however if it would be possible to compute such a bias correction one should do it in the first place to arrive at a better estimator our working hypothesis here is that the bias component of the confidence intervals is negligible in comparison with the variance component there do exist methods that claim to give confidence intervals that are second see eg the discussion order correct ie up to and including terms of order lpdata after since we do not know how to handle the bias component anyways such precise confidence intervals which require a tremendous amount of bootstrap samples are too ambitious for our purposes first order correct intervals up to and including terms of order pclata are always symmetric and can be derived by assuming a gaussian distribution pfgm the variance of this distribution can be estimated from the variance in the outputs rtln tru n i i of the rtrun networks this is the crux of the bootstrap method see eg since the distribution of pfm is a gaussian so is the inverse distribution pmlf to find the regression m by randomly drawing data sets consisting of pdata data points according to the prescription not knowing the true distribution of inputs and corresponding targets the best we can do is to define the empirical distribution as explained before and estimate pmf from the distribution poglm this then yields the estimate so following this bootstrap procedure we arrive at the confidence intervals m cconfidence f rem cconfidence where cconfidenc e depends on the desired confidence level c the factors cconfidenc e can be taken from a table with the percentage points of the students t distribution with number of degrees of freedom equal to the number of bootstrap runs nrun a more direct alternative is to choose cconfiaence such that for no more than a of all nrun x pclata network predictions of m cconfidence ttin this paper we assume that both the inputs and the outputs are stochastic for the case of deterministic input variables other bootstrapping techniques see eg are more appropriate since the statistical intervals resulting from naive bootstrapping may be too conservative practical confidence and prediction intervals prediction intervals confidence intervals deal with the accuracy of our prediction of the regression ie of the mean of the target probability distribution prediction intervals consider the accuracy with which we can predict the targets themselves ie they are based on estimates of the distribution ptzlm we propose the following method the two noise components f rn and in are independent the variance of the first component has been estimated in our bootstrap procedure to arrive at confidence intervals the remaining task is to estimate the noise inherent to the regression problem we assume that this noise is more or less gaussian such that it again suffices to compute its variance which may however depend on the input in mathematical symbols of course we are interested in prediction intervals for new points for which we do not know the targets t suppose that we had left aside a set of test patterns t that we had never used for training nor for validating our neural networks then we could try and estimate a model x to fit the remaining residuals r maxt m a using minus the loglikelihood as the error measure l elog vrx exp x of course leaving out these test patterns is a waste of data and luckily our bootstrap procedure offers an alternative each pattern is in about of all bootstrap runs not part of the training set let us write q if pattern is in the validation set of run i and q otherwise if we for each pattern use the average mvalidation nqo nq i instead of the average rn we get as close as possible to an unbiased estimate for the residual on independent test patterns as we can without wasting any training data so summarizing we suggest to find a function x that minimizes the error yet not by leaving out test patterns which would be a waste of data nor by straightforwardly using the training data which would underestimate the error but by exploiting the information about the residuals on the validation patterns once we have found the function x we can compute for any both the mean m and the deviation s which are combined in the prediction interval m cpredictions t m qcpredictions again the factor cprediction can be found in a students t table or chosen such that for no more than a of all pdata patterns it mvalidation t i cprediction st the function x may be modelled by a separate neural network similar to the method proposed in with an exponential instead of a linear transfer function for the output unit to ensure that the variance is always positive t heskes a input i c i i i i i t t input b o x x x x o ool l input i input figure prediction intervals for a synthetic problem a training set crosses true regression solid line and network prediction dashed line b validation residuals crosses training residuals circles true variance solid line estimated variance based on validation residuals dashed line and based on training residuals dash dotted line c width of standard error bars for the more advanced method dashed line the simpler procedure dash dotted line and what it should be solid line d prediction intervals solid line network prediction dashed line and test points dots illustration we consider a synthetic problem similar to the one used in with this example we will demonstrate the desirability to incorporate the inaccuracy of the regression estimator in the prediction intervals inputs x are drawn from the interval with probability density px ixl ie more examples are drawn at the boundary than in the middle targets t are generated according to t sinrxcosrx x with x sinrx the regression is the solid line in figure la the variance of the target distribution the solid line in figure lb following this prescription we obtain a training set of pdata data points the crosses in figure la on which we train an ensemble of nru n networks each having hidden units with tanh transfer function and one linear output unit the average network output rex is the dashed line practical confidence and prediction intervals in figure la and d in the following we compare two methods to arrive at prediction intervals the more advanced method described in section ie taking into account the uncertainty of the estimator and correcting for the tendency to overfit on the training data and a simpler procedure similar to which disregards both effects we compute the squared validation residuals t crosses in figinvalid at ion ure lb based on runs in which pattern was part of the validation set and the training residuals mtrai n tts circles based on runs in which pattern was part of the training set the validation residuals are most of the time somewhat larger than the training residuals for our more advanced method we substract the uncertainty of our model from the validation residuals as in the other procedure simply keeps the training residuals to estimate the variance of the target distribution it is obvious that the distribution of residuals in figure lb does not allow for a complex model here we take a feedforward network with one hidden unit xx exp vl tanhvx v v the parameters v vx v v are found through minimization of the error both for the advanced method dashed line and for the simpler procedure dashdotted line the variance of the target distribution is estimated to be a step function the former being based on the validation residuals minus the uncertainty of the estimator is slightly more conservative than the latter being based on the training residuals both estimates are pretty far from the truth solid line especially for x yet considering such a limited amount of noisy residuals we can hardly expect anything better figure c considers the width of standard error bars ie of prediction intervals for error level a for the simpler procedure the width of the prediction interval dash dotted line in figure c follows directly from the estimate of the variance of the target distribution our more advanced method adds the uncertainty of the estimator to arrive at the dashed line the correct width of the prediction interval ie the width that would include of all targets for a particular input is given by the solid line the prediction intervals obtained through the more advanced procedure are displayed in figure ld together with a set of test points visualizing the probability distribution of inputs and corresponding targets the method proposed in section has several advantages the prediction intervals of the advanced method include of the test points in figure ld pretty close to the desired confidence level of the simpler procedure is too liberal with an actual confidence level of only this difference is mainly due to the use of validation residuals instead of training residuals incorporation of the uncertainty of the estimator is important in regions of input space with just a few training data in this example the density of training data affects both extrapolation and interpolation for ix i the prediction intervals obtained with the advanced method become wider and wider whereas those obtained through the simpler procedure remain more or less constant the bump in the prediction interval dashed line near the origin is a result of the relatively large variance in the network predictions in this region it shows that our method also incorporates the effect that the density of training data has on the accuracy of interpolation t heskes conclusion and discussion we have presented a novel method to compute prediction intervals for applications with a limited amount of data the uncertainty of the estimator itself has been taken into account by the computation of the confidence intervals this explains the qualitative improvement over existing methods in regimes with a low density of training data usage of the residuals on validation instead of on training patterns yields prediction intervals with a better coverage the price we have to pay is in the computation time we have to train an ensemble of networks on about to different bootstrap replicates there are other good reasons for resampling averaging over networks improves the generalization performance and early stopping is a natural strategy to prevent overfitting it would be interesting to see how our frequentist method compares with bayesian alternatives see eg prediction intervals can also be used for the detection of outliers with regard to the training set it is straightforward to point out the targets that are not enclosed by a prediction interval of error level say c a wide prediction interval for a new test pattern indicates that this test pattern lies in a region of input space with a low density of training data making any prediction completely unreliable a weak point in our method is the assumption of unbiasedness in the computation of the confidence intervals this assumption makes the confidence intervals in general too liberal however as discussed in such bootstrap methods tend to perform better than other alternatives based on the computation of the hessian matrix partly because they incorporate the variability due to the random initialization furthermore when we model the prediction interval as a function of the input we will to some extent repair this deficiency but still incorporating even a somewhat inaccurate confidence interval ensures that we can never severely overestimate our accuracy in regions of input space where we have never been before references c bishop and c qazaz regression with input dependent noise a bayesian treatment these proceedings l breiman bagging predictors machine learning b efron and r tibshirani an
we study generalization capability of the mixture of experts learning from examples generated by another network with the same architecture when the number of examples is smaller than a critical value the network shows a symmetric phase where the role of the experts is not specialized upon crossing the critical point the system undergoes a continuous phase transition to a symmetry breaking phase where the gating network partitions the input space effectively and each expert is assigned to an appropriate subspace we also find that the mixture of experts with multiple level of hierarchy shows multiple phase transitions i
results of a study of the worst case learning curves for a particular class of probability distribution on input space to mlp with hard threshold hidden units are presented it is shown in particular that in the thermodynamic limit for scaling by the number of connections to the first hidden layer although the true learning curve behaves as a for a its vc dimension based bound is trivial and its vc entropy bound is trivial for a it is also shown that bounds following the true learning curve can be derived from a formalism based on the density of error patterns
in this paper we apply the method of complexity regularization to derive estimation bounds for nonlinear function estimation using a single hidden layer radial basis function network our approach differs from the previous complexity regularization neural network function learning schemes in that we operate with random covering numbers and l metric entropy making it possible to consider much broader families of activation functions namely functions of bounded variation some constraints previously imposed on the network parameters are also eliminated this way the network is trained by means of complexity regularization involving empirical risk minimization bounds on the expected risk in terms of the sample size are obtained for a large class of loss functions rates of convergence to the optimal loss are also derived
we study a mistake driven variant of an on line bayesian learning algorithm similar to one studied by cesa bianchi helmbold and panizza chp this variant only updates its state learns on trials in which it makes a mistake the algorithm makes binary classifications using a linear threshold classifier and runs in time linear in the number of attributes seen by the learner we have been able to show theoretically and in simulations that this algorithm performs well under assumptions quite different from those embodied in the prior of the original bayesian algorithm it can handle situations that we do not know how to handle in linear time with bayesian algorithms we expect our techniques to be useful in deriving and analyzing other apobayesian algorithms i
we exhibit e novel wey of simuleting sigmoidel neurel nets by networks of noisy spiking neurons in tempotell coding furthermore it is shown theit networks of noisy spiking neurons with tempotell coding helve e strictly lerger computetionel power thein sigmoidel neure nets with the seme number of units
we introduce a model for noise robust analog computations with disirete time that is flexible enough to cover the most important concrete cases such as computations in noisy analog neural nets and networks of noisy spiking neurons we show that the presence of arbitrarily small amounts of analog noise reduces the power of analog computational models to that of finite automata and we also prove a new type of upper bound for the vc dimension of computational models with analog noise
we present an algorithm which is expected to realise bayes optimal predictions in large feed forward networks it is based on mean field methods developed within statistical mechanics of disordered systems we give a derivation for the single layer perceptton and show that the algorithm also provides a leave one out cross validation test of the predictions simulations show excellent agreement with theoretical results of statistical mechanics i
stochastic on line learning can be faster than batch learning howevex at late times the learning rate must be annealed to remove the noise present in the stochastic weight updates in this annealing phase tile convergence rate in mean square is at best proportional to where is the number of input presentations an alternative is to increase the batch size to remove the noise in this paper we explore convergence for lms using small but fixed batch sizes and an adaptive batch size we show that the best adaptive batch schedule is exponential and has a rate of convergence which is the same as for annealing ie at best proportional to i
it is shown that conventional computers can be exponentiall x faster than planar hopfield networks although there are planar hopfield networks that take exponential time to converge a stable state of an arbitrary planar hopfield network can be found by a conventional computer in polynomial time the theory of p completeness gives strong evidence that such a separation is unlikely for nonplanar hopfield networks and it is demonstrated that this is also the case for several restricted classes of nonplanar hopfield networks including those who interconnection graphs are the class of bipartite graphs graphs of degree the dual of the knights graph the neighbor mesh the hypercube the butterfly the cube connected cycles and the shuffle exchange graph i
this paper investigates the stationary points of a hebb learning rule with a sigmoid nonlinearity in it we show mathematically that when the input has a low information content as measured by the inputs variance this learning rule suppresses learning that is forces the weight vector to converge to the zero vector when the information content exceeds a certain value the rule will automatically begin to learn a feature in the input our analysis suggests that under certain conditions it is the first principal component that is learned the weight vector length remains bounded provided the variance of the input is finite simulations confirm the theoretical results derived i
given unlimited computational resources it is best to use a criterion of minimal expected generalisation error to select a model and determine its parameters however it may be worthwhile to sacrifice some generalisation performance for higher learning speed a method for quantifying sub optimality is set out here so that this choice can be made intelligently furthermore the method is applicable to a broad class of models including the ultra fast memory based methods such as ramnets this brings the added benefit of providing for the first time the means to analyse the generalisation properties of such models in a bayesian framework i
we study the effect of noise and regularization in an on line gradient descent learning scenario for a general two layer student network with an arbitrary number of hidden units training examples are randomly drawn input vectors labeled by a two layer teacher network with an arbitrary number of hidden units the examples are corrupted by gaussian noise affecting either the output or the model itself we examine the effect of both types of noise and that of weight decay regularization on the dynamical evolution of the order parameters and the generalization error in various phases of the learning process i
given a multidimensional data set and a model of its density we consider how to define the optimal interpolation between two points this is done by assigning a cost to each path through space based on two competing goals one to interpolate through regions of high density the other to minimize arc length from this path functional we derive the euler lagrange equations for extremal motion given two points the desired interpolation is found by solving a boundary value problem we show that this interpolation can be done efficiently in high dimensions for gaussian dirichlet and mixture models i
we analyse online learning from finite training sets at noninfinitesimal learning rates r by an extension of statistical mechanics methods we obtain exact results for the time dependent generalization error of a linear network with a large number of weights n we find for example that for small training sets of size p n larger learning rates can be used without compromising asymptotic generalization performance or convergence speed encouragingly for optimal settings of r and less importantly weight decay a at given final learning time the generalization performance of online learning is essentially as good as that of offiine learning i
the support vector sv method was recently proposed for estimating regressions constructing multidimensional splines and solving linear operator equations vapnik in this presentation we report results of applying the sv method to these problems i
the learning properties of a universal approximator a normalized committee machine with adjustable biases are studied for on line back propagation learning within a statistical mechanics framework numerical studies show that this model has features which do not exist in previously studied two layer network models without adjustable biases eg attractive suboptimal symmetric phases even for realizable cases and noiseless data
for neural networks with a wide class of weight priors it can be shown that in the limit of an infinite number of hidden units the prior over functions tends to a gaussian process in this paper analytic forms are derived for the covariance function of the gaussian processes corresponding to networks with sigmoidal and gaussian hidden units this allows predictions to be made efficiently using networks with an infinite number of hidden units and shows that somewhat paradoxically it may be easier to compute with infinite networks than finite ones i
we consider the microscopic equations for learning problems in neural networks the aligning fields of an example are obtained from the cavity fields which are the fields if that example were absent in the learning process in a rough energy landscape we assume that the density of the local minima obey an exponential distribution yielding macroscopic properties agreeing with the first step replica symmetry breaking solution iterating the microscopic equations provide a learning algorithm which results in a higher stability than conventional algorithms i
we consider the problem of prediction of stationary time series using the architecture known as mixtures of experts mem here we suggest a mixture which blends several autoregressive models this study focuses on some theoretical foundations of the prediction problem in this context more precisely it is demonstrated that this model is a universal approximator with respect to learning the unknown prediction function this statement is strengthened as upper bounds on the mean squared error are established based on these results it is possible to compare the mem to other families of models eg neural networks and state dependent models it is shown that a degenerate version of the mem is in fact equivalent to a neural network and the number of experts in the architecture plays a similar role to the number of hidden units in the latter model a j zeevi r meir and r j adler

a classifier is called consistent with respect to a given set of classlabeled points if it correctly classifies the set we consider classitiers defined by unions of local separators and propose algorithms for consistent classifier reduction the expected complexities of the proposed algorithms are derived along with the expected classifier sizes in particular the proposed approach yields a consistent reduction of the nearest neighbor classifier which performs firm classification assigning each new object to a class regardless of the data structure the proposed reduction method suggests a notion of soft classification allowing for indecision with respect to objects which are insufficiently or ambiguously supported by the data the performances of the proposed classifiers in predicting stock behavior are compared to that achieved by the nearest neighbor method i

the full bayesian method for applying neural networks to a prediction problem is to set up the priorhyperprior structure for the net and then perform the necessary integrals however these integrals are not tractable analytically and markov chain monte carlo mcmc methods are slow especially if the parameter space is high dimensional using gaussian processes we can approximate the weight space integral analytically so that only a small number of hyperparameters need be integrated over by mcmc methods we have applied this idea to classification problems obtaining excellent results on the real world problems investigated so far i
in most treatments of the regression problem it is assumed that the distribution of target data can be described by a deterministic function of the inputs together with additive gaussian noise having constant variance the use of maximum likelihood to train such models then corresponds to the minimization of a sum of squares error function in many applications a more realistic model would allow the noise variance itself to depend on the input variables however the use of maximum likelihood to train such models would give highly biased results in this paper we show how a bayesian treatment can allow for an input dependent variance while overcoming the bias of maximum likelihood i
the self organizing map som algorithm has been extensively studied and has been applied with considerable success to a wide variety of problems however the algorithm is derived from heuristic ideas and this leads to a number of significant limitations in this paper we consider the problem of modelling the probability density of data in a space of several dimensions in terms of a smaller number of latent or hidden variables we introduce a novel form of latent variable model which we call the gtm algorithm for generarive topographic mapping which allows general non linear transformations from latent space to data space and which is trained using the em expectation maximization algorithm our approach overcomes the limitations of the som while introducing no significant disadvantages we demonstrate the performance of the gtm algorithm on simulated data from flow diagnostics for a multi phase oil pipeline i
the power of sampling methods in bayesian reconstruction of noisy signals is well known the extension of sampling to temporal problems is discussed efficacy of sampling over time is demonstrated with visual tracking i
the problem of assigning m points in the n dimensional real space r n to k clusters is formulated as that of determining k centers in r n such that the sum of distances of each point to the nearest center is minimized if a polyhedral distance is used the problem can be formulated as that of minimizing a piecewise linear concave function on a polyhedral set which is shown to be equivalent to a bilinear program minimizing a bilinear function on a polyhedral set a fast finite k median algorithm consisting of solving few linear programs in closed form leads to a stationary point of the bilinear program computational testing on a number of realworld databases was carried out on the wisconsin diagnostic breast cancer wdbc database k median training set correctness was comparable to that of the k mean algorithm however its testing set correctness was better additionally on the wisconsin prognostic breast cancer wpbc database distinct and clinically important survival curves were extracted by the k median algorithm whereas the k mean algorithm failed to obtain such distinct survival curves for the same database i
support vector learning machines svm are finding application in pattern recognition regression estimation and operator inversion for ill posed problems against this very general backdrop any methods for improving the generalization performance or for improving the speed in test phase of svms are of increasing interest in this paper we combine two such techniques on a pattern recognition problem the method for improving generalization performance the virtual support vector method does so by incorporating known invariances of the problem this method achieves a drop in the error rate on nist test digit images of to the method for improving the speed the reduced set method does so by approximating the support vector decision surface we apply this method to achieve a factor of fifty speedup in test phase over the virtual support vector machine the combined approach yields a machine which is both times faster than the original machine and which has better generalization performance achieving error the virtual support vector method is applicable to any svm problem with known invariances the reduced set method is applicable to any support vector machine i
we describe the notion of equivalent kernels and suggest that this provides a framework for comparing different classes of regression models including neural networks and both parametric and non parametric statistical techniques unfortunately standard techniques break down when faced with models such as neural networks in which there is more than one layer of adjustable parameters we propose an algorithm which overcomes this limitation estimating the equivalent kernels for neural network models using a data perturbation approach experimental results indicate that the networks do not use the maximum possible number of degrees of freedom that these can be controlled using regularisation techniques and that the equivalent kernels learnt by the network vary both in size and in shape in different regions of the input space
in supervised learning there is usually a clear distinction between inputs and outputs inputs are what you will measure outputs are what you will predict from those measurements this paper shows that the distinction between inputs and outputs is not this simple some features are more useful as extra outputs than as inputs by using a feature as an output we get more than just the case values but calx learn a mapping from the other inputs to that feature for many features this mapping may be more useful than the feature value itself we present two regression problems and one classification problem where performance improves if features that could have been used as inputs are used as extra outputs instead this result is surprising since a feature used as an output is not used during testing
the paper is developed in two parts where we discuss a new approach to self organization in a single layer linear feed forward network first two novel algorithms for self organization are derived from a two layer linear hetero associative network performing a one of m classification and trained with the constrained least mean squared classification error criterion second two adaptive algorithms are derived from these selforganizing procedures to compute the principal generalized eigenvectors of two correlation matrices from two sequences of random vectors these novel adaptive algorithms can be implemented in a single layer linear feed forward network we give a rigorous convergence analysis of the adaptive algorithms by using stochastic approximation theory as an example we consider a problem of online signal detection in digital mobile communications
this work investigates the representational and inductive capabilities of time delay neural networks tdnns in general and of two subclasses of tdnn those with delays only on the inputs idnn and those which include delays on hidden units hdnn both architectures are capable of representing the same class of languages the definite memory machine dmm languages but the delays on the hidden units in the hdnn helps it outperform the idnn on problems composed of repeated features over short time windows i
a globally convergent homotopy method is defined that is capable of sequentially producing large numbers of stationary points of the multi layer perceptron mean squared error surface using this algorithm large subsets of the stationary points of two test problems are found it is shown empirically that the mlp neural network appears to have an extreme ratio of saddle points compared to local minima and that even small neural network problems have extremely large numbers of solutions
i describe a querying criterion that attempts to minimize the error of a learner by minimizing its estimated squared bias i describe experiments with locally weighted regression on two simple problems and observe that this bias only approach outperforms the more common variance only exploration approach even in the presence of noise
in many optimization problems the structure of solutions reflects complex relationships between the different input parameters for example experience may tell us that certain parameters are closely related and should not be explored independently similarly experience may establish that a subset of parameters must take on particular values any search of the cost landscape should take advantage of these relationships we present mimic a framework in which we analyze the global structure of the optimization landscape a novel and efficient algorithm for the estimation of this structure is derived we use knowledge of this structure to guide a randomized search through the solution space and in turn to refine our estimate of the structure our technique obtains significant speed gains over other randomized optimization procedures i
a modification is described to the use of mean field approximations in the e step of em algorithms for analysing data from latent structure models as described by ghahramani among others the modification involves second order taylor approximations to expectations computed in the e step the potential benefits of the method are illustrated using very simple latent profile models i
this paper describes a new framework for relational graph matching the starting point is a recently reported bayesian consistency measure which gauges structural differences using hamming distance the main contributions of the work are threefold firstly we demonstrate how the discrete components of the cost function can be softened the second contribution is to show how the softened cost function can be used to locate matches using continuous nonlinear optimisation finally we show how the resulting graph matching algorithm relates to the standard quadratic assignment problem i
the limitations of using self organizing maps som for either clusteringvector quantization vq or multidimensional scaling mds are being discussed by reviewing recent empirical findings and the relevant theory soms remaining ability of doing both vq and mds at the same time is challenged by a new combined technique of online k means clustering plus sammon mapping of the cluster centroids som are shown to perform significantly worse in terms of quantization error in recovering the structure of the clusters and in preserving the topology in a comprehensive empirical study using a series of multivariate normal clustering problems
real valued random hidden variables can be useful for modelling latent structure that explains correlations among observed variables i propose a simple unit that adds zero mean gaussian noise to its input before passing it through a sigmoidal squashing function such units can produce a variety of useful behaviors ranging from deterministic to binary stochastic to continuous stochastic i show how slice sampling can be used for inference and learning in top down networks of these units and demonstrate learning on two simple problems i
we propose a novel approach to automatically growing and pruning hierarchical mixtures of experts the constructive algorithm proposed here enables large hierarchies consisting of several hundred experts to be trained effectively we show that hmes trained by our automatic growing procedure yield better generalization performance than traditional static and balanced hierarchies evaluation of the algorithm is performed on vowel classification and within a hybrid version of the janus speech recognition system using a subset of the switchboard large vocabulary speaker independent continuous speech recognition database
we compare different methods to combine predictions from neural networks trained on different bootstrap samples of a regression problem one of these methods introduced in and which we here call balancing is based on the analysis of the ensemble generalization error into an ambiguity term and a term incorporating generalization performances of individual networks we show how to estimate these individual errors from the residuals on validation patterns weighting factors for the different networks follow from a quadratic programming problem on a real world problem concerning the prediction of sales figures and on the well known boston housing data set balancing clearly outperforms other recently proposed alternatives as bagging and bumping i early stopping and bootstrapping stopped training is a popular strategy to prevent overfitting in neural networks the complete data set is split up into a training and a validation set through learning the weights are adapted in order to minimize the error on the training data training is stopped when the error on the validation data starts increasing the final network depends on the accidental subdivision in training and validation set and often also on the usually random initial weight configuration and chosen minimization procedure in other words early stopped neural networks are highly unstable small changes in the data or different initial conditions can produce large changes in the estimate as argued in with unstable estimators it is advisable to resample ie to apply the same procedure several times using different subdivisions in training and validation set and perhaps starting from different initial rwcp real world computing partnership snn foundation for neural networks balancing between bagging and bumping configurations in the neural network literature resampling is often referred to as training ensembles of neural networks in this paper we will discuss methods for combining the outputs of networks obtained through such a repetitive procedure first however we have to choose how to generate the subdivisions in training and validation sets options are among others k fold cross validation subsampling and bootstrapping in this paper we will consider bootstrapping which is based on the idea that the available data set is nothing but a particular realization of some probability distribution in principle one would like to do inference on this true yet unknown probability distribution a natural thing to do is then to define an empirical distribution with so called naive bootstrapping the empirical distribution is a sum of delta peaks on the available data points each with probability content pdata with pdata the number of patterns a bootstrap sample is a collection of pdata patterns drawn with replacement from this empirical probability distribution some of the data points will occur once some twice and some even more than twice in this bootstrap sample the bootstrap sample is taken to be the training set all patterns that do not occur in a particular bootstrap sample constitute the validation set for large pdata the probability that a pattern becomes part of the validation set is pdata pdata e an advantage of bootstrapping over other resampling techniques is that most statistical theory on resampling is nowadays based on the bootstrap using naive bootstrapping we generate nrun training and validation sets out of our complete data set of pdata input output combinations t in this paper we will restrict ourselves to regression problems with for notational convenience just one output variable we keep track of a matrix with components q indicating whether patternt is part of the validation set for run i q or of the training set q on each subdivision we train and stop a neural network with one layer of nhidden hidden units the output o of network i with weight vector wi on input reads o wji tanh wklnhddenjiirk woi jl i ko where we use the definition xg the validation error for run i can be written evalidationi qr pi with pi yt q pdata the number of validation patterns in run i and r o t the error of network i on pattern after training we are left with nrun networks with in practice quite different performances on the complete data set how should we combine all these outputs to get the best possible performance on new data combining estimators several methods have been proposed to combine estimators see eg for a review in this paper we will only consider estimators with the same architecture t heskes but trained and stopped on different subdivisions of the data in training and validation sets recently two such methods have been suggested for bootstrapped estimators bagging an acronym for bootstrap aggregating and bumping meaning bootstrap umbrella of model parameters with bagging the prediction on a newly arriving input vector is the average over all network predictions bagging completely disregards the performance of the individual networks on the data used for training and stopping bumping on the other hand throws away all networks except the one with the lowest error on the complete data set in the following we will describe an intermediate form due to which we here call balancing a theoretical analysis of the implications of this idea can be found in suppose that after training we receive a new set of ptest test patterns for which we do not know the true targets but can calculate the network output i for each network i we give each network a weighting factor ci and define the prediction of all networks on pattern y as the weighted average the goal is to find the weighting factors ai subject to the constraints run and ai o vi i yielding the smallest possible generalization error i ptest ptest the problem of course is our ignorance about the targets iv bagging simply takes ci nrun for all networks whereas bumping implies ai ji with pdata argmin e o t i pdata yl as in we write the generalization error in the form etest ij i ij if v the last term depends only on the network outputs and can thus be calculated this ambiguity term favors networks with conflicting outputs the first part the idea behind bumping is more general and involved than discussed here the interested reader is referred to in this paper we will only consider its naive version balancing between bagging and bumping containing the generalization errors etest i for individual networks depends on the targets iv and is thus unknown it favors networks that by themselves already have a low generalization error in the next section we will find reasonable estimates for these generalization errors based on the network performances on validation data once we have obtained these estimates finding the optimal weighting factors ai under the constraints is a straightforward quadratic programming problem estimating the generalization error at first sight a good estimate for the generalization error of network i could be the performance on the validation data not included during training however the validation error evalidationi strongly depends on the accidental subdivision in training and validation set for example if there are a few outliers which by pure coincidence are part of the validation set the validation error will be relatively large and the training error relatively small to correct for this bias as a result of the random subdivision we introduce the expected validation error for run i first we define n as the number of runs in which pattern is part of the validation set and evalidation as the error averaged over these runs validation tt i i the expected validation error then follows from validation i pdata pi qi evalidation the ratio between the observed and the expected validation error indicates whether the validation error for network i is relatively high or low our estimate for the generalization error of network i is this ratio multiplied by an overall scaling factor being the estimated average generalization error pdata evalidation i e e bresti validationi pdat validation note that we implicitly make the assumption that the bias introduced by stopping at the minimal error on the validation patterns is negligible ie that the validation patterns used for stopping a network can be considered as new to this network as the completely independent test patterns simulations we compare the following methods for combining neural network outputs individual the average individual generalization error ie the generalization error we will get on average when we decide to perform only one run it serves as a reference with which the other methods will be compared bumping the generalization of the network with the lowest error on the data available for training and stopping t heskes unfair unfair bumping bagging ambiguity balancing bumping balancing store i store store store store store mean table decrease in generalization error relative to the average individual generalization error as a result of several methods for combining neural networks trained to predict the sales figures for several stores bagging the generalization error when we take the average of all rru n network outputs as our prediction ambiguity the generalization error when the weighting factors are chosen to maximize the ambiguity ie taking identical estimates for the individual generalization errors of all networks in expression balancing the generalization error when the weighting factors are chosen to minimize our estimate of the generalization error unfair bumping the smallest generalization error for an individual error ie the result of bumping if we had indeed chosen the network with the smallest generalization error unfair balancing the lowest possible generalization error that we could obtain if we had perfect estimates of the individual generalization errors the last two methods unfair bumping and unfair balancing only serve as some kind of reference and can never be used in practice we applied these methods on a real world problem concerning the prediction of sales figures for several department stores in the netherlands for each store networks with hidden units were trained and stopped on bootstrap samples of about patterns the test set on which the performances of the various methods for combination were measured consists of about patterns inputs include weather conditions day of the week previous sales figures and season the results are summarized in table where we give the decrease in the generalization error relative to the average individual generalization error as can be seen in table bumping hardly improves the performance the reason is that the error on the data used for training and stopping is a lousy predictor of the generalization error since some amount of overfitting is inevitable the generalization performance obtained through bagging ie first averaging over all outputs can be proven to be always better than the average individual generalization error balancing between bagging and bumping o t m number of replicates number of replicates figure decrease of generalization error relative to the average individual generalization error as a function of the number of bootstrap replicates for different combination methods bagging dashdot star ambiguity dotted star bumping dashed star balancing solid star unfair bumping dashed circle unfair balancing solid circle shown are the mean left and the standard deviation right of the decrease in percentages networks are trained and tested on the boston housing database on these data bagging is definitely better than bumping but also worse than maximizing the ambiguity in all cases except for store where maximization of the ambiguity is slightly better balancing is a clear winner among the fair methods the last column in table shows how much better we can get if we could find more accurate estimates for the generalization errors of individual networks the method of balancing discards most of the networks ie the solution to the quadratic programming problem under constraints yields just a few weighting factors different from zero on average about for this set of simulations balancing is thus indeed a compromise between bagging taking all networks into acount and bumping keeping just one network we also compared these methods on the well known boston housing data set concerning the median housing price in several tracts based on mainly socio economic predictor variables see eg for more information we left out of the available cases for assessment of the generalization performance all other cases were used for training and stopping neural networks with hidden units the average individual mean squared error over all bootstrap runs is which is comparable to the mean squared error reported in to study how the performance depends on the number of bootstrap replicates we randomly drew sets of n and bootstrap replicates out of our ensemble of replicates and applied the combination methods on these sets for each n we did this times figure shows the mean decrease in the generalization error relative to the average individual generalization error and its standard deviation again balancing comes out best especially for a larger number of bootstrap replicates it seems that beyond say replicates both bumping and bagging are hardly helped by more runs whereas both maximization of the ambiguity and balancing still increase their performance bagging fully taking into account all network pre t heskes dictions yields the smallest variation bumping keeping just one of them by far the largest balancing and maximization of the ambiguity combine several predictions and thus yield a variation that is somewhere in between conclusion and discussion balancing a compromise between bagging and bumping is an attempt to arrive at better performances on regression problems the crux in all this is to obtain reasonable estimates for the quality of the different networks and to incorporate these estimates in the calculation of the proper weighting factors see for similar ideas and related work in the context of stacked generalization obtaining several estimators is computationally expensive however the notorious instability of feedforward neural networks hardly leaves us a choice furthermore an ensemble of bootstrapped neural networks can also be used to deduce approximate confidence and prediction intervals see eg to estimate the relevance of input fields and so on it has also been argued that combination of several estimators destroys the structure that may be present in a single estimator having hardly any interpretable structure neural networks do not seem to have a lot they can lose it is a challenge to show that an ensemble of neural networks does not only give more accurate predictions but also reveals more information than a single network references l breiman bagging predictors machine learning b efron and r tibshirani an

neural one unit learning rules for the problem of independent component analysis ica and blind source separation are introduced in these new algorithms every ica neuron develops into a separator that finds one of the independent components the learning rules use very simple constrained hebbiananti hebbian learning in which decorrelating feedback may be added to speed up the convergence of these stochastic gradient descent rules a novel computationally efficient fixed point algorithm is introduced
we develop a recursive node elimination formalism for efficiently approximating large probabilistic networks no constraints are set on the network topologies yet the formalism can be straightforwardly integrated with exact methods whenever they arebecome applicable the approximations we use are controlled they maintain consistently upper and lower bounds on the desired quantities at all times we show that boltzmann machines sigmoid belief networks or any combination ie chain graphs can be handled within the same framework the accuracy of the methods is verified experimentally i
to obtain classification systems with both good generalization performance and efficiency in space and time we propose a learning method based on combinations of weak classifiers where weak classifters are linear classifiers percepttons which can do a little better than making random guesses a randomized algorithm is proposed to find the weak classifiers they are then combined through a majority vote as demonstrated through systematic experiments the method developed is able to obtain combinations of weak classifiers with good generalization performance and a fast training time on a variety of test problems and real applications
we study a time series model that can be viewed as a decision tree with markov temporal structure the model is intractable for exact calculations thus we utilize variational approximations we consider three different distributions for the approximation one in which the markov calculations are performed exactly and the layers of the decision tree are decoupled one in which the decision tree calculations are performed exactly and the time steps of the markov chain are decoupled and one in which a viterbi like assumption is made to pick out a single most likely state sequence we present simulation results for artificial data and the bach chorales i
in the present paper we propose a method to unify information maximization and minimization in hidden units the information maximization and minimization are performed on two different levels collective and individual level thus two kinds of information collective and individual information are defined by maximizing collective information and by minimizing individual information simple networks can be generated in terms of the number of connections and the number of hidden units obtained networks are expected to give better generalization and improved interpretation of internal representations this method was applied to the inference of the maximum onset principle of an artificial language in this problem it was shown that the individual information minimization is not contradictory to the collective information maximization in addition experimental results confirmed improved generalization performance because over training can significantly be suppressed i
unsupervised leaxning algorithms based on convex and conic encoders are proposed the encoders find the closest convex or conic combination of basis vectors to the input the learning algorithms produce basis vectors that minimize the reconstruction error of the encoders the convex algorithm develops locally linear models of the input while the conic algorithm discovers features both algorithms are used to model handwritten digits and compared with vector quantization and principal component analysis the neural network implementations involve feedback connections that project a reconstruction back to the input layer i
we introduce arc lh a new algorithm for improvement of ann classifter performance which measures the importance of patterns by aggregated network output errors on several artificial benchmark problems this algorithm compares favorably with other resample and combine techniques i
multilayer architectures such as those used in bayesian belief networks and helmholtz machines provide a powerful framework for representing and learning higher order statistical relations among inputs because exact probability calculations with these models are often intractable there is much interest in finding approximate algorithms we present an algorithm that efficiently discovers higher order structure using em and gibbs sampling the model can be interpreted as a stochastic recurrent network in which ambiguity in lower level states is resolved through feedback from higher levels we demonstrate the performance of the algorithm on benchmark problems i
we couple the tasks of source separation and density estimation by extracting the local geometrical structure of distributions obtained from mixtures of statistically independent sources our modifications of the self organizing map som algorithm results in purely digital learning rules which perform non parametric histogram density estimation the non parametric nature of the separation allows for source separation of non linear mixtures an anisotropic coupling is introduced into our om with the role of aligning the network locally with the independent component contours this approach provides an exact verification condition for source separation with no prior on the source distributions i
dimension reducing feature extraction neural network techniques which also preserve neighbourhood relationships in data have traditionally been the exclusive domain of kohonen self organising maps recently we introduced a novel dimension reducing feature extraction process which is also topographic based upon a radial basis function architecture it has been observed that the generalisation performance of the system is broadly insensitive to model order complexity and other smoothing factors such as the kernel widths contrary to intuition derived from supervised neural network models in this paper we provide an effective demonstration of this property and give a theoretical justification for the apparent self regularising behaviour of the neuroscale architecture neuroscale a feed forward neural network topographic transformation recently an important class of topographic neural network based feature extraction approaches which can be related to the traditional statistical methods of sammon mappings sammon and multidimensional scaling kruskal have been introduced mao and jain lowe webb lowe and tipping these novel alternatives to kohonen like approaches for topographic feature extraction possess several interesting properties for instance the neuroscale architecture has the empirically observed property that the generalisation perfor d lowe and m e tipping mance does not seem to depend critically on model order complexity contrary to intuition based upon knowledge of its supervised counterparts this paper presents evidence for their self regularising behaviour and provides an explanation in terms of the curvature of the trained models we now provide a brief

when triangulating a belief network we aim to obtain a junction tree of minimum state space according to rose searching for the optimal triangulation can be cast as a search over all the permutations of the graphs vertices our approach is to embed the discrete set of permutations in a convex continuous domain d by suitably extending the cost function over d and solving the continous nonlinear optimization task we hope to obtain a good triangulation with respect to the aformentioned cost this paper presents two ways of embedding the triangulation problem into continuous domain and shows that they perform well compared to the best known heuristic
when combining a set of learned models to form an improved estimator the issue of redundancy or multicollinearity in the set of models must be addressed a progression of existing approaches and their limitations with respect to the redundancy is discussed a new approach pcr based on principal components regression is proposed to address these limitations an evaluation of the new approach on a collection of domains reveals that pcr was the most robust combination method as the redundancy of the learned models increased redundancy could be handled without eliminating any of the learned models and the principal components of the learned models provided a continuum of regularized weights from which pcr could choose
we address statistical classifier design given a mixed training set consisting of a small labelled feature set and a generally larger set of unlabelledfeatures this situation arises eg for medical images where although training features may be plentiful expensive expertise is required to extract their class labels we propose a classifier structure and learning algorithm that make effective use of unlabelled data to improve performance the learning is based on maximization of the total data likelihood ie over both the labelled and unlabelled data subsets two disinc em learning algorithms are proposed differing in the em formalism applied for unlabelled data the classifier based on a joint probability model for features and labels is a mixture of experts structure that is equivalent to the radial basis function rbf classifier but unlike pubfs is amenable to likelihood based training the scope of application for the new method is greatly extended by the observation that test data or any new data to classify is in fact additional unlabelled daathus a combined learningclassification operation much akin to what is done in image segmentation can be invoked whenever there is new data to classify experiments with data sets from the uc irvine database demonstrate that the new learning algorithms and structure achieve substantial performance gains over alternative approaches i
in this paper we propose a method for learning bayesian belief networks from data the method uses artificial neural networks as probability estimators thus avoiding the need for making prior assumptions on the nature of the probability distributions governing the relationships among the participating variables this new method has the potential for being applied to domains containing both discrete and continuous variables arbitrarily distributed we compare the learning performance of this new method with the performance of the method proposed by cooper and herskovits in the experimental results show that although the learning scheme based on the use of ann estimators is slower the learning accuracy of the two methods is comparable category algorithms and architectures
smoothing regularizers for radial basis functions have been studied extensively but no general smoothing regularizers for projective basis functions pbfs such as the widely used sigmoidal pbfs have heretofore been proposed we derive new classes of algebraically simple rn h order smoothing regularizers for networks of the form fw x yjl uj x tvj vj it with general projectire basis functions these reguladzers are n rw vjll global form jl n rtwm u i lvj al form these regularizers bound the corresponding rn order smoothing integral where w denotes all the network weights u uo v vo and x is a weighting function on the d dimensional input space the global and local cases are distinguished by different choices of the simple algebraic forms rw m enable the direct enforcement of smoothness without the need for cosily monte carlo integrations of sw m the new regularizers are shown to yield better generalization errors than weight decay when the implicit assumptions in the latter are wrong unlike weight decay the new regularizers distinguish between the roles of the input and output weights and capture the interactions between them address as of september centre for computer architecture university of halmstad pobox s halmstad sweden j e moody and t s rgnvaldsson
the separation of generalization error into two types bias and variance geman bienenstock doursat leads to the notion of error reduction by averaging over a committee of classifiers perrone committee performance decreases with both the average error of the constituent classifiers and increases with the degree to which the misclassifications are correlated across the committee here a method for reducing correlations is introduced that uses a winner take all procedure similar to competitive learning to drive the individual networks to different minima in weight space with respect to the training set such that correlations in generalization performance will be reduced thereby reducing committee error
an adaptive on line algorithm extending the learning of learning idea is proposed and theoretically motivated relying only on gradient flow information it can be applied to learning continuous functions or distributions even when no explicit loss function is given and the hessian is not available its efficiency is demonstrated for a non stationary blind separation task of acoustic signals i
we present an algorithm for fast stochastic gradient descent that uses a nonlinear adaptive momentum scheme to optimize the late time convergence rate the algorithm makes effective use of curvature information requires only n storage and computation and delivers convergence rates close to the theoretical optimum we demonstrate the technique on linear and large nonlinear backprop networks improving stochastic search learning algorithms that perform gradient descent on a cost function can be formulated in either stochastic on line or batch form the stochastic version takes the form where cot is the current weight estimate pt is the learning rate g is minus the instantaneous gradient estimate and xt is the input at time t x one obtains the corresponding batch mode learning rule by taking constant and averaging g over all x stochastic learning provides several advantages over batch learning for large datasets the batch average is expensive to compute stochastic learning eliminates the averaging the stochastic update can be regarded as a noisy estimate of the batch update and this intrinsic noise can reduce the likelihood of becoming trapped in poor local optima we assume that the inputs are iid this is achieved by random sampling with replacement from the training data using curvature information for fast stochastic search the noise must be reduced late in the training to allow weights to converge after settling within the basin of a local optimum w learning rate annealing allows convergence of the weight error v w w it is well known that the expected squared weight error ev decays at its maximal rate oc it with the annealing schedule iot furthermore to achieve this rate one must have icrit amin where amin is the smallest eigenvalue of the hessian at w and references therein finally the optimal io which gives the lowest possible value of ev is a in multiple dimensions the optimal learning rate matrix is t lt where is the hessian at the local optimum incorporating this curvature information into stochastic learning is difficult for two reasons first the hessian is not available since the point of stochastic learning is not to perform averages over the training data second even if the hessian were available optimal learning requires its inverse which is prohibitively expensive to compute the primary result of this paper is that one can achieve an algorithm that behaves optimally ie as if one had incorporated the inverse of the full hessian without the storage or computational burden the algorithm which requires only on storage and computation n number of weights in the network uses an adaptive momentum parameter extending our earlier work to fully non linear problems we demonstrate the performance on several large back prop networks trained with large datasets implementations of stochastic learning typically use a constant learning rate during the early part of training what darken and moody call the search phase to obtain exponential convergence towards a local optimum and then switch to annealed learning called the converge phase we use darken and moodys adaptive search then converge astc algorithm to determine the point at which to switch to it annealing astc was originally conceived as a means to insure icrit during the annealed phase and we compare its performance with adaptive momentum as well we also provide a comparison with conjugate gradient optimization momentum in stochastic gradient descent the adaptive momentum algorithm we propose was suggested by earlier work on convergence rates for annealed learning with constant momentum in this section we summarize the relevant results of that work extending to include momentum leaves the learning rule where is the momentum parameter constrained so that analysis of the dynamics of the expected squared weight error e ivl with it iot learning rate annealing shows that at late times learning proceeds as for the algorithm without momentum but with a scaled or effective learning rate uo this result is consistent with earlier work on momentum learning with small constant where the same result holds venter proposed a d algorithm for optimizing the convergence rate that estimates the hessian by time averaging finite differences of the gradient and scalin the learning rate by the inverse its extension to multiple dimensions would require on storage and on time for inversion both are prohibitive for large models g b orr and t k leen if we allow the effective learning rate to be a matrix then following our comments in the

the softassign quadratic assignment algorithm has recently emerged as an effective strategy for a variety of optimization problems in pattern recognition and combinatorial optimization while the effectiveness of the algorithm was demonstrated in thousands of simulations there was no known proof of convergence here we provide a proof of convergence for the most general form of the algorithm i
this paper compares three penalty terms with respect to the efficiency of supervised learning by using firstand second order learning algorithms our experiments showed that for a reasonably adequate penalty factor the combination of the squared penalty term and the second order learning algorithm drastically improves the convergence performance more than times over the other combinations at the same time bringing about a better generalization performance i
a hint is any piece of side information about the target function to be learned we consider the monotonicity hint which states that the function to be learned is monotonic in some or all of the input variables the application of monotonicity hints is demonstrated on two real world problemsa credit card application task and a problem in medical diagnosis a measure of the monotonicity error of a candidate function is defined and an objective function for the enforcement of monotonicity is derived from bayesian principles we report experimental results which show that using monotonicity hints leads to a statistically significant improvement in performance on both problems
we present new algorithms for parameter estimation of hmms by adapting a framework used for supervised learning we construct iterative algorithms that maximize the likelihood of the observations while also attempting to stay close to the current estimated parameters we use a bound on the relative entropy between the two hmms as a distance measure between them the result is new iterative training algorithms which are similar to the em baum welch algorithm for training hmms the proposed algorithms are composed of a step similar to the expectation step of baum welch and a new update of the parameters which replaces the maximization re estimation step the algorithm takes only negligibly more time per iteration and an approximated version uses the same expectation step as baum welch we evaluate experimentally the new algorithms on synthetic and natural speech pronunciation data for sparse models ie models with relatively small number of non zero parameters the proposed algorithms require significantly fewer iterations preliminaries we use the numbers from to n to name the states of an hmm state is a special initial state and state n is a special final state any state sequence denoted by s starts with the initial state but never returns to it and ends in the final state observations symbols are also numbers in m and observation sequences are denoted by x a discrete output hidden markov model hmm is parameterized by two matrices a and b the first matrix is of dimension n n and aij i n j n denotes the probability of moving from state i to state j the second matrix is of dimension n m and bil is the probability of outputting symbol k at state i the set of parameters of an hmm is denoted by t a b the initial state distribution vector is represented by the first row of a an hmm is a probabilistic generator of sequences it starts in the initial state it then iteratively does the following until the final state is reached if i is the current state then a next state j is chosen according to the transition probabilities out of the current state row i of matrix a after arriving at state j a symbol is output according to the output probabilities of that state row j of matrix b let px s denote the probability likelihood that an hmm generates the observation sequence x on the path s starting at state and ending at state n px slls i ixl s sis i n def i ixllasttstbstxt for the sake of brevity we omit the conditions on s and x throughout the paper we assume that the hmms are absorbing that is from every state there is a path to the final state with a y singer and m k warmuth non zero probability similar parameter estimation algorithms can be derived for ergodic hmms absorbing hmms induce a probability over all state observation sequences ie exs px s the likelihood of an observation sequence x is obtained by summing over all possible hidden paths state sequences px es px s to obtain the likelihood for a set r of observations we simply multiply the likelihood values for the individual sequences we seek an hmm t that maximizes the likelihood for a given set of observations r or equivalently maximizes the log likelihood llxi ixl xx in pxo to simplify our notation we denote the generic parameter in o by oi where i ranges from to the total number of parameters in a and b there might be less if some are clamped to zero we denote the total number of parameters of o by and leave the fixed correspondence between the oi and the entries of a and b unspecified the indices are naturally partitioned into classes corresponding to the rows of the matrices we denote by i the class of parameters to which i belongs and by ill the vector of all oj st j i if j g i then both oi and oj are parameters from the same row of one of the two matrices whenever it is clear from the context we will use i to denote both a class of parameters and the row number ie state associated with the class we now can rewrite px so as i iz xs where rtix s is the number of times parameter i is used along the path s with observation sequence x note that this value does not depend on the actual parameters we next compute partial derivatives of the likelihood and the log likelihood using this notation oogpx sl o llxi ooi nxo i o px slo xx s pxlo ngx s pslx xx s nxs px s hix px s ixl g px xx s exx xi here fiix def es mxspslx is the expected number of occurrences of the transitionoutput that corresponds to oi over all paths that produce x in these values are calculated in the expectation step of the expectation maximization em training algorithm for hmms also known as the baum welch or the forwardbackward algorithm in the next sections we use the additional following expectations io dej exs nixspx slo and fiio dej eji jo note that the summation here is over all legal x and s of arbitrary length and fiji is the expected number of times the state i was visited entropic distance functions for hmms our training algorithms are based on the following framework of kivinen and warmuth for motivating iterative updates assume we have already done a number of iterations and our current parameters are assume further that r is the set of observations to be processed in the current iteration in the batch case this set never changes and in the on line case x is typically a single observation the new parameters should stay close to which incorporates all the knowledge obtained in past iterations but it should also maximize the log likelihood on the current date set r thus instead of maximizing the loglikelihood we maximize u rllri d see for further motivation training algorithms for hidden markov models here d measures the distance between the old and new parameters and r is a trade off factor maximizing u is usually difficult since both the distance function and the loglikelihood depend on t as in we approximate the log likelihood by a first order taylor expansion around and add lagrange multipliers for the constraints that the parameters of each class must sum to one i i a commonly used distance function is the relative entropy to calculate the relative entropy between two hmms we need to sum over all possible hidden state sequence which leads to the following definition dreo deal e pxl in p l e px sl in es px sly x x however the above divergence is very difficult to calculate and is not a convex function in to avoid the computational difficulties and the non convexity of dre we upper bound the relative entropy using the log sum inequality px s daoo too def e px sio lnpxs e px sl n f i izl xs z xs i iz xs px sl nixs in xs i eln e px slo nixs ehi ln i xs i note that for the distance function eo an hmm is viewed as a joint distribution between observation sequences and hidden state sequences we can further simplify the bound on the relative entropy using the following lemma proof omitted lemma for any absorbing hmm o andany parameteroi o oifiilo this gives the following new formula e zl ii in which can be rewritten as e i idbq eq in equation is still difficult to solve since the vmables i depend on the new set of pmeters which e not known we therefore fuher approximate by the distance function i fii ii in new parameter updates we now would like to use the distce functions discuss in previous section in u we first derive our mn update using this distce function is is done by replacing d in u with db and setting the derivatives of the resulting u wrt i to s gives the following set of equations i e z mxm hioln ai which e equivent to v ig in ai y singer and m k warmuth we now can solve for ti and replace ai by a normalization factor which ensures that the sum of the parameters in i is the above re estimation rule is the entropic update for hmms we now derive an alternate to the update of the mixture weights fii which approximate the original mixture weights i b in j lead to a state dependent learning rate of for the parameters of class i if computation time is limited see discussion below then the expectations fiji can be approximated by values that are readily available one possible choice is to use the sample based expectations j an approximation for fiil these weights are needed for calculating the gradient and are evaluated in the expectation step of baum welch let ilxl dej zjei hjxl then this approximation leads to the following distance function e exv fiix e j in j q ix i which results in an update which we call the approximated entropic update for hmms oi exp xexl x ji oj eocp xexulxi given a current set of parameters t and a learning rate rwe obtain a new set of parameters by iteratively evaluating the right hand side of the entropic update or the approximated entropic update we calculate the expectations ix as done in the expectation step of baum welch the weights fijix are obtained by averaging fix for j i this lets us evaluate the right hand side of the approximated entropic update the entropic update is slightly more involved and requires an additional calculation of fiil recall that hq is the expected number of times state i is visited unconditionedon the data to compute these expectations we need to sum over all possible sequences of state observation pairs since the probability of outputting the possible symbols at a given state sum to one calculating i reduces to evaluating the probability of reaching a state for each possible time and sequence length for absorbing hmms fiji i can be approximated efficiently using dynamic programming we compute i by summing the probabilities of all legal state sequences s of up to length cn typically c proved to be sufficient to obtain very accurate approximations of fiq therefore the time complexity of calculating fiq depends only on the number of states regardless of the dimension of the output vector m and the training data a subtle improvement is possible over the update by treating the transition probabilities and output probabilities differently first the transition probabilities are updated based on then the state probabilities hlo hil are recomputed based on the new parameters this is possible since the state probabilities depend only on the transition probabilities and not on the output probabilities finally the output probabilities are updated with where the fib are used in place of the il training algorithms for hidden markov models the relation to em and convergence properties we first show that the em algorithm for hmms can be derived using our framework to do so we approximate the relative entropy by the x distance see dre p dx p def i p and use this distance to approximate re ei p re x de e i dxi il i o dxglril lxl o dxrglrl l by minimizing u with the last version of the x heredxi i jil distance function and following the same derivation steps as for the approximated entropic update we arrive at what we call the approximated x update for hmms xx setting results in the update i yxx fiixyxx ix which is the maximization re estimation step of the em algorithm although omitted from this paper due to the lack of space it is can be shown that for r the entropic updates and the x update improve the likelihood on each iteration therefore these updates belong to the family of generalized em gem algorithms which are guaranteed to converge to a local maximum given some additional conditions furthermore using infinitesimal analysis and second order approximation of the likelihood function at the local maximum similar to it can be shown that the approximated x update is a contraction mapping and close to the local maximum there exists a learning rate r which results in a faster rate of convergence than when using experiments ith artificial anti natural data in order to test the actual convergence rate of the algorithms and to compare them to baum welch we created synthetic data using hmms in our experiments we mainly used sparse models that is models with many parameters clamped to zero previous work eg might suggest that the entropic updates will perform better on sparse models indeed when we used dense models to generate the data the algorithms showed almost the same performance the training algorithms however were started from a randomly chosen dense model when comparing the algorithms we used the same initial model due to different trajectories in parameter space each algorithm may converge to a different local maximum for the clarity of presentation we show here results for cases where all updates converged to the same maximum which often occur when the hmm generating the data is sparse and there are enough examples typically tens of observations per non zero parameter we tested both the entropic updates and the x updates learning rates greater than one speed up convergence the two entropic updates converge almost equally fast on synthetic data generated by an hmm for natural data the entropic update converges slightly faster than the approximated version the x update also benefits from learning rates larger than one however the x update need to be used carefully since it does not necessarily ensure non negativeness of the new parameters for this problems is exaggerated when the data is not generated by an hmm we therefore used the entropic updates in our experiments with natural data in order to have a fair comparison we did not tune the learning rate rand set it to in figure we give a comparison of the entropic update the approximated entropic update and baum welch left figure using an hmm to generate the random observation sequences where n m but only parameters on the average for each transitionobservation vector of the parameters of the y singer and m k warmuth hmm are non zero the performance of the entropic update and the approximated entropic update are practically the same and both updates clearly outperform baum welch one reason the performance of the two entropic updates is the same is that the observations were indeed generated by an hmm in this case approximating the expectations fiji by the sample based expectations seems reasonable these results suggest a valuable alternative to using baum welch with a predetermined sparse potentially biased hmm where a large number of parameters is clamped to zero instead we suggest starting with a full model and let one of the entropic updates find the relevant parameters this approach is demonstrated on the right part of figure in this example the data was generated by a sparse hmm with states and possible output symbols only of the hmms parameters were nonzero three log likelihood curves are given in the figure one is the log likelihood achieved by baum welch when only those parameters that are non zero in the hmm generating the data are initialized to random non zero values the other two are the log likelihood of the entropic update and baum welch when all the parameters are initialized randomly the curves show that the entropic update compensates for its inferior initialization in less than iterations see horizontal line in figure and from this point on it requires only more iterations to converge compared to baum welch which is given prior knowledge of the non zero parameters in contrast when baum welch is started with a full model then its convergence is much slower than the entropic update ox entropic update entropic update em baum welch iteration iteration figure comparison of the entropic updates and baum welch we next tested the updates on speech pronunciation data in natural speech a word might be pronounced differently by different speakers a common practice is to construct a set of stochastic models in order to capture the variability of the possible pronunciations alternative pronunciations of a given word this problem was studied previously in using a state merging algorithm for hmms and in using a subclass of probabilistic finite automata the purpose of the experiments discussed here is not to compare the above algorithms to the entropic updates but rather compare the entropic updates to baum welch nevertheless the resulting hmm pronunciation models are usually sparse typically only two or three phonemes have a non zero output probability at a given state and the average number of states that in practice can follow a states is about therefore the entropic updates may provide a good alternative to the algorithms presented in we used the timit texas instruments mit database as in this database contains the acoustic waveforms of continuous speech with phone labels from an alphabet of phones which constitute a temporally aligned phonetic transcription to the uttered words for the purpose of building pronunciation models the acoustic data was ignored and we partitioned the phonetic labels according to the words that appeared in the data the data was filtered and partitioned so that words occurring between and times in the dataset were used for training and evaluation according to the following partition of the occurrences of each word were used as training data for the learning algorithm and the remaining were used for evaluation we then built for each word three pronunciation models by training a fully connected hmm whose number of states was set to and times the longest sample denoted by n the models were evaluated by calculating training algorithms for hidden markov models the log likelihood averaged over different random parameter initializations of each hmm on the phonetic transcription of each word in the test set in table we give the negative log likelihood achieved on the test data together with the average number of iterations needed for training overall the differences in the log likelihood are small which means that the results should be interpreted with some caution nevertheless the entropic update obtained the highest likelihood on the test data while needing the least number of iterations the approximated entropic update and baum welch achieve similar results on the test data but the latter requires more iterations checking the resulting models reveals one reason why the entropic update achieves higher likelihood values namely it does a better job in setting the irrelevant parameters to zero and it does it faster negative log likelihood iterations states nm n n n n n b aum welch approx eu entropic update table comparison of the entropic updates and baum welch on speech pronunciation data conclusions and future research in this paper we have showed how the framework of kivinen and warmuth can be used to derive parameter updates algorithms for hmms we view an hmm as a joint distribution between the observation sequences and hidden state sequences and use a bound on relative entropy as a distance between the new and old parameter settings if we approximate of the relative entropy by the x distance replace the exact state expectations by a sample based approximation and fix the learning rate to one then the framework yields an alternative derivation of the em algorithm for hmms since the em update uses sample based estimates of the state expectations it is hard to use it in an on line setting in contrast the on line versions of our updates can be easily derived using only one observation sequence at a time also there are alternative gradient descent based methods for estimating the parameters of hmms such methods usually employ an exponential parameterization such as soft max of the parameters see for the case of learning one set of mixture coefficients an exponential parameterization led to an algorithm with a slower convergence rate compared to algorithms derived using entropic distances however it is not clear whether this is still the case for hmms our future goals is to perform a comparative study of the different updates with emphasis on the on line versions acknowledgments we thank anders krogh for showing us the simple derivative calculations used in this paper and thank fernando peteira and yasubumi sakakibara for interesting discussions references p baldi and y chauvin smooth on line learning algorithms for hidden markov models neural computation le baum and t pelrie statistical inference for probabilistic functionsof finite state markovchains annals of mathematical statisitics t cover and j thomas elements oflnformation theor wiley ap dempster nm laird and d b rubin maximum likelihood from incomplete data via the em algorithm journal of the royal statistical society b d p helmbold r e schapire y singer and m k warmuth a comparison of new and old algorithms for a mixture estimation problem in proceedingsof the eighth annual workshop on computationai learning theory pages j kivinen and mo k warmuth exponentiated gradient versus gradient descent for linear predictors informationa and computation to appear lr rabiner and b h juang an
this paper discusses a probabilistic model based approach to clustering sequences using hidden markov models hmms the problem can be framed as a generalization of the standard mixture model approach to clustering in feature space two primary issues are addressed first a novel parameter initialization procedure is proposed and second the more difficult problem of determining the number of clusters k from the data is investigated experimental results indicate that the proposed techniques are useful for revealing hidden cluster structure in data sets of sequences i
the algorithm described in this article is based on the obs algorithm by hassibi stork and wolff and the main disadvantage of obs is its high complexity obs needs to calculate the inverse hessian to delete only one weight thus needing much time to prune a big net a better algorithm should use this matrix to remove more than only one weight because calculating the inverse hessian takes the most time in the obs algorithm the algorithm called unit obs described in this article is a method to overcome this disadvantage this algorithm only needs to calculate the inverse hessian once to remove one whole unit thus drastically reducing the time to prune big nets a further advantage of unit obs is that it can be used to do a feature extraction on the input data this can be helpful on the understanding of unknown problems i
we seek to analyze and manipulate two factors which we call style and content underlying a set of observations we fit training data with bilinear models which explicitly represent the two factor structure these models can adapt easily during testing to new styles or content allowing us to solve three general tasks extrapolation of a new style to unobserved content classification of content observed in a new style and translation of new content observed in a new style for classification we embed bilinear models in a probabilistic framework separable mixture models smms which generalizes earlier work on factoffal mixture models significant performance improvement on a benchmark speech dataset shows the benefits of our approach i
optimal brain damage obd is a method for reducing the number of weights in a neural network obd estimates the increase in cost function if weights are pruned and is a valid approximation if the learning algorithm has converged into a local minimum on the other hand it is often desirable to terminate the learning process before a local minimum is reached early stopping in this paper we show that obd estimates the increase in cost function incorrectly if the network is not in a local minimum we also show how obd can be extended such that it can be used in connection with early stopping we call this new approach early brain damage ebd ebd also allows to revive already pruned weights we demonstrate the improvements achieved by ebd using three publicly available data sets i
alexandre pouget alexsalk edu we present a theoretical framework for population codes which generalizes naturally to the important case where the population provides information about a whole probability distribution over an underlying quantity rather than just a single value we use the framework to analyze two existing models and to suggest and evaluate a third model for encoding such probability distributions
two dimensional image motion detection neural networks have been implemented using a general purpose analog neural computer the neural circuits perform spatiotemporal feature extraction based on the cortical motion detection model of adelson and bergen the neural computer provides the neurons synapses and synaptic time constants required to realize the model in vlsi hardware results show that visual motion estimation can be implemented with simple sum andthreshold neural hardware with temporal computational capabilities the neural circuits compute general d visual motion in real time
many popular learning rules are formulated in terms of continuous analog inputs and outputs biological systems however use action potentials which are digital amplitude events that encode analog information in the inter event interval action potential representations are now being used to advantage in neuromorphic vlsi systems as well we report on a simple learning rule based on the riccati equation described by kohonen modified for action potential neuronal outputs we demonstrate this learning rule in an analog vlsi chip that uses volatile capacitive storage for synaptic weights we show that our time dependent learning rule is sufficient to achieve approximate weight normalization and can detect temporal correlations in spike trains a spike based learning neuron in analog vlsi
we use the constant statistics constraint to calibrate an array of sensors that contains gain and offset variations this algorithm has been mapped to analog hardware and designed and fabricated with a um cmos technology measured results from the chip show that the system achieves invariance to gain and offset variations of the input signal i
a one dimensional visual tracking chip has been implemented using neuromorphic analog vlsi techniques to model selective visual attention in the control of saccadic and smooth pursuit eye movements the chip incorporates focal plane processing to compute image saliency and a winner take all circuit to select a feature for tracking the target position and direction of motion are reported as the target moves across the array we demonstrate its functionality in a closed loop system which performs saccadic and smooth pursuit tracking movements using a one dimensional mechanical eye i
the major problem that has prevented practical application of analog neuro lsis has been poor accuracy due to fluctuating analog device characteristics inherent in each device as a result of manufacturing this paper proposes a dynamic control architecture that allows analog silicon neural networks to compensate for the fluctuating device characteristics and adapt to a change in input dc level we have applied this architecture to compensate for input offset voltages of an analog cmos wta winner take all chip that we have fabricated experimental data show the effectiveness of the architecture

we describe the implementation of a hidden markov model state decoding system a component for a wordspotting speech recognition system the key specification for this state decoder design is microwatt power dissipation this requirement led to a continuoustime analog circuit implementation we characterize the operation of a word state state decoder test chip
we propose a neuromorphic architecture for real time processing of acoustic transients in analog vlsi we show how judicious normalization of a time frequency signal allows an elegant and robust implementation of a correlation algorithm the algorithm uses binary multiplexing instead of analog analog multiplication this removes the need for analog storage and analog multiplication simulations show that the resulting algorithm has the same out of sample classification performance correct as a baseline template matching algorithm
detection of the periodicity of amplitude modulation is a major step in the determination of the pitch of a sound in this article we will present a silicon model that uses synchronicity of spiking neurons to extract the fundamental frequency of a sound it is based on the observation that the so called choppers in the mammalian cochlear nucleus synchronize well for certain rates of amplitude modulation depending on the cells intrinsic chopping frequency our silicon model uses three different circuits ie an artificial cochlea an inner hair cell circuit and a spiking neuron circuit
humans use visual as well as auditory speech signals to recognize spoken words a variety of systems have been investigated for performing this task the main purpose of this research was to systematically compare the performance of a range of dynamic visual features on a speechreading task we have found that normalization of images to eliminate variation due to translation scale and planar rotation yielded substantial improvements in generalization performance regardless of the visual representation used in addition the dynamic information in the difference between successive frames yielded better performance than optical flow based approaches and compression by local low pass filtering worked surprisingly better than global principal components analysis pca these results are examined and possible explanations are explored

this paper discusses a fairly general adaptation algorithm which augments a standard neural network to increase its recognition accuracy for a specific user the basis for the algorithm is that the output of a neural network is characteristic of the input even when the output is incorrect we exploit this characteristic output by using an output adaptation module oam which maps this output into the correct user dependent confidence vector the oam is a simplified resource allocating network which constructs radial basis functions on line we applied the oam to construct a writer adaptive character recognition system for on line handprinted characters the oam decreases the word error rate on a test set by an average of while creating only to basis functions for each writer in the test set i
this paper presents a new approach to speech recognition with hybrid hmmann technology while the standard approach to hybrid hmmann systems is based on the use of neural networks as posterior probability estimators the new approach is based on the use of mutual information neural networks trained with a special learning algorithm in order to maximize the mutual information between the input classes of the network and its resulting sequence of firing output neurons during training it is shown in this paper that such a neural network is an optimal neural vector quantizer for a discrete hidden markov model system trained on maximum likelihood principles one of the main advantages of this approach is the fact that such neural networks can be easily combined with hmms of any complexity with context dependent capabilities it is shown that the resulting hybrid system achieves very high recognition rates which are now already on the same level as the best conventional hmm systems with continuous parameters and the capabilities of the mutual information neural networks are not yet entirely exploited
time series prediction is one of the major applications of neural networks after a short
to reduce the computational complexity of classification systems using tangent distance hastie et al hss developed an algorithm to devise rich models for representing large subsets of the data which computes automatically the best associated tangent subspace schwenk milgram proposed a discriminant modular classification system diabolo based on several autoassociative multilayer percepttons which use tangent distance as error reconstruction measure we propose a gradient based constructive learning algorithm for building a tangent subspace model with discriminant capabilities which combines several of the the advantages of both hss and diabolo devised tangent models hold discriminant capabilities space requirements are improved with respect to hss since our algorithm is discriminant and thus it needs fewer prototype models dimension of the tangent subspace is determined automatically by the constructive algorithm and our algorithm is able to learn new transformations i
prediction estimation and smoothing are fundamental to signal processing to perform these interrelated tasks given noisy data we form a time series model of the process that generates the data taking noise in the system explicitly into account maximumlikelihood and kalman frameworks are discussed which involve the dual process of estimating both the model parameters and the underlying state of the system we review several established methods in the linear case and propose several extensions utilizing dual kalman filters dkf and forward backward fb filters that are applicable to neural networks methods are compared on several simulations of noisy time series we also include an example of nonlinear noise reduction in speech
this paper investigates a number of ensemble methods for improving the performance of phoneme classification for use in a speech recognition system two ensemble methods are described boosting and mixtures of experts both in isolation and in combination results are presented on two speech recognition databases an isolated word database and a large vocabulary continuous speech database these results show that principled ensemble methods such as boosting and mixtures provide superior performance to more naive ensemble methods such as averaging
brandyn webb the future fieldgate rd oceanside ca brandyn brainstormcom we have combined an artificial neural network ann character classifier with context driven search over character segmentation word segmentation and word recognition hypotheses to provide robust recognition of hand printed english text in new models of apple computers newton messagepad we present some innovations in the training and use of anns as character classifiers for word recognition including normalized output error frequency balancing error emphasis negative training and stroke warping a recurring theme of reducing a priori biases emerges and is discussed


field has suggested that neurons with line and edge selectivities found in primary visual cortex of cats and monkeys form a sparse distributed representation of natural scenes and barlow has reasoned that such responses should emerge from an unsupervised learning algorithm that attempts to find a factorial code of independent visual features we show here that non linear infomax when applied to an ensemble of natural scenes produces sets of visual filters that are localised and oriented some of these filters are gabor like and resemble those produced by the sparseness maximisation network of olshausen field in addition the outputs of these filters are as independent as possible since the infomax network is able to perform independent components analysis ica we compare the resulting ica filters and their associated basis functions with other decorrelating filters produced by principal components analysis pca and zero phase whitening filters zca the ica filters have more sparsely distributed kurtotic outputs on natural scenes they also resemble the receptive fields of simple cells in visual cortex which suggests that these neurons form an information theoretic co ordinate system for images i

this paper describes a new technique for object recognition based on learning appearance models the image is decomposed into local regions which are described by a new texture representation called generalized second moments that are derived from the output of multiscale multiorientation filter banks class characteristic local texture features and their global composition is learned by a hierarchical mixture of experts architecture jordan jacobs the technique is applied to a vehicle database consisting of general car categories sedan van with back doors van without back doors old sedan and volkswagen bug this is a difficult problem with considerable in class variation the new technique has a misclassification rate compared to eigen images which give misclassification rate and nearest neighbors which give misclassification rate
in this paper we propose a model for the lateral connectivity of orientation selective cells in the visual cortex based on informationtheoretic considerations we study the properties of the input signal to the visual cortex and find new statistical structures which have not been processed in the retino geniculate pathway applying the idea that the system optimizes the representation of incoming signals we derive the lateral connectivity that will achieve this for a set of local orientation selective patches as well as the complete spatial structure of a layer of such patches we compare the results with various physiological measurements i
we study the spatiotemporal correlation in natural time varying images and explore the hypothesis that the visual system is concerned with the optimal coding of visual representation through spatiotemporal decorrelation of the input signal based on the measured spatiotemporal power spectrum the transform needed to decorrelate input signal is derived analytically and then compared with the actual processing observed in psychophysical experiments
local disparity information is often sparse and noisy which creates two conflicting demands when estimating disparity in an image region the need to spatially average to get an accurate estimate and the problem of not averaging over discontinuities we have developed a network model of disparity estimation based on disparityselective neurons such as those found in the early stages of processing in visual cortex the model can accurately estimate multiple disparities in a region which may be caused by transparency or occlusion in real images and random dot stereograms the use of a selection mechanism to selectively integrate reliable local disparity estimates results in superior performance compared to standard back propagation and cross correlation approaches in addition the representations learned with this selection mechanism are consistent with recent neurophysiological results of von der heydt zhou friedman and poggio for cells in cortical visual area v combining multi scale biologically plausible image processing with the power of the mixture of experts learning algorithm represents a promising approach that yields both high performance and new insights into visual system function selective integration a model for disparity estimation i
a self organizing architecture is developed for image region classification the system consists of a preprocessor that utilizes multiscale filtering competition cooperation and diffusion to compute a vector of image boundary and surface properties notably texture and brightness properties this vector inputs to a system that incrementally learns noisy multidimensional mappings and their probabilities the architecture is applied to difficult real world image classification problems including classification of synthetic aperture radar and natural texture images and outperforms a recent state of the art system at classifying natural textures i
this paper describes how the early visual process of contour organisation can be realised using the em algorithm the underlying computational representation is based on fine spline coverings according to our em approach the adjustment of spline parameters draws on an iterative weighted least squares fitting process the expectation step of our em procedure computes the likelihood of the data using a mixture model defined over the set of spline coverings these splines are limited in their spatial extent using gaussian windowing functions the maximisation of the likelihood leads to a set of linear equations in the spline parameters which solve the weighted least squares problem we evaluate the technique on the localisation of road structures in aerial infra red images
a simple mathematical model for the large scale circuitry of primary visual cortex is introduced it is shown that a basic cortical architecture of recurrent local excitation and lateral inhibition can account quantitatively for such properties as orientation tuning the model can also account for such local effects as cross orientation suppression it is also shown that nonlocal state dependent coupling between similar orientation patches when added to the model can satisfactorily reproduce such effects as non local iso orientation suppression and non local crossorientation enhancement following this an account is given of perceptual phenomena involving object segmentation such as popout and the direct and indirect tilt illusions i
we compare the generalization performance of three distinct representation schemes for facial emotions using a single classification strategy neural network the face images presented to the classifters are represented as full face projections of the dataset onto their eigenvectors eigenfaces a similar projection constrained to eye and mouth areas eigenfeatures and finally a projection of the eye and mouth areas onto the eigenvectors obtained from x random image patches from the dataset the latter system achieves generalization on novel face images individuals the networks were not trained on drawn from a database in which human subjects consistently identify a single emotion for the face i
we have investigated the possibility that rapid processing in the visual system could be achieved by using the order of firing in different neurones as a code rather than more conventional firing rate schemes using spikenet a neural net simulator based on integrate and fire neurones and in which neurones in the input layer function as analogto delay converters we have modeled the initial stages of visual processing initial results are extremely promising even with activity in retinal output cells limited to one spike per neuron per image effectively ruling out any form of rate coding sophisticated processing based on asynchronous activation was nonetheless possible
a central theme of computational vision research has been the realization that reliable estimation of local scene properties requires propagating measurements across the image many authors have therefore suggested solving vision problems using architectures of locally connected units updating their activity in parallel unfortunately the convergence of traditional relaxation methods on such architectures has proven to be excruciatingly slow and in general they do not guarantee that the stable point will be a global minimum in this paper we show that an architecture in which bayesian beliefs about image properties are propagated between neighboring units yields convergence times which are several orders of magnitude faster than traditional methods and avoids local minima in particular our architecture is non iterative in the sense of marr at every time step the local estimates at a given location are optimal given the information which has already been propagated to that location we illustrate the algorithms performance on real images and compare it to several existing methods i theory the essence of our approach is shown in figure figure la shows the prototypical ill posed problem interpolation of a function from sparse data figure lb shows a traditional relaxation approach to the problem a dense array of units represents the value of the interpolated function at discretely sampled points the activity of a unit is updated based on the local data in those points where data is available and the activity of the neighboring points as discussed below the local update rule can interpreting images by propagating bayesian beliefs figure a a prototypical ill posed problem b traditional relaxation approach dense array of units represent the value of the interpolated function units update their activity based on local information and the activity of neighboring units c the bayesian belief propagation bbp approach units transmit probabilities and combine them according to probability calculus in two non interacting streams be defined such that the network converges to a state in which the activity of each unit corresponds to the value of the globally optimal interpolating function figure lc shows the bayesian belief propagation bbp approach to the problem as in the traditional approach the function is represented by the activity of a dense array of units however the units transmit probabilities rather than single estimates to their neighbors and combine the probabilities according to the probability calculus to formalize the above discussion let y represent the activity of a unit at location k and let y be noisy samples from the true function a typical interpolation problem would be to minimize jy wkyk y q yi yii k i where we have defined w for grid points with no data and w for points with data since j is quadratic any local update in the direction of the gradient will converge to the optimal estimate this yields updates of the sort w relaxation algorithms differ in their choice of r r a w corresponds to gauss seidel relaxation and r a w corresponds to successive over relaxation sor which is the method of choice for such problems to derive a bbp update rule for this problem note that that minimizing jy is equivalent to maximizing the posterior probability of y given y assuming the following generative model yil yi q y wiyi rl where n an r n az the ratio of az to an plays a role similar to that of a in the original cost functional the advantage of considering the cost functional as a posterior is that it enables us to use the methods of hidden markov models bayesian belief nets and optimal estimation to derive local update rules cf denote the posterior by piu py uly the markovian property allows us to factor piu into three terms one depending on the local data another depending on data to the left of i and a third depending on data to the right of i thus where aiu pyi ulyii iu pyi ulyijvliu pyiy u and c denotes a normalizing constant now denoting the conditional ciu v y weiss p ulyl v ciu can be written in terms of cilv u c f i lvciu vlilv where c denotes another normalizing constant a symmetric equation can be written for iu this suggests a propagation scheme where units represent the probabilities given in the left hand side of equations and updates are bed on the right hand side ie on the activities of neighboring units specifically for a gaussian generating process the probabilities can be represented by their mean and variance thus denote pi nii and similarly i nand n performing the integration in gives a kalman filter like update for the parameters d i i wi axpi l a i g i wxx vg vw v he update rules for he parameters of are analogous so far we have considered continuous estimation problems bu identical issues arise in labeling problems where he k is o estimate a label l which can ake on m discrete values we will denote lm i if he label akes on value m and zero otherwise typically one minimizes functionms of the form m k m k adifionm relation labeling algorithms minimize his cost funcfionm wih updaes of he form fv again differen relaion labeling mgorihms differ in heir choice of f a linear sum followed by a hreshold gives the discrete hopfield network updates a linear sum followed by a sof threshold gives he continuous or mean field hop field updates nd yet another form gives the relaxation labeling algorithm of rosenfeld e m see for a review of relaxation labeling methods to derive a bbp algorithm for this ce one cn again rewrite the posterior of a markov generating process and cmculae plm for this process this gives he same expressions in equations wih he integral replaced by a linear sum since he probabilities here are no gaussian he i i pi will no be represented by heir mean nd wriances but rather by a vector of length m thus he update rule for i will be tc and similarly for vo vi owig v i o mi fo ooig sequence of labels that minimizes j in those ces one should do beef revision rather than propagation interpreting images by propagating bayesian beliefs figure a the first frame of a sequence the hand is translated to the left b contour extracted using standard methods convergence equations are mathematical identities hence it is possible to show that after n iterations the activity of units pi will converge to the correct posteriors where n is the maximal distance between any two units in the architecture and an iteration refers to one update of all units furthermore we have been able to show that after n n iterations the activity of unit pi is guaranteed to represent the probability of the hidden state at location i given all data within distance n this guarantee is significant in the light of a distinction made by mart regarding local propagation rules in a scheme where units only communicate with their neighbors there is an obvious limit on how fast the information can reach a given unit ie after n iterations the unit can only know about information within distance n thus there is a minimal number of iterations required for all data to reach all units mart distinguished between two types of iterations those that are needed to allow the information to reach the units versus those that are used to refine an estimate based on information that has already arrived the significance of the guarantee on pi is that it shows that bbp only uses the first type of iteration iterations are used only to allow more information to reach the units once the information has arrived pi represents the correct posterior given that information and no further iterations are needed to refine the estimate moreover we have been able to show that propagations schemes that do not propagate probabilities such as those in equations will in general not represent the optimal estimate given information that has already arrived to summarize both traditional relaxation updates as in equation and bbp updates as in equations give simple rules for updating a units activity based on local data and activities of neighboring units however the fact that bbp updates are based on the probability calculus guarantees that a units activity will be optimal given information that has already arrived and gives rise to a qualitative difference between the convergence of these two types of schemes in the next section we will demonstrate this difference in image interpretation problems results figure a shows the first frame of a sequence in which the hand is translated to the left figure b shows the bounding contour of the hand extracted using standard techniques motion propagation along contours local measurements along the contour are insufficient to determine the motion hildreth suggested to overcome the local ambiguity by minimizing the following y weiss figure a local estimate of velocity along the contour b performance of sor gradient descent and bbp as a function of time bbp converges orders of magnitude faster than sor c motion estimate of sor after iterations d motion estimate of bbp after iterations cost functional jv q dt q iivv k k where dx dt denote the spatial and temporal image derivatives and va denotes the velocity at point k along the contour this functional is analogous to the interpolation functional eq and the derivation of the relaxation and bbp updates are also analogous figure a shows the estimate of motion based solely on local information the estimates are wrong due to the aperture problem figure b shows the performance of three propagation schemes gradient descent sor and bbp gradient descent converges so slowly that the improvement in its estimate can not be discerned in the plot sor converges much faster than gradient descent but still has significant error after iterations bbp gets the correct estimate after iterations here and in all subsequent plots an iteration refers to one update of all units in the network this is due to the fact that after iterations the estimate at location k is the optimal one given data in the interval k k in this case there is enough data in every such interval along the contour to correctly estimate the motion figure c shows the estimate produced by sor after iterations even with simple visual inspection it is evident that the estimate is quite wrong figure d shows the correct estimate produced by bbp after iterations direction of figure propagation the extracted contour in figure bounds a dark and a light region direction of figure dof eg refers to which of these two regions is figure and which is ground a local cue for dof is convexity given three neighboring points along the contour we prefer the dof that makes the angle defined by those points acute interpreting images by propagating bayesian beliefs a b figure a local estimate of dof along the contour b performance of hopfieldgradient descent relaxation labeling and bbp as a function of time bbp is the only method that converges to the global minimum c dof estimate of hopfield net after convergence d dof estimate of bbp after convergence rather than obtuse figure a shows the results of using this local cue on the hand contour the local cue is not sufficient we can overcome the local ambiguity by minimizing a cost functional that takes into account the dof at neighboring points in addition to the local convexity denote by lrn the dof at point k along the contour and define jl e vml mx z ylmlxm m k rn k with vm determined by the acuteness of the angle at location k figure b shows the performance of four propagation algorithms on this task three traditional relaxation labeling algorithms mf hop field rosenfeld et al constrained gradient descent and bbp all three traditional algorithms converge to a local minimum while the bbp converges to the global minimum figure c shows the local minimum reached by the hop field network and figure d shows the correct solution reached by the bbp algorithm recall section that bbp is guaranteed to converge to the correct posterior given all the data extensions to d in the previous two examples ambiguity was reduced by combining information from other points on the same contour there exist however cases when information should be propagated to all points in the image unfortunately such propagation problems correspond to markov random field mrf generative models for which calculation of the posterior cannot be done efficiently however willsky and his y weiss colleagues have recently shown that mrfs can be approximated with hierarchical or multi resolution models in current work we have been using the multi resolution generative model to derive local bbp rules in this case the bayesian beliefs are propagated between neighboring units in a pyramidal representation of the image although this work is still in preliminary stages we find encouraging results in comparison with traditional d relaxation schemes discussion the update rules in equations differ slightly from those derived by pearl in that the quantities c are conditional probabilities and hence are constantly normalized to sum to unity using pearls original algorithm for sequences as long as the ones we are considering will lead to messages that become vanishingly small likewise our update rules differ slightly from the forward backward algorithm for hmms in that ours are based on the assumption that all states are equally likely apriore and hence the updates are symmetric in c and finally equation can be seen as a variant of a riccati equation in addition to these minor notational differences the context in which we use the update rules is different while in hmms and kalman filters the updates are seen as interim calculations toward calculating the posterior we use these updates in a parallel network of local units and are interested in how the estimates of units in this network improve as a function of iteration we have shown that an architecture that propagates bayesian beliefs according to the probability calculus yields orders of magnitude improvements in convergence over traditional schemes that do not propagate probabilities thus image interpretation provides an important example of a task where it pays to be a bayesian acknowledgments i thank e adelson p dayan j tenenbaum and g galperin for comments on versions of this manuscript mi jordan for stimulating discussions and for introducing me to bayesian nets supported by a training grant from nigms references arthur gelb editor applied optimal estimation mit press e c hildreth the measurement of visual motion mit press sz li markov random field modeling in computer vision springer verlag mark r luettgen w clem karl and allan s willsky efficient multiscale regularization with application to the computation of optical flow ieee transactions on image processing d mart vision h freeman and co judea pearl probabilistic reasoning in intelligent systems networks of plausible inference morgan kaufmann lawrence rabiner and biing hwang juang fundamentals of speech recognition ptr prentice hall a rosenfeld r hummel and s zucker scene labeling by relaxation operations ieee transactions on systems man and cybernetics p sajda and l h finkel intermediate level visual representations and the construction of surface perception journal of cognitive neuroscience gilbert strang
it has been suggested that long range intrinsic connections in striate cortex may play a role in contour extraction gilbert et al a number of rent physiological and psychophysical studies have examined the possible role of long range connections in the modulation of contrast detection thresholds polat and sagi kapadia et al kovfics and julesz and various pre attentive detection tasks kovfics and julesz field et al we have developed a network architture based on the anatomical connectivity of striate cortex as well as the temporal dynamics of neuronal processing that is able to reproduce the observed experimental results the network has been tested on real images and has applications in terms of identifying salient contours in automatic image processing systems
we present an algorithm for identifying linear patterns on a twodimensional lattice based on the concept of an orientation selective cell a concept borrowed from neurobiology of vision constructing a multi layered neural network with fixed architecture which implements orientation selectivity we define output elements corresponding to different orientations which allow us to make a selection decision the algorithm takes into account the granularity of the lattice as well as the presence of noise and inefficiencies the method is applied to a sample of data collected with the zeus detector at hera in order to identify cosmic muons that leave a linear pattern of signals in the segmented calorimeter a two dimensional representation of the relevant part of the detector is used the algorithm performs very well given its architecture this system becomes a good candidate for fast pattern recognition in parallel processing devices i
this paper presents a method that decides which combinations of traffic can be accepted on a packet data link so that quality of service qos constraints can be met the method uses samples of qos results at different load conditions to build a neural network decision function previous similar approaches to the problem have a significant bias this bias is likely to occur in any real system and results in accepting loads that miss qos targets by orders of magnitude preprocessing the data to either remove the bias or provide a confidence level the method was applied to sources based on difficult to analyze ethernet data traces with this data the method produces an accurate access control function that dramatically outperforms analytic alternatives interestingly the results depend on throwing away more than of the data

artificial neural networks can be used to predict future returns of stocks in order to take financial decisions should one build a separate network for each stock or share the same network for all the stocks in this paper we also explore other alternatives in which some layers are shared and others are not shared when the prediction of future returns for different stocks are viewed as different tasks sharing some parameters across stocks is a form of multi task learning in a series of experiments with canadian stocks we obtain yearly returns that are more than above various benchmarks i

this paper shows how the prices of option contracts traded in financial markets can be tracked sequentially by means of the extended kalman filter algorithm i consider call and put option pairs with identical strike price and time of maturity as a two output nonlinear system the black scholes approach popular in finance literature and the radial basis functions neural network are used in modelling the nonlinear system generating these observations i show how both these systems may be identified recursively using the ekf algorithm i present results of simulations on some ftse index options data and discuss the implications of viewing the pricing problem in this sequential manner
epidemiological data is traditionally analyzed with very simple techniques flexible models such as neural networks have the potential to discover unanticipated features in the data however to be useful flexible models must have effective control on overfitting this paper reports on a comparative study of the predictive quality of neural networks and other flexible models applied to real and artificial epidemiological data the results suggest that there are no major unanticipated complex features in the real data and also demonstrate that mackays bayesian neural network methodology provides effective control on overfitting while retaining the ability to discover complex features in the artificial data i

the mortality related to cervical cancer can be substantially reduced through early detection and treatment however current detection techniques such as pap smear and colposcopy fail to achieve a concurrently high sensitivity and specificity in vivo fluorescence spectroscopy is a technique which quickly noninvasively and quantitatively probes the biochemical and morphological changes that occur in pre cancerous tissue rbf ensemble algorithms based on such spectra provide automated and near realtime implementation of pre cancer detection in the hands of nonexperts the results are more reliable direct and accurate than those achieved by either human experts or multivariate statistical algorithms i
we present a mixture of experts me approach to interpolate sparse spatially correlated earth science data kriging is an interpolation method which uses a global covariation model estimated from the data to take account of the spatial dependence in the data based on the close relationship between kriging and the radial basis function rbf network wan bone we use a mixture of generalized rbf networks to partition the input space into statistically correlated regions and learn the local covariation model of the data in each region applying the me approach to simulated and real world data we show that it is able to achieve good partitioning of the input space learn the local covariation models and improve generalization
high frequency foreign exchange data can be decomposed into three components the inventory effect component the surprise information news component and the regular information component the presence of the inventory effect and news can make analysis of trends due to the diffusion of information regular information component difficult we propose a neural net based independent component analysis to separate high frequency foreign exchange data into these three components our empirical results show that our proposed multi effect decomposition can reveal the intrinsic price behavior
dynamic programming q learning and other discrete markov decision process solvers can be applied to continuous d dimensional state spaces by quantizing the state space into an array of boxes this is often problematic above two dimensions a coarse quantization can lead to poor policies and fine quantization is too expensive possible solutions are variable resolution discretization or function approximation by neural nets a third option which has been little studied in the reintbrcement learning literature is interpolation on a coarse grid in this paper we study interpolation techniques that can result in vast improvements in the online behavior of the resulting control systems multilinear interpolation and an interpolation algorithm based on an interesting regular triangulation of d dimensional space we adapt these interpolators under three reinforcement learning paradigms i offline value iteration with a known model ii q learning and iii online value iteration with a previously unknown model learned from data we describe empirical results and the resulting implications for practical learning of continuous non linear dynamic control grid based interpolation techniques reinforcement learning algorithms generate functions that map states to cost togo values when dealing with continuous state spaces these functions must be approximated the following approximators are frequently used fine grids may be used in one or two dimensions above two dimensions fine grids are too expensive value functions can be discontinuous which as we will see can lead to suboptimalities even with very fine discretization in two dimensions neural nets have been used in conjunction with td sutton and q learning watkins in very high dimensional spaces tesauro crites and barto while promising it is not always clear that they produce the accurate value functions that might be needed for fine nearoptimal control of dynamic systems and the most commonly used methods of applying value iteration or policy iteration with a neural net value function are often unstable boyan and moore s davies interpolation over points on a coarse grid is another potentially useful approximator for value functions that has been little studied for reinforcement learning this paper attempts to rectify this omission interpolation schemes may be particularly attractive because they are local averagers and convergence has been proven in such cases for offline value iteration gordon all of the interpolation methods discussed here split the state space into a regular grid of d dimensional boxes data points are associated with the centers or the corners of the resulting boxes the value at a given point in the continuous state space is computed as a weighted average of neighboring data points multilinear interpolation when using multilinear interpolation data points are situated at the corners of the grids boxes the interpolated value within a box is an appropriately weighted average of the d datapoints on that boxs corners the weighting scheme assures global continuity of the interpolated surface and also guarantees that the interpolated value at any grid corner matches the given value of that corner in one dimensional space multilinear interpolation simply involves piecewise linear interpolations between the data points in a higher dimensional space a recursive though not terribly efficient implementation can be described as follows pick an arbitrary axis project the query point along this axis to each of the two opposite faces of the box containing the query point use two d dimensional multihnear interpolations over the datapoints on each of these two faces to calculate the values at both of these projected points linearly interpolate between the two values generated in the previous step multilinear interpolation processes d data points for every query which becomes prohibitively expensive as d increases simplex based interpolation it is possible to interpolate over d of the data points for any given query in only odlog d time and still achieve a continuous surface that fits the datapoints exactly each box is broken into d hyperdimensional triangles or simplexes according to the coxeter freudenthal kuhn triangulation moore assume that the box is the unit hypercube with one corner at xx x xd and the diagonally opposite corner at then each simplex in the kuhn triangulation corresponds to one possible permutation p of d and occupies the set of points satisfying the equation xpx xp xpd i triangulating each box into d simplexes in this manner generates a conformal mesh any two elements with a d dimensional surface in common have entire faces in common which ensures continuity across element boundaries when interpolating we use the kuhn triangulation for interpolation as follows translate and scale to a coordinate system in which the box containing the query point is the unit hypercube let the new coordinate of the query point be as as this tells us the simplex of the use a sorting algorithm to rank as through as kuhn triangulation in which the query point hes triangulation and interpolation for reinforcement learning express cc as a convex combination of the coordinates of the relevant simplexs d corners use the coefficients determined in the previous step as the weights for a weighted sum of the data values stored at the corresponding corners at no point do we explicitly represent the d different simplexes all of the above steps can be performed in od time except the second which can be done in od log d time using conventional sorting routines problem domains car on hill in the hillcar domain the goal is to park a car near the top of a one dimensional hill the hill is steep enough that the driver needs to back up in order to gather enough speed to get to the goal the state space is two dimensional positionvelocity see moore and atkeson for further details but note that our formulation is harder than the usual formulation in that the goal region is restricted to a narrow range of velocities around and trials start at random states the task is specified by a reward of for any action taken outside the goal region and inside the goal no discounting is used and two actions are available maximum thrust backwards and maximum thrust forwards acrobot the acrobot is a two link planar robot acting in the vertical plane under gravity with a weak actuator at its elbow joint joint the shoulder is unactuated the goal is to raise the hand to at least one links height above the unactuated pivot sutton the state space is four dimensional two angular positions and two angular velocities trials always start from a stationary position hanging straight down this task is formulated in the same way as the car on thehill the only actions allowed are the two extreme elbow torques applying interpolation three cases case i offline value iteration with a known model first we precalculate the effect of taking each possible action from each state corresponding to a datapoint in the grid then as suggested in gordon we use these calculations to derive a completely discrete mdp taking any action from any state in this mdp results in c possible successor states where c is the number of datapoints used per interpolation without interpolation c is with multilinear interpolation d with simplex based interpolation d we calculate the optimal policy for this derived mdp offline using value iteration ross because the value iteration can be performed on a completely discrete mdp the calculations are much less computationally expensive than they would have been with many other kinds of function approximators the value iteration gives us values for the datapoints of our grid which we may then use to interpolate the values at other states during online control hillcar results value iteration with known model we tested the two interpolation methods on a variety of quantization levels by first performing value iteration offline and then starting the car from random states and averaging the number of steps taken to the goal from those states we also recorded the number of backups required before convergence as well as the execution time required for the entire value iteration on a mhz sparc see figure for the results all steps to goal values are means with an expected error of steps s davies grid size interpolation method none steps to goal backups k k k m time sec multilin steps to goal backups k k k m time sec simplex steps to goal backups k k k m time sec figure hillcar value iteration with known model grid size interpolation method none steps to goal backups k k m time sec muitilin steps to goal backups k m k m m m m m time sec simplex steps to goal backups k m k m m m m m time sec figure acrobot value iteration with known model the interpolated functions require more backups for convergence but this is amply compensated by dramatic improvement in the policy surprisingly both interpolation methods provide improvements even at extremely high grid resolutions the noninterpolated grid with datapoints along each axis fared no better than the interpolated grids with only datapoints along each axis acrobot results value iteration with known model we used the same value iteration algorithm in the acrobot domain in this case our test trials always began from the same start state but we ran tests for a larger set of grid sizes figure grids with different resolutions place grid cell boundaries at different locations and these boundary locations appear to be important in this problem the performance varies unpredictably as the grid resolution changes however in all cases interpolation was necessary to arrive at a satisfactory solution without interpolation the value iteration often failed to converge at all with relatively coarse grids it may be that any trajectory to the goal passes through some grid box more than once which would immediately spell disaster for any algorithm associating a constant value over that entire grid box controllers using multilinear interpolation consistently fared better than those employing the simplex based interpolation the smoother value function provided by multilinear interpolation seems to help however value iteration with the simplexbased interpolation was about twice as fast as that with multilinear interpolation in higher dimensions this speed ratio will increase triangulation and interpolation for reinforcement learning case ii q learning under a second reinforcement learning paradigm we do not use any model rather we learn a q function that directly maps state action pairs to long term rewards watkins does interpolation help here too in this implementation we encourage exploration by optimistically initializing the q function to zero everywhere after travelling a sufficient distance from our last decision point we perform a single backup by changing the grid point values according to a perceptron like update rule and then we greedily select the action for which the interpolated q function is highest at the current state hillcar results q learning we used q learning with a grid size of figure shows learning curves for three learners using the three different interpolation techniques both interpolation methods provided a significant improvement in both initial and final online performance the learner without interpolation achieved a final average performance of about steps to the goal with multilinear interpolation with simplex based interpolation note that these are all significant improvements over the corresponding results for offiine value iteration with a known model inaccuracies in the interpolated functions often cause controllers to enter cycles because the q learning backups are being performed online however the q learning controller can escape from these control cycles by depressing the q values in the vicinities of such cycles acrobot results q learning we used the same algorithms on the acrobot domain with a grid size of results are shown in figure ooooo no intpllatltn o z o figure left cumulative performance of q learning hillcar on an grid multilinear interpolation comes out on top no interpolation on the bottom right q learning acrobot on a grid the two interpolations come out on top with nearly identical performance for each learner the y axis shows the sum of rewards for all trials to date the better the average performance the shallower the gradient gradients are always negative because each state transition before reaching the goal results in a reward of both q learners using interpolation improved rapidly and eventually reached the goal in a relatively small number of steps per trial the learner using multilinear interpolation eventually achieved an average of steps to the goal per trial the learner using simplex based interpolation achieved steps per trial on the other hand the learner not using any interpolation fared much worse taking s davies an average of more than steps per trial a controller that chooses actions randomly typically takes about the same number of steps to reach the goal simplex based interpolation provided on line performance very close to that provided by multilinear interpolation but at roughly half the computational cost case iii value iteration with model learning here we use a mode of the system but we do not assume that we have one to start with instead we earn a model of the system as we interact with it we assume this model is adequate and calculate a value function via the same algorithms we would use if we knew the true mode this approach may be particularly beneficial for tasks in which data is expensive and computation is cheap here models are learned using very simple grid based function approximators without interpolation for both the reward and transition functions of the model the same grid resolution is used for the value function grid and the model approximator we strongly encourage exploration by initializing the model so that every state is initially assumed to be an absorbing state with zero reward while making transitions through the state space we update the model and use prioritized sweeping moore and atkeson to concentrate backups on relevant parts of the state space we also occasionally stop to recalculate the effects of all actions under the updated model and then run value iteration to convergence as this is fairly time consuming it is done rather rarely we rely on the updates performed by prioritized sweeping to guide the system in the meantime figure left cumulative performance model learning on hillcar with a grid right acrobot with a grid in both cases multilinear interpolation comes out on top while no interpolation winds up on the bottom hillcar results value iteration with learned model we used the algorithm described above with an by grid an average of about two prioritized sweeping backups were performed per transition the complete recalculations were performed every steps throughout the first two trims and every steps thereafter figure shows the results for the first trials over the first trims the learner using simplex based interpolation didnt fare much better than the learner using no interpolation however its performance on trims not shown was close to that of the learner using multilinear interpolation taking an average of steps to the goal per trim while the learner using multilinear interpolation took the learner using no interpolation did significantly worse than the others in these later trials taking steps per trial triangulation and interpolation for reinforcement learning the model learners performance improved more quickly than the q learners over the first few trials on the other hand their final performance was significantly worse that the q learners acrobot results value iteration with learned model we used the same algorithm with a grid on the acrobot domain this time performing the complete recalculations every steps through the first two trials and every thereafter figure shows the results in this case the learner using no interpolation took so much time per trial that the experiment was aborted early after trials it was still taking an average of more than steps to reach the goal the learners using interpolation however fared much better the learner using multilinear interpolation converged to a solution taking steps per trial the learner using simplex based interpolation averaged about steps again as the graphs show these three learners initially improve significantly faster than did the q learners using similar grid sizes conclusions we have shown how two interpolation schemes one based on a weighted average of the d points in a square cell the other on a ddimensional triangulation may be used in three reinforcement learning paradigms optimal policy computation with a known model q learning and online value iteration while learning a model in each case our empirical studies demonstrate interpolation resoundingly decreasing the quantization level necessary for a satisfactory solution future extensions of this research will explore the use of variable resolution grids and triangulations multiple low dimensional interpolations in place of one high dimension interpolation in a manner reminiscent of cmac albus memory based approximators and more intelligent exploration this research was funded in part by a national science foundation graduate fellowship to scott davies and a research initiation award to andrew moore references albus j s albus brains behawour and robotics byte books mcgraw hill boyan and moore j a boyan and a w moore generalization in reinforcement learning safely approximating the value function in neural information processing systems crites and barto r h crites and a g barto improving elevator performance using reinforcement learning in d touretzky m mozer and m hasselmo editors neural information processing systems gordon g gordon stable function approximation in dynamic programming in proceedings of the th international conference on machine learning morgan kaufmann june moore and atkeson a w moore and c g atkeson prioritized sweeping reinforcement learning with less data and less real time machane learning moore and atkeson a w moore and c g atkeson the parti game algorithm for variable resolution reinforcement learning in multidimensional state spaces machine learning moore d w moore simplical mesh generation with applications phd thesis report no cornell university ross s ross
a new reinforcement learning architecture for nonlinear control is proposed a direct feedback controller or the actor is trained by a value gradient based controller or the tutor this architecture enables both efficient use of the value function and simple computation for real time implementation good performance was verified in multi dimensional nonlinear control tasks using gaussian softmax networks i
in general procedures for determining bayes optimal adaptive controls for markov decision processes mdps require a prohibitive amount of computation the optimal learning problem is intractable this paper proposes an approximate approach in which bandit processes are used to model in a certain local sense a given mdp bandit processes constitute an important subclass of mdps and have optimal learning strategies defined in terms of gittins indices that can be computed relatively efficiently thus one scheme for achieving approximately optimal learning for general mdps proceeds by taking actions suggested by strategies that are optimal with respect to local bandit models
closed loop control relies on sensory feedback that is usually assumed to be free but if sensing incurs a cost it may be costeffective to take sequences of actions in open loop mode we describe a reinforcement learning algorithm that learns to combine open loop and closed loop control when sensing incurs a cost although we assume reliable sensors use of open loop control means that actions must sometimes be taken when the current state of the controlled system is uncertain this is a special case of the hidden state problem in reinforcement learning and to cope our algorithm relies on short term memory the main result of the paper is a rule that significantly limits exploration of possible memory states by pruning memory states for which the estimated value of information is greater than its cost we prove that this rule allows convergence to an optimal policy i
reinforcement learning methods for discrete and semi markov decision problems such as real time dynamic programming can be generalized for controlled diffusion processes the optimal control problem reduces to a boundary value problem for a fully nonlinear second order elliptic differential equation of hamiltonjacobi bellman hjb type numerical analysis provides multigrid methods for this kind of equation in the case of learning control however the systems of equations on the various grid levels are obtained using observed information transitions and local cost to ensure consistency special attention needs to be directed toward the type of time and space discretization during the observation an algorithm for multi grid observation is proposed the multi grid algorithm is demonstrated on a simple queuing problem
by now it is widely accepted that learning a task from scratch ie without any prior knowledge is a daunting undertaking humans however rarely attempt to learn from scratch they extract initial biases as well as strategies how to approach a learning problem from instructions andor demonstrations of other humans for learning control this paper investigates how learning from demonstration can be applied in the context of reinforcement learning we consider priming the q function the value function the policy and the model of the task dynamics as possible areas where demonstrations can speed up learning in general nonlinear learning problems only model based reinforcement learning shows significant speed up after a demonstration while in the special case of linear quadratic regulator lqr problems all methods profit from the demonstration in an implementation of pole balancing on a complex anthropomorphic robot arm we demonstrate that when facing the complexities of real signal processing model based reinforcement learning offers the most robustness for lqr problems using the suggested methods the robot learns pole balancing in just a single trial after a second long demonstration of the human instructor
model learning combined with dynamic programming has been shown to be effective for learning control of continuous state dynamic systems the simplest method assumes the learned model is correct and applies dynamic programming to it but many approximators provide uncertainty estimates on the fit how can they be exploited this paper addresses the case where the system must be prevented from having catastrophic failures during learning we propose a new algorithm adapted from the dual control literature and use bayesian locally weighted regression models with dynamic programming a common reinforcement learning assumption is that aggressive exploration should be encouraged this paper addresses the converse case in which the system has to reign in exploration the algorithm is illustrated on a dimensional simulated control problem i
we have calculated analytical expressions for how the bias and variance of the estimators provided by various temporal difference value estimation algorithms change with offline updates over trials in absorbing markov chains using lookup table representations we illustrate classes of learning curve behavior in various chains and show the manner in which td is sensitive to the choice of its stepsize and eligibility trace parameters i
probability models can be used to predict outcomes and compensate for missing data but even a perfect model cannot be used to make decisions unless the utility of the outcomes or preferences between them are also provided this arises in many real world problems such as medical diagnosis where the cost of the test as well as the expected improvement in the outcome must be considered relatively little work has been done on learning the utilities of outcomes for optimal decision making in this paper we show how temporal difference reinforcement learning td can be used to determine decision theoretic utilities within the context of a mixture model and apply this new approach to a problem in medical diagnosis tdq learning of utilities reduces the number of tests that have to be done to achieve the same level of performance compared with the probability model alone which results in significant cost savings and increased efficiency
we present a monte carlo simulation algorithm for real time policy improvement of an adaptive controller in the monte carlo simulation the long term expected reward of each possible action is statistically measured using the initial policy to make decisions in each step of the simulation the action maximizing the measured expected reward is then taken resulting in an improved policy our algorithm is easily parallelizable and has been implemented on the ibm sp and sp parallel risc supercomputers we have obtained promising initial results in applying this algorithm to the domain of backgammon results are reported for a wide variety of initial policies ranging from a random policy to td gammon an extremely strong multi layer neural network in each case the monte carlo algorithm gives a substantial reduction by as much as a factor of or more in the error rate of the base players the algorithm is also potentially useful in many other adaptive control applications in which it is possible to simulate the environment i
we present new results about the temporal difference learning algorithm as applied to approximating the cost to go function of a markov chain using linear function approximators the algorithm we analyze performs on line updating of a parameter vector during a single endless trajectory of an aperiodic irreducible finite state markov chain results include convergence with probability a characterization of the limit of convergence and a bound on the resulting approximation error in addition to establishing new and stronger results than those previously available our analysis is based on a new line of reasoning that provides new intuition about the dynamics of temporal difference learning furthermore we discuss the implications of two counter examples with regards to the significance of on line updating and linearly parameterized function approximators
we propose and analyze an algorithm that approximates solutions to the problem of optimal stopping in a discounted irreducible aperiodic markov chain the scheme involves the use of linear combinations of fixed basis functions to approximate a q function the weights of the linear combination are incrementally updated through an iterative process similar to q learning involving simulation of the underlying markov chain due to space limitations we only provide an overview of a proof of convergence with probability and bounds on the approximation error this is the first theoretical result that establishes the soundness of a q learninglike algorithm when combined with arbitrary linear function approximators to solve a sequential decision problem though this paper focuses on the case of finite state spaces the results extend naturally to continuous and unbounded state spaces which are addressed in a forthcoming full length paper i
we have developed a neural network architecture that implements a theory of attention learning and trans cortical communication based on adaptive synchronization of hz and hz oscillations between cortical areas here we present a specific higher order cortical model of attentional networks rhythmic expectancy and the interaction of hlgherorder and primary cortical levels of processing it accounts for the mismatch negativity of the auditory erp and the results of psychological experiments of jones showing that auditory stream segregation depends on the rhythmic structure of inputs the timing mechanisms of the model allow us to explain how relative timing information such as the relative order of events between streams is lost when streams are formed the model suggests how the theories of auditory perception and attention of jones andbregman may be reconciled
a novel neural network model of pre attention processing in visualsearch tasks is presented using displays of line orientations taken from wolfes experiments we study the hypothesis that the distinction between parallel versus serial processes arises from the availability of global information in the internal representations of the visual scene the model operates in two phases first the visual displays are compressed via principal component analysis second the compressed data is processed by a target detector module in order to identify the existence of a target in the display our main finding is that targets in displays which were found experimentally to be processed in parallel can be detected by the system while targets in experimentally serial displays cannot this fundamental difference is explained via variance analysis of the compressed representations providing a numerical criterion distinguishing parallel from serial displays our model yields a mapping of response time slopes that is similar to duncan and humphreyss search surface providing an explicit formulation of their intuitive notion of feature similarity it presents a neural realization of the processing that may underlie the classical metaphorical explanations of visual search on parallel versus serial processing a computational study of visual search



human subjects are known to adapt their motor behavior to a shift of the visual field brought about by wearing prism glasses over their eyes we have studied the analog of this effect in speech using a device that can feed back transformed speech signals in real time we exposed subjects to alterations of their own speech feedback we found that speakers learn to adjust their production of a vowel to compensate for feedback alterations that change the vowels perceived phonetic identity moreover the effect generalizes across consonant contexts and to different vowels
singular value decomposition svd can be viewed as a method for unsupervised training of a network that associates two classes of events reciprocally by linear connections through a single hidden layer svd was used to learn and represent relations among very large numbers of words k k and very large numbers of natural text passages lkk in which they occurred the result was dimensional semantic spaces in which any trained or newly added word or passage could be represented as a vector and similarities were measured by the cosine of the contained angle between vectors good accuracy in simulating human judgments and behaviors has been demonstrated by performance on multiple choice vocabulary and domain knowledge tests emulation of expert essay evaluations and in several other ways examples are also given of how the kind of knowledge extracted by this method can be applied
motivated by the findings of modular structure in the association cortex we study a multi modular model of associative memory that can successfully store memory patterns with different levels of activity we show that the segregation of synaptic conductances into intra modular linear and inter modular nonlinear ones considerably enhances the networks memory retrieval performance compared with the conventional single module associative memory network the multi modular network has two main advantages it is less susceptible to damage to columnar input and its response is consistent with the cognitive data pertaining to category specific impairment i
dual route and connectionist single route models of reading have been at odds over claims as to the correct explanation of the reading process recent dual route models predict that subjects should show an increased naming latency for irregular words when the irregularity is earlier in the word eg chef is slower than glow a prediction that has been confirmed in human experiments since this would appear to be an effect of the left to right reading process coltheart rastle claim that single route parallel connectionist models cannot account for it a refutation of this claim is presented here consisting of network models which do show the interaction along with orthographic neighborhood statistics that explain the effect i
accounts of neurological disorders often posit damage to a specific functional pathway of the brain farah has proposed an alternative class of explanations involving partial damage to multiple pathways we explore this explanation for optic aphasia a disorder in which severe performance deficits are observed when patients are asked to name visually presented objects but surprisingly performance is relatively normal on naming objects from auditory cues and on gesturing the appropriate use of visually presented objects we model this highly specific deficit through partial damage to two pathways one that maps visual input to semantics and the other that maps semantics to naming responses the effect of this damage is superadditive meaning that tasks which require one pathway or the other show little or no performance deficit but the damage is manifested when a task requires both pathways ie naming visually presented objects our model explains other phenomena associated with optic aphasia and makes testable experimental predictions neuropsychology is the study of disrupted cognition resulting from damage to functional systems in the brain generally accounts of neuropsychological disorders posit damage to a particular functional system or a disconnection between systems farah suggested an alternative class of explanations for neuropsychological disorders partial damage to multiple systems which is manifested through interactions among the loci of damage we explore this explanation for the neuropsychological disorder of optic aphasia optic aphasia arising from unilateral left posterior lesions including occipital cortex and the splenium of the corpus callosum schnider benson scharre is marked by a deficit in naming visually presented objects hereafter referred to as visual naming farah however patients can demonstrate recognition of visually presented objects nonverbally for example by gesturing the appropriate use of an object or sorting visual items into their proper superordinate categories hereafter visual gesturing patients can also name objects by nonvisual cues such as a verbal definition or typical sounds made by the objects hereafter auditory naming the highly specific nature of the deficit rules out an explanation in terms of damage to a single pathway in a standard model of visual naming figure suggesting that a more complex model is required involving a superadditiveimpairment theory of optic aphasia figure a standard box and arrow model of visual naming the boxes denote levels of representation and the arrows denote pathways mapping from one level of representation to another although optic aphasia cannot be explained by damage to the vision to semantics pathway or the semantics to naming pathway farah proposed an explanation in terms of partial damage to both pathways the xs i semantic i multiple semantic systems or multiple pathways to visual naming however a more parsimonious account is suggested by farah optic aphasia might arise from partial lesions to two pathways in the standard model those connecting visual input to semantics and semantics to naming and the effect of damage to these pathways is superadditive meaning that tasks which require only one of these pathways eg visual gesturing or auditory naming will be relatively unimpaired whereas tasks requiring both pathways eg visual naming will show a significant deficit a model of superadditive impairments we present a computational model of the superadditive impairment theory of optic aphasia by elaborating the architecture of figure the architecture has four pathways visual input to semantics v s auditory input to semantics a s semantics to naming s n and semantics to gesturing s g each pathway acts as an associative memory the critical property of a pathway that is required to explain optic aphasia is a speed accuracy trade off the initial output of a pathway appears rapidly but it may be inaccurate this quick and dirty guess is refined over time and the pathway output asymptotically converges on the best interpretation of the input we implement a pathway using the architecture suggested by mathis and mozer in this architecture inputs are mapped to their best interpretations by means of a two stage process figure first a quick one shot mapping is performed by a multilayer feedforward connectionist network to transform the input directly to its corresponding output this is followed by a slower iterative clean up process carried out by a recurrent attractor network this architecture shows a speed accuracy trade off by virtue of the assumption that the feedforward mapping network does not have the capacity to produce exactly the right output to every input especially when the inputs are corrupted by noise or are otherwise incomplete consequently the clean up stage is required to produce a sensible interpretation of the noisy output of the mapping network fully distributed attractor networks have been used for similar purposes eg plaut shallice for simplicity we adopt a localist attractor network with a layer of state units and a layer of radial basis function rbf units one rbf unit per attractor each rbf or attractor unit measures the distance of the current state to the attractor that it represents the activity of attractor unit i a i is figure connectionist implementation of a processing pathway the pathway consists of feedforward mapping network followed by a recurrent cleanup or attractor network circles denote connectionist processing units and arrows denote connections between units or between layers of units way output clean up network mapping network pathway input m c mozer m sitton and m farah it exp llst ll ait ait where s is the state unit activity vector at time t gi is the vector denoting the location of attractor i and i is the strength of the attractor the strength determines the region of the state space over which an attractor will exert its pull and also the rate at which the state vill converge to the attractor the state units receive input from the mapping network and from the attractor units and are updated as follows sit diteit ditajt gji j where sio is the activity of state unit i at time t e i is the ith output of the mapping net is the ith element of attractor j and dis given by dit hi bit j where h is a linear threshold function that bounds activity between and b i is a weighted time average of the ith output of the mapping net it cteit ctit in all simulations ct the activity of the state units are governed by two forces the external input from the feedforward net first term in equation and the attractor unit activities second term the parameter di acts as a kind of attentional mechanism that modulates the relative influence of these two forces the basic idea is that when the input coming from the mapping net is changing the system should be responsive to the input and should not yet be concerned with interpreting the input in this case the input is copied straight through to the state units and hence di should have a value close to when the input begins to stabilize however the focus shifts to interpreting the input and following the dynamics of the attractor network this shift corresponds to d i being lowered to zero the weighted time average in the update rule for di is what allows for the smooth transition of the function to its new value for certain constructions of the function d zemel and mozer in preparation have proven convergence of the algorithm to an attractor apart from speed accuracy trade off these dynamics have another important consequence for the present model particularly with respect to cascading pathways if pathway feeds into pathway such as v s feeding into s on then the state unit activities of act as the input to because these activities change over time as the state approaches a well formed state the dynamics of pathway can be quite complex as it is forced to deal with an unstable input this property is important in explaining several phenomena associated with optic aphasia pattern generation patterns were constructed for each of the five representational spaces visual and auditory input semantic name and gesture responses each representational space was arbitrarily made to be dimensional we generated binary valued patterns in each space which were meant to correspond to known entities of that representational domain for the visual auditory and semantic spaces patterns were partitioned into similarity clusters with siblings per cluster patterns were chosen randomly subject to two constraints patterns in different clusters had to be at least apart and siblings had to be between and apart because similarity of patterns in the name and gesture spaces was irrelevant to our modeling we did not impose a similarity structure on these spaces a superadditive impairment theory of optic aphasia instead we generated patterns in these spaces at random subject to the constraint that every pattern had to be at least from every other after generating patterns in each of the representational spaces we established arbitrary correspondences among the patterns such that visual pattern n auditory pattern n semantic pattern n name pattern n and gesture pattern n all represented the same concept that is the appropriate response in a visual naming task to visual pattern n would be semantic pattern n and name pattern n training procedure the feedforward networks in the four pathways v s a s s n and s g were independently trained on all associations using back propagation each of these networks contained a single hidden layer of units and all units in the network used the symmetric activation function to give activities in the range the amount of training was chosen such that performance on the training examples was not perfect usually several elements in the output would be erroneous ie have the wrong sign and others would not be exactly correct ie or this was done to embody the architectural assumption that the feedforward net does not have the capacity to map every input to exactly the right output and hence the clean up process is required training was not required for the clean up network due to the ocalist representation of attractors in the clean up network it was trivial to hand wire each clean up net with the attractors for its domain along with one rest state attractor all attractor strengths were initialized to the same value except the rest state attractor for which the rest state attractor required a lower strength so that even a weak external input would be sufficient to kick the attractor network out of the rest state simulation methodology after each pathway had been trained the model was damaged by lesioning or removing a fraction of the connections in the v s and s n mapping networks the lesioned connections were chosen at random and an equal fraction was removed from the two pathways the clean up nets were not damaged the architecture was damaged a total of different times creating simulated patients who were tested on each of the four tasks and on all input patterns for a task the results we report come from averaging across simulated patients and input patterns responses were determined after the system had been given sufficient time to relax into a name or gesture attractor which was taken to be the response each response was classified as one of the following mutually exclusive response types correct perseveration response is the same as that produced on any of the three immediately preceding trials visual error the visual pattern corresponding to the incorrect response is a sibling of the visual pattern corresponding to the correct response semantic error visualsemantic error or other error priming mechanism priming the increased availability of recently experienced stimuli has been found across a wide variety of tasks in normal subjects we included priming in our model as a strengthening increasing the i parameter of recently visited attractors see mathis mozer for details and becker behrmann moscovitch for a related approach in the damaged model this mechanism often gave rise to perseverations results we have examined the models behavior as we varied the amount of damage quantified by the parameter y however we report on the performance of simulated patients with y this intermediate amount of damage yielded no floor or ceiling effects and also produced error rates for the visual naming task in the range of roughly the median performance of patients in the literature m c mozer m sitton and m farah table error rate of the damaged model on various tasks task error rate auditory gesturing auditory naming visual gesturing visual naming table presents the error rates of the model on four tasks the pattern of errors shows a qualitative fit to human patient data the model produced no errors on the auditory gesturing task because the two component pathways a s and s g were undamaged relatively few errors were made on the auditory naming and visual gesturing tasks each of which involved one damaged pathway because the clean up nets were able to compensate for the damage however the error rate for the visual naming task was quite large due to damage on both of its component pathways v s and s n the error rate for visual naming cannot be accounted for by summing the effects of the damage to the two component pathways because the sum of the error rates for auditory naming and visual gesturing each of which involves one of the two partially damaged pathways is nearly four times smaller rather the effects of damage on these pathways interact and their interaction leads to superadditive impairments when a visual pattern is presented to the model it is mapped by the damaged v s pathway into a corrupted semantic representation which is then cleaned up while the corruption is sufficiently minor that clean up will eventually succeed the clean up process is slowed considerably by the corruption during the period of time in which the semantic clean up network is searching for the correct attractor the corrupted semantic representation is nonetheless fed into the damaged s n pathway the combined effect of the initially noisy semantic representation serving as input to a damaged pathway leads to corruption of the naming representation past the point where it can be cleaned up properly interactions in the architecture are inevitable and are not merely a consequence of some arbitrary assumption that is built into our model to argue this point we consider two modifications to the architecture that might eliminate the interaction in the damaged model first if we allowed the v s pathway to relax into a well formed state before feeding its output into the s n pathway there would be little interaction the effects of the damage would be additive however cortical pathways do not operate sequentially one stage finishing its computation and then turning on the next stage moreover in the undamaged brain such a processing strategy is unadaptive as cascading partial results from one pathway to the next can speed processing without the
a rich body of data exists showing that recollection of specific information makes an important contribution to recognition memory which is distinct from the contribution of familiarity and is not adequately captured by existing unitary memory models furthermore neuropsychological evidence indicates that recollection is subserved by the hippocampus we present a model based largely on known features of hippocampal anatomy and physiology that accounts for the following key characteristics of recollection false recollection is rare ie participants rarely claim to recollect having studied nonstudied items and increasing interference leads to less recollection but apparently does not compromise the quality of recollection ie the extent to which recollected information veridically reflects events that occurred at study
given a set of objects in the visual field how does the the visual system learn to attend to a particular object of interest while ignoring the rest how are occlusions and background clutter so effortlessly discounted for when recognizing a familiar object in this paper we attempt to answer these questions in the context of a kalman filter based model of visual recognition that has previously proved useful in explaining certain neurophysiological phenomena such as endstopping and related extra classical receptive field effects in the visual cortex by using results from the field of robust statistics we describe an extension of the kalman filter model that can handle multiple objects in the visual field the resulting robust kalman filter model demonstrates how certain forms of attention can be viewed as an emergent property of the interaction between top down expectations and bottom up signals the model also suggests functional interpretations of certain attentionrelated effects that have been observed in visual cortical neurons experimental results are provided to help demonstrate the ability of the model to perform robust segmentation and recognition of objects and image sequences in the presence of varying degrees of occlusions and clutter
recently researchers have derived formal complexity analysis of analog computation in the setting of discrete time dynamical systems as an empirical conswast training recurrent neural networks rnns produces seff organizea systems that are realizations of analog mechanisms previous work showed that a rnn can learn to process a simple context free language cfl by counting herein we extend that work to show that a rnn can learn a harder cfl a simple palindrome by organizing its resources into a symbol sensitive counting solution and we provide a dynamical systems analysis which demonstrates how the network can not only count but also copy and store counting information
we present a study which is concerned with word recognition rates for heavily degraded documents we compare human with machine reading capabilities in a series of experiments which explores the interaction of wordnon word recognition word frequency and legality of non words with degradation level we also study the influence of character segmentation and compare human performance with that of our artificial neural network model for reading we found that the proposed computer model uses word context as efficiently as humans but performs slightly worse on the pure character recognition task i
it is known that humans can make finer discriminations between familiar sounds eg syllables than between unfamiliar ones eg different noise segments here we show that a corresponding enhancement is present in early auditory processing stages based on previous work which demonstrated that natural sounds had robust statistical properties that could be quantified we hypothesize that the auditory system exploits those properties to construct efficient neural codes to test this hypothesis we measure the information rate carried by auditory spike trains on narrow band stimuli whose amplitude modulation has naturalistic characteristics and compare it to the information rate on stimuli with non naturalistic modulation we find that naturalistic inputs significantly enhance the rate of transmitted information indicating that auditiory neural responses are matched to characteristics of natural auditory scenes i natural scene statistics and the neural code a primary goal of hearing research is to understand how complex sounds that occur in natural scenes are processed by the auditory system however natural sounds are difficult to describe quantitatively and the complexity of auditory responses they evoke makes it hard to gain insight into their processing hence most studies of auditory physiology are restricted to pure tones and noise stimuli resulting in a limited understanding of auditory encoding in this paper we pursue a novel approach to the study of natural sound encoding in auditory spike trains our corresponding author e mail hagaiphy ucsf edu t e mail chrisphy ucsf edu h attias and c e schreiner illlllll i hi i iltll illltll t iiiiiiiii iiiiiiii iii iiiiiiii t figure left amplitude modulation stimulus drawn from a naturalistic stimulus set and the evoked spike train of an inferior colliculus neuron right amplitude modulation from a non naturalistic set and the evoked spike train of the same neuron method consists of measuring statistical characteristics of natural auditory scenes and incorporating them into simple stimuli in a systematic manner thus creating naturalistic stimuli which enable us to study the encoding of natural sounds in a controlled fashion the first stage of this program has been described in attias and schreiner the second is reported below fig i shows two segments of long stimuli and the corresponding spike trains of the same neuron elicited by pure tones that were amplitude modulated by these stimuli while both stimuli appear to be random and to have the same mean and both spike trains have the same firing rate one may observe that high and low amplitudes are more likely to occur in the stimulus on the left indeed these stimuli are drawn from two stimulus sets with different statistical properties our present study of auditory coding focuses on assessing the efficiency of this neural code for a given stimulus set how well can the animal reconstruct the input sound and discriminate between similar sound segments based on the evoked spike train and how those abilities are affected by changing the stimulus statistics we quantify the discrimination capability of auditory neurons in the inferior colliculus of the cat using concepts from information theory bialek et al rieke et al this leads to the issue of optimal coding atick theoretically given an auditory scene with particular statistical properties it is possible to design an encoding scheme that would exploit those properties resulting in a neural code that is optimal for that scene but is consequently less efficient for other scenes here we investigate the hypothesis that the auditory system uses a code that is adapted to natural auditory scenes this question is addressed by comparing the discrimination capability of auditory neurons between sound segments drawn from a naturalistic stimulus set to the one for a non naturalistic set statistics of natural sounds as a first step in investigating the relation between neural responses and auditory inputs we studied and quantified temporal statistics of natural auditory scenes attias and schreiner it is well known that different locations on the basal membrane respond selectively to different frequency components of the incoming sound xt eg pickles hence the frequency y corresponds to a spatial coordinate in analogy with retinal location in vision we therefore analyzed a large database of sounds including speech music animal vocalizations and background sounds using various filter banks comprising khz in each frequency band y the amplitude at and phase bt of the band limited signal xt at cost bt were extracted and the amplitude probability distribution pa and auto correlation coding of naturalistic stimuli by auditory midbrain neurons piano music symphonic music speech o cat vocaiizatlons bird songs loga loga loga ignre og mpiude disfibufio i sever sound esembes differed curves for give esembe correspond o differed frequec bds he low mpimde pek i he c po refiec budance of sile sees he heorefical curve is pioed for compariso dhed function c atat were computed as well as those of the instantaneous frequency dqbt dt those statistics were found to be nearly identical in all bands and across all examined sounds in particular the distribution of the log amplitude log a normalized to have zero mean and unit variance could be well fitted to the form pa exp a a e a with normalization constants a and which should however be corrected at large amplitude a several examples are displayed in fig the log amplitude distribution corresponds mathematically to the amplitude distribution of musical instruments and vocalizations found to be pa e a known as the laplace distribution in speech signal processing as well as that of background sounds where pa ae which can be shown to be the band amplitude distribution for a gaussian signal the power spectra of at fourier transform of c were found to have a modified if form together with the results for bt those findings show that natural sounds are distinguished from arbitrary ones by robust characteristics in the present paper we explore to what extent the auditory system exploits them in constructing efficient neural codes another important point made by attias and schreiner as well as by ruderman and bialek regarding visual signals is that natural inputs are very often not gaussian eg unlike the signals used by conventional system identification methods often applied to the nervous system in this paper we use non gaussian stimuli to study auditory coding measuring the rate of information transfer experiment based on our results for temporal statistics of natural auditory scenes we can construct naturalistic stimuli by starting with a simple signal and systematically incorporate successively more complicated characteristics of natural sounds into it h attias and c e schreiner we chose to use narrow band stimuli consisting of amplitude modulated carriers at cosvt at sound frequencies khz with no phase modulation focusing on one point amplitude statistics we constructed a white naturalistic amplitude by choosing at from an exponential distribution with a cutoff p a ac cr e a pa ac at each time point t independently using a cutoff modulation frequency of f hz ie f f const af f where af is the fourier transform of at we also used a non naturalistic stimulus set where at was chosen from a uniform distribution p a b be pa b o with b adjusted so that both stimulus sets had the same mean a short segment from each set is shown in fig and the two distributions are plotted in figs right stimuli of min duration were played to ketamine anesthetized cats to minimize adaptation effects we alternated between the two sets using sec long segments single unit recordings were made from the inferior colliculus ic a subthalamic auditory processing stage eg pickles each ic unit responds best to a narrow range of sound frequencies the center of which is called its best frequency bf neighboring units have similar bfs in accord with the topographic frequency organization of the auditory system for each unit stimuli with carrier frequency y at most hz away from the units bf were used firing rates in response to those stimuli were between hz the stimulus and the electrode signal were recorded simultaeneously at a sampling rate of khz after detecting and sotring the spikes and extracting the stimulus amplitude both amplitude and spike train were down sampled to khz analysis in order to assess the ability to discriminate between different inputs based on the observed spike train we computed the mutual information ir s between the spike train response rt y i t ti where ti are the spike times and the stimulus amplitude st i consists of two terms irs hs hslr where hs is the stimulus entropy the log number of different stimuli and hslr is the entropy of the stimulus conditioned on the response the log number of different stimuli that could elicit a given response and thus could not be discriminated based on that response averaged over all responses our approach generally follows the ideas of bialek et al rieke et al to simplify the calculation we first modified the stimuli st to get st fst where the function fs was chosen so that s was gaussian hence for exponential stimuli fs verfi e s and for uniform stimuli fs verfisbc where erfi is the inverse error function this gaussianization has two advantages first the expression for the mutual information ir s it s is now simpler being given by the frequency dependent signal to noise ratio snrf see below since hs depends only on the power spectrum of st second and more importantly the noise distribution was observed to become closer to gaussian following this transformation to compute hslr we bound it from above by fo dfhsf f the calculation of which requires the conditional distribution psf i f note that these variables are complex hence this is the joint ditribution of the real and imaginary parts the latter is approximated by a gaussian with mean srf and variance nrf this variance is in fact the power spectrum of the noise nrf i firf which we define by nrt st srt computing the mutual information for those gaussian distributions is straightforward and provides a lower bound on the coding of naturalistic stimuli by auditory midbrain neurons f o figure left signal to noise ratio snrf vs modulation frequency f for naturalistic stimuli right normalized noise distribution solid line amplitude distribution of stimuli dashed line and of gaussianized stimuli dashed dotted line true its fc is is f df log snrf o the signal to noise ratio is given by snrf sfnf where sf i f is the spectrum of the gaussianized stimulus and the averaging is performed over all responses the main object here is gf which is an estimate of the stimulus from the elicited spike train and would optimally be given by the conditional mean f dp at each f kay for gaussian p this estimator which is generally non linear becomes linear in f and is given by ff where f f ff f is the wiener filter however since our distributions were only approximately gaussians we used the conditional mean obtained by the kernel estimate f kiff where k is a gaussian kernel rf is the spectrum of the spike train and i indexes the data points obtained by computing fft using a sliding window the scaling by v v reflects the assumption that the distributions at all f differ only by their variance which enables us to use the data points at all frequencies to estimate at a given f our estimate produced a slightly higher snrf than the wiener r estimate used by bialek et al rieke et al and others information on naturalistic stimuli the snrf for exponential stimuli is shown in fig left for one of our units ic neurons have a preferred modulation frequency fm eg pickles which is about hz for this unit notice that generally snrf with equality when the stimulus and response are completely independent thus stimulus components at frequencies higher than hz effectively cannot be estimated from the spike train the stimulus amplitude distribution is shown in fig right dashed line together with the noise distribution normalized to have unit variance solid line which is nearly gaussian h attias and c e chreiner f oo o figure left signal to noise ratio snrf vs modulation frequency f for nonnaturalistic stimuli solid line compared with naturalistic stimuli dotted line right normalized noise distribution solid line amplitude distribution of stimuli dashed line compared with that of naturalistic stimuli dotted line and of gaussianized stimuli dashed dotted line using we obtain an information rate of irs bitsec for the spike rate of spikesec measured in this unit this translates into bitspike averaging across units we have qbitspike for naturalistic stimuli although this information rate was computed using the conditional mean estimator it is interesting to examine the wiener filter ht which provides the optimal linear estimator of the stimulus as discussed in the previous section this filter is displayed in fig solid line and has a temporal width of several tens of milliseconds information on non naturalistic stimuli the snrf for uniform stimuli is shown in fig left solid line for the same unit as in fig and is significantly lower than the corresponding snrf for exponential stimuli plotted for comparison dashed line for the mutual information rate we obtain it bitsec which amounts to bitspike averaging across units we have qbitspike for non naturalistic stimuli the stimulus amplitude distribution is shown in fig right dashed line together with the exponential distribution dotted line plotted for comparison as well as the noise distribution normalized to have unit variance the noise in this case is less gaussian than for exponential stimuli suggesting that our calculated bound on is may be lower for uniform stimuli fig shows the stimulus reconstruction filter dashed line it has a similar time course as the filter for exponential stimuli but the decay is significantly slower and its temporal width is more than msec conclusion we measured the rate at which auditory neurons carry information on simple stimuli with naturalistic amplitude modulation and found that it was higher than for stimuli with non naturalistic modulation a result along the same lines for the frog was obtained by rieke et al using gaussian signals whose spectrum was shaped according to the frog call spectrum similarly work in vision laughlin field atick and redlich ruderman and bialek dong and atick suggests that visual receptive field properties are consistent with optimal coding predictions based on characteristics of natural images future work will explore coding of stimuli with more complex natural statistical characteristics and coding of naturalistic stimuli by auditory midbrain neurons t figure impulse response of wiener reconstruction filter for naturalistic stimuli solid line and non naturalistic stimuli dashed line will extend to higher processing stages acknowledgements we thank w bialek k miller s nagarajan and f theunissen for useful discussions and b bonham m escabi m kvale l miller and h read for experimental support supported by the office of naval research n nidcd r and the sloan foundation references jj atick and n redlich towards a theory of early visual processing neural comput jj atick could information theory provide an ecological theory of sensory processing network h attias and ce schreiner temporal low order statistics of natural sounds in advances in neural information processing systems mit press w bialek f rieke r de ruyter van steveninck and d warland reading the neural code science dw dong and jj atick temporal decorrelation a theory of lagged and non lagged responses in the lateral geniculate nucleus network dj field relations between the statistics of natural images and the response properties of cortical cells j opt soc am sm kay fundamentals of statistical signal processing estimation theory prentice hall new jersey sb laughlin a simple coding procedure enhances a neurons information capacity z naturforsch c jo pickles an
the relationship between a neurons refractory period and the precision of its response to identical stimuli was investigated we constructed a model of a spiking neuron that combines probabilistic firing with a refractory period for realistic refractoriness the model closely reproduced both the average firing rate and the response precision of a retinal ganglion cell the model is based on a free firing rate which exists in the absence of refractoriness this function may be a better description of a spiking neurons response than the peri stimulus time histogram
conditioning experiments probe the ways that animals make predictions about rewards and punishments and use those predictions to control their behavior one standard model of conditioning paradigms which involve many conditioned stimuli suggests that individual predictions should be added together various key results show that this model fails in some circumstances and motivate an alternative model in which there is attentional selection between different available stimuli the new model is a form of mixture of experts has a close relationship with some other existing psychological suggestions and is statistically well founded
while the understanding of the functional role of different classes of neurons in the awake primary visual cortex has been extensively studied since the time of hubel and wiesel hubel and wiesel our understanding of the feature selectivity and functional role of neurons in the primary auditory cortex is much farther from complete moving bars have long been recognized as an optimal stimulus for many visual cortical neurons and this finding has recently been confirmed and extended in detail using reverse correlation methods jones and palmer reid and alonso reid et al ringach et al in this study we recorded from neurons in the primary auditory cortex of the awake primate and used a novel reverse correlation technique to compute receptive fields or preferred stimuli encompassing both multiple frequency components and ongoing time these spectrotemporal receptive fields make clear that neurons in the primary auditory cortex as in the primary visual cortex typically show considerable structure in their feature processing properties often including multiple excitatory and inhibitory regions in their receptive fields these neurons can be sensitive to stimulus edges in frequency composition or in time and sensitive to stimulus transitions such as changes in frequency these neurons also show strong responses and selectivity to continuous frequency modulated stimuli analogous to visual drifting gratings
one of the current challenges to understanding neural information processing in biological systems is to decipher the code carried by large populations of neurons acting in parallel we present an algorithm for automated discovery of stochastic firing patterns in large ensembles of neurons the algorithm from the helmholtz machine family attempts to predict the observed spike patterns in the data the model consists of an observable layer which is directly activated by the input spike patterns and hidden units that are activated through ascending connections from the input layer the hidden unit activity can be propagated down to the observable layer to create a prediction of the data pattern that produced it hidden units are added incrementally and their weights are adjusted to improve the fit between the predictions and data that is to increase a bound on the probability of the data given the model this greedy strategy is not globally optimal but is computationally tractable for large populations of neurons we show benchmark data on artificially constructed spike trains and promising early results on neurophysiological data collected from our chronic multi electrode cortical implant i
nystagmus is a pattern of eye movement characterized by smooth rotations of the eye in one direction and rapid rotations in the opposite direction that reset eye position periodic alternating nystagmus pan is a form of uncontrollable nystagmus that has been described as an unstable but amplitude limited oscillation pan has been observed previously only in subjects with vestibulo cerebellar damage we describe results in which pan can be produced in normal subjects by prolonged rotation in darkness we propose a new model in which the neural circuits that control eye movement are inherently unstable but this instability is kept in check under normal circumstances by the cerebellum circumstances which alter this cerebellar restraint such as vestibulocerebellar damage or plasticity due to rotation in darkness can lead to pan
we provide a model of the standard watermaze task and of a more challenging task involving novel platform locations in which rats exhibit one trial learning after a few days of training the model uses hippocampal place cells to support reinforcement learning and also in an integrated manner to build and use allocentric coordinates
the initial activity independent formation of a topographic map in the retinotectal system has long been thought to rely on the matching of molecular cues expressed in gradients in the retina and the tectum however direct experimental evidence for the existence of such gradients has only emerged since the new data has provoked the discussion of a new set of models in the experimental literature here the capabilities of these models are analyzed and the gradient shapes they predict in vivo are derived
in the developing nervous system gradients of target derived diffusible factors play an important role in guiding axons to appropriate targets in this paper the shape that such a gradient might have is calculated as a function of distance from the target and the time since the start of factor production using estimates of the relevant parameter values from the experimental literature the spatiotemporal domain in which a growth cone could detect such a gradient is derived for large times a value for the maximum guidance range of about mm is obtained this value fits well with experimental data for smaller times the analysis predicts that guidance over longer ranges may be possible this prediction remains to be tested

we propose a model for early visual processing in primates the model consists of a population of linear spatial filters which interact through non linear excitatory and inhibitory pooling statistical estimation theory is then used to derive human psychophysical thresholds from the responses of the entire population of units the model is able to reproduce human thresholds for contrast and orientation discrimination tasks and to predict contrast thresholds in the presence of masks of varying orientation and spatial frequency
in this paper we present a new method for studying auditory systems based on m sequences the method allows us to perturbatively study the linear response of the system in the presence of various other stimuli such as speech or sinusoidal modulations this allows one to construct linear kernels receptive fields at the same time that other stimuli are being presented using the method we calculate the modulation transfer function of single units in the inferior colliculus of the cat at different operating points and discuss nonlinearities in the response i
in normal vision the inputs from the two eyes are integrated into a single percept when dissimilar images are presented to the two eyes however perceptual integration gives way to alternation between monocular inputs a phenomenon called binocular rivalry although recent evidence indicates that binocular rivalry involves a modulation of neuronal responses in extrastriate cortex the basic mechanisms responsible for differential processing of conflicting and congruent stimuli remain unclear using a neural network that models the mammalian early visual system i demonstrate here that the desynchronized firing of cortical like neurons that first receive inputs from the two eyes results in rivalrous activity patterns at later stages in the visual pathway by contrast synchronization of firing among these cells prevents such competition the temporal coordination of cortical activity and its effects on neural competition emerge naturally from the network connectivity and from its dynamics these results suggest that input related differences in relative spike timing at an early stage of visual processing may give rise to the phenomena both of perceptual integration and rivalry in binocular vision

here we analyze synaptic transmission from an information theoretic perspective we derive closed form expressions for the lower bounds on the capacity of a simple model of a cortical synapse under two explicit coding paradigms under the signal estimation paradigm we assume the signal to be encoded in the mean firing rate of a poisson neuron the performance of an optimal linear estimator of the signal then provides a lower bound on the capacity for signal estimation under the signal detection paradigm the presence or absence of the signal has to be detected performance of the optimal spike detector allows us to compute a lower bound on the capacity for signal detection we find that single synapses for empirically measured parameter values transmit information poorly but significant improvement can be achieved with a small amount of redundancy
hubel and wiesel proposed that complex cells in visual cortex are driven by a pool of simple cells with the same preferred orientation but different spatial phases however a wide variety of experimental results over the past two decades have challenged the pure hierarchical model primarily by demonstrating that many complex cells receive monosynaptic input from unoriented lgn cells or do not depend on simple cell input we recently showed using a detailed biophysical model that nonlinear interactions among synaptic inputs to an excitable dendritic tree could provide the nonlinear subunit computations that underlie complex cell responses mel ruderman archie this work extends the result to the case of complex cell binocular disparity tuning by demonstrating in an isolated model pyramidal cell disparity tuning at a resolution much finer than the the overall dimensions of the cells receptive field and systematically shifted optimal disparity values for rivalrous pairs of light and dark bars both in good agreement with published reports ohzawa deangelis freeman our results reemphasize the potential importance of intradendritic computation for binocular visual processing in particular and for cortical neurophysiology in general singlecell account for binocular disparity tuning
in macaque inferotemporal cortex it neurons have been found to respond selectively to complex shapes while showing broad tuning invariance with respect to stimulus transformations such as translation and scale changes and a limited tuning to rotation in depth training monkeys with novel paperclip like objects logothetis et al could investigate whether these invariance properties are due to experience with exhaustively many transformed instances of an object or if there are mechanisms that allow the cells to show response invariance also to previously unseen instances of that object they found object selective cells in anterior it which exhibited limited invariance to various transformations after training with single object views while previous models accounted for the tuning of the cells for rotations in depth and for their selectivity to a specific object relative to a population of distractor objects the model described here attempts to explain in a biologically plausible way the additional properties of translation and size invariance using the same stimuli as in the experiment we find that model it neurons exhibit invariance properties which closely parallel those of real neurons simulations show that the model is capable of unsupervised learning of view tuned neurons we thank peter dayan marcus dill shimon edelman nikos logothetis jonathan mumick and randy oreilly for useful discussions and comments m riesenhuber and t poggio
we discuss a solution to the problem of separating waveforms produced by multiple cells in an extracellular neural recording we take an explicitly probabilistic approach using latent variable models of varying sophistication to describe the distribution of waveforms produced by a single cell the models range from a single gaussian distribution of waveforms for each cell to a mixture of hidden markov models we stress the overall statistical structure of the approach allowing the details of the generative model chosen to depend on the specific neural preparation i
we have studied the application of an independent component analysis ica approach to the identification and possible removal of artifacts from a magnetoencephalographic meg recording this statistical technique separates components according to the kurtosis of their amplitude distributions over time thus distinguishing between strictly periodical signals and regularly and irregularly occurring signals many artifacts belong to the last category in order to assess the effectiveness of the method controlled artifacts were produced which included saccadic eye movements and blinks increased muscular tension due to biting and the presence of a digital watch inside the magnetically shielded room the results demonstrate the capability of the method to identify and clearly isolate the produced artifacts
we model the responses of cells in visual area v during natural vision our model consists of a classical energy mechanism whose output is divided by nonclassical gain control and texture contrast mechanisms we apply this model to review movies a stimulus sequence that replicates the stimulation a cell receives during free viewing of natural images data were collected from three cells using five different review movies and the model was fit separately to the data from each movie for the energy mechanism alone we find modest but significant correlations rr between model and data these correlations are improved somewhat when we allow for suppressive surround effects rrg in one case the inclusion of a delayed suppressive surround dramatically improves the fit to the data by modifying the time course of the models response i
we prove that the canonical distortion measure cdm is the optimal distance measure to use for nearest neighbour nn classification and show that it reduces to squared euclidean distance in feature space for function classes that can be expressed as linear combinations of a fixed set of features pac like bounds are given on the samplecomplexity required to learn the cdm an experiment is presented in which a neural network cdm was learnt for a japanese ocr environment and then used to do nn classification
we introduce a new boolean computing element related to the linear threshold element which is the boolean version of the neuron instead of the sign function it computes an arbitrary with polynomialy many transitions boolean function of the weighted sum of its inputs we call the new computing element an ltm element which stands for linear threshold with multiple transitions the paper consists of the following main contributions related to our study of ltm circuits i the creation of efficient designs of ltm circuits for the addition of a multiple number of integers and the product of two integers in particular we show how to compute the addition of m integers with a single layer of ltm elements ii a proof that the area of the vlsi layout is reduced from on in lt circuits to on in ltm circuits for n inputs symmetric boolean functions and iii the characterization of the computing power of ltm relative to lt circuits i
recent theoretical results for pattern classification with thresholded real valued functions such as support vector machines sigmoid networks and boosting give bounds on misclassification probability that do not depend on the size of the classifier and hence can be considerably smaller than the bounds that follow from the vc theory in this paper we show that these techniques can be more widely applied by representing other boolean functions as two layer neural networks thresholded convex combinations of boolean functions for example we show that with high probability any decision tree of depth no more than d that is consistent with m training examples has misclassification probability no more than neff vcdiml log mlogd whereg is the class of node decision functions and neff n can be thought of as the effective number of leaves it becomes small as the distribution on the leaves induced by the training data gets far from uniform this bound is qualitatively different from the vc bound and can be considerably smaller we use the same technique to give similar results for dnf formulae author to whom correspondence should be addressed m gotea p bartlett w s lee and l mason
a simple linear averaging of the outputs of several networks as eg in bagging seems to follow naturally from a biasvariance decomposition of the sum squared error the sum squared error of the average model is a quadratic function of the weighting factors assigned to the networks in the ensemble suggesting a quadratic programming algorithm for finding the optimal weighting factors if we interpret the output of a network as a probability statement the sum squared error corresponds to minus the loglikelihood or the kullback leibler divergence and linear averaging of the outputs to logarithmic averaging of the probability statements the logarithmic opinion pool the crux of this paper is that this whole story about model averaging biasvariance decompositions and quadratic programming to find the optimal weighting factors is not specific for the sumsquared error but applies to the combination of probability statements of any kind in a logarithmic opinion pool as long as the kullback leibler divergence plays the role of the error measure as examples we treat model averaging for classification models under a cross entropy error measure and models for estimating variances i
we derive a first order approximation of the density of maximum entropy for a continuous d random variable given a number of simple constraints this results in a density expansion which is somewhat similar to the classical polynomial density expansions by gram charlier and edgeworth using this approximation of density an approximation of d differential entropy is derived the approximation of entropy is both more exact and more robust against outliers than the classical approximation based on the polynomial density expansions without being computationally more expensive the approximation has applications for example in independent component analysis and projection pursuit
we present a new approximate learning algorithm for boltzmann machines using a systematic expansion of the gibbs free energy to second order in the weights the linear response correction to the correlations is given by the hessian of the gibbs free energy the computational complexity of the algorithm is cubic in the number of neurons we compare the performance of the exact bm learning algorithm with first order weiss mean field theory and second order tap mean field theory the learning task consists of a fully connected ising spin glass model on neurons we conclude that the method works well for paramagnetic problems the tap correction gives a significant improvement over the weiss mean field theory both for paramagnetic and spin glass problems and that the inclusion of diagonal weights improves the weiss approximation for paramegnetic problems but not for spin glass problems i
we study on line generalized linear regression with multidimensional outputs ie neural networks with multiple output nodes but no hidden nodes we allow at the final layer transfer functions such as the softmax function that need to consider the linear activations to all the output neurons we use distance functions of a certain kind in two completely independent roles in deriving and analyzing on line learning algorithms for such tasks we use one distance function to define a matching loss function for the possibly multidimensional transfer function which allows us to generalize earlier results from one dimensional to multidimensional outputs we use another distance function as a tool for measuring progress made by the on line updates this shows how previously studied algorithms such as gradient descent and exponentiated gradient fit into a common framework we evaluate the performance of the algorithms using relative loss bounds that compare the loss of the on line algoritm to the best off line predictor from the relevant model class thus completely eliminating probabilistic assumptions about the data
the generalization ability of a neural network can sometimes be improved dramatically by regularization to analyze the improvement one needs more refined results than the asymptotic distribution of the weight vector here we study the simple case of one dimensional linear regression under quadratic regularization ie ridge regression we study the random design misspecified case where we derive expansions for the optimal regularization parameter and the ensuing improvement it is possible to construct examples where it is best to use no regularization
we employ both master equation and order parameter approaches to analyze the asymptotic dynamics of on line learning with different learning rate annealing schedules we examine the relations between the results obtained by the two approaches and obtain new results on the optimal decay coefficients and their dependence on the number of hidden nodes in a two layer architecture i


we present a method for determining the globally optimal on line learning rule for a soft committee machine under a statistical mechanics framework this work complements previous results on locally optimal rules where only the rate of change in generalization error was considered we maximize the total reduction in generalization error over the whole learning process and show how the resulting rule can significantly outperform the locally optimal rule i

perceptron decision trees a known as linear machine dts etc are analysed in order that data dependent structural risk minlmlation can be applied data dependent analysis is performed which indicates that choosing the maximal margin hyperplancs at the decision nodes will improve the generalization the analysis uses a novel technique to bound the generalization error in terms of the margins at individual nodes experiments performed on real data sets confirm the validity of the approach
we derive the correspondence between regularization operators used in regularization networks and hilbert schmidt kernels appearing in support vector machines more specifically we prove that the greens functions associated with regularization operators are suitable support vector kernels with equivalent regularization properties as a by product we show that a large number of radial basis functions namely conditionally positive definite functions may be used as support vector kernels
a simple but powerful modification of the standard gaussian distribution is studied the variables of the rectified gaussian are constrained to be nonnegative enabling the use of nonconvex energy functions two multimodal examples the competitive and cooperative distributions illustrate the representationai power of the rectified gaussian since the cooperative distribution can represent the translations of a pattern it demonstrates the potential of the rectified gaussian for modeling pattern manifolds i
online learning is one of the most common forms of neural network training we present an analysis of online learning from finite training sets for non linear networks namely soft committee machines advancing the theory to more realistic learning scenarios dynamical equations are derived for an appropriate set of order parameters these are exact in the limiting case of either linear networks or infinite training sets preliminary comparisons with simulations suggest that the theory captures some effects of finite training sets but may not yet account correctly for the presence of local minima i
we apply a general algorithm for merging prediction strategies the aggregating algorithm to the problem of linear regression with the square loss our main assumption is that the response variable is bounded it turns out that for this particular problem the aggregating algorithm resembles but is slightly different from the wellknown ridge estimation procedure from general results about the aggregating algorithm we deduce a guaranteed bound on the difference between our algorithms performance and the best in some sense linear regression functions performance we show that the aa attains the optimal constant in our bound whereas the constant attained by the ridge regression procedure in general can be times worse i
we demonstrate that the problem of training neural networks with small average squared error is computationally intractable consider a data set of m points xi y i m where xi are input vectors from r d y are real outputs y r for a netm work f in some class of neural networks ei fxi in f fem imfxi yi is the avarage relative error occurs when one tries to fit the data set by f we will prove for several classes y of neural networks that achieving a relative error smaller than some fixed positive threshold independent from the size of the data set is np hard
we study the storage capacity of a fully connected committee machine with a large number k of hidden nodes the storage capacity is obtained by analyzing the geometrical structure of the weight space related to the internal representation by examining the asymptotic behavior of order parameters in the limit of large k the storage capacity ac is found to be proportional to k l up to the leading order this result satisfies the mathematical bound given by mitchison and durbin whereas the replica symmetric solution in a conventional gardners approach violates this bound i
the inverse of the fisher information matrix is used in the natural gradient descent algorithm to train single layer and multi layer perceptrons we have discovered a new scheme to represent the fisher information matrix of a stochastic multi layer perceptron based on this scheme we have designed an algorithm to compute the natural gradient when the input dimension n is much larger than the number of hidden neurons the complexity of this algorithm is of order on it is confirmed by simulations that the natural gradient descent learning rule is not only efficient but also robust i
bayesian treatments of learning in neural networks are typically based either on local gaussian approximations to a mode of the posterior weight distribution or on markov chain monte carlo simulations a third approach called ensemble learning was introduced by hinton and van camp it aims to approximate the posterior distribution by minimizing the kullback leibler divergence between the true posterior and a parametric approximating distribution however the derivation of a deterministic algorithm relied on the use of a gaussian approximating distribution with a diagonal covariance matrix and so was unable to capture the posterior correlations between parameters in this paper we show how the ensemble learning approach can be extended to fullcovariance gaussian distributions while remaining computationally tractable we also extend the framework to deal with hyperparameters leading to a simple re estimation procedure initial results from a standard benchmark problem are encouraging i
bayesian methods have been successfully applied to regression and classification problems in multi layer percepttons we present a novel application of bayesian techniques to radial basis function networks by developing a gaussian approximation to the posterior distribution which for fixed basis function widths is analytic in the parameters the setting of regularization constants by crossvalidation is wasteful as only a single optimal parameter estimate is retained we treat this issue by assigning prior distributions to these constants which are then adapted in light of the data under a simple re estimation formula i
recently a model for supervised learning of probabilistic transducers represented by suffix trees was introduced however this algorithm tends to build very large trees requiring very large amounts of computer memory in this paper we propose a new more compact transducer model in which one shares the parameters of distributions associated to contexts yielding similar conditional output distributions we illustrate the advantages of the proposed algorithm with comparative experiments on inducing a noun phrase recognizer i
exact inference in densely connected bayesian networks is computationally intractable and so there is considerable interest in developing effective approximation schemes one approach which has been adopted is to bound the log likelihood using a mean field approximating distribution while this leads to a tractable algorithm the mean field distribution is assumed to be factorial and hence unimodal in this paper we demonstrate the feasibility of using a richer class of approximating distributions based on mixtures of mean field distributions we derive an efficient algorithm for updating the mixture parameters and apply it to the problem of learning in sigmoid belief networks our results demonstrate a systematic improvement over simple mean field theory as the number of mixture components is increased
we study several statistically and biologically motivated learning rules using the same visual environment one made up of natural scenes and the same single cell neuronal architecture this allows us to concentrate on the feature extraction and neuronal coding properties of these rules included in these rules are kurtosis and skewness maximization the quadratic form of the bcm learning rule and single cell ica using a structure removal method we demonstrate that receptive fields developed using these rules depend on a small portion of the distribution we find that the quadratic form of the bcm rule behaves in a manner similar to a kurtosis maximization rule when the distribution contains kurtotic directions although the bcm modification equations are computationally simpler b s biais iv intrator h shouvai and l iv cooper i
we derive and analyse robust optimization schemes for noisy vector quantization on the basis of deterministic annealing starting from a cost function for central clustering that incorporates distortions from channel noise we develop a soft topographic vector quantization algorithm stvq which is based on the maximum entropy principle and which performs a maximum likelihood estimate in an expectationmaximization em fashion annealing in the temperature parameter leads to phase transitions in the existing code vector representation during the cooling process for which we calculate critical temperatures and modes as a function of eigenvectors and eigenvalues of the covariance matrix of the data and the transition matrix of the channel noise a whole family of vector quantization algorithms is derived from stvq among them a deterministic annealing scheme for kohonens self organizing map som this algorithm which we call ssom is then applied to vector quantization of image data to be sent via a noisy binary symmetric channel the algorithms performance is compared to those of lbg and stvq while it is naturally superior to lbg which does not take into account channel noise its results compare very well to those of stvq which is computationally much more demanding
in many applications such as credit default prediction and medical image recognition test inputs are available in addition to the labeled training examples we propose a method to incorporate the test inputs into learning our method results in solutions having smaller test errors than that of simple training solution especially for noisy problems or small training sets
this paper considers the problem of learning the ranking of a set of alternatives based upon incomplete information eg a limited number of observations we describe two algorithms for hypothesis ranking and their application for probably approximately correct pac and expected loss el learning criteria empirical results are provided to demonstrate the effectiveness of these ranking procedures on both synthetic datasets and real world data from a spacecraft design optimization problem i
there are many applications in which it is desirable to order rather than classify instances here we consider the problem of learning how to order given feedback in the form of preference judgments ie statements to the effect that one instance should be ranked ahead of another we outline a two stage approach in which one first learns by conventional means a preference function of the form prefu v which indicates whether it is advisable to rank u before v new instances are then ordered so as to maximize agreements with the learned preference function we show that the problem of finding the ordering that agrees best with a preference function is np complete even under very restrictive assumptions nevertheless we describe a simple greedy algorithm that is guaranteed to find a good approximation we then discuss an on line learning algorithm based on the hedge algorithm for finding a good linear combination of ranking experts we use the ordering algorithm combined with the on line learning algorithm to find a combination of search experts each of which is a domain specific query expansion strategy for a www search engine and present experimental results that demonstrate the merits of our approach
in this paper we discuss regularisation in onlinesequential learning algorithms in environments where data arrives sequentially techniques such as cross validation to achieve regularisation or model selection are not possible further bootstrapping to determine a confidence level is not practical to surmount these problems a minimum variance estimation approach that makes use of the extended kalman algorithm for training multi layer perceptrons is employed the novel contribution of this paper is to show the theoretical links between extended kalman filtering suttons variable learning rate algorithms and mackays bayesian estimation framework in doing so we propose algorithms to overcome the need for heuristic choices of the initial conditions and noise covariance matrices in the kalman approach
classification of finite sequences without explicit knowledge of their statistical nature is a fundamental problem with many important applications we propose a new information theoretic approach to this problem which is based on the following ingredients i sequences are similar when they are likely to be generated by the same source ii cross entropies can be estimated via universal compression iii markovian sequences can be asymptotically optimally merged with these ingredients we design a method for the classification of discrete sequences whenever they can be compressed we introduce the method and illustrate its application for hierarchical clustering of languages and for estimating similarities of protein sequences i


we first describe a hierarchical generafive model that can be viewed as a non linear generalisation of factor analysis and can be implemented in a neural network the model performs perceptual inference in a probabilistically consistent manner by using top down bottom up and lateral connections these connections can be learned using simple rules that require only locally available information we then show how to incorporate lateral connections into the generafive model the model extracts a sparse distributed hierarchical representation of depth from simplified random dot stereograms and the localised disparity detectors in the first hidden layer form a topographic map when presented with image patches from natural scenes the model develops topographically organised local feature detectors i

some learning techniques for classification tasks work indirectly by first trying to fit a full probabilistic model to the observed data whether this is a good idea or not depends on the robustness with respect to deviations from the postulated model we study this question experimentally in a restricted yet non trivial and interesting case we consider a conditionally independent attribute cia model which postulates a single binary valued hidden variable z on which all other attributes ie the target and the observables depend in this model finding the most likely value of any one variable given known values for the others reduces to testing a linear function of the observed values we learn cia with two techniques the standard em algorithm and a new algorithm we develop based on covariances we compare these in a conuolled fashion against an algorithm a version of winnow that attempts to find a good linear classifier directly our conclusions help delimit the fragility of using the cia model for classification once the data departs from this model performance quickly degrades and drops below that of the directly learned linear classifier
we discuss a strategy for polychotomous classification that involves estimating class probabilities for each pair of classes and then coupling the estimates together the coupling model is similar to the bradley terry method for paired comparisons we study the nature of the class probability estimates that arise and examine the performance of the procedure in simulated datasets the classifiers used include linear discriminants and nearest neighbors application to support vector machines is also briefly described i
an adaptive on line algorithm is proposed to estimate hierarchical data structures for non stationary data sources the approach is based on the principle of minimum cross entropy to derive a decision tree for data clustering and it employs a metalearning idea learning to learn to adapt to changes in data characteristics its efficiency is demonstrated by grouping non stationary artifical data and by hierarchical segmentation of landsat images i
we address the problem of learning structure in nonlinear markov networks with continuous variables this can be viewed as non gaussian multidimensional density estimation exploiting certain conditional independencies in the variables markov networks are a graphical way of describing conditional independencies well suited to model relationships which do not exhibit a natural causal ordering we use neural network structures to model the quantitative relationships between variables the main focus in this paper will be on learning the structure for the purpose of gaining insight into the underlying process using two data sets we show that interesting structures can be found using our approach inference will be briefly addressed
active data clustering is a novel technique for clustering of proximity data which utilizes principles from sequential experiment design in order to interleave data generation and data analysis the proposed active data sampling strategy is based on the expected value of information a concept rooting in statistical decision theory this is considered to be an important step towards the analysis of largescale data sets because it offers a way to overcome the inherent data sparseness of proximity data we present applications to unsupervised texture segmentation in computer vision and information retrieval in document databases i
we present a computationally efficient algorithm for function approximation with piecewise linear sigmoidal nodes a one hidden layer network is constructed one node at a time using the method of fitting the residual the task of fitting individual nodes is accomplished using a new algorithm that searchs for the best fit by solving a sequence of quadratic programming problems this approach offers significant advantages over derivative based search algorithms eg backpropagation and its extensions unique characteristics of this algorithm include finite step convergence a simple stopping criterion a deterministic methodology for seeking good local minima good scaling properties and a robust numerical implementation i
a new class of plug in classification techniques have recently been developed in the statistics and machine learning literature a plug in classiftcation technique pact is a method that takes a standard classifier such as lda or trees and plugs it into an algorithm to produce a new classifier the standard classifier is known as the plug in classifier pic these methods often produce large improvements over using a single classifier in this paper we investigate one of these methods and give some motivation for its success
the s map is a network with a simple learning algorithm that combines the self organization capability of the self organizing map som and the probabilistic interpretability of the generative topographic mapping gtm the simulations suggest that the smap algorithm has a stronger tendency to self organize from random initial configuration than the gtm the s map algorithm can be further simplified to employ pure hebbian learning without changing the qualitative behaviour of the network i


multiple instance learning is a variation on supervised learning where the task is to learn a concept given positive and negative bags of instances each bag may contain many instances but a bag is labeled positive even if only one of the instances in it falls within the concept a bag is labeled negative only if all the instances in it are negative we describe a new general framework called diverse density for solving multiple instance learning problems we apply this framework to learn a simple description of a person from a series of images bags containing that person to a stock selection problem and to the drug activity prediction problem
applications of gaussian mixture models occur frequently in the fields of statistics and artificial neural networks one of the key issues arising from any mixture model apphcation is how to estimate the optimum number of mixture components this paper extends the reversible jump markov chain monte carlo mcmc algorithm to the case of multivariate spherical gaussian mixtures using a hierarchical prior model using this method the number of mixture components is no longer fixed but becomes a parmeter of the model which we shall estimate the reversible jump mcmc algorithm is capable of moving between parameter subspaces which correspond to models with different numbers of mixture components as a result a sample from the full joint distribution of all unknown model parameters is generated the technique is then demonstrated on a simulated example and a well known vowel dataset
this paper introduces a probability model the mixture of trees that can account for sparse dynamically changing dependence relationships we present a family of efficient algorithms that use em and the minimum spanning tree algorithm to find the ml and map mixture of trees for a variety of priors including the dirichlet and the mdl priors
several effective methods for improving the performance of a single learning algorithm have been developed recently the general approach is to create a set of learned models by repeatedly applying the algorithm to different versions of the training data and then combine the learned models predictions according to a prescribed voting scheme little work has been done in combining the predictions of a collection of models generated by many learning algorithms having different representation andor search strategies this paper describes a method which uses the strategies of stacking and correspondence analysis to model the relationship between the learning examples and the way in which they are classified by a collection of learned models a nearest neighbor method is then applied within the resulting representation to classify previously unseen examples the new algorithm consistently performs as well or better than other combining techniques on a suite of data sets
we propose diffusion networks a type of recurrent neural network with probabilistic dynamics as models for learning natural signals that are continuous in time and space we give a formula for the gradient of the log likelihood of a path with respect to the drift parameters for a diffusion network this gradient can be used to optimize diffusion networks in the nonequilibrium regime for a wide variety of problems paralleling techniques which have succeeded in engineering fields such as system identification state estimation and signal filtering an aspect of this work which is of particular interest to computational neuroscience and hardware design is that with a suitable choice of activation function eg quasi linear sigmoidal the gradient formula is local in space and time i

we consider the general problem of learning multi category classification from labeled examples we present experimental results for a nearest neighbor algorithm which actively selects samples from different pattern classes according to a querying rule instead of the a priori class probabilities the amount of improvement of this query based approach over the passive batch approach depends on the complexity of the bayes rule the principle on which this algorithm is based is general enough to be used in any learning algorithm which permits a model selection criterion and for which the error rate of the classifier is calculable in terms of the complexity of the model
existing proofs demonstrating the computational limitations of recurrent cascade correlation and similar networks fahlman bachtach mozer explicitly limit their results to units having sigmoidal or hard threshold transfer functions giles et al and kremer the proof given here shows that for any finite discrete transfer function used by the units of an rcc network there are finite state automata fsa that the network cannot model no matter how many units are used the proof also applies to continuous transfer functions with a finite number of fixed points such as sigmoid and radial basis functions

if globally high dimensional data has locally only low dimensional distributions it is advantageous to perform a local dimensionality reduction before further processing the data in this paper we examine several techniques for local dimensionality reduction in the context of locally weighted linear regression as possible candidates we derive local versions of factor analysis regression principle component regression principle component regression on joint distributions and partial least squares regression after outlining the statistical bases of these methods we perform monte carlo simulations to evaluate their robustness with respect to violations of their statistical assumptions one surprising outcome is that locally weighted partial least squares regression offers the best average results thus outperforming even factor analysis the theoretically most appealing of our candidate techniques troduction regression tasks involve mapping a n dimensional continuous input vector x filn onto a m dimensional output vector y fil m they form a ubiquitous class of problems found in fields including process control sensorimotor control coordinate transformations and various stages of information processing in biological nervous systems this paper will focus on spatially localized learning techniques for example kernel regression with gaussian weighting functions local learning offer advantages for real time incremental learning problems due to fast convergence considerable robustness towards problems of negative interference and large tolerance in model selection atkeson moore schaal schaal atkeson in press local learning is usually based on interpolating data from a local neighborhood around the query point for high dimensional learning problems however it suffers from a biasvariance dilemma caused by the nonintuitive fact that in high dimensions if neighborhoods are local then they are almost surely empty whereas if a neighborhood is not empty then it is not local scott p global learning methods such as sigmoidal feedforward networks do not face this s schaai s vijayakumar and c g atkeson problem as they do not employ neighborhood relations although they require strong prior knowledge about the problem at hand in order to be successful assuming that local learning in high dimensions is a hopeless however is not necessarily warranted being globally high dimensional does not imply that data remains high dimensional if viewed locally for example in the control of robot arms and biological arms we have shown that for estimating the inverse dynamics of an arm a globally dimensional space reduces on average to dimensions locally vijayakumar schaal a local learning system that can robustly exploit such locally low dimensional distributions should be able to avoid the curse of dimensionality in pursuit of the question of what in the context of local regression is the fight method to perform local dimensionality reduction this paper will derive and compare several candidate techniques under i perfectly fulfilled statistical prerequisites eg gaussian noise gaussian input distributions perfectly linear data and ii less perfect conditions eg non gaussian distributions slightly quadratic data incorrect guess of the dimensionality of the true data distribution we will focus on nonlinear function approximation with locally weighted linear regression lwr as it allows us to adapt a variety of global linear dimensionality reduction techniques and as lwr has found widespread application in several local learning systems atkeson moore schaal jordan jacobs xu jordan hinton in particular we will derive and investigate locally weighted principal component regression lwpcr locally weighted joint data principal component analysis lwpca locally weighted factor analysis lwfa and locally weighted partial least squares lwpls section will briefly outline these methods and their theoretical foundations while section will empirically evaluate the robustness of these methods using synthetic data sets that increasingly violate some of the statistical assumptions of the techniques methods of dimensionaiity reduction we assume that our regression data originate from a generating process with two sets of observables the inputs i and the outputs the characteristics of the process ensure a functional relation fi both i and are obtained through some measurement device that adds independent mean zero noise of different magnitude in each observable such that x i c x and y y cy for the sake of simplicity we will only focus on one dimensional output data rnl and functions f that are either linear or slightly quadratic as these cases are the most common in nonlinear function approximation with locally linear models locality of the regression is ensured by weighting the error of each data point with a weight from a gaussian kernel x denotes the query point and d a positive semi definite distance metric which deterrmnes the size and shape of the neighborhood contributing to the regression atkeson et al the parameters xq and d can be determined in the framework of nonparametric statistics schaal atkeson in press or parametric maximum likelihood estimations xu et al for the present study they are determined manually since their origin is secondary to the results of this paper without loss of generality all our data sets will set xq to the zero vector compute the weights and then translate the input data such that the lbcally weighted mean e w xi e wi is zero the output data is equally translated to be mean zero mean zero data is necessary for most of techniques considered below the translated input data is summarized in the rows of the matrix x the corresponding translated outputs are the elements of the vector y and the corresponding weights are in the diagonal matrix w in some cases we need the joint input and output data denoted as zx y local dimensionaiity reduction factor analysis lwfa factor analysis everitt is a technique of dimensionality reduction which is the most appropriate given the generating process of our regression data it assumes the observed data z was produced by a mean zero independently distributed k dimensional vector of factors v transformed by the matrix u and contaminated by mean zero independent noise e with diagonal covariance matrix zuve where z xry and e ex if both v and are normally distributed the parameters c and u can be obtained iteratively by the expectation maximization algorithm em rubin thayer for a linear regression problem one assumes that z was generated with ui r and v where denotes the vector of regression coefficients of the linear model y yx and i the identity matrix after calculating c and u by em in joint data space as formulated in an estimate of can be derived from the conditional probability pyl x as all distributions are assumed to be normal the expected value ofy is the mean of this conditional distribution the locally weighted version lwfa of can be obtained together with an estimate of the factors v from the joint weighted covariance matrix w of z and v u r mk xn mkxmk where e denotes the expectation operator and b a matrix of coefficients involved in estimating the factors v note that unless the noise e is zero the estimated is different from the true as it tries to average out the noise in the data joint space principal component analysis lwpca an alternative way of determining the parameters in a reduced space employs locally weighted principal component analysis lwpca in the joint data space by defining the largest kl principal components of the weighted covariance matrix of z as u ueigertvectorszwizixzitzwimaxlkl and noting that the eigenvectors in u are unit length the matrix inversion theorem horn johnson provides a means to derive an efficient estimate of ux n x k uuy r i where uymxkj uy t uyuy t vyuyt e in our one dimensional output case uy is just a x k dimensional row vector and the evaluation of does not require a matrix inversion anymore but rather a division if one assumes normal distributions in all variables as in lwfa lwpca is the special case of lwfa where the noise covariance f is spherical ie the same magnitude of noise in all observables under these circumstances the subspaces spanned by u in both methods will be the same however the regression coefficients of lwpca will be different from those of lwfa unless the noise level is zero as lwfa optimizes the coefficients according to the noise in the data equation thus for normal distributions and a correct guess of k lwpca is always expected to perform worse than lwfa chaal vijayakumar and c g atkeson partial least squares lwpls lwplsi partial least squares wold frank friedman recursively computes orthogonal projections of the input data and performs single variable regressions along these projections on the residuals of the previous iteration step a locally weighted version of partial least squares lwpls proceeds as shown in equation below as all single variable regressions are ordinary unifor training for lookup variate least squares minim izations lwpls initialize initialize makes the same statistical assumption as ordinary linear regressions ie that only output variables do x e y d o x y have additive noise but input variables are noisefor i to k for i to k less the choice of the projections u however introduces an element in lwpls that remains staffsui dr wei s dru tically still debated frank friedman als diu i y y is though interestingly there exists a strong similarsrwei d d sip ity with the way projections are chosen in cascade i correlation fahlman lebiere a peculisrws arity of lwpls is that it also regresses the inputs drws of the previous step against the projected inputs s pi srws in order to ensure the orthogonality of all the projections u since lwpls chooses projections in a d d sip r very powerful way it can accomplish optimal function fits with only one single projections ie k l for certain input distributions we will address this issue in our empirical evaluations by comparing k step lwpls with step lwpls abbreviated lwplsi principal component regression lwpcr although not optimal a computationally efficient techniques of dimensionality reduction for linear regression is principal component regression lwpcr massy the inputs are projected onto the largest k principal components of the weighted covariance matrix of the input data by the matrix u geigertvectofszwixi xxi tewimaxlk the regression coefficients are thus calculated as urxrwxu urxrwy equation is inexpensive to evaluate since after projecting x with u urxrwxu becomes a diagonal matrix that is easy to invert lwpcr assumes that the inputs have additive spherical noise which includes the zero noise case as during dimensionality reduction lwpcr does not take into account the output data it is endangered by clipping input dimensions with low variance which nevertheless have important contribution to the regression output however from a statistical point of view it is less likely that low variance inputs have significant contribution in a linear regression as the confidence bands of the regression coefficients increase inversely proportionally with the variance of the associated input if the input data has non spherical noise lwpcr is prone to focus the regression on irrelevant projections monte carlo evaluations in order to evaluate the candidate methods data sets with inputs and output were randomly generated each data set consisted of training points and test points distributed either uniformly or nonuniformly in the unit hypercube the outputs were local dimensionality reduction generated by either a linear or quadratic function afterwards the dimensional input space was projected into a dimensional space by a randomly chosen distance preserving linear transformation finally gaussian noise of various magnitudes was added to both the dimensional inputs and one dimensional output for the test sets the additive noise in the outputs was omitted each regression technique was localized by a gaussian kernel equation with a dimensional distance metric di d was manually chosen to ensure that the gaussian kernel had sufficiently many data points and no data holes in the fringe areas of the kernel the precise experimental conditions followed closely those suggested by frank and friedman kinds of linear functions y irx for i i r ii i r t t t kinds of quadratic functions y iix axl x x x xs for i i r and qua r and ii z r and q r kinds of noise conditions each with sub conditions i only output noise a low noise local signalnoise ratio lsnr and b high noise lsnr ii equal noise in inputs and outputs a low noise ex y n n and b high noise ex n n iii unequal noise in inputs and outputs a low noise e nn n and lsnr and b high noise e nln n and lsnr kinds of input distributions i uniform in unit hyper cube ii uniform in unit hyper cube excluding data points which activate a gaussian weighting function at c r with di more than w this forms a hyper kidney shaped distribution every algorithm was ran times on each of the combinations of the conditions additionally the complete test was repeated for three further conditions varying the di mensionality called factors in accordance with lwfa that the algorithms assumed to be the true dimensionality of the o dimensional data from k to ie too few correct and too many factors the average results are summarized in figure figure abc show the summary results of the three factor conditions besides averaging over the trials per condition each mean of these charts also averages over the two input distribution conditions and the linear and quadratic function condition as these four cases are frequently observed violations of the statistical assumptions in nonlinear function approximation with locally linear models in figure lb the number of factors equals the underlying dimensionality of the problem and all algorithms are essentially performing equally well for perfectly gaussian distributions in all random variables not shown separately lwfas assumptions are perfectly fulfilled and it achieves the best results however almost indistinguishable closely followed by lwpls for the unequal noise condition the two pca based techniques lwpca and lwpcr perform the worst since as expected they choose suboptimal projections however when violating the statistical assumptions lwfa loses parts of its advantages such that the summary results become fairly balanced in figure b the quality of function fitting changes significantly when violating the correct number of factors as illustrated in figure i ac for too few factors figure a lwpcr performs worst because it randomly omits one of the principle components in the input data without respect to how important it is for the regression the second worse is lwfa according to its assumptions it believes that the signal it cannot model must be noise leading to a degraded estimate of the datas subspace and consequently degraded regression results lwpls has a clear lead in this test closely followed by lwpca and lwplsi except for lwfa all methods can evaluate a data set in non iterative calculations lwfa was mined with em for maximally iterations or until the log likelihood increased less than le in one iteration s schaai s vijayakumar and c g atkeson for too many factors than necessary figure c it is now lwpca which degrades this effect is due to its extracting one very noise contaminated projection which strongly influences the recovery of the regression parameters in equation all other algorithms perform almost equally well with lwfa and lwpls taking a small lead only output equal noise in all unequal noise in all noise inputs and outputs inputs and outputs u io tu o o o lro o o jlao jlao o lao a regression results with factors o ool ooolll b regrssslon results with factors c regression results with factors oi tu d summary results figure average summary results of monte carlo experiments each chart is primarily divided into the three major noise conditions of headers in chart a in each noise condition there are four further subdivision i coefficients of linear or quadratic model are equal with low added noise ii like i with high added noise iii coefficients of linear or quadratic model are different with low noise added iv like iii with high added noise refer to text and descriptions of monte carlo studies for further explanations local dimensionaiity reduction summary and conclusions figure d summarizes all the monte carlo experiments in a final average plot except for lwpls every other technique showed at least one clear weakness in one of our robustness tests it was particularly an incorrect number of factors which made these weaknesses apparent for high dimensional regression problems the local dimensionality ie the number of factors is not a clearly defined number but rather a varying quantity depending on the way the generating process operates usually this process does not need to generate locally low dimensional distributions however it often chooses to do so for instance as human ann movements follow stereotypic patterns despite they could generate arbitrary ones thus local dimensionality reduction needs to find autonomously the appropriate number of local factor locally weighted partial least squares turned out to be a surprisingly robust technique for this purpose even outperforming the statistically appealing probabilistic factor analysis as in principal component analysis lwplss number of factors can easily be controlled just based on a variance cutoff threshold in input space frank friedman while factor analysis usually requires expensive cross validation techniques simple variance based control over the number of factors can actually improve the results of lwpca and lwpcr in practice since as shown in figure a lwpcr is more robust towards overestimating the number of factors while lwpca is more robust towards an underestimation if one is interested in dynamically growing the number of factors while obtaining already good regression results with too few factors lwpca and especially lwpls seem to be appropriate it should be noted how well one factor lwpls lwplsi already performed in figure in conclusion since locally weighted partial least squares was equally robust as local weighted factor analysis towards additive noise in both input and output data and moreover superior when mis guessing the number of factors it seems to be a most favorable technique for local dimensionality reduction for high dimensional regressions acknowledgments the authors are grateful to geoffrey hinton for reminding them of parfal least squares this work was supported by the atr human information processing research laboratories s schaals support includes the german research association the alexander von humboldt foundation and the german scholarship foundation s vijayakumar was supported by the japanese ministry of education science and culture monbusho c g atkeson acknowledges the air force office of scientific research grant f and a national science foundation presidential young investigators award references atkeson c g moore a w schaal s a locally weighted learning artificial intelligence review pp atkeson c g moore a w schaal s c locally weighted learning for control artificial intelligence review pp belsley d a kuh e welsch r e regression diagnostics identiing influential data and sources ofcollinearity new york wiley everitt b s an
we explore methods for incorporating prior knowledge about a problem at hand in support vector learning machines we show that both invariances under group transformations and prior knowledge about locality in images can be incorporated by constructing appropriate kernel functions
boosting is a general method for improving the performance of any learning algorithm that consistently generates classifiers which need to perform only slightly better than random guessing a recently proposed and very promising boosting algorithm is adaboost it has been applied with great success to several benchmark machine learning problems using rather simple learning algorithms and decision trees in this paper we use adaboost to improve the performances of neural networks we compare training methods based on sampling the training set and weighting the cost function our system achieves about error on a data base of online handwritten digits from more than writers adaptive boosting of a multi layer network achieved error on the uci letters and error on the uci satellite data set

monotonicity is a constraint which arises in many application domains we present a machine learning model the monotonic network for which monotonicity can be enforced exactly ie by virtue of functional form a straightforward method for implementing and training a monotonic network is described monotonic networks are proven to be universal approximators of continuous differentiable monotonic functions we apply monotonic networks to a real world task in corporate bond rating prediction and compare them to other approaches i
in this paper the technique of stacking previously only used for supervised learning is applied to unsupervised learning specifically it is used for non parametric multivariate density estimation to combine finite mixture model and kernel density estimators experimental results on both simulated data and real world data sets clearly demonstrate that stacked density estimation outperforms other strategies such as choosing the single best model based on cross validation combining with uniform weights and even the single best model chosen by cheating by looking at the data used for independent testing
similarity based fault tolerant retrieval in neural associative memories nam has not lead to wiedespread applications a drawback of the efficient willshaw model for sparse patterns ste wblh is that the high asymptotic information capacity is of little practical use because of high cross talk noise arising in the retrieval for finite sizes here a new bidirectional iterative retrieval method for the willshaw model is presented called crosswise bidirectional cb retrieval providing enhanced performance we discuss its asymptotic capacity limit analyze the first step and compare it in experiments with the willshaw model applying the very efficient cb memory model either in information retrieval systems or as a functional model for reciprocal cortico cortical pathways requires more than robustness against random noise in the input our experiments show also the segmentation ability of cb retrieval with addresses containing the superposition of pattens provided even at high memory load
nonlinear dimensionality reduction is formulated here as the problem of trying to find a euclidean feature space embedding of a set of observations that preserves as closely as possible their intrinsic metric structurethe distances between points on the observation manifold as measured along geodesic paths our isometric feature mapping procedure or isomap is able to reliably recover low dimensional nonlinear structure in realistic perceptual data sets such as a manifold of face images where conventional global mapping methods find only local minima the recovered map provides a canonical set of globally meaningful features which allows perceptual transformations such as interpolation extrapolation and analogy highly nonlinear transformations in the original observation space to be computed with simple linear operations in feature space
our aim in this paper is to develop a bayesian flamework for matching hierarchical relational models the goal is to make discrete label assignments so as to optimise a global cost function that draws information concerning the consistency of match from different levels of the hierarchy our bayesian development naturally distinguishes between intra level and inter level constraints this allows the impact of reassigning a match to be assessed not only at its own or peer level of representation but also upon its parents and children in the hierarchy i
for blind source separation when the fisher information matrix is used as the riemannian metric tensor for the parameter space the steepest descent algorithm to maximize the likelihood function in this riemannian parameter space becomes the serial updating rule with equivariant property this algorithm can be further simplified by using the asymptotic form of the fisher information matrix around the equilibrium i
an asynchronous pdm pulse density modulating digital neural network system has been developed in our laboratory it consists of one thousand neurons that are physically interconnected via one million bit synapses it can solve one thousand simultaneous nonlinear first order differential equations in a fully parallel and continuous fashion the performance of this system was measured by a winner take all network with one thousand neurons although the magnitude of the input and network parameters were identical for each competing neuron one of them won in milliseconds this processing speed amounts to billion connections per second a broad range of neural networks including spatiotemporal filtering feedforward and feedback networks can be run by loading appropriate network parameters from a host system i

we have a developed an analog vlsi system that models the coordination of neurobiological segmental oscillators we have implemented and tested a system that consists of a chain of eleven pattern generating circuits that are synaptically coupled to their nearest neighbors each pattern generating circuit is implemented with two silicon morris lecar neurons that are connected in a reciprocally inhibitory network we discuss the mechanisms of oscillations in the two cell network and explore system behavior based on isotropic and anisotropic coupling and frequency gradients along the chain of oscillators
we describe the design fabrication and test results of an analog cmos vlsi neural network prototype chip intended for phase based machine vision algorithms the chip implements an image filtering operation similar to gabor filtering because a gabor filters output is complex valued it can be used to define a phase at every pixel in an image this phase can be used in robust algorithms for disparity estimation and binocular stereo vergence control in stereo vision and for image motion analysis the chip reported here takes an input image and generates two outputs at every pixel corresponding to the real and imaginary parts of the output
we present a method for the analysis of nonstationary time series with multiple operating modes in particular it is possible to detect and to model both a switching of the dynamics and a less abrupt time consuming drift from one mode to another this is achieved in two steps first an unsupervised training method provides prediction experts for the inherent dynamical modes then the trained experts are used in a hidden markov model that allows to model drifts an application to physiological wakesleep data demonstrates that analysis and modeling of real world time series can be improved when the drift paradigm is taken into account i
we discuss the problem of catastrophic fusion in multimodal recognition systems this problem arises in systems that need to fuse different channels in non stationary environments practice shows that when recognition modules within each modality are tested in contexts inconsistent with their assumptions their influence on the fused product tends to increase with catastrophic results we explore a principled solution to this problem based upon bayesian ideas of competitive models and inference robustification each sensory channel is provided with simple white noise context models and the perceptual hypothesis and context are jointly estimated consequenty context deviations are interpreted as changes in white noise contamination strength automatically adjusting the influence of the module the approach is tested on a fixed lexicon automatic audiovisual speech recognition problem with very good results i
hidden markov models hmms for automatic speech recognition rely on high dimensional feature vectors to summarize the shorttime properties of speech correlations between features can arise when the speech signal is non stationary or corrupted by noise we investigate how to model these correlations using factor analysis a statistical method for dimensionality reduction factor analysis uses a small number of parameters to model the covariance structure of high dimensional data these parameters are estimated by an expectation maximization em algorithm that can be embedded in the training procedures for hmms we evaluate the combined use of mixture densities and factor analysis in hmms that recognize alphanumeric strings holding the total number of parameters fixed we find that these methods properly combined yield better models than either method on its own i
we apply information maximization maximum likelihood blind source separation to complex valued signals mixed with complex valued nonstationary matrices this case arises in radio communications with baseband signals we incorporate known source signal distributions in the adaptation thus making the algorithms less blind this results in drastic reduction of the amount of data needed for successful convergence adaptation to rapidly changing signal mixing conditions such as to fading in mobile communications becomes now feasible as demonstrated by simulations i
in this paper we present a novel hybrid architecture for continuous speech recognition systems it consists of a continuous hmm system extended by an arbitrary neural network that is used as a preprocessor that takes several frames of the feature vector as input to produce more discriminative feature vectors with respect to the underlying hmm system this hybrid system is an extension of a state of the art continuous hmm system andin fact it is the first hybrid system that really is capable ofoutperforming these standard systems with respect to the recognition accuracy o experimental results show an relative error reduction of about yo that we achieved on a remarkably good recognition system based on continuous hmms for the resource management word continuous speech recognition task
the observed distribution of natural images is far from uniform on the contrary real images have complex and important structure that can be exploited for image processing recognition and analysis there have been many proposed approaches to the principled statistical modeling of images but each has been limited in either the complexity of the models or the complexity of the images we present a non parametric multi scale statistical model for images that can be used for recognition image de noising and in a generatire mode to synthesize high quality textures
this paper describes a new approach to extracting d perspective structure from d point sets the novel feature is to unify the tasks of estimating transformation geometry and identifying pointcorrespondence matches unification is realised by constructing a mixture model over the bi partite graph representing the correspondence match and by effecting optimisation using the em algorithm according to our em framework the probabilities of structural correspondence gate contributions to the expected likelihood function used to estimate maximum likelihood perspective pose parameters this provides a means of rejecting structural outliers i
image intensity variations can result from several different object surface effects including shading from dimensional relief of the object or paint on the surface itself an essential problem in vision which people solve naturally is to attribute the proper physical cause eg surface relief or paint to an observed image we addressed this problem with an approach combining psychophysical and bayesian computational methods we assessed human performance on a set of test images and found that people made fairly consistent judgements of surface properties our computational model assigned simple prior probabilities to different relief or paint explanations for an image and solved for the most probable interpretation in a bayesian framework the ratings of the test images by our algorithm compared surprisingly well with the mean ratings of our subjects i
an image is often represented by a set of detected features we get an enormous compression by representing images in this way furthermore we get a representation which is little affected by small amounts of noise in the image however features are typically chosen in an ad hoc manner we show how a good set of features can be obtained using sufficient statistics the idea of sparse data representation naturally arises we treat the dimensional and dimensional signal reconstruction problem to make our ideas concrete i
a model of motion detection is presented the model contains three stages the first stage is unoriented and is selective for contrast polarities the next two stages work in parallel a phase insensitive stage pools across different contrast polarities through a spatiotemporal filter and thus can detect first and second order motion a phase sensitive stage keeps contrast polarities separate each of which is filtered through a spatiotemporal filter and thus only first order motion can be detected differential phase sensitivity can therefore account for the detection of first and second order motion phase insensitive detectors correspond to cortical complex cells and phase sensitive detectors to simple cells
a neural network approach to stereovision is presented based on aliasing effects of simple disparity estimators and a fast coherencedetection scheme within a single network structure a dense disparity map with an associated validation map and additionally the fused cyclopean view of the scene are available the network operations are based on simple biological plausible circuitry the algorithm is fully parallel and non iterative

we implement a model of obstacle avoidance in flying insects on a small monocular robot the result is a system that is capable of rapid navigation through a dense obstacle field the key to the system is the use of zigzag behavior to articulate the body during movement it is shown that this behavior compensates for a parallax blind spot surrounding the focus of expansion normally found in systems without parallax behavior the system models the cooperation of several behaviors halteres ocular response similar to vor optomotor response and the parallax field computation and mapping to motor system the resulting system is neurally plausible very simple and should be easily hosted on avlsi hardware
converging evidence has shown that human object recognition depends on familiarity with the images of an object further the greater the similarity between objects the stronger is the dependence on object appearance and the more important twodimensional d image information becomes these findings however do not rule out the use of d structural information in recognition and the degree to which d information is used in visual memory is an important issue liu knill kersten showed that any model that is restricted to rotations in the image plane of independent d templates could not account for human performance in discriminating novel object views we now present results from models of generalized radial basis functions grbf d nearest neighbor matching that allows d affine transformations and a bayesian statistical estimator that integrates over all possible d affine transformations the performance of the human observers relative to each of the models is better for the novel views than for the familiar template views suggesting that humans generalize better to novel views from template views the bayesian estimator yields the optimal performance with d arline transformations and independent d templates therefore models of d arline matching operations with independent d templates are unlikely to account for human recognition performance i
scale invariance is a fundamental property of ensembles of natural images their non gaussian properties are less well understood but they indicate the existence of a rich statistical structure in this work we present a detailed study of the marginal statistics of a variable related to the edges in the images a numerical analysis shows that it exhibits extended self similarity this is a scaling property stronger than self similarity all its moments can be expressed as a power of any given moment more interesting all the exponents can be predicted in terms of a multiplicative log poisson process this is the very same model that was used very recently to predict the correct exponents of the structure functions of turbulent flows these results allow us to study the underlying multifractal singularities in particular we find that the most singular structures are one dimensional the most singular manifold consists of sharp edges category visual processing i
the ability to rely on similarity metrics invariant to image transformations is an important issue for image classification tasks such as face or character recognition we analyze an invariant metric that has performed well for the latter the tangent distance and study its limitations when applied to regular images showing that the most significant among these convergence to local minima can be drastically reduced by computing the distance in a multiresolution setting this leads to the multiresolution tangent distance which exhibits significantly higher invariance to image transformations and can be easily combined with robust estimation procedures

in many real world tasks only a small fraction of the available inputs are important at any particular time this paper presents a method for ascertaining the relevance of inputs by exploiting temporal coherence and predictability the method proposed in this paper dynamically allocates relevance to inputs by using expectations of their future values as a model of the task is learned the model is simultaneously extended to create task specific predictions of the future values of inputs inputs which are either not relevant and therefore not accounted for in the model or those which contain noise will not be predicted accurately these inputs can be de emphasized and in turn a new improved model of the task created the techniques presented in this paper have yielded significant improvements for the vision based autonomous control of a land vehicle vision based hand tracking in cluttered scenes and the detection of faults in the etching of semiconductor wafers
a new algorithm is presented which approximates the perceived visual similarity between images the images are initially transformed into a feature space which captures visual structure texture and color using a tree of filters similarity is the inverse of the distance in this perceptual feature space using this algorithm we have constructed an image database system which can perform example based retrieval on large image databases using carefully constructed target sets which limit variation to only a single visual characteristic retrieval rates are quantitatively compared to those of standard methods i
a x pixel general purpose vision chip for spatial focal plane processing is presented the size and configuration of the processing receptive field are programmable the chips architecture allows the photoreceptor cells to be small and densely packed by performing all computation on the read out away from the array in addition to the raw intensity image the chip outputs four processed images in parallel also presented is an application of the chip to line segment orientation detection as found in the retinal receptive fields of toads
flies are capable of rapidly detecting and integrating visual motion information in behaviorly relevant ways the first stage of visual motion processing in flies is a retinotopic array of functional units known as elementary motion detectors emds several decades ago reichardt and colleagues developed a correlation based model of motion detection that described the behavior of these neural circuits we have implemented a variant of this model in a tin analog cmos vlsi process the result is a low power continuous time analog circuit with integrated photoreceptors that responds to motion in real time the responses of the circuit to drifting sinusoidal gratings qualitatively resemble the temporal frequency response spatial frequency response and direction selectivity of motion sensitive neurons observed in insects in addition to its possible engineering applications the circuit could potentially be used as a building block for constructing hardware models of higher level insect motion integration
melonet i is a multi scale neural network system producing baroque style melodic variations given a melody the system invents a four part chorale harmonization and a variation of any chorale voice after being trained on music pieces of composers like j s bach and j pachelbel unlike earlier approaches to the learning of melodic structure the system is able to learn and reproduce high order structure like harmonic motif and phrase structure in melodic sequences this is achieved by using mutually interacting feedforward networks operating at different time scales in combination with kohonen networks to classify and recognize musical structure the results are chorale partitas in the style of j pachelbel their quality has been judged by experts to be comparable to improvisations invented by an experienced human organist i
severe contamination of electroencephalographic eeg activity by eye movements blinks muscle heart and line noise is a serious problem for eeg interpretation and analysis rejecting contaminated eeg segments results in a considerable loss of information and may be impractical for clinical data many methods have been proposed to remove eye movement and blink artifacts from eeg recordings often regression in the time or frequency domain is performed on simultaneous eeg and electrooculographic eog recordings to derive parameters characterizing the appearance and spread of eog artifacts in the eeg channels however eog records also contain brain signals so regressing out eog activity inevitably involves subtracting a portion of the relevant eeg signal from each recording as well regression cannot be used to remove muscle noise or line noise since these have no reference channels here we propose a new and generally applicable method for removing a wide variety of artifacts from eeg records the method is based on an extended version of a previous independent component analysis ica algorithm for performing blind source separation on linear mixtures of independent source signals with either sub gaussian or super gaussian distributions our results show that ica can effectively detect separate and remove activity in eeg records from a wide variety of artifactual sources with results comparing favorably to those obtained using regression based methods extended ica removes artifacts from eeg recordings
we present a novel generic approach to the problem of event related potential identification and classification based on a competitive neural net architecture the network weights converge to the embedded signal patterns resulting in the formation of a matched filter bank the network performance is analyzed via a simulation study exploring identification robustness under low snr conditions and compared to the expected performance from an information theoretic perspective the classifier is applied to real event related potential data recorded during a classic odd ball type paradigm for the first time withinsession variable signal patterns are automatically identified dismissing the strong and limiting requirement of a priori stimulus related selective grouping of the recorded data d h lange h t siegelmann h pratt and g f inbar
we have constructed an inexpensive video based motorized tracking system that learns to track a head it uses real time graphical user inputs or an auxiliary infrared detector as supervisory signals to train a convolutional neural network the inputs to the neural network consist of normalized luminance and chrominance images and motion information from frame differences subsampled images are also used to provide scale invariance during the online training phase the neural network rapidly adjusts the input weights depending upon the reliability of the different channels in the surrounding environment this quick adaptation allows the system to robustly track a head even when other objects are moving within a cluttered background i
in this work we tackle the problem of time series modeling of video traffic different from the existing methods which model the timeseries in the time domain we model the wavelet coefficients in the wavelet domain the strength of the wavelet model includes a unified approach to model both the long range and the short range dependence in the video traffic simultaneously a computationally efficient method on developing the model and generating high quality video traffic and feasibility of performance analysis using the model i
in integrated service communication networks an important problem is to exercise call admission control and routing so as to optimally use the network resources this problem is naturally formulated as a dynamic programming problem which however is too complex to be solved exactly we use methods of reinforcement learning rl together with a decomposition approach to find call admission control and routing policies the performance of our policy for a network with approximately different feature configurations is compared with a commonly used heuristic policy
program execution speed on modem computers is sensitive by a factor of two or more to the order in which instructions are presented to the processor to realize potential execution efficiency an optimizing compiler must employ a heuristic algorithm for instruction scheduling such algorithms are painstakingly hand crafted which is expensive and time consuming we show how to cast the instruction scheduling problem as a learning task obtaining the heuristic scheduling algorithm automatically our focus is the narrower problem of scheduling straight line code also called basic blocks of instructions our empirical results show that just a few features are adequate for quite good performance at this task for a real modem processor and that any of several supervised learning methods perform nearly optimally with respect to the features used
this paper enhances the q learning algorithm for optimal asset allocation proposed in netmeier the new formulation simplifies the approach by using only one value function for many assets and allows model free policy iteration after testing the new algorithm on real data the possibility of risk management within the framework of markov decision problems is analyzed the proposed methods allows the construction of a multi period portfolio management system which takes into account transaction costs the risk preferences of the investor and several constraints on the allocation
with the rapid expansion of computer networks during the past few years security has become a crucial issue for modern computer systems a good way to detect illegitimate use is through monitoring unusual user activity methods of intrusion detection based on hand coded rule sets or predicting commands on line are laborous to build or not very reliable this paper proposes a new way of applying neural networks to detect intrusions we believe that a user leaves a print when using the system a neural network can be used to learn this print and identify each user much like detectives use thumbprints to place people at crime scenes if a users behavior does not match hisher print the system administrator can be alerted of a possible security breech a backpropagation neural network called nnid neural network intrusion detector was trained in the identification task and tested experimentally on a system of users the system was accurate in detecting unusual activity with false alarm rate these results suggest that learning user profiles is an effective way for detecting intrusions
in this paper we propose a technique to incorporate contextual information into object classification in the real word there are cases where the identity of an object is ambiguous due to the noise in the measurements based on which the classification should be made it is helpful to reduce the ambiguity by utilizing extra information referred to as context which in our case is the identities of the accompanying objects this technique is applied to white blood cell classification comparisons are made against no context approach which demonstrates the superior classification performance achieved by using context in our particular application it significantly reduces false alarm rate and thus greatly reduces the cost due to expensive clinical tests author for correspondence incorporating contextual information in white blood cell identification
we describe a system for learning j s bachs rules of musical harmony these rules are learned from examples and are expressed as rule based neural networks the rules are then applied in realtime to generate new accompanying harmony for a live performer real time functionality imposes constraints on the learning and harmonizing processes including limitations on the types of information the system can use as input and the amount of processing the system can perform we demonstrate algorithms for generating and refining musical rules from examples which meet these constraints we describe a method for including a priori knowledge into the rules which yields significant performance gains we then describe techniques for applying these rules to generate new music in real time we conclude the paper with an analysis of experimental results i
this paper reports about an application of bayes inferred neural network classifiers in the field of automatic sleep staging the reason for using bayesian learning for this task is two fold first bayesian inference is known to embody regularization automatically second a side effect of bayesian learning leads to larger variance of network outputs in regions without training data this results in well known moderation effects which can be used to detect outliers in a fold cross validation experiment the full bayesian solution found with r neals hybrid monte carlo algorithm was not better than a single maximum a posteriori map solution found with dj mackays evidence approximation in a second experiment we studied the properties of both solutions in rejecting classification of movement artefacts experiences with bayesian learning in a real world application

we discuss the development of a multi layer perceptron neural network classifier for use in preoperative differentiation between benign and malignemt ovarian tumors as the mean squared classiftcation error is not sufficient to make correct and objective assessments about the performance of the neural classifier the concepts of sensitivity and specificity are introduced and combined in receiver operating characteristic curves based on objective observations such as sonomorphologic criteria color doppler imaging and results from serum tumor markers the neural network is able to make reliable predictions with a discriminating performemce comparable to that of experienced gynecologists i
this paper presents a new approach to the problem of modelling daily rainfall using neural networks we first model the conditional distributions of rainfall amounts in such a way that the model itself determines the order of the process and the time dependent shape and scale of the conditional distributions after integrating over particular weather patterns we are able to extract seasonal variations and long term trends
we explain how the training data can be separated into clean information and unexplainable noise analogous to the data the neural network is separated into a time invariant structure used for forecasting and a noisy part we propose a unified theory connecting the optimization algorithms for cleaning and learning together with algorithms that control the data noise and the parameter noise the combined algorithm allows a data driven local control of the liability of the network parameters and therefore an improvement in generalization the approach is proven to be very useful at the task of forecasting the german bond market
prioritized sweeping is a model based reinforcement learning method that attempts to focus an agents limited computational resources to achieve a good estimate of the value of environment states to choose effectively where to spend a costly planning step classic prioritized sweeping uses a simple heuristic to focus computation on the states that are likely to have the largest errors in this paper we introduce generalized prioritized sweeping a principled method for generating such estimates in a representation specific manner this allows us to extend prioritized sweeping beyond an explicit state based representation to deal with compact representations that are necessary for dealing with large state spaces we apply this method for generalized model approximators such as bayesian networks and describe preliminary experiments that compare our approach with classical prioritized sweeping
this paper describes some of the interactions of model learning algorithms and planning algorithms we have found in exploring model based reinforcement learning the paper focuses on how local trajectory optimizers can be used effectively with learned nonparametric models we find that trajectory planners that are fully consistent with the learned model often have difficulty finding reasonable plans in the early stages of learning trajectory planners that balance obeying the learned model with minimizing cost or maximizing reward often do better even if the plan is not fully consistent with the learned model
a new policy iteration algorithm for partially observable markov decision processes is presented that is simpler and more efficient than an earlier policy iteration algorithm of sondik the key simplification is representation of a policy as a finite state controller this representation makes policy evaluation straightforward the papers contribution is to show that the dynamic programming update used in the policy improvement step can be interpreted as the transformation of a finite state controller into an improved finite state controller the new algorithm consistently outperforms value iteration as an approach to solving infinite horizon problems i
initial experiments described here were directed toward using reinforcement learning rl to develop an automated recovery system ars for high agility aircraft an ars is an outer loop flight control system designed to bring an aircraft from a range of out of control states to straightand level flight in minimum time while satisfying physical and physiological constraints here we report on results for a simple version of the problem involving only single axis pitch simulated recoveries through simulated control experience using a medium fidelity aircraft simulation the rl system approximates an optimal policy for pitch stick inputs to produce minimum time transitions to straight and level flight in unconstrained cases while avoiding ground strike the rl system was also able to adhere to a pilot station acceleration constraint while executing simulated recoveries automated aircraft recovery via reinforcement learning
this paper is concerned with the problem of reinforcement learning rl for continuous state space and time stochastic control problems we state the hamilton jacobi bellman equation satisfied by the value function and use a finite difference method for designing a convergent approximation scheme then we propose a rl algorithm based on this scheme and prove its convergence to the optimal solution
we propose local error estimates together with algorithms for adaptive a posteriori grid and time refinement in reinforcement learning we consider a deterministic system with continuous state and time with infinite horizon discounted cost functional for grid refinement we follow the procedure of numerical methods for the bellman equation for time refinement we propose a new criterion based on consistency estimates of discrete solutions of the bellmanequation we demonstrate that an optimal ratio of time to space discretization is crucial for optimal learning rates and accuracy of the approximate optimal value function i
we present a new approach to reinforcement learning in which the policies considered by the learning process are constrained by hierarchies of partially specified machines this allows for the use of prior knowledge to reduce the search space and provides a framework in which knowledge can be transferred across problems and in which component solutions can be recombined to solve larger and more complicated problems our approach can be seen as providing a link between reinforcement learning and behavior based or teleo reactive approaches to control we present provably convergent algorithms for problem solving and learning with hierarchical machines and demonstrate their effectiveness on a problem with several thousand states
planning doina precup richard s sutton university of massachusetts amherst ma dprecuprich csumassedu abstract planning and learning at multiple levels of temporal abstraction is a key problem for artificial intelligence in this paper we summarize an approach to this problem based on the mathematical framework of markov decision processes and reinforcement learning current model based reinforcement learning is based on one step models that cannot represent common sense higher level actions such as going to lunch grasping an object or flying to denver this paper generalizes prior work on temporally abstract models sutton and extends it from the prediction setting to include actions control and planning we introduce a more general form of temporally abstract model the multi time model and establish its suitability for planning and learning by virtue of its relationship to the bellman equations this paper summarizes the theoretical framework of multi time models and illustrates their potential advantages in a gridworld planning task the need for hierarchical and abstract planning is a fundamental problem in ai see eg sacerdoti laird et al korf kaelbling dayan hinton model based reinforcement learning offers a possible solution to the problem of integrating planning with real time learning and decision making peng williams moore atkeson sutton and barto however current model based reinforcement learning is based on one step models that cannot represent common sense higher level actions modeling such actions requires the ability to handle different interrelated levels of temporal abstraction a new approach to modeling at multiple time scales was introduced by sutton based on prior work by singh dayan and sutton and pinette this approach enables models of the environment at different temporal scales to be intermixed producing temporally abstract models however that work was concerned only with predicting the environment this paper summarizes an extension of the approach including actions and control of the environment precup sutton in particular we generalize the usual notion of a multi time models for temporally abstract planning primitive one step action to an abstract action an arbitrary closed loop policy whereas prior work modeled the behavior of the agent environment system under a single given policy here we learn different models for a set of different policies for each possible way of behaving the agent learns a separate model of what will happen then in planning it can choose between these overall policies as well as between primitive actions to illustrate the kind of advance we are trying to make consider the example shown in figure this is a standard grid world in which the primitive actions are to move from one grid cell to a neighboring cell imagine the learning agent is repeatedly given new tasks in the form of new goal locations to travel to as rapidly as possible if the agent plans at the level of primitive actions then its plans will be many actions long and take a relatively long time to compute planning could be much faster if abstract actions could be used to plan for moving from room to room rather than from cell to cell for each room the agent learns two models for two abstract actions one for traveling efficiently to each adjacent room we do not address in this paper the question of how such abstract actions could be discovered without help instead we focus on the mathematical theory of abstract actions in particular we define a very general semantics for themina property that seems to be required in order for them to be used in the general kind of planning typically used with markov decision processes at the end of this paper we illustrate the theory in this example problem showing how room to room abstract actions can substantially speed planning unreliable primitive actions up left right fail o the time down abstract actions to each rooms haftways figure example task the natural abstract actions are to move from room to room reinforcement learning mdp framework in reinforcement learning a learning agent interacts with an environment at some discrete lowest level time scale t on each time step the agent perceives the state of the environment st and on that basis chooses a primitive action at in response to each primitive action at the environment produces one step later a numerical reward rtl and a next state st the agents objective is to learn a policy a mapping tom states to probabilities of taking each action that maximizes the expected discounted future reward from each state s i where is a discount rate parameter and e denotes an expectation implicitly conditional on the policy being followed the quantity vs is called the value of state s under policy and v x is called the value function for policy the value under the optimal policy is denoted vs max vrs planning in reinforcement learning refers to the use of models of the effects of actions to compute value functions particularly v d precup and r s sutton we assume that the states are discrete and form a finite set st m this is viewed as a temporary theoretical convenience it is not a limitation of the ideas we present this assumption allows us to alternatively denote the value functions v r and v as column vectors v and v each having m components that contain the values of the m states in general for any m vector x we will use the notation zs to refer to its sth component the model of an action a whether primitive or abstract has two components one is an m x m matrix p predicting the state that will result from executing the action in each state the other is a vector g predicting the cumulative reward that will be received along the way in the case of a primitive action p is the matrix of step transition probabilities of the environment times where pts denotes the sth column of pt these are the predictions corresponding to state s and st denotes the unit basis m vector corresponding to st the reward prediction g for a primitive action contains the expected immediate rewards gs e rt i s sa a vs for any stochastic policy w we can similarly define its l step model gr pr as ps erst l st s and gs errt is s vs suitability for planning in conventional planning one step models are used to compute value functions via the bellman equations for prediction and control in vector notation the prediction and control bellman equations are v r gr pv and v maxg pv respectively where the max function is applied component wise in the control equation in planning these equalities are turned into updates eg v gr rye which converge to the value functions thus the bellman equations are usually used to define and compute value functions given models of actions following sutton here we reverse the roles we take the value functions as given and use the bellman equations to define and compute models of new abstract actions in particular a model can be used in planning only if it is stable and consistent with the bellman equations it is useful to define special terms for consistency with each bellman equation let g denote an arbitrary model an m vector and an m x m matrix then this model is said to be valid for policy r sutton if and only if limkoo pk and v r g pv any valid model can be used to compute v via the iteration algorithm v g pv this is a direct sense in which the validity of a model implies that it is suitable for planning we introduce here a parallel definition that expresses consistency with the control bellman equation the model g is said to be non overpromising nop if and only if has only positive elements limoo pk and v g pv where the relation holds component wise if a nop model is added inside the max operator in the control bellman equation this condition ensures that the true value v will not be exceeded for any state thus any model that does not promise more than it multi time models for temporally abstract planning is achievable is not verpromising can serve as an option for planning purposes the one step models of primitive actions are obviously nop due to it is similarly straightforward to show that the one step model of any policy is also nop for some purposes it is more convenient to write a model g p as a single m x m matrix p we say that the model m has been put in homogeneous coordinates the vectors corresponding to the value functions can also be put into homogeneous coordinates by adding an initial element that is always using this notation new models can be combined using two basic operations composition and averaging two models m and m can be composed by matrix multiplication yielding a new model m m m a set of models mi can be averaged weighted by a set of diagonal matrices di such that yi di i to yield a new model m yi dimi sutton showed that the set of models that are valid for a policy is closed under composition and averaging this enables models acting at different time scales to be mixed together and the resulting model can still be used to compute v we have proven that the set of nop models is also closed under composition and averaging precup sutton these operations permit a richer variety of combinations for nop models than they do for valid models because the nop models that are combined need not correspond to a particular policy multi time models the validity and nop ness of a model do not imply each other precup sutton nevertheless we believe a good model should be both valid and nop we would like to describe a class of models that in some sense includes all the interesting models that are valid and non overpromising and which is expressive enough to include common sense notions of abstract action these goals have led us to the notion of a multi time model the simplest example of multi step model called the n step model for policy r predicts the n step truncated return and the state n steps into the future times n if different nstep models of the same policy are averaged the result is called a mixture model mixtures are valid and non overpromising due to the closure properties established in the previous section one kind of mixture suggested in sutton allows an exponential decay of the weights over time controlled by a parameter figure two hypothetical markov environments are mixture models expressive enough for capturing the properties of the environment in order to get some intuition about the expressive power that a model should have let us consider the example in figure if we are only interested if state g is attained then the two environments presented should be characterized by significantly different models however n step models or e ny linear mixture of n step models cannot achieve this goal in order to remediate this problem models should average differently over all the different trajectories that are possible through the state space a full r model sutton can d precup and r s sutton distinguish between these two situations a model is a more general form of mixture model in which a different parameter is associated with each state for a state i i can be viewed as the probability that the trajectory through the state space ends in state i although models seem to have more expressive power they cannot describe n step models we would like to have a more general form of model that unifies both classes this goal is achieved by accurate multi time models multi time models are defined with respect to a policy just as the one step model for a policy is defined by we define g to be an accurate multi time model if and only if for some r for all s and for some sequence of random weights w w such that wt and wt the weights are random variables chosen according to a distribution that depends only on states visited at or before time t the weight wt is a measure of the importance given to the t th state of the trajectory in particular if wt t then state t has no weight associated with it if wt io wi all the remaining weight along the trajectory is given to state t the effect is that state t is the outcome state for the trajectory the random weights along each trajectory make this a very general form of model the only necessary constraint is that the weights depend only on previously visited states in particular we can choose weighting sequences that generate the types of multi step models described in sutton if the weighting variables are such that wl and wt vt n we obtain n step models a weighting sequence of the form wt iiioi where i is the parameter associated to the state visited on time step i describes a full model the main result for multi time models is that they satisfy the two criteria defined in the previous section any accurate multi time model is also nop and valid for r the proofs of these results are too long to include here ihustrative example in order to illustrate the way in which multi time models can be used in practice let us return to the gridworld example figure the cells of the grid correspond to the states of the environment from any state the agent can perform one of four primitive actions up down eft or rght with probability the actions cause the agent to move one cell in the corresponding direction unless this would take the agent into a wall in which case it stays in the same state with probability the agent instead moves in one of the other three directions unless this takes it into a wall of course there is no penalty for bumping into walls in each room we also defined two abstract actions for going to each of the adjacent hallways each abstract action has a set of input states the states in the room and two outcome states the target hallway which corresponds to a successful outcome and the state adjacent to the other hallway which corresponds to failure the agent has wandered out of the room each abstract action is given by its complete model g p where r is the optimal policy for getting into the target hallway and the weighting variables w along any trajectory have the value for the outcome states and everywhere else multi time models for temporally abstract planning iteration ii ll lo ii iteration iteration iteration iteration iteration figure value iteration using primitive and abstract actions the goal state can have an arbitrary position in any of the rooms but for this illustration let us suppose that the goal is two steps down from the right hallway the value of the goal state is there are no rewards along the way and the discounting factor is we performed planning according to the standard value iteration method vkz t maxga pavk a where vos for all the states except the goal state which starts at l in one experiment a ranged only over the primitive actions in the other it ranged over the set including both the primitive and the abstract actions when using only primitive actions the values are propagated one step away on each iteration after six iterations for instance only the states that are at most six steps away from the goal will be attributed non zero values the models of abstract actions produce a significant speed up in the propagation of values at each step figure shows the value function after each iteration using both primitive and abstract actions for planning the area of the circle drawn in each state is proportional to the value attributed to the state the first three iterations are identical with the case when only primitive actions are used however once the values are propagated to the first hallway all the states in the rooms adjacent to that hallway will receive values as well for the states in the room containing the goal these values correspond to performing the abstract action of getting into the right hallway and then following the optimal primitive actions to get to the goal at this point a path to the goal is known from each state in the right half of the environment even if the path is not optimal for all states after six iterations an optimal policy is known for all the states in the environment the models of the abstract actions do not need to be given a priori they can be learned from experience in fact the abstract models that were used in this experiment have been learned during a step random walk in the environment the starting point for d precup and r s sutton learning was represented by the outcome states of each abstract action along with the hypothetical utilities u associated with these states we used q learning watkins to learn the optimal state action value function q associated with each abstract action ub the greedy policy with respect to q is the policy associated with the abstract action ub at the same time we used the model learning algorithm presented in sutton to compute the model corresponding to the policy the learning algorithm is completely online and incremental and its complexity is comparable to that of regular step tdlearning models of abstract actions can be built while an agent is acting in the environment without any additional effort such models can then be used in the planning process as if they would represent primitive actions ensuring more efficient learning and planning especially if the goal is changing over time acknowledgments the authors thank amy mcgovern and andy fagg for helpful discussions and comments contributing to this paper this research was supported in part by nsf grant ecs to andrew g barto and richard s sutton and by afosr grant afosr f to andrew g barto and richard s sutton doina precup also acknowledges the support of the fulbright foundation references dayan p improving generalization for temporal difference learning the successor representation neural computation dayan p hinton g e feudal reinibrcement learning in advances in neural information processing systems volume pp san mateo ca morgan kaufmann kaelbling l p hierarchical learning in stochastic domains preliminary results in proceedings of the tenth international conference on machine learning icml pp san mateo ca morgan kaufmann korf r e learning to solve problems by searching for macro operators london pitman publishing ltd laird j e rosenbloom ps newell a chunking in soar the anatomy of a general learning mechanism machine learning moore a w atkeson c g prioritized sweeping reinforcement learning with less data and less real time machine learning peng j williams j efficient learning and planning within the dyna framework adaptive behavior precup d sutton r s multi time models for reinforcement learning in icml workshop the role of models in reinforcement learning sacerdoti e d a structure for plans and behavior north holland ny elsevier singh s p scaling reinforcement learning by learning variable temporal resolution models in proceedings of the ninth international conference on machine learning icml pp san mateo ca morgan kaufmann sutton r s td models modeling the world as a mixture of time scales in proceedings of the twelfth international conference on machine learning icml pp san mateo ca morgan kaufmann sutton r s barto a g reinforcement learning an

in this paper we show that for discounted mdps with discount factor the asymptotic rate of convergence of q learning is ot r v if r and ovloglogtt otherwise provided that the state action pairs are sampled from a fixed probability distribution here r pminpmax is the ratio of the minimum and maximum state action occupation frequencies the results extend to convergent on line learning provided that pmin where pmin and pmax now become the minimum and maximum state action occupation frequencies corresponding to the stationary distribution i
a learning system composed of linear control modules reinforcement learning modules and selection modules a hybrid reinforcement learning system is proposed for the fast learning of real world control problems the selection modules choose one appropriate control module dependent on the state this hybrid learning system was applied to the control of a stilt type biped robot it learned the control on a sloped floor more quickly than the usual reinforcement learning because it did not need to learn the control on a fiat floor where the linear control module can control the robot when it was trained by a step learning during the first learning step the selection module was trained by a training procedure controlled only by the linear controller it learned the control more quickly the average number of trials about is so small that the learning system is applicable to real robot control
based on computational principles the concept of an internal model for adaptive control has been divided into a forward and an inverse model however there is as yet little evidence that learning control by the cns is through adaptation of one or the other here we examine two adaptive control architectures one based only on the inverse model and other based on a combination of forward and inverse models we then show that for reaching movements of the hand in novel force fields only the learning of the forward model results in key characteristics of performance that match the kinematics of human subjects in contrast the adaptive control system that relies only on the inverse model fails to produce the kinematic patterns observed in the subjects despite the fact that it is more stable our results provide evidence that learning control of novel dynamics is via formation of a forward model i
as a benchmark task the spiral problem is well known in neural networks unlike previous work that emphasizes learning we approach the problem from a generic perspective that does not involve learning we point out that the spiral problem is intrinsically connected to the insideoutside problem a generic solution to both problems is proposed based on oscillatory correlation using a time delay network our simulation results are qualitatively consistent with human performance and we interpret human limitations in terms of synchrony and time delays both biologically plausible as a special case our network without time delays can always distinguish these figures regardless of shape position size and orientation
despite the fact that mental arithmetic is based on only a few hundred basic facts and some simple algorithms humans have a difficult time mastering the subject and even experienced individuals make mistakes associative multiplication the process of doing multiplication by memory without the use of rules or algorithms is especially problematic humans exhibit certain characteristic phenomena in performing associative multiplications both in the type of error and in the error frequency we propose a model for the process of associative multiplication and compare its performance in both these phenomena with data from normal humans and from the model proposed by anderson et al

humans demonstrate a remarkable ability to generate accurate and appropriate motor behavior under many different and often uncertain emqronmental conditions this paper describes a new modular approach to human motor learning and control based on multiple pairs of inverse controller and forward predictor models this architecture simultaneously learns the multiple inverse models necessary for control as well as how to select the inverse models appropriate for a given environment simulations of object manipulation demonstrates the ability to learn multiple objects appropriate generalization to novel objects and the inappropriate activation of motor programs based on visual cues followed by on line correction seen in the size weight illusion
historically connectionist systems have not excelled at representing and manipulating complex structures how can a system composed of simple neuron like computing elements encode complex relations recently researchers have begun to appreciate that representations can extend in both time and space many researchers have proposed that the synchronous firing of units can encode complex representations i identify the limitations of this approach and present an asynchronous model of binding that effectively represents complex structures the asynchronous model extends the synchronous approach i argue that our cognitive architecture utilizes a similar mechanism
the learning of many visual perceptual tasks has been shown to be specific to practiced stimuli while new stimuli require re learning from scratch here we demonstrate generalization using a novel paradigm in motion discrimination where learning has been previously shown to be specific we trained subjects to discriminate the directions of moving dots and verified the previous results that learning does not transfer from the trained direction to a new one however by tracking the subjects performance across time in the new direction we found that their rate of learning doubled therefore learning generalized in a task previously considered too difficult for generalization we also replicated in the second experiment transfer following training with easy stimuli the specificity of perceptual learning and the dichotomy between learning of easy vs difficult tasks were hypothesized to involve different learning processes operating at different visual cortical areas here we show how to interpret these results in terms of signal detection theory with the assumption of limited computational resources we obtain the observed phenomena direct transfer and change of learning rate for increasing levels of taskdifficulty it appears that human generalization concurs with the expected behavior of a generic discrimination system i
structure in a visual scene can be described at many levels of granularity at a coarse level the scene is composed of objects at a finer level each object is made up of parts and the parts of subparts in this work i propose a simple principle by which such hierarchical structure can be extracted from visual scenes regularity in the relations among different parts of an object is weaker than in the internal structure of a part this principle can be applied recursively to define part whole relationships among elements in a scene the principle does not make use of object models categories or other sorts of higher level knowledge rather part whole relationships can be established based on the statistics of a set of sample visual scenes i illustrate with a model that performs unsupervised decomposition of simple scenes the model can account for the results from a human learning experiment on the ontogeny of partwhole relationships
i consider the problem of learning concepts from small numbers of positive examples a feat which humans perform routinely but which computers are rarely capable of bridging machine learning and cognitive science perspectives i present both theoretical analysis and an empirical study with human subjects for the simple task of learning concepts corresponding to axis aligned rectangles in a multidimensional feature space existing learning models when applied to this task cannot explain how subjects generalize from only a few examples of the concept i propose a principled bayesian model based on the assumption that the examples are a random sample from the concept to be learned the model gives precise fits to human behavior on this simple task and provides qualitative insights into more complex realistic cases of concept learning
recent experimental data indicate that the strengthening or weakening of synaptic connections between neurons depends on the relative timing of preand postsynaptic action potentials a hebbian synaptic modification rule based on these data leads to a stable state in which the excitatory and inhibitory inputs to a neuron are balanced producing an irregular pattern of firing it has been proposed that neurons in vivo operate in such a mode
the contrast response function crf of many neurons in the primary visual cortex saturates and shifts towards higher contrast values following prolonged presentation of high contrast visual stimuli using a recurrent neural network of excitatory spiking neurons with adapting synapses we show that both effects could be explained by a fast and a slow component in the synaptic adaptation i fast synaptic depression leads to saturation of the crf and phase advance in the cortical response to high contrast stimuli ii slow adaptation of the synaptic transmitter release probability is derived such that the mutual information between the input and the output of a cortical neuron is maximal this component given by infomax learning rule explains contrast adaptation of the averaged membrane potential dc component as well as the surprising experimental result that the stimulus modulated component f component of a cortical cells membrane potential adapts only weakly based on our results we propose a new experiment to estimate the strength of the effective excitatory feedback to a cortical neuron and we also suggest a relatively simple experimental test to justify our hypothesized synaptic mechanism for contrast adaptation
visually guided arm reaching movements are produced by distributed neural networks within parietal and frontal regions of the cerebral cortex experimental data indicate that single neurons in these regions are broadly tuned to parameters of movement appropriate commands are elaborated by populations of neurons the coordinated action of neurons can be visualized using a neuronal population vector npv however the npv provides only a rough estimate of movement parameters direction velocity and may even fail to reflect the parameters of movement when arm posture is changed we designed a model of the cortical motor command to investigate the relation between the desired direction of the movement the actual direction of movement and the direction of the npv in motor cortex the model is a two layer self organizing neural network which combines broadly tuned muscular proprioceptive and cartesian visual information to calculate angular motor commands for the initial part of the movement of a two link arm the network was trained by motor babbling in positions simulations showed that the network produced appropriate movement direction over a large part of the workspace small deviations of the actual trajectory from the desired trajectory existed at the extremities of the workspace these deviations were accompanied by large deviations of the npv from both trajectories these results suggest the npv does not give a faithful image of cortical processing during arm reaching movements to whom correspondence should be addressed p baraduc e guigon and y burnod
cortical amplification has been proposed as a mechanism for enhancing the selectivity of neurons in the primary visual cortex less appreciated is the fact that the same form of amplification can also be used to de tune or broaden selectivity using a network model with recurrent cortical circuitry we propose that the spatial phase invariance of complex cell responses arises through recurrent amplification of feedforward input neurons in the network respond like simple cells at low gain and complex ceils at high gain similar recurrent mechanisms may play a role in generating invariant representations of feedforward input elsewhere in the visual processing pathway
human and animal studies show that mammalian brain undergoes massive synaptic pruning during childhood removing about half of the synapses until puberty we have previously shown that maintaining network memory performance while synapses are deleted requires that synapses are properly modified and pruned removing the weaker synapses we now show that neuronal regulation a mechanism recently observed to maintain the average neuronal input field results in weight dependent synaptic modification under the correct range of the degradation dimension and synaptic upper bound neuronal regulation removes the weaker synapses and judiciously modifies the remaining synapses it implements near optimal synaptic modification and maintains the memory performance of a network undergoing massive synaptic pruning thus this paper shows that in addition to the known effects of hebbian changes neuronal regulation may play an important role in the self organization of brain networks during development i

determining the relationship between the activity of a single nerve cell to that of an entire population is a fundamental question that bears on the basic neural computation paradigms in this paper we apply an information theoretic approach to quantify the level of cooperative activity among cells in a behavioral context it is possible to discriminate between synergetic activity of the cells vs redundant activity depending on the difference between the information they provide when measured jointly and the information they provide independently we define a synergy value that is positive in the first case and negative in the second and show that the synergy value can be measured by detecting the behavioral mode of the animal from simultaneously recorded activity of the cells we observe that among cortical cells positive synergy can be found while cells from the basal ganglia active during the same task do not exhibit similar synergetic activity litay tishby cshujiacil permanent address institute of computer science and center for neural computation the hebrew university jerusalem israel i gat and n tishby
event related potentials erps are portions of electroencephalographic eeg recordings that are both timeand phase locked to experimental events erps are usually averaged to increase their signalnoise ratio relative to non phase locked eeg activity regardless of the fact that response activity in single epochs may vary widely in time course and scalp distribution this study applies a linear decomposition tool independent component analysis ica to multichannel single trial eeg records to derive spatial filters that decompose single trial eeg epochs into a sum of temporally independent and spatially fixed components arising from distinct or overlapping brain or extra brain networks our results on normal and autistic subjects show that ica can separate artifactual stimulus locked response locked anal non event related background eeg activities into separate components allowing removal of pervasive artifacts of all types from single trial eeg records and identification of both stimulusand responselocked eeg components second this study proposes a new visualization tool the erp image for investigating variability in latencies and amplitudes of event evoked responses in spontaneous eeg or meg records we show that sorting single trial erp epochs in order of reaction time and plotting the potentials in d clearly reveals underlying patterns of response variability linked to performance these analysis and visualization tools appear broadly applicable to electrophyiological research on both normal and clinical populations analyzing and visualizing single trial event related potentials
a correlation based learning rule at the spike level is formulated mathematically analyzed and compared to learning in a firing rate description a differential equation for the learning dynamics is derived under the assumption that the time scales of learning and spiking can be separated for a linear poissonian neuron model which receives time dependent stochastic input we show that spike correlations on a millisecond time scale play indeed a role correlations between input and output spikes tend to stabilize structure formation provided that the form of the learning window is in accordance with hebbs principle conditions for an intrinsic normalization of the average synaptic weight are discussed
here we derive measures quantifying the information loss of a synaptic signal due to the presence ofneuronal noise sources as it electrotonically propagates along a weakly active dendrite we model the dendrite as an infinite linear cable with noise sources distributed along its length the noise sources we consider are thermal noise channel noise arising from the stochastic nature of voltage dependent ionic channels k and na and synaptic noise due to spontaneous background activity we assess the efficacy of information transfer using a signal detection paradigm where the objective is to detect the presenceabsence of a presynaptic spike from the post synaptic membrane voltage this allows us to analytically assess the role of each of these noise sources in information transfer for our choice of parameters we find that the synaptic noise is the dominant noise source which limits the maximum length over which information be reliably transmitted
lateral competition within a layer of neurons sharpens and localizes the response to an input stimulus here we investigate a model for the activity dependent development of ocular dominance maps which allows to vary the degree of lateral competition for weak competition it resembles a correlation based learning model and for strong competition it becomes a self organizing map thus in the regime of weak competition the receptive fields are shaped by the second order statistics of the input patterns whereas in the regime of strong competition the higher moments and features of the individual patterns become important when correlated localized stimuli from two eyes drive the cortical development we find i that a topographic map and binocular localized receptive fields emerge when the degree of competition exceeds a critical value and ii that receptive fields exhibit eye dominance beyond a second critical value for anti correlated activity between the eyes the second order statistics drive the system to develop ocular dominance even for weak competition but no topography emerges topography is established only beyond a critical degree of competition
a new paradigm is proposed for sorting spikes in multi electrode data using ratios of transfer functions between cells and electrodes it is assumed that for every cell and electrode there is a stable linear relation these are dictated by the properties of the tissue the electrodes and their relative geometries the main advantage of the method is that it is insensitive to variations in the shape and amplitude of a spike spike sorting is carried out in two separate steps first templates describing the statistics of each spike type are generated by clustering transfer function ratios then spikes are detected in the data using the spike statistics these techniques were applied to data generated in the escape response system of the cockroach


we study the effect of correlated noise on the accuracy of population coding using a model of a population of neurons that are broadly tuned to an angle in two dimension the fluctuations in he neuronat activity is modeled as a gaussian noise with pairwise correlations which decays exponentially with the difference between the preferred orientations of the pair by calculating the fisher information of the system we show that in the biologically relevant regime of parameters positive correlations decrease the estimation capability of the network relative to the uncorrelated population moreover strong positive correlations result in information capacity which saturates to a finite value as the number of cells in the population grows in contrast negative correlations substantially increase the information capacity of the neuronal population

graphical models provide a broad probabilistic flamework with applications in speech recognition hidden markov models medical diagnosis belief networks and artificial intelligence boltzmann machines however the computing time is typically exponential in the number of nodes in the graph within the variational flamework for approximating these models we present two classes of distributions decimatable boltzmann machines and tractable belief networks that go beyond the standard factorized approach we give generalised mean field equations for both these directed and undirected approximations simulation results on a small benchmark problem suggest using these richer approximations compares favorably against others previously reported in the literature i

we study the dynamics of supervised learning in layered neural networks in the regime where the size p of the training set is proportional to the number n of inputs here the local fields are no longer described by gaussian distributions we use dynamical replica theory to predict the evolution of macroscopic observables including the relevant error measures incorporating the old formalism in the limit pin oz
the kernel parameter is one of the few tunable parameters in support vector machines controlling the complexity of the resulting hypothesis its choice amounts to model selection and its value is usually found by means of a validation set we present an algorithm which can automatically perform model selection with little additional computational cost and with no need of a validation set in this procedure model selection and learning are not separate but kernels are dynamically adjusted during the learning process to find the kernel parameter which provides the best possible upper bound on the generalisation error theoretical results motivating the approach and experimental results confirming its validity are presented i
we solve the dynamics of hopfield type neural networks which store sequences of patterns close to saturation the asymmetry of the interaction matrix in such models leads to violation of detailed balance ruling out an equilibrium statistical mechanical analysis using generating functional methods we derive exact closed equations for dynamical order parameters viz the sequence overlap and correlation and response functions in the limit of an infinite system size we calculate the time translation invariant solutions of these equations describing stationary limit cycles which leads to a phase diagram the effective retarded self interaction usually appearing in symmetric models is here found to vanish which causes a significantly enlarged storage capacity of acm compared to acm for hopfield networks soring static patterns our results are tested against extensive computer simulations and excellent agreement is found a diring a c c coolen and d sherrington
gaussian process gp prediction suffers from on scaling with the data set size n by using a finite dimensional basis to approximate the gp predictor the computational complexity can be reduced we derive optimal finite dimensional predictors under a number of assumptions and show the superiority of these predictors over the projected bayes regression method which is asymptotically optimal we also show how to calculate the minimal model size for a given n the calculations are backed up by numerical experiments i
we describe a unifying method for proving relative loss bounds for online linear threshold classification algorithms such as the perceptron and the winnow algorithms for classification problems the discrete loss is used ie the total number of prediction mistakes we introduce a continuous loss function called the linear hinge loss that can be employed to derive the updates of the algorithms we first prove bounds wrt the linear hinge loss and then convert them to the discrete loss we introduce a notion of average margin of a set of examples we show how relative loss bounds based on the linear hinge loss can be converted to relative loss bounds ito the discrete loss using the average margin
recent works in parameter estimation and neural coding have demonstrated that optimal performance are related to the mutual information between parameters and data we consider the mutual information in the case where the dependency in the parameter a vector of the conditional pdf of each observation a vector is through the scalar product only we derive bounds and asymptotic behaviour for the mutual information and compare with results obtained on the same model with the replica technique
the w s wake sleep algorithm is a simple learning rule for the models with hidden variables it is shown that this algorithm can be applied to a factor analysis model which is a linear version of the helmholtz machine but even for a factor analysis model the general convergence is not proved theoretically in this article we describe the geometrical understanding of the w s algorithm in contrast with the em expectationmaximization algorithm and the era algorithm as the result we prove the convergence of the w s algorithm for the factor analysis model we also show the condition for the convergence in general models
we show the similarity between belief propagation and tap for decoding corrupted messages encoded by sourlass method the latter is a special case of the gallager error correcting code where the code word comprises products of ix bits selected randomly from the original message we examine the efficacy of solutions obtained by the two methods for various values of ix and show that solutions for i may be sensitive to the choice of initial conditions in the case of unbiased patterns good approximations are obtained generally for i and for biased patterns in the case of ix especially when nishimoris temperature is being used i
following recent results showing the importance of the fatshattering dimension in explaining the beneficial effect of a large margin on generalization performance the current paper investigates the implications of these results for the case of imbalanced datasets and develops two approaches to setting the threshold the approaches are incorporated into thetaboost a boosting algorithm for dealing with unequal loss functions the performance of thetaboost and the two approaches are tested experimentally keywords computational learning theory generalization fat shattering large margin pac estimates unequal loss imbalanced datasets i
we study probabilistic inference in large layered bayesian networks represented as directed acyclic graphs we show that the intractability of exact inference in such networks does not preclude their effective use we give algorithms for approximate probabilistic inference that exploit averaging phenomena occurring at nodes with large numbers of parents we show that these algorithms compute rigorous lower and upper bounds on marginal probabilities of interest prove that these bounds become exact in the limit of large networks and provide rates of convergence i
we analyze the asymptotic behavior of autoregressive neural network ar nn processes using techniques from markov chains and non linear time series analysis it is shown that standard ar nns without shortcut connections are asymptotically stationary if linear shortcut connections are allowed only the shortcut weights determine whether the overall system is stationary hence standard conditions for linear ar processes can be used i
symmetrically connected recurrent networks have recently been used as models of a host of neural computations however because of the separation between excitation and inhibition biological neural networks are asymmetrical we study characteristic differences between asymmetrical networks and their symmetrical counterparts showing that they have dramatically different dynamical behavior and also how the differences can be exploited for computational ends we illustrate our results in the case of a network that is a selective amplifier
we consider recurrent analog neural nets where each gate is subject to gaussian noise or any other common noise distribution whose probability density function is nonzero on a large set we show that many regular languages cannot be recognized by networks of this type for example the language w c i w begins with and we give a precise characterization of those languages which can be recognized this result implies severe constraints on possibilities for constructing recurrent analog neural nets that are robust against realistic types of analog noise on the other hand we present a method for constructingfeedforward analog neural nets that are robust with regard to analog noise of this type
oo o i o i margin cumulative training margin distributions for adaboost versus our direct optimization of margins doom algorithm the dark curve is adaboost the light curve is doom doom sacrifices significant training error for improved test error horizontal marks on margin line i
we study the approximation of functions by two layer feedforward neural networks focusing on incremental algorithms which greedily add units estimating single unit parameters at each stage as opposed to standard algorithms for fixed architectures the optimization at each stage is performed over a small number of parameters mitigating many of the difficult numerical problems inherent in high dimensional non linear optimization we establish upper bounds on the error incurred by the algorithm when approximating functions from the sobolev class thereby extending previous results which only provided rates of convergence for functions in certain convex hulls of functional spaces by comparing our results to recently derived lower bounds we show that the greedy algorithms are nearly optimal combined with estimation error results for greedy algorithms a strong case can be made for this type of approach
based on a simple convexity lemma we develop bounds for different types of bayesian prediction errors for regression with gaussian processes the basic bounds are formulated for a fixed training set simpler expressions are obtained for sampling from an input distribution which equals the weight function of the covariance kernel yielding asymptotically tight results the results are compared with numerical experiments i

we solve the dynamics of on line hebbian learning in perceptrons exactly for the regime where the size of the training set scales linearly with the number of inputs we consider both noiseless and noisy teachers our calculation cannot be extended to nonhebbian rules but the solution provides a nice benchmark to test more general and advanced theories for solving the dynamics of learning with restricted training sets
owss log d q logdqhs and owshs log qqlogdqhs are upper bounds for the vc dimension of a set of neural networks of units with piecewise polynomial activation functions where s is the depth of the network h is the number of hidden units w is the number of adjustable parameters q is the maximum of the number of polynomial segments of the activation function and d is the maximum degree of the polynomials also flwslogdqhs is a lower bound for the vc dimension of such a network set which are tight for the cases s h and s is constant for the special case q the vc dimension is ws log d
a new algorithm for support vector regression is described for a priori chosen it automatically adjusts a flexible tube of minimal radius to the data such that at most a fraction of the data points lie outside moreover it is shown how to use parametric tube shapes with non constant radius the algorithm is analysed theoretically and experimentally
we present exact analytical equilibrium solutions for a class of recurrent neural network models with both sequential and parallel neuronal dynamics in which there is a tunable competition between nearestneighbour and long range synaptic interactions this competition is found to induce novel coexistence phenomena as well as discontinuous transitions between pattern recall states cycles and non recall states
i consider the problem of calculating learning curves ie average generalization performance of gaussian processes used for regression a simple expression for the generalization error in terms of the eigenvalue decomposition of the covariance function is derived and used as the starting point for several approximation schemes i identify where these become exact and compare with existing bounds on learning curves the new approximations which can be used for any input space dimension generally get substantially closer to the truth
i present a theory of mean field approximation based on information geometry this theory includes in a consistent way the naive mean field approximation as well as the tap approach and the linear response theorem in statistical physics giving clear information theoretic interpretations to them
many belief networks have been proposed that are composed of binary units however for tasks such as object and speech recognition which produce real valued data binary network models are usually inadequate independent component analysis ica learns a model from real data but the descriptive power of this model is severly limited we begin by describing the independent factor analysis ifa technique which overcomes some of the limitations of ica we then create a multilayer network by cascading singlelayer ifa models at each level the ifa network extracts realvalued latent variables that are non linear functions of the input data with a highly adaptive functional form resulting in a hierarchical distributed representation of these data whereas exact maximum likelihood learning of the network is intractable we derive an algorithm that maximizes a lower bound on the likelihood based on a variational approach
we introduce a semi supervised support vector machine svm method given a training set of labeled data and a working set of unlabeled data sgvm constructs a support vector machine using both the training and working sets we use sgvm to solve the transduction problem using overall risk minimization orm posed by vapnik the transduction problem is to estimate the value of a classification function at the given points in the working set this contrasts with the standard inductive learning problem of estimating the classification function at all possible values and then using the fixed function to deduce the classes of the working set data we propose a general sgvm model that minimizes both the misclassification error and the function capacity based on all the available data we show how the svm model for i norm linear support vector machines can be converted to a mixed integer program and then solved exactly using integer programming resuits of sgvm and the standard norm support vector machine approach are compared on ten data sets our computational results support the statistical learning theory results showing that incorporating working data improves generalization when insufficient training information is available in every case sgvm either improved or showed no significant difference in generalization compared to the traditional approach semi supervised support vector machines
lazy learning is a memory based technique that once a query is received extracts a prediction interpolating locally the neighboring examples of the query which are considered relevant according to a distance measure in this paper we propose a data driven method to select on a query by query basis the optimal number of neighbors to be considered for each prediction as an efficient way to identify and validate local models the recursive least squares algorithm is introduced in the context of local approximation and lazy learning furthermore beside the winner takes all strategy for model selection a local combination of the most promising models is explored the method proposed is tested on six different datasets and compared with a state of the art approach
the technique of principal component analysis pca has recently been expressed as the maximum likelihood solution for a generative latent variable model in this paper we use this probabilistic reformulation as the basis for a bayesian treatment of pca our key result is that effective dimensionality of the latent space equivalent to the number of retained principal components can be determined automatically as part of the bayesian inference procedure an important application of this framework is to mixtures of probabilistic pca models in which each component can determine its own effective complexity
standard techniques eg yule walker are available for learning auto regressive process models of simple directly observable dynamical processes when sensor noise means that dynamics are observed only approximately learning can still been achieved via expectation maximisation em together with kalman filtering however this does not handle more complex dynamics involving multiple classes of motion for that problem we show here how em can be combined with the condensation algorithm which is based on propagation of random sample sets experiments have been performed with visually observed juggling and plausible dynamical models are found to emerge from the learning process i
inference is a key component in learning probabilistic models from partially observable data when learning temporal models each of the many inference phases requires a traversal over an entire long data sequence furthermore the data structures manipulated are exponentially large making this process computationally expensive in we describe an approximate inference algorithm for monitoring stochastic processes and prove bounds on its approximation error in this paper we apply this algorithm as an approximate forward propagation step in an em algorithm for learning temporal bayesian networks we provide a related approximation for the backward step and prove error bounds for the combined algorithm we show empirically that for a real life domain em using our inference algorithm is much faster than em using exact inference with almost no degradation in quality of the learned model we extend our analysis to the online learning task showing a bound on the error resulting from restricting attention to a small window of observations we present an online em learning algorithm for dynamic systems and show that it learns much faster than standard offiine em
we present monte carlo generalized em equations for learning in nonlinear state space models the difficulties lie in the monte carlo e step which consists of sampling from the posterior distribution of the hidden variables given the observations the new idea presented in this paper is to generate samples from a gaussian approximation to the true posterior from which it is easy to obtain independent samples the parameters of the gaussian approximation are either derived from the extended kalman filter or the fisher scoring algorithm in case the posterior density is multimodal we propose to approximate the posterior by a sum of gaussians mixture of modes approach we show that sampling from the approximate posterior densities obtained by the above algorithms leads to better models than using point estimates for the hidden states in our experiment the fisher scoring algorithm obtained a better approximation of the posterior mode than the ekf for a multimodal distribution the mixture of modes approach gave superior results
we propose a novel strategy for training neural networks using sequential sampling importance resampling algorithms this global optimisation strategy allows us to learn the probability distribution of the network weights in a sequential framework it is well suited to applications involving on line nonlinear non gaussian or non stationary signal processing
we examine the problem of estimating the parameters of a multinomial distribution over a large number of discrete outcomes most of which do not appear in the training data we analyze this problem from a bayesian perspective and develop a hierarchical prior that incorporates the assumption that the observed outcomes constitute only a small subset of the possible outcomes we show how to efficiently perform exact inference with this form of hierarchical prior and compare it to standard approaches
we present a stochastic clustering algorithm based on pairwise similarity of datapoints our method extends existing deterministic methods including agglomerative algorithms min cut graph algorithms and connected components thus it provides a common framework for all these methods our graph based method differs from existing stochastic methods which are based on analogy to physical systems the stochastic nature of our method makes it more robust against noise including accidental edges and small spurious clusters we demonstrate the superiority of our algorithm using an example with spiraling bands and a lot of noise i
the expectation maximization em algorithm is an iterative procedure for maximum likelihood parameter estimation from data sets with missing or hidden variables it has been applied to system identification in linear stochastic state space models where the state variables are hidden from the observer and both the state and the parameters of the model have to be estimated simultaneously we present a generalization of the em algorithm for parameter estimation in nonlinear dynamical systems the expectation step makes use of extended kalman smoothing to estimate the state while the maximization step re estimates the parameters using these uncertain state estimates in general the nonlinear maximization step is dicult because it requires integrating out the uncertainty in the states however if gaussian radial basis function rbf approximators are used to model the nonlinearities the integrals become tractable and the maximization step can be solved via systems of linear equations stochastic nonlinear dynamical systems we examine inference and learning in discrete time dynamical systems with hidden state xt inputs ut and outputs yt the state evolves according to stationary nonlinear dynamics driven by the inputs and by additive noise xt fxt ut w all lowercase characters except indices denote vectors matrices are represented by uppercase characters z ghahramani and s t rowels where w is zero mean gaussian noise with covariance q the outputs are nonlinearly related to the states and inputs by yt xt v where v is zero mean gaussian noise with covariance r the vector valued nonlinearlties f and g are assumed to be differentiable but otherwise arbitrary models of this kind have been examined for decades in various communities most notably nonlinear state space models form one of the cornerstones of modern systems and control engineering in this paper we examine these models within the framework of probabilistic graphical models and derive a novel learning algorithm for them based on em with one exception this is to the best of our knowledge the first paper addressing learning of stochastic nonlinear dynamical systems of the kind we have described within the framework of the em algorithm the classical approach to system identification treats the parameters as hidden variables and applies the extended kalman filtering algorithm described in section to the nonlinear system with the state vector augmented by the parameters this approach is inherently on line which may be important in certain applications furthermore it provides an estimate of the covariance of the parameters at each time step in contrast the em algorithm we present is a batch algorithm and does not attempt to estimate the covariance of the parameters there are three important advantages the em algorithm has over the classical approach first the em algorithm provides a straightforward and principled method for handing missing inputs or outputs second em generalizes readily to more complex models with combinations of discrete and real valued hidden variables for example one can formulate em for a mixture of nonlinear dynamical systems third whereas it is often very difficult to prove or analyze stability within the classical on line approach the em algorithm is always attempting to maximize the likelihood which acts as a lyapunov function for stable learning in the next sections we will describe the basic components of the learning algorithm for the expectation step of the algorithm we infer the conditional distribution of the hidden states using extended kalman smoothing section for the maximization step we first discuss the general case section and then describe the particular case where the nonlinearities are represented using gaussian radial basis function rbf networks section extended kalman smoothing given a system described by equations and we need to infer the hidden states from a history of observed inputs and outputs the quantity at the heart of this inference problem is the conditional density pxtul ut yi yt for i t t which captures the fact that the system is stochastic and therefore our inferences about x will be uncertain the gaussian noise assumption is less restrictive for nonlinear systems than for linear systems since the nonlinearity can be used to generate non gaussian state noise the authors have just become aware that briegel and tresp this volume have applied em to essentially the same model briegel and tresps method uses multilayer perceptrons mlp to approximate the nonlinearities and requires sampling from the hidden states to fit the mlp we use gaussian radial basis functions rbfs to model the nonlinearities which can be fit analytically without sampling see section it is important not to confuse this use of the extended kalman algorithm to simultaneously estimate parameters and hidden states with our use of eks to estimate just the hidden state as part of the e step of em learning nonlinear dynamics using em for linear dynamical systems with gaussian state evolution and observation noises this conditional density is gaussian and the recursive algorithm for computing its mean and covariance is known as kalman smoothin kalman smoothing is directly analogous to the forward backward algorithm for computing the conditional hidden state distribution in a hidden markov model and is also a special case of the belief propagation algorithm s for nonlinear systems this conditional density is in general non gaussian and can in fact be quite complex multiple approaches exist for inferring the hidden state distribution of such nonlinear systems including sampling methods and variational approximations we focus instead in this paper on a classic approach from engineering extended kalman smoothing eks extended kalman smoothing simply applies kalman smoothing to a local linearization of the nonlinear system at every point in x space the derivatives of the vector valued functions f and define the matrices a and c ag x x x respectively the dynamics are linearized about st the mean of the kalman filter state estimate at time t xt ft ut da xt t dw the output equation can be similarly linearized if the prior distribution of the hidden state at t was gaussian then in this linearized system the conditional distribution of the hidden state at any time t given the history of inputs and outputs will also be gaussian thus kalman smoothing can be used on the linearized system to infer this conditional distribution see figure left panel learning the m step of the em algorithm re estimates the parameters given the observed inputs outputs and the conditional distributions over the hidden states for the model we have described the parameters define the nonlinearities f and and the noise covariances q and r two complications arise in the m step first it may not be computationally feasible to fully re estimate f and for example if they are represented by neural network regressors a single full m step would be a lengthy training procedure using backpropagation conjugate gradients or some other optimization method alternatively one could use partial m steps for example each consisting of one or a few gradient steps the second complication is that f and have to be trained using the uncertain state estimates output by the eks algorithm consider fitting f which takes as inputs xt and ut and outputs xt for each t the conditional density estimated by eks is a full covariance gaussian in xt xt space so f has to be fit not to a set of data points but instead to a mixture of full covariance gaussians in input output space gaussian clouds of data integrating over this type of noise is non trivial for almost any form of f one simple but inefficient approach to bypass this problem is to draw a large sample from these gaussian clouds of uncertain data and then fit f to these samples in the usual way a similar situation occurs with in the next section we show how by choosing gaussian radial basis functions to model f and both of these complications vanish the forward part of the kalman smoother is the kalman filter z ghahramani and s t roweis fitting radial basis functions to gaussian clouds we will present a general formulation of an rbf network from which it should be clear how to fit special forms for f and g consider the following nonlinear mapping from input vectors x and u to an output vector z i z zhi pix ax bu b w i where w is a zero mean gaussian noise variable with covariance q for example one form of f can be represented using with the substitutions x xt u ut and z xt another with x xtut u and z xt the parameters are the coefficients of the i rbfs hi the matrices a and b multiplying inputs x and u respectively and an output bias vector b each rbf is assumed to be a gaussian in x space with center ci and width given by the covariance matrix si c sixci pix tri exp x i the goal is to fit this model to data ux z the complication is that the data set comes in the form of a mixture of gaussian distributions here we show how to analytically integrate over this mixture distribution to fit the rbf model assume the data set is pxzu uj min ttafjxzz ox uj q x z ox uj dx dz jln we rewrite this in a slightly different notation using angled brackets j to denote expectation over aj and defining h h h a b px pxpix x u then the objective can be written min z jln iqi s that is we observe samples from the u variables each paired with a gaussian cloud of data jvj over x z the gaussian aj has mean uj and covariance matrix cj let ox u i eii hi pix qax qbu b where is the set of parameters h hi a b b the log likelihood of a single data point under the model is z ox u q z ox u in it qconst the maximum likelihood rbf fit to the mixture of gaussian data is obtained by minimizing the following integrated quadratic form learning nonlinear dynamics using em taking derivatives with respect to premultiplying by q l and setting to zero gives the linear equations ydz otij which we can solve for and q ztj tj ezztj eztj j j in other words given the expectations in the angled brackets the optimal parameters can be solved for via a set of linear equations in appendix a we show that these expectations can be computed analytically the derivation is somewhat laborious but the intuition is very simple the gaussian rbfs multiply with the gaussian densities a to form new unnormalized gaussians in x y space expectations under these new gaussians are easy to compute this fitting algorithm is illustrated in the right panel of figure xt i gaussian evidence for t xt gaussian evidence xt gaussian evidence om t i from tl uty t inputs and outputs at ume t input dimension figure illustrations of the e and m steps of the algorithm the left panel shows the information used in extended kalman smoothing eks which infers the hidden state distribution during the e step the right panel illustrates the regression technique employed during the m step a fit to a mixture of gaussian densities is required if gaussian rbf networks are used then this fit can be solved analytically the dashed line shows a regular rbf fit to the centres of the four gaussian densities while the solid line shows the analytic rbf fit using the covariance information the dotted lines below show the support of the rbf kernels results we tested how well our algorithm could learn the dynamics of a nonlinear system by observing only its inputs and outputs the system consisted of a single input state and output variable at each time where the relation of the state from one time step to the next was given by a tanh nonlinearity sample outputs of this system in response to white noise are shown in figure left panel we initialized the nonlinear model with a linear dynamical model trained with em which in turn we initialized with a variant of factor analysis the model was given rbfs in xt space which were uniformly spaced within a range which was automatically determined from the density of points in xt space after the initialization was over the algorithm discovered the sigmoid nonlinearity in the dynamics within less than iterations of em figure middle and right panels further experiments need to be done to determine how practical this method will be in real domains z ghahramani and s t roweis figure left data set used for training first half and testing rest which consists of a time series of inputs u a and outputs y b middle representative plots of log likelihood vs iterations of em for linear dynamical systems dashed line and nonlinear dynamical systems trained as described in this paper solid line note that the actual likelihood for nonlinear dynamical systems cannot generally be computed analytically what is shown here is the approximate likelihood computed by eks the kink in the solid curve comes when initialization with linear dynamics ends and the nonlinearity starts to be learned right means of xx gaussian posteriors computed by eks dots along with the sigmoid nonlinearity dashed line and the rbf nonlinearity learned by the algorithm at no point does the algorithm actually observe x x pairs these are inferred from inputs outputs and the current model parameters discussion this paper brings together two classic algorithms one from statistics and another from systems engineering to address the learning of stochastic nonlinear dynamical systems we have shown that by pairing the extended kalman smoothing algorithm for state estimation in the e step with a radial basis function learning model that permits analytic solution of the m step the em algorithm is capable of learning a nonlinear dynamical model from data as a side effect we have derived an algorithm for training a radial basis function network to fit data in the form of a mixture of gaussians our initial approach has three potential limitations first the m step presented does not modify the centres or widths of the rbf kernels it is possible to compute the expectations required to change the centres and widths but it requires resorting to a partial m step for low dimensional state spaces filling the space with pre fixed kernels is feasible but this strategy needs exponentially many rbfs in high dimensions second em training can be slow especially if initialized poorly understanding how different hidden variable models are related can help devise sensible initialization heuristics for example for this model we used a nested initialization which first learned a simple linear dynamical system which in turn was initialized with a variant of factor analysis third the method presented here learns from batches of data and assumes stationary dynamics we have recently extended it to handle online learning of nonstationary dynamics the belief network literature has recently been dominated by two methods for approximate inference markov chain monte carlo and variational approximations to our knowledge this paper is the first instance where extended kalman smoothing has been used to perform approximate inference in the e step of em while eks does not have the theoretical guarantees of variational methods its simplicity has gained it wide acceptance in the estimation and control literatures as a method for doing inference in nonlinear dynamical systems we are now exploring generalizations of this method to learning nonlinear multilayer belief networks learning nonlinear dynamics using em acknowledgements zg would like to acknowledge the support of the cito ontario and the gatsby charitable fund str was supported in part by the nsf center for neuromorphic systems engineering and by an nserc of canada award a expectations required to fit the rbfs the expectations we need to compute pxj x z starting with some of the easier ones xr j for equation are xj zj xx xz j zz j that do not depend on the rbf kernel p j j cj observe that when we multiply the gaussian rbf kernel pix equation and jv we get a gaussian density over x z with mean and covariance s ci and cij s i c lj and an extra constant due to lack of normalization rio w al cl col exp sij where i csici ujcuj iciui using fiij and ij we can evaluate the other expectations pix fiij x pixj fiiuis and finally pix ptxj exp it where cit s s and it cit c c and itj c s ci c sct c i j j itjitj references t briegel and v tresp fisher scoring and a mixture of modes approach for approximate inference and learning in nonlinear state space models in this volume mit press ap dempster nm laird and db rubin maximum likelihood from incomplete data via the em algorithm j royal statistical society series b m i jordan z ghahramani t s jaakkola and l k saul an
we investigate the problem of learning a classification task on data represented in terms of their pairwise proximities this representation does not refer to an explicit feature representation of the data items and is thus more general than the standard approach of using euclidean feature vectors from which pairwise proximities can always be calculated our first approach is based on a combined linear embedding and classification procedure resulting in an extension of the optimal hyperplane algorithm to pseudo euclidean data as an alternative we present another approach based on a linear threshold model in the proximity values themselves which is optimized using structural risk minimization we show that prior knowledge about the problem can be incorporated by the choice of distance measures and examine different metrics wrt their generalization finally the algorithms are successfully applied to protein structure data and to data from the cats cerebral cortex they show better performance than k nearest neighbor classification
adaptive ridge is a special form of ridge regression balancing the quadratic penalization on each parameter of the model it was shown to be equivalent to lasso least absolute shrinkage and selection operator in the sense that both procedures produce the same estimate lasso can thus be viewed as a particular quadratic penalizer from this observation we derive a fixed point algorithm to compute the lasso solution the analogy provides also a new hyper parameter for tuning effectively the model complexity we finally present a series of possible extensions of lasso performing sparse regression in kernel smoothing additive modeling and neural net training
cluster analysis is a fundamental principle in exploratory data analysis providing the user with a description of the group structure of given data a key problem in this context is the interpretation and visualization of clustering solutions in high dimensional or abstract data spaces in particular probabilistic descriptions of the group structure essential to capture inter cluster relationships are hardly assessable by simple inspection of the probabilistic assignment variables we present a novel approach to the visualization of group structure it is based on a statistical model of the object assignments which have been observed or estimated by a probabilistic clustering procedure the objects or data points are embedded in a low dimensional euclidean space by approximating the observed data statistics with a gaussian mixture model the algorithm provides a new approach to the visualization of the inherent structure for a broad variety of data types eg histogram data proximity data and co occurrence data to demonstrate the power of the approach histograms of textured images are visualized as an example of a large scale data mining application i
this paper reveals a previously ignored connection between two important fields regularization and independent component analysis ica we show that at least one representative of a broad class of algorithms regularizers that reduce network complexity extracts independent features as a by product this algorithm is flat minimum search fms a recent general method for finding low complexity networks with high generalization capability fms works by minimizing both training error and required weight precision according to our theoretical analysis the hidden layer of an fms trained autoassociator attempts at coding each input by a sparse code with as few simple features as possible in experiments the method extracts optimal codes for difficult versions of the noisy bars benchmark problem by separating the underlying sources whereas ica and pca fail real world images are coded with fewer bits per pixel than by ica or pca
dyadzc data refers to a domain with two finite sets of objects in which observations are made for dyads ie pairs with one element from either set this type of data arises naturally in many application ranging from computational linguistics and information retrieval to preference analysis and computer vision in this paper we present a systematic domain independent framework of learning from dyadic data by statistical mixture models our approach covers different models with fiat and hierarchical latent class structures we propose an annealed version of the standard em algorithm for model fitting which is empirically evaluated on a variety of data sets from different domains i
sparse coding is a method for finding a representation of data in which each of the components of the representation is only rarely significantly active such a representation is closely related to redundancy reduction and independent component analysis and has some neurophysiological plausibility in this paper we show how sparse coding can be used for denoising using maximum likelihood estimation of nongaussian variables corrupted by gaussian noise we show how to apply a shrinkage nonlinearity on the components of sparse coding so as to reduce noise furthermore we show how to choose the optimal sparse coding basis for denoising our method is closely related to the method of wavelet shrinkage but has the important benefit over wavelet methods that both the features and the shrinkage parameters are estimated directly from the data
the task in text retrieval is to find the subset of a collection of documents relevant to a users information request usually expressed as a set of words classically documents and queries are represented as vectors of word counts in its simplest form relevance is defined to be the dot product between a document and a query vector a measure of the number of common terms a central difficulty in text retrieval is that the presence or absence of a word is not sufficient to determine relevance to a query linear dimensionality reduction has been proposed as a technique for extracting underlying structure from the document collection in some domains such as vision dimensionality reduction reduces computational complexity in text retrieval it is more often used to improve retrieval performance we propose an alternative and novel technique that produces sparse representations constructed from sets of highly related words documents and queries are represented by their distance to these sets and relevance is measured by the number of common clusters this technique significantly improves retrieval performance is efficient to compute and shares properties with the optimal linear projection operator and the independent components of documents
generative probability models such as hidden markov models provide a principled way of treating missing information and dealing with variable length sequences on the other hand discriminative methods such as support vector machines enable us to construct flexible decision boundaries and often result in classification performance superior to that of the model based approaches an ideal classifier should combine these two complementary approaches in this paper we develop a natural xvay of achieving this combination by deriving kernel functions for use in discriminative methods such as support vector machines from generative probability models we provide a theoretical justification for this combination as well as demonstrate a substantial improvement in the classification performance in the context of dna and protein sequence analysis
we present the cem conditional xpectation maximization algorithm as an extension of the em xpectation maximization algorithm to conditional density estimation under missing data a bounding and maximization process is given to specifically optimize conditional likelihood instead of the usual joint likelihood we apply the method to conditioned mixture models and use bounding techniques to derive the models update rules monotonic convergence computational efficiency and regression results superior to em are demonstrated i
principal curves have been defined as self consistent smooth curves which pass through the middle of a d dimensional probability distribution or data cloud recently we have offered a new approach by defining principal curves as continuous curves of a given length which minimize the expected squared distance between the curve and points of the space randomly chosen according to a given distribution the new definition made it possible to carry out a theoretical analysis of learning principal curves from training data in this paper we propose a practical construction based on the new definition simulation results demonstrate that the new algorithm compares favorably with previous methods both in terms of performance and computational complexity
we present an unsupervised classification algorithm based on an ica mixture model the ica mixture model assumes that the observed data can be categorized into several mutually exclusive data classes in which the components in each class are generated by a linear mixture of independent sources the algorithm finds the independent sources the mixing matrix for each class and also computes the class membership probability for each data point this approach extends the gaussian mixture model so that the classes can have non gaussian structure we demonstrate that this method can learn efficient codes to represent images of natural scenes and text the learned classes of basis functions yield a better approximation of the underlying distributions of the data and thus can provide greater coding efficiency we believe that this method is well suited to modeling structure in high dimensional data and has many potential applications i
a directed generative model for binary data using a small number of hidden continuous units is investigated a clipping nonlinearity distinguishes the model from conventional principal components analysis the relationships between the correlations of the underlying continuous gaussian variables and the binary output variables are utilized to learn the appropriate weights of the network the advantages of this approach are illustrated on a translationally invariant binary distribution and on handwritten digit images
we introduce two new techniques for density estimation our approach poses the problem as a supervised learning task which can be performed using neural networks we introduce a stochastic method for learning the cumulative distribution and an analogous deterministic technique we demonstrate convergence of our methods both theoretically and experimentally and provide comparisons with the parzen estimate our theoretical results demonstrate better convergence properties than the parzen estimate
two developments of nonlinear latent variable models based on radial basis functions are discussed in the first the use of priors or constraints on allowable models is considered as a means of preserving data structure in low dimensional representations for visualisation purposes also a resampling approach is introduced which makes more effective use of the latent samples in evaluating the likelihood


we present a new energy minimization framework for the graph isomorphism problem which is based on an equivalent maximum clique formulation the approach is centered around a fundamental result proved by motzkin and straus in the mid s and recently expanded in various ways which allows us to formulate the maximum clique problem in terms of a standard quadratic program to solve the program we use replicator equations a class of simple continuousand discrete time dynamical systems developed in various branches of theoretical biology we show how despite their inability to escape from local solutions they nevertheless provide experimental results which are competitive with those obtained using more elaborate mean field annealing heuristics
training a support vector machine svm requires the solution of a very large quadratic programming qp problem this paper proposes an algorithm for training svms sequential minimal optimization or smo smo breaks the large qp problem into a series of smallest possible qp problems which are analytically solvable thus smo does not require a numerical qp library smos computation time is dominated by evaluation of the kernel hence kernel optimizations substantially quicken smo for the mnist database smo is times as fast as pcg chunking while for the uci adult database and linear svms smo can be times faster than the pcg chunking algorithm
boosting methods maximize a hard classification margin and are known as powerful techniques that do not exhibit overfitting for low noise cases also for noisy data boosting will try to enforce a hard margin and thereby give too much weight to outliers which then leads to the dilemma of non smooth fits and overfitting therefore we propose three algorithms to allow for soft margin classification by introducing regularization with slack variables into the boosting concept adaboostreg and regularized versions of linear and quadratic programming adaboost experiments show the usefulness of the proposed algorithms in comparison to another soft margin classifier the support vector machine
signal processing and pattern recognition algorithms make extensive use of convolution in many cases computational accuracy is not as important as computational speed in feature extraction for instance the features of interest in a signal are usually quite distorted this form of noise justifies some level of quantization in order to achieve faster feature extraction our approach consists of approximating regions of the signal with low degree polynomials and then differentiating the resulting signals in order to obtain impulse functions or derivatives of impulse functions with this representation convolution becomes extremely simple and can be implemented quite effectively the true convolution can be recovered by integrating the result of the convolution this method yields substantial speed up in feature extraction and is applicable to convolutional neural networks i
we describe a new iterative method for parameter estimation of gaussian mixtures the new method is based on a framework developed by kivinen and warmuth for supervised on line learning in contrast to gradient descent and em which estimate the mixtures covariance matrices the proposed method estimates the inverses of the covariance matrices furthermore the new parameter estimation procedure can be applied in both on line and batch settings we show experimentally that it is typically faster than em and usually requires about half as many iterations as em
semiparametric models are useful tools in the case where domain knowledge exists about the function to be estimated or emphasis is put onto understandability of the model we extend two learning algorithms support vector machines and linear programming machines to this case and give experimental results for sv machines i
we present a probabilistic latent variable framework for data visualisation a key feature of which is its applicability to binary and categorical data types for which few established methods exist a variational approximation to the likelihood is exploited to derive a fast algorithm for determining the model parameters illustrations of application to real and synthetic binary data sets are given i
we present a split and merge em smem algorithm to overcome the local maximum problem in parameter estimation of finite mixture models in the case of mixture models non global maxima often involve having too many components of a mixture model in one part of the space and too few in another widely separated part of the space to escape from such configurations we repeatedly perform simultaneous split and merge operations using a new criterion for efficiently selecting the split and merge candidates we apply the proposed algorithm to the training of gaussian mixtures and mixtures of factor analyzers using synthetic and real data and show the effectiveness of using the split and merge operations to improve the likelihood of both the training data and of held out test data
the hierarchical representation of data has various applications in domains such as data mining machine vision or information retrieval in this paper we inlxoduce an extension of the expectation maximization em algorithm that learns mixture hierarchies in a computationally efficient manner efficiency is achieved by progressing in a bottom up fashion ie by clustering the mixture components of a given level in the hierarchy to obtain those of the level above this clustering requires only knowledge of the mixture parameters there being no need to resort to intermediate samples in addition to practical applications the algorithm allows a new interpretation of em that makes clear the relationship with non parametric kernel based estimation methods provides explicit conixol over the trade off between the bias and variance of em estimates and offers new insights about the behavior of deterministic annealing methods commonly used with em to escape local minima of the likelihood
in gaussian process regression the covariance between the outputs at input locations x and x is usually assumed to depend on the distance x x t w x x where w is a positive definite matrix w is often taken to be diagonal but if we allow w to be a general positive definite matrix which can be tuned on the basis of training data then an eigen analysis of w shows that we are effectively creating hidden features where the dimensionality of the hidden feature space is determined by the data we demonstrate the superiority of predictions using the general matrix over those based on a diagonal matrix on two test problems i
we propose a new in sample cross validation based method randomized gacv for choosing smoothing or bandwidth parameters that govern the bias variance or fit complexity tradeoff in soft classification soft classification refers to a learning procedure which estimates the probability that an example with a given attribute vector is in class vs class the target for optimizing the the tradeoff is the kullback liebler distance between the estimated probability distribution and the true probability distribution representing knowledge of an infinite population the method uses a randomized estimate of the trace of a hessian and mimics cross validation at the cost of a single relearning with perturbed outcome data
a wavelet basis selection procedure is presented for wavelet regression both the basis and threshold are selected using crossvalidation the method includes the capability of incorporating prior knowledge on the smoothness or shape of the basis functions into the basis selection procedure the results of the method are demonstrated using widely published sampled functions the results of the method are contrasted with other basis function based methods
in this paper we introduce a new class of image models which we call dynamic trees or dts a dynamic tree model specifies a prior over a large number of trees each one of which is a tree structured belief net tsbn experiments show that dts are capable of generating images that are less blocky and the models have better translation invariance properties than a fixed balanced tsbn we also show that simulated annealing is effective at finding trees which have high posterior probability
this paper formulates the problem of visual search as bayesian inference and defines a bayesian ensemble of problem instances in particular we address the problem of the detection of visual contours in noiseclutter by optimizing a global criterion which combines local intensity and geometry information we analyze the convergence rates of a search algorithms using results from information theory to bound the probability of rare events within the bayesian ensemble this analysis determines characteristics of the domain which we call order parameters that determine the convergence rates in particular we present a specific admissible a algorithm with pruning which converges with high probability with expected time on in the size of the problem in addition we briefly summarize extensions of this work which address fundamental limits of target contour detectability ie algorithm independent results and the use of non admissible heuristics i
in this paper we present a novel approach to multichannel blind separationgeneralized deconvolution assuming that both mixing and demixing models are described by stable linear state space systems we decompose the blind separation problem into two process separation and state estimation based on the minimization of kullback leibler divergence we develop a novel learning algorithm to train the matrices in the output equation to estimate the state of the demixing model we introduce a new concept called hidden innovation to numerically implement the kalman filter computer simulations are given to show the validity and high effectiveness of the state space approach i
we present an analog vlsi cellular architecture implementing a simpli fled version of the boundary contour system bcs for real time image processing inspired by neuromorphic models across several layers of visual cortex the design integrates in each pixel the functions of simple cells complex cells hyper complex cells and bipole cells in three orientations interconnected on a hexagonal grid analog current mode cmos circuits are used throughout to perform edge detection local inhibition directionally selective long range diffusive kernels and renormalizing global gain control experimental results from a fabricated x pixel prototype in zm cmos technology demonstrate the robustness of the architecture in selecting image contours in a cluttered and noisy background
a modular analogue neuro chip set with on chip learning capability is developed for active noise canceling the analogue neuro chip set incorporates the error backpropagation learning rule for practical applications and allows pin to pin interconnections for multi chip boards the developed neuro board demonstrated active noise canceling without any digital signal processor multi path fading of acoustic channels random noise and nonlinear distortion of the loud speaker are compensated by the adaptive learning circuits of the neuro chips experimental results are reported for cancellation of car noise in real time
in this paper we describe the architecture implementation and experimental results for an intracardiac electrogram iceg classification and compression chip the chip processes and vector quantises dimensional analogue vectors while consuming a maximum of w power for a heart rate of beats per minute vector per second from a v supply this represents a significant advance on previous work which achieved ultra low power supervised morphology classification since the template matching scheme used in this chip enables unsupervised blind classification of abnormal rhythms and the computational support for low bit rate data compression the adaptive template matching scheme used is tolerant to amplitude variations and interand intra sample time shifts
the performance of dedicated vlsi neural processing hardware depends critically on the design of the implemented algorithms we have previously proposed an algorithm for acoustic transient classification having implemented and demonstrated this algorithm in a mixed mode architecture we now investigate variants on the algorithm using time and frequency channel differencing input and output normalization and schemes to binarize and train the template values with the goal of achieving optimal classification performance for the chosen hardware
a circuit for fast compact and low power focal plane motion centroid localization is presented this chip which uses mixed signal cmos components to implement photodetection edge detection on set detection and centroid localization models the retina and superior colliculus the centroid localization circuit uses time windowed asynchronously triggered row and column address events and two linear resistive grids to provide the analog coordinates of the motion centroid this vlsi chip is used to realize fast lightweight autonavigating vehicles the obstacle avoiding line following algorithm is discussed
we describe the first single microphone sound localization system and its inspiration from theories of human monaural sound localization reflections and diffractions caused by the external ear pinna allow humans to estimate sound source elevations using only one ear our single microphone localization model relies on a specially shaped reflecting structure that serves the role of the pinna specially designed analog vlsi circuitry uses echo time processing to localize the sound a cmos integrated circuit has been designed fabricated and successfully demonstrated on actual sounds
a robust integrative algorithm is presented for computing the position of the focus of expansion or axis of rotation the singular point in optical flow fields such as those generated by self motion measurements are shown of a fully parallel cmos analog vlsi motion sensor array which computes the direction of local motion sign of optical flow at each pixel and can directly implement this algorithm the flow field singular point is computed in real time with a power consumption of less than row computation of the singular point for more general flow fields requires measures of field expansion and rotation which it is shown can also be computed in real time hardware again using only the sign of the optical flow field these measures along with the location of the singular point provide robust real time self motion information for the visual guidance of a moving platform such as a robot

this paper presents a novel and fast k nn classifier that is based on a binary cmm correlation matrix memory neural network a robust encoding method is developed to meet cmm input requirements a hardware implementation of the cmm is described which gives over times the speed of a current mid range workstation and is scaleable to very large problems when tested on several benchmarks and compared with a simple k nn method the cmm classifier gave less than lower tccuracy and over and times speed up in software and hardware respectively

a common way to represent a time series is to divide it into shortduration blocks each of which is then represented by a set of basis functions a limitation of this approach however is that the temporal alignment of the basis functions with the underlying structure in the time series is arbitrary we present an algorithm for encoding a time series that does not require blocking the data the algorithm finds an efficient representation by inferring the best temporal positions for functions in a kernel basis these can have arbitrary temporal extent and are not constrained to be orthogonal this allows the model to capture structure in the signal that may occur at arbitrary temporal positions and preserves the relative temporal structure of underlying events the model is shown to be equivalent to a very sparse and highly overcomplete basis under this model the mapping from the data to the representation is nonlinear but can be computed efficiently this form also allows the use of existing methods for adapting the basis itself to data this approach is applied to speech data and results in a shift invariant spike like representation that resembles coding in the cochlear nerve i
this paper introduces a method for regularization of hmm systems that avoids parameter overfitting caused by insufficient training data regularization is done by augmenting the em training method by a penalty term that favors simple and smooth hmm systems the penalty term is constructed as a mixture model of negative exponential distributions that is assumed to generate the state dependent emission probabilities of the hmms this new method is the successful transfer of a well known regularization approach in neural networks to the hmm domain and can be interpreted as a generalization of traditional state tying for hmm systems the effect of regularization is demonstrated for continuous speech recognition tasks by improving overfitted triphone models and by speaker adaptation with limited training data
we describe maximum likelihood continuity mapping malcom an alternative to hidden markov models hmms for processing sequence data such as speech while hmms have a discrete hidden space constrained by a fixed finite automaton architecture malcom has a continuous hidden space a continuity map that is constrained only by a smoothness requirement on paths through the space malcom fits into the same probabilistic framework for speech recognition as hmms but it represents a more realistic model of the speech production process to evaluate the extent to which malcom captures speech production information we generated continuous speech continuity maps for three speakers and used the paths through them to predict measured speech articulator data the median correlation between the malcom paths obtained from only the speech acoustics and articulator measurements was on an independent test set not used to train malcom or the predictor this unsupervised model achieved correlations over speakers and articulators only to lower than those obtained using an analogous supervised method which used articulatory measurements as well as acoustics
we investigate a probabilistic framework for automatic speech recognition based on the intrinsic geometric properties of curves in particular we analyze the setting in which two variables one continuous a one discrete s evolve jointly in time we suppose that the vector a traces out a smooth multidimensional curve and that the variable s evolves stochastically as a function of the arc length traversed along this curve since arc length does not depend on the rate at which a curve is traversed this gives rise to a family of markov processes whose predictions prsla are invariant to nonlinear warpings of time we describe the use of such models known as markov processes on curves mpcs for automatic speech recognition where a are acoustic feature trajectories and s are phonetic transcriptions on two tasks recognizing new jersey town names and connected alpha digits we find that mpcs yield lower word error rates than comparably trained hidden markov models
there has been much recent work on measuring image statistics and on learning probability distributions on images we observe that the mapping from images to statistics is many to one and show it can be quantified by a phase space factor this phase space approach throws light on the minimax entropy technique for learning gibbs distributions on images with potentials derived from image statistics and elucidates the ambiguities that are inherent to determining the potentials in addition it shows that if the phase factor can be approximated by an analytic distribution then this approximation yields a swift minutemax algorithm that vastly reduces the computation time for minimax entropy learning an illustration of this concept using a gaussian to approximate the phase factor gives a good approximation to the results of zhu and mumford in just seconds of cpu time the phase space approach also gives insight into the multi scale potentials found by zhu and mumford and suggests that the forms of the potentials are influenced greatly by phase space considerations finally we prove that probability distributions learned in feature space alone are equivalent to minimax entropy learning with a multinomial approximation of the phase factor i
we present a method for learning complex appearance mappings such as occur with images of articulated objects traditional interpolation networks fail on this case since appearance is not necessarily a smooth function nor a linear manifold for articulated objects we define an appearance mapping from examples by constructing a set of independently smooth interpolation networks these networks can cover overlapping regions of parameter space a set growing procedure is used to find example clusters which are well approximated within their convex hull interpolation then proceeds only within these sets of examples with this method physically valid images are produced even in regions of parameter space where nearby examples have different appearances we show results generating both simulated and real arm images
we seek the scene interpretation that best explains image data for example we may want to infer the projected velocities scene which best explain two consecutive image frames image from synthetic data we model the relationship between image and scene patches and between a scene patch and neighboring scene patches given a new image we propagate likelihoods in a markov network ignoring the effect of loops to infer the underlying scene this yields an efficient method to form low level scene interpretations we demonstrate the technique for motion analysis and estimating high resolution images from low resolution ones i
finding articulated objects like people in pictures presents a particularly difficult object recognition probleln we show how to find people by finding putative body segments and then constructing assemblies of those segments that are consistent with the constraints on the appearance of a person that result froill kinematic properties since a reasonable model of a person requires at least nine segments it is not possible to present every group to a classifier instead the search can be pruned by using projected versions of a classifier that accepts groups corresponding to people we describe an efficient projection algorithm for one popular classifier and demonstrate that our approach can be used to deterlnine whether images of real scenes contain people
we previously proposed a quantitative model of early visual processing in primates based on non linearly interacting visual filters and statistically efficient decision we now use this model to interpret the observed modulation of a range of human psychophysical thresholds with and without focal visual attention our model calibrated by an automatic fitting procedure simultaneously reproduces thresholds for four classical pattern discrimination tasks performed while attention was engaged by another concurrent task our model then predicts that the seemingly complex improvements of certain thresholds which we observed when attention was fully available for the discrimination tasks can best be explained by a strengthening of competition among early visual filters
visual search is the task of finding a target in an image against a background of distractors unique features of targets enable them to pop out against the background while targets defined by lacks of features or conjunctions of features are more difficult to spot it is known that the ease of target detection can change when the roles of figure and ground are switched the mechanisms underlying the ease of pop out and asymmetry in visual search have been elusive this paper shows that a model of segmentation in v based on intracortical interactions can explain many of the qualitative aspects of visual search
face recognition is a k class problem where k is the number of known individuals and support vector machines svms are a binary classification method by reformulating the face recognition problem and reinterpreting the output of the svm classifier we developed a svm based face recognition algorithm the face recognition problem is formulated as a problem in difference space which models dissimilarities between two facial images in difference space we formulate face recognition as a two class problem the classes are dissimilarities between faces of the same person and dissimilarities between faces of different people by modifying the interpretation of the decision surface generated by svm we generated a similarity metric between faces that is learned from examples of differences between faces the svm based algorithm is compared with a principal component analysis pca based algorithm on a difficult set of images from the feret database performance was measured for both verification and identification scenarios the identification performance for svm is versus for pca for verification the equal error rate is for svm and for pca
one of the most important problems in visual perception is that of visual invariance how are objects perceived to be the same despite undergoing transformations such as translations rotations or scaling in this paper we describe a bayesian method for learning invariances based on lie group theory we show that previous approaches based on first order taylor series expansions of inputs can be regarded as special cases of the lie group approach the latter being capable of handling in principle arbitrarily large transformations using a matrixexponential based generative model of images we derive an unsupervised algorithm for learning lie group operators from input data containing infinitesimal transformations the on line unsupervised learning algorithm maximizes the posterior probability of generating the training data we provide experimental results suggesting that the proposed method can learn lie group operators for handling reasonably large d translations and d rotations

we present a probabilistic method for fusion of images produced by multiple sensors the approach is based on an image formation model in which the sensor images are noisy locally linear functions of an underlying true scene a bayesian framework then provides for maximum likelihood or maximum a posteriori estimates of the true scene from the sensor images maximum likelihood estimates of the parameters of the image formation model involve local second order image statistics and thus are related to local principal component analysis we demonstrate the efficacy of the method on images from visible band and infrared sensors
a recent neural model of illusory contour formation is based on a distribution of natural shapes traced by particles moving with constant speed in directions given by brownian motions the input to that model consists of pairs of position and direction constraints and the output consists of the distribution of contours joining all such pairs in general these contours will not be closed and their distribution will not be scale invariant in this paper we show how to compute a scale invariant distribution of closed contours given position constraints alone and use this result to explain a well known illusory contour effect i

this paper describes a simple and efficient method to make template based object classification invariant to in plane rotations the task is divided into two parts orientation discrimination and classification the key idea is to pertbrm the orientation discrimination before the classification this can be accomplished by hypothesizing in turn that the input image belongs to each class of interest the image can then be rotated to maximize its similarity to the training images in each class these contain the prototype object in an upright orientation this process yields a set of images at least one of which will have the object in an upright position the resulting images can then be classified by models which have been trained with only upright examples this approach has been successfully applied to two real world vision based tasks rotated handwritten digit recognition and rotated face detection in cluttered scenes
this paper presents probabilistic modeling methods to solve the problem of discriminating between five facial orientations with very little labeled data three models are explored the first model maintains no inter pixel dependencies the second model is capable of modeling a set of arbitrary pair wise dependencies and the last model allows dependencies only between neighboring pixels we show that for all three of these models the accuracy of the learned models can be greatly improved by augmenting a small number of labeled training images with a large set of unlabeled images using expectation maximization this is important because it is often difficult to obtain image labels while many unlabeled images are readily available through a large set of empirical tests we examine the benefits of unlabeled data for each of the models by using only two randomly selected labeled examples per class we can discriminate between the five facial orientations with an accuracy of with six labeled examples we achieve an accuracy of
gaussian processes provide good prior models for spatial data but can be too smooth in many physical situations there are discontinuities along bounding surfaces for example fronts in near surface wind fields we describe a modelling method for such a constrained discontinuity and demonstrate how to infer the model parameters in wind fields with mcmc sampling
in high energy physics experiments one has to sort through a high flux of events at a rate of tens of mhz and select the few that are of interest one of the key factors in making this decision is the location of the vertex where the interaction that led to the event took place here we present a novel solution to the problem of finding the location of the vertex based on two feedforward neural networks with fixed architectures whose parameters are chosen so as to obtain a high accuracy the system is tested on simulated data sets and is shown to perform better than conventional algorithms i
the artmap fd neural network performs both identification placing test patterns in classes encountered during training and familiarity discrimination judging whether a test pattern belongs to any of the classes encountered during training the performance of artmap fd is tested on radar pulse data obtained in the field and compared to that of the nearest neighbor based nen algorithm and to a k i extension of nen i
computer animation through the numerical simulation of physics based graphics models offers unsurpassed realism but it can be computationally demanding this paper demonstrates the possibility of replacing the numerical simulation of nontrivial dynamic models with a dramatically more efficient neuroanimator that exploits neural networks neuroanimators are automatically trained off line to emulate physical dynamics through the observation of physics based models in action depending on the model its neural network emulator can yield physically realistic animation one or two orders of magnitude faster than conventional numerical simulation we demonstrate neuroanimators for a variety of physics based models
fraud causes substantial losses to telecommunication carriers detection systems which automatically detect illegal use of the network can be used to alleviate the problem previous approaches worked on features derived from the call patterns of individual users in this paper we present a call based detection system based on a hierarchical regime switching model the detection problem is formulated as an inference problem on the regime probabilities inference is implemented by applying the junction tree algorithm to the underlying graphical model the dynamics are learned from data using the em algorithm and subsequent discriminative training the methods are assessed using fraud data from a real mobile communication network
this paper describes a bayesian graph matching algorithm for data mining from large structural data bases the matching algorithm uses edge consistency and node attribute similarity to determine the a posterjori probability of a query graph for each of the candidate matches in the data base the node feature vectors are constructed by computing normalised histograms of pairwise geometric attributes attribute similarity is assessed by computing the bhattacharyya distance between the histograms recognition is realised by selecting the candidate from the data base which has the largest a posterjori probability we illustrate the recognition technique on a data base containing line patterns extracted from real world imagery here the recognition technique is shown to significantly outperform a number of algorithm alternatives i
the execution order of a block of computer instructions can make a difference in its running time by a factor of two or more in order to achieve the best possible speed compilers use heuristic schedulers appropriate to each specific architecture implementation however these heuristic schedulers are time consuming and expensive to build in this paper we present results using both rollouts and reinforcement learning to construct heuristics for scheduling basic blocks the rollout scheduler outperformed a commercial scheduler and the reinforcement learning scheduler performed almost as well as the commercial scheduler
in previous work we advanced a new technique for direct visual matching of images for the purposes of face recognition and image retrieval using a probabilistic measure of similarity based primarily on a bayesian map analysis of image differences leading to a dual basis similar to eigenfaces the performance advantage of this probabilistic matching technique over standard euclidean nearest neighbor eigenface matching was recently demonstrated using results from darpas feret face recognition competition in which this probabilistic matching algorithm was found to be the top performer we have further developed a simple method of replacing the costly compution of nonlinear online bayesian similarity measures by the relatively inexpensive computation of linear offiine subspace projections and simple online euclidean norms thus resulting in a significant computational speed up for implementation with very large image databases as typically encountered in real world applications
we propose to train trading systems by optimizing financial objective functions via reinforcement learning the performance functions that we consider are profit or wealth the sharpe ratio and our recently proposed differential sharpe ratio for online learning in moody wu we presented empirical results that demonstrate the advantages of reinforcement learning relative to supervised learning here we extend our previous work to compare q learning to our recurrent reinforcement learning rrl algorithm we provide new simulation results that demonstrate the presence of predictability in the monthly sp stock index for the year period through as well as a sensitivity analysis that provides economic insight into the traders structure
we describe a real time computer vision and machine learning system for modeling and recognizing human actions and interactions two different domains are explored recognition of two handed motions in the martial art tai chi and multiple person interactions in a visual surveillance task our system combines top down with bottom up information using a feedback loop and is formulated with a bayesian framework two different graphical models hmms and coupled hmms are used for modeling both individual actions and multiple agent interactions and chmms are shown to work more efficiently and accurately for a given amount of training finally to overcome the limited amounts of training data we demonstrate that synthetic agents alife style agents can be used to develop flexible prior models of the person to person interactions i
calcium cais an ubiquitous intracellular messenger which regulates cellular processes such as secretion contraction and cell proliferation a number of different cell types respond to hormonal stimuli with periodic oscillations of the intracellular free calcium concentration cai these ca signals are often organized in complex temporal and spatial patterns even under conditions of sustained stimulation here we study the spario temporal aspects of intracellular calcium cai oscillations in clonal cells hamster insulin secreting cells hit under pharmacological stimulation schsfl et al we use a novel fast fixed point algorithm hyvirinen and oja for independent component analysis ica to blind source separation of the spario temporal dynamics of cai in a hit cell using this approach we find two significant independent components out of five differently mixed input signals one cai signal with a mean oscillatory period of s and a high frequency signal with a broadband power spectrum with considerable spectral density this results is in good agreement with a study on high frequency cai oscillations palus et al further theoretical and experimental studies have to be performed to resolve the question on the functional impact of intracellular signaling of these independent cai signals k prank et al
we have previously presented a coarse to fine hierarchical pyramidneural network hpnn architecture which combines multiscale image processing techniques with neural networks in this paper we present applications of this general architecture to two problems in mammographic computer aided diagnosis cad the first application is the detection of microcalcifications the coarse to fine hpnn was designed to learn large scale context infbrmation fbr detecting small objects like microcalcifications receiver operating characteristic roc analysis suggests that the hierarchical architecture improves detection performance of a well established cad system by roughly the second application is to detect mammographic masses directly since masses are large extended objects the coarse to fine hpnn architecture is not suitable for this problem instead we construct a fine to coarse hpnn architecture which is designed to learn small scale detail structure associated with the extended objects our initial results applying the fine to coarse hpnn to mass detection are encouraging with detection perfbrmance improvements of about we conclude that the ability of the hpnn architecture to integrate information across scales both coarse to fine and fine to coarse makes it well suited tbr detecting objects which may have contextual clues or detail structure occurring at scales other than the natural scale of the object
this paper applies the mixture of gaussians probabilistic model combined with expectation maximization optimization to the task of summarizing three dimensional range data for a mobile robot this provides a flexible way of dealing with uncertainties in sensor information and allows the
a collective intelligence coin is a set of interacting reinforcement learning rl algorithms designed in an automated fashion so that their collective behavior optimizes a global utility function we summarize the theory of coins then present experiments using that theory to design coins to control internet traffic routing these experiments indicate that coins outperform all previously investigated rl based shortest path routing algorithms i
patti game moore a moore b moore and atkeson is a reinforcement learning rl algorithm that has a lot of promise in overcoming the curse of dimensionality that can plague rl algorithms when applied to high dimensional problems in this paper we introduce modifications to the algorithm that further improve its performance and robustness in addition while patti game solutions can be improved locally by standard local path improvement techniques we introduce an add on algorithm in the same spirit as parti game that instead tries to improve solutions in a non local manner
a simple learning rule is derived the vaps algorithm which can be instantiated to generate a wide range of new reinforcementlearning algorithms these algorithms solve a number of open problems define several new approaches to reinforcement learning and unify different approaches to reinforcement learning under a single theory these algorithms all have guaranteed convergence and include modifications of several existing algorithms that were known to fail to converge on simple mdps these include qlearning sarsa and advantage learning in addition to these value based algorithms it also generates pure policy search reinforcement learning algorithms which learn optimal policies without learning a value function in addition it allows policysearch and value based algorithms to be combined thus unifying two very different approaches to reinforcement learning into a single value and policy search vaps algorithm and these algorithms converge for pomdps without requiring a proper belief state simulations results are given and several areas for future research are discussed convergence of greedy exploration many reinforcement learning algorithms are known that use a parameterized function approximator to represent a value function and adjust the weights incrementally during learning examples include q learning sarsa and advantage learning there are simple mdps where the original form of these algorithms fails to converge as summarized in table for the cases with the algorithms are guaranteed to converge under reasonable assumptions such as gradient descent for general reinforcement learning table current convergence results for incremental value based rl algorithms residual algorithms changed every x in the first two columns to x the new algorithms proposed in this taper change evei y x to a x fixed fixel i usuallydistribution distrution i greedy on policy distribution lookup table markov averager chain linear nonlinear lookup table mdp averager linear nonlinear lookup table pomdp averager nonlinear convergence guaranteed xcounterexample is known that either diverges or oscillates between the best and worst possible policies decaying learning rates for the cases with x there are known counterexamples where it will either diverge or oscillate between the best and worst possible policies which have very different values this can happen even with infinite training time and slowly decreasing learning rates baird gordon each x in the first two columns can be changed to a and made to converge by using a modified form of the algorithm the residual form baird but this is only possible when learning with a fixed training distribution and that is rarely practical for most large problems it is useful to explore with a policy that is usually greedy with respect to the current value function and that changes as the value function changes in that case the rightmost column of the chart the current convergence guarantees are not very good one way to guarantee convergence in all three columns is to modify the algorithm so that it is performing stochastic gradient descent on some average error function where the average is weighted by state visitation frequencies for the current usually greedy policy then the weighting changes as the policy changes it might appear that this gradient is difficult to compute consider qlearning exploring with a boltzman distribution that is usually greedy with respect to the learned q function it seems difficult to calculate gradients since changing a single weight will change many q values changing a single q value will change many action choice probabilities in that state and changing a single action choice probability may affect the frequency with which every state in the mdp is visited although this might seem difficult it is not surprisingly unbiased estimates of the gradients of visitation distributions with respect to the weights can be calculated quickly and the resulting algorithms can put a x in every case in table i derivation of the vaps equation consider a sequence of transitions observed while following a particular stochastic policy on an mdp let st xouoro xlulrl xt lut lrt l xtutrt be the sequence of states actions and reinforcements up to time t where performing action u in state x yields reinforcement r and a transition to state xl the l baird and a w moore stochastic policy may be a function of a vector of weights w assume the mdp has a single start state named x if the mdp has terminal states and x is a terminal state then xlx let st be the set of all possible sequences from time to t let est be a given error function that calculates an error on each time step such as the squared bellman residual at time t or some other error occurring at time t if e is a function of the weights then it must be a smooth function of the weights consider a period of time starting at time and ending with probability pendlst after the sequence st occurs the probabilities must be such that the expected squared period length is finite let b be the expected total error during that period where the expectation is weighted according to the state visitation frequencies generated by the given policy b pperiod ends at time t after trajectory s r es es ps stag t where pst put i sprt i sti pu i spr i sps i sl pend l s o note that on the first line for a particular st the error est will be added in to b once for every sequence that starts with st each of these terms will be weighted by the probability of a complete trajectory that starts with st the sum of the probabilities of all trajectories that start with st is simply the probability of st being observed since the period is assumed to end eventually with probability one so the second line equals the first the third line is the probability of the sequence of which only the pu lx factor might be a function of w if so this probability must be a smooth function of the weights and nonzero everywhere the partial derivative of b with respect to w a particular element of the weight vector w is c b s es psesps z pu s tgst es es lneu is jl space here is limited and it may not be clear from the short sketch of this derivation but summing over an entire period does give an unbiased estimate of b the expected total error during a period an incremental algorithm to perform stochastic gradient descent on b is the weight update given on the left side of table where the summation over previous time steps is replaced with a trace tt for each weight this algorithm is more general than previously published algorithms of this form in that e can be a function of all previous states actions and reinforcements rather than just the current reinforcement this is what allows vaps to do both value and policy search every algorithm proposed in this paper is a special case of the vaps equation on the left side of table note that no model is needed for this algorithm the only probability needed in the algorithm is the policy not the transition probability from the mdp this is stochastic gradient descent on b and the update rule is only correct if the observed transitions are sampled from trajectories found by following gradient descent for general reinforcement learning table the general vaps algorithm left and several instantiations of it right this single algorithm includes both value based and policy search approaches and their combination and ives guaranteed convergence in ever case aw aes est lnpu i s at er qx u qx u e s s e c te g s r max q x u q x u r y m ax u ax u eveo s m v the current stochastic policy both e and p should be smooth functions of w and for any given w vector e should be bounded the algorithm is simple but actually generates a large class of different algorithms depending on the choice of e and when the trace is reset to zero for a single sequence sampled by following the current policy the sum of aw along the sequence will give an unbiased estimate of the true gradient with finite variance therefore during learning if weight updates are made at the end of each trial and if the weights stay within a bounded region and the learning rate approaches zero then b will converge with probability one adding a weight decay term a constant times the norm of the weight vector onto b will prevent weight divergence for small initial learning rates there is no guarantee that a global minimum will be found when using general function approximators but at least it will converge this is true for backprop as well instantiating the vaps algorithm many reinforcement learning algorithms are value based they try to learn a value function that satisfies the bellman equation examples are q learning which learns a value function actor critic algorithms which learn a value function and the policy which is greedy with respect to it and td which learns a value function based on future rewards other algorithms are pure policy search algorithms they directly learn a policy that returns high rewards these include reinforce williams backprop through time learning automata and genetic algorithms the algorithms proposed here combine the two approaches they perform value and policy search vaps the general vaps equation is instantiated by choosing an expression for e this can be a bellman residual yielding value based the reinforcement yielding policy search or a linear combination of the two yielding value and policy search the single vaps update rule on the left side of table generates a variety of different types of algorithms some of which are described in the following sections reducing mean squared residual per trial if the mdp has terminal states and a trial is the time from the start until a terminal state is reached then it is possible to minimize the expected total error per trial by resetting the trace to zero at the start of each trial then a convergent form of sarsa q learning incremental value iteration or advantage learning can be generated by choosing e to be the squared bellman residual as shown on the right side of table in each case the expected value is taken over all possible xt utrt l baird and a w moore triplets given stl the policy must be a smooth nonzero function of the weights so it could not be an e greedy policy that chooses the greedy action with probability l e and chooses uniformly otherwise that would cause a discontinuity in the gradient when two q values in a state were equal but the policy could be something that approaches e greedy as a positive temperature c approaches zero e qxoc where n is the number of possible actions in each state for each instance in table other than value iteration the gradient of e can be estimated using two independent unbiased estimates of the expected value for example esars a s esae s s q qx u qx u when qbl this is an estimate of the true gradient when qbl this is a residual algorithm as described in baird and it retains guaranteed convergence but may learn more quickly than pure gradient descent for some values of qb note that the gradient of qxu at time t uses primed variables that means a new state and action at time t were generated independently from the state and action at time t of course if the mdp is deterministic then the primed variables are the same as the unprimed if the mdp is nondeterministic but the model is known then the model must be evaluated one additional time to get the other state if the model is not known then there are three choices first a model could be learned from past data and then evaluated to give this independent sample second the issue could be ignored simply reusing the unprimed variables in place of the primed variables this may affect the quality of the learned function depending on how random the mdp is but doesnt stop convergence and be an acceptable approximation in practice third all past transitions could be recorded and the primed variables could be found by searching for all the times xtltltl has been seen before and randomly choosing one of those transitions and using its successor state and action as the primed variables this is equivalent to learning the certainty equivalence model and sampling from it and so is a special case of the first choice for extremely large state action spaces with many starting states this is likely to give the same result in practice as simply reusing the unprimed variables as the primed variables note that when weights do not effect the policy at all these algorithms reduce to standard residual algorithms baird it is also possible to reduce the mean squared residual per step rather than per trial this is done by making period lengths independent of the policy so minimizing error per period will also minimize the error per step for example a period might be defined to be the first steps after which the traces are reset and the state is returned to the start state note that if every state action pair has a positive chance of being seen in the first steps then this will not just be solving a finite horizon problem it will be actually be solving the discounted infinite horizon problem by reducing the bellman residual in every state but the weighting of the residuals will be determined only by what happens during the first steps many different problems can be solved by the vaps algorithm by instantiating the definition of period in different ways policy search and value based learning it is also possible to add a term that tries to maximize reinforcement directly for example e could be defined to be esas ocy rather than esas from table and gradient descent for general reinforcement learning ooo oo beta figure a pomdp and the number of trials needed to learn it vs a combination of policy search and value based rl outperforms either alone the trace reset to zero after each terminal state is reached the constant b does not affect the expected gradient but does affect the noise distribution as discussed in williams when the algorithm will try to learn a q function that satisfies the bellman equation just as before when it directly learns a policy that will minimize the expected total discounted reinforcement the resulting q function may not even be close to containing true q values or to satisfying the bellman equation it will just give a good policy when is in between this algorithm tries to both satisfy the bellman equation and give good greedy policies a similar modification can be made to any of the algorithms in table in the special case where this algorithm reduces to the reinforce algorithm williams reinforce has been rederived for the special case of gaussian action distributions tresp hofman and extensions of it appear in marbach this case of pure policy search is particularly interesting because for there is no need for any kind of model or of generating two independent successors other algorithms have been proposed for finding policies directly such as those given in gullapalli and the various algorithms from learning automata theory summarized in narendra thathachar the vaps algorithms proposed here appears to be the first one unifying these two approaches to reinforcement learning finding a value function that both approximates a bellman equation solution and directly optimizes the greedy policy figure shows simulation results for the combined algorithm a run is said to have learned when the greedy policy is optimal for consecutive trials the graph shows the average plot of runs with different initial random weights between the learning rate was optimized separately for each value ri when leaving state a r when leaving state b or entering end and r otherwise the algorithm used was the modified q learning from table with exploration as in equation and qc b e states a and b share the same parameters so ordinary sarsa or greedy q learning could never converge as shown in gordon when pure value based the new algorithm converges but of course it cannot learn the optimal policy in the start state since those two q values learn to be equal when pure policy search learning converges to optimality but slowly since there is no value function caching the results in the long sequence of states near the end by combining the two approaches the new algorithm learns much more quickly than either alone it is interesting that the vaps algorithms described in the last three sections can be applied directly to a partially observable markov decision process pomdp where the true state is hidden and all that is available on each time step is an l baird and a w moore ambiguous observation which is a function of the true state normally an algorithm such as sarsa only has guaranteed convergance when applied to an mdp the vaps algorithms will converge in such cases conclusion a new algorithm has been presented special cases of it give new algorithms similar to q learning sarsa and advantage learning but with guaranteed convergence for a wider range of problems than was previously possible including pomdps for the first time these can be guaranteed to converge even when the exploration policy changes during learning other special cases allow new approaches to reinforcement learning where there is a tradeoff between satisfying the bellman equation and improving the greedy policy for one mdp simulation showed that this combined algorithm learned more quickly than either approach alone this unified theory unifying for the first time both value based and policysearch reinforcement learning is of theoretical interest and also was of practical value for the simulations performed future research with this unified framework may be able to empirically or analytically address the old question of when it is better to learn value functions and when it is better to learn the policy directly it may also shed light on the new question of when it is best to do both at once acknowledgments this research was sponsored in part by the us air force references baird l c residual algorithms reinforcement learning with function approximation in armand prieditis stuart russell eds machine learning proceedings of the twelfth international conference july morgan kaufman publishers san francisco ca gordon g stable fitted reinforcement learning in g tesauro m mozer and m hasselmo eds advances in neural information processing systems pp mit press cambridge ma gullapalli v reinforcement learning and its application to control dissertation and coins technical report university of massachusetts amherst ma kaelbling l p littman m l cassandra a planning and acting in partially observable stochastic domains artificial intelligence to appear available now at httpwwwcsbrownedupeoplelpk marbach p simulation based optimization of markov decision processes thesis lids th massachusetts institute of technology mccallum a reinforcement learning with selective perception and hidden state dissertation department of computer science university of rochester rochester ny narendra k thathachar mal learning automata an

this paper examines the application of reinforcement learning to a telecommunications networking problem the problem requires that revenue be maximized while simultaneously meeting a quality of service constraint that forbids entry into certain states we present a general solution to this multi criteria problem that is able to earn significantly higher revenues than alternatives
classifier systems are now viewed disappointing because of their problems such as the rule strength vs rule set performance problem and the credit assignment problem in order to solve the problems we have developed a hybrid classifier system gls generalization learning system in designing gls we view css as model free learning in pomdps and take a hybrid approach to finding the best generalization given the total number of rules gls uses the policy improvement procedure by jaakkola et al for an locally optimal stochastic policy when a set of rule conditions is given gls uses ga to search for the best set of rule conditions
in this paper we address two issues of long standing interest in the reinforcement learning literature first what kinds of performance guarantees can be made for q learning after only a finite number of actions second what quantitative comparisons can be made between q learning and model based indirect approaches which use experience to estimate next state distributions for off line value iteration we first show that both q learning and the indirect approach enjoy rather rapid convergence to the optimal policy as a function of the number of state transitions observed in particular on the order of only nlogeelogn loglogie transitions are sufficient for both algorithms to come within e of the optimal policy in an idealized model that assumes the observed transitions are well mixed throughout an n state mdp thus the two approaches have roughly the same sample complexity perhaps surprisingly this sample complexity is far less than what is required for the model based approach to actually construct a good approximation to the next state distribution the result also shows that the amount of memory required by the model based approach is closer to n than to n for either approach to remove the assumption that the observed transitions are well mixed we consider a model in which the transitions are determined by a fixed arbitrary exploration policy bounds on the number of transitions required in order to achieve a desired level of performance are then related to the stationary distribution and mixing time of this policy
learning real time a lrta is a popular control method that interleaves planning and plan execution and has been shown to solve search problems in known environments efficiently in this paper we apply lrta to the problem of getting to a given goal location in an initially unknown environment uninformed lrta with maximal lookahead always moves on a shortest path to the closest unvisited state that is to the closest potential goal state this was believed to be a good exploration heuristic but we show that it does not minimize the worst case plan execution time compared to other uninformed exploration methods this result is also of interest to reinforcement learning researchers since many reinforcement learning methods use asynchronous dynamic programming interleave planning and plan execution and exhibit optimism in the face of uncertainty just like lrta
agents acting in the real world are confronted with the problem of making good decisions with limited knowledge of the environment partially observable markov decision processes pomdps model decision problems in which an agent tries to maximize its reward in the face of limited sensor feedback recent work has shown empirically that a reinforcement learning rl algorithm called sarsa can efficiently find optimal memoryless policies which map current observations to actions for pomdp problems loch and singh the sarsa algorithm uses a form of short term memory called an eligibility trace which distributes temporally delayed rewards to observation action pairs which lead up to the reward this paper explores the effect of eligibility traces on the ability of the sarsa algorithm to find optimal memoryless policies a variant of sarsa called k step truncated sarsa is applied to four test problems taken from the recent work of littman littman cassandra and kaelbling parr and russell and chrisman the empirical results show that eligibility traces can be significantly truncated without affecting the ability of sarsa to find optimal memoryless policies for pomdps
reinforcement learning methods can be used to improve the performance of local search algorithms for combinatorial optimization by learning an evaluation function that predicts the outcome of search the evaluation function is therefore able to guide search to low cost solutions better than can the original cost function we describe a reinforcement learning method for enhancing local search that combines aspects of previous work by zhang and dietterich and boyan and moore boyan in an off line learning phase a value function is learned that is useful for guiding search for multiple problem sizes and instances we illustrate our technique by developing several such functions for the dial a ride problem our learning enhanced local search algorithm exhibits an improvement of more then over a standard local search algorithm
in order to find the optimal control of continuous state space and time reinforcement learning rl problems we approximate the value function vf with a particular class of functions called the barycentric interpolators we establish sufficient conditions under which a rl algorithm converges to the optimal vf even when we use approximate models of the state dynamics and the reinforcement functions i

in order to grasp an object we need to solve the inverse kinematics problem ie the coordinate transformation from the visual coordinates to the joint angle vector coordinates of the arm although several models of coordinate transformation learning have been proposed they suffer from a number of drawbacks in human motion control the learning of the hand position error feedback controller in the inverse kinematics solver is important this paper proposes a novel model of the coordinate transformation learning of the human visual feedback controller that uses the change of the joint angle vector and the corresponding change of the square of the hand position error norm the feasibility of the proposed model is illustrated using numerical simulations
we present a method for automatically constructing macro actions from scratch from primitive actions during the reinforcement learning process the overall idea is to reinforce the tendency to perform action b after action a if such a pattern of actions has been rewarded we test the method on a bicycle task the car on the hill task the race track task and some grid world tasks for the bicycle and race track tasks the use of macro actions approximately halves the learning time while for one of the grid world tasks the learning time is reduced by a factor of the method did not work for the car on the hill task for reasons we discuss in the conclusion
in this article we propose a new reinforcement learning rl method based on an actor critic architecture the actor and the critic are approximated by normalized gaussian networks ngnet which are networks of local linear regression units the ngnet is trained by the on line em algorithm proposed in our previous paper we apply our rl method to the task of swinging up and stabilizing a single pendulum and the task of balancing a double pendulum near the upright position the experimental results show that our rl method can be applied to optimal control problems having continuous stateaction spaces and that the method achieves good control with a small number of trial and errors
we describe a reinforcement learning algorithm for partially observable environments using short term memory which we call blht since blht learns a stochastic model based on bayesian learning the overfitting problem is reasonably solved moreover blht has an efficient implementation this paper shows that the model learned by blht converges to one which provides the most accurate predictions of percepts and rewards given short term memory

partially observable markov decision processes pomdps constitute an important class of reinforcement learning problems which present unique theoretical and computational difficulties in the absence of the markov property popular reinforcement learning algorithms such as q learning may no longer be effective and memory based methods which remove partial observability via state estimation are notoriously expensive an alternative approach is to seek a stochastic memoryless policy which for each observation of the environment prescribes a probability distribution over available actions that maximizes the average reward per timestep a reinforcement learning algorithm which learns a locally optimal stochastic memoryless policy has been proposed by jaakkola singh and jordan but not empirically verified we present a variation of this algorithm discuss its implementation and demonstrate its viability using four test problems
virtual reality vr provides immersive and controllable experimental environments it expands the bounds of possible evoked potential ep experiments by providing complex dynamic environments in order to study cognition without sacrificing environmental control vr also serves as a safe dynamic testbed for brain computer interface bci research however there has been some concern about detecting ep signals in a complex vr environment this paper shows that eps exist at red green and yellow stop lights in a virtual driving environment experimental results show the existence of the p ep at go and stop lights and the contingent negative variation cnv ep at slow down lights in order to test the feasibility of on line recognition in vr we looked at recognizing the p ep at red stop lights and the absence of this signal at yellow slow down lights recognition results show that the p may successfully be used to control the brakes of a vr car at stop lights
the psychophysical evidence for selective attention originates mainly from visual search experiments in this work we formulate a hierarchical system of interconnected modules consisting in populations of neurons for modeling the underlying mechanisms involved in selective visual attention we demonstrate that our neural system for visual search works across the visual field in parallel but due to the different intrinsic dynamics can show the two experimentally observed modes of visual attention namely the serial and the parallel search mode in other words neither explicit model of a focus of attention nor saliencies maps are used the focus of attention appears as an emergent property of the dynamic behavior of the system the neural population dynamics are handled in the framework of the mean field approximation consequently the whole process can be expressed as a system of coupled differential equations
spatial information comes in two forms direct spatial information for example retinal position and indirect temporal contiguity information since objects encountered sequentially are in general spatially close the acquisition of spatial information by a neural network is investigated here given a spatial layout of several objects networks are trained on a prediction task networks using temporal sequences with no direct spatial information are found to develop internal representations that show distances correlated with distances in the external layout the influence of spatial information is analyzed by providing direct spatial information to the system during training that is either consistent with the layout or inconsistent with it this approach allows examination of the relative contributions of spatial and temporal contiguity
quantitative data on the speed with which animals acquire behavioral responses during classical conditioning experiments should provide strong constraints on models of learning however most models have simply ignored these data the few that have attempted to address them have failed by at least an order of magnitude we discuss key data on the speed of acquisition and show how to account for them using a statistically sound model of learning in which differential reliabilities of stimuli play a crucial role
in many classification tasks recognition accuracy is low because input patrems are corrupted by noise or are spatially or temporally overlapping we propose an approach to overcoming these limitations based on a model of human selective attention the model an early selection filter guided by top down attentional control entertains each candidate output class in sequence and adjusts attentional gain coefficients in order to produce a strong response for that class the chosen class is then the one that obtains the strongest response with the least modulation of attention we present simulation results on classification of corrupted and superimposed handwritten digit pattems showing a significant improvement in recognition rates the algorithm has also been applied in the domain of speech recognition with comparable results
a figure ground segregation network is proposed based on a novel boundary pair representation nodes in the network are boundary segments obtained through local grouping each node is excitatorily coupled with the neighboring nodes that belong to the same region and inhibitorily coupled with the corresponding paired node gestalt grouping rules are incorporated by modulating connections the status of a node represents its probability being figural and is updated according to a differential equation the system solves the figure ground segregation problem through temporal evolution different perceptual phenomena such as modal and amodal completion virtual contours grouping and shape decomposition are then explained through local diffusion the system eliminates combinatorial optimization and accounts for many psychophysical results with a fixed set of parameters i
we examine a psychophysical law that describes the influence of stimulus and context on perception according to this law choice probability ratios factorize into components independently controlled by stimulus and context it has been argued that this pattern of results is incompatible with feedback models of perception in this paper we examine this claim using neural network models defined via stochastic differential equations we show that the law is related to a condition named channel separability and has little to do with the existence of feedback connections in essence channels are separable if they converge into the response units without direct lateral connections to other channels and if their sensors are not directly contaminated by external inputs to the other channels implications of the analysis for cognitive and computational neurosicence are discussed
we introduce a novel method of constructing language models which avoids some of the problems associated with recurrent neural networks the method of creating a prediction fractal machine pfm is briefly described and some experiments are presented which demonstrate the suitability of pfms for language modeling pfms distinguish reliably between minimal pairs and their behavior is consistent with the hypothesis that wellformedness is graded not absolute a discussion of their potential to offer fresh insights into language acquisition and processing follows
this paper argues that two apparently distinct modes of generalizing concepts abstracting rules and computing similarity to exemplars should both be seen as special cases of a more general bayesian learning framework bayes explains the specific workings of these two modes which rules are abstracted how similarity is measured as well as why generalization should appear ruleor similarity based in different situations this analysis also suggests why the rulessimilarity distinction even if not computationally fundamental may still be useful at the algorithmic level as part of a principled approximation to fully bayesian learning
recent theories suggest that language acquisition is assisted by the evolution of languages towards forms that are easily learnable in this paper we evolve combinatorial languages which can be learned by a recurrent neural network quickly and from relatively few examples additionally we evolve languages for generalization in different worlds and for generalization from specific examples we find that languages can be evolved to facilitate different forms of impressive generalization for a minimally biased general purpose learner the results provide empirical support for the theory that the language itself as well as the language environment of a learner plays a substantial role in learning that there is far more to language acquisition than the language acquisition device i
in this paper we question the necessity of levels of expert guided abstraction in learning hard statistically neutral classification tasks we focus on two tasks date calculation and parity that are claimed to require intermediate levels of abstraction that must be defined by a human expert we challenge this claim by demonstrating empirically that a single hidden layer bp som network can learn both tasks without guidance moreover we analyze the networks solution for the parity task and show that its solution makes use of an elegant intermediary checksum computation i

we investigate the short term dynamics of the recurrent competition and neural activity in the primary visual cortex in terms of information processing and in the context of orientation selectivity we propose that after stimulus onset the strength of the recurrent excitation decreases due to fast synaptic depression as a consequence the network shifts from an initially highly nonlinear to a more linear operating regime sharp orientation tuning is established in the first highly competitive phase in the second and less competitive phase precise signaling of multiple orientations and long range modulation eg by intraand inter areal connections becomes possible surround effects thus the network first extracts the salient features from the stimulus and then starts to process the details we show that this signal processing strategy is optimal if the neurons have limited bandwidth and their objective is to transmit the maximum amount of information in any time interval beginning with the stimulus onset
this paper revisits the classical neuroscience paradigm of hebbian learning we find that a necessary requirement for effective associative memory learning is that the efcacies of the incoming synapses should be uncorrelated this requirement is difficult to achieve in a robust manner by hebbian synaptic learning since it depends on network level information effective learning can yet be obtained by a neuronal process that maintains a zero sum of the incoming synaptic efcacies this normalization drastically improves the memory capacity of associative networks from an essentially bounded capacity to one that linearly scales with the networks size it also enables the effective storage of patterns with heterogeneous coding levels in a single network such neuronal normalization can be successfully carried out by activity dependent homeostasis of the neurons synaptic efcacies which was recently observed in cortical tissue thus our findings strongly suggest that effective associative learning with hebbian synapses alone is biologically implausible and that hebbian synapses must be continuously remodeled by neuronally driven regulatory processes in the brain i

i consider a topographic projection between two neuronal layers with different densities of neurons given the number of output neurons connected to each input neuron divergence or fan out and the number of input neurons synapsing on each output neuron convergence or fan in i determine the widths of axonal and dendritic arbors which minimize the total volume of axons and dendrites my analytical results can be summarized qualitatively in the following rule neurons of the sparser layer should have arbors wider than those of the denser layer this agrees with the anatomical data from retinal and cerebellar neurons whose morphology and connectivity are known the rule may be used to infer connectivity of neurons from their morphology
the encoding accuracy of a population of stochastically spiking neurons is studied for different distributions of their tuning widths the situation of identical radially symmetric receptive fields for all neurons which is usually considered in the literature turns out to be disadvantageous from an information theoretic point of view both a variability of tuning widths and a fragmentation of the neural population into specialized subpopulations improve the encoding accuracy

we investigate the behavior of a hebbian cell assembly of spiking neurons formed via a temporal synaptic learning curve this learning function is based on recent experimental findings it includes potentiation for short time delays between preand post synaptic neuronal spiking and depression for spiking events occuring in the reverse order the coupling between the dynamics of the synaptic learning and of the neuronal activation leads to interesting results we find that the cell assembly can fire asynchronously but may also function in complete synchrony or in distributed synchrony the latter implies spontaneous division of the hebbian cell assembly into groups of cells that fire in a cyclic manner we invetigate the behavior of distributed synchrony both by simulations and by analytic calculations of the resulting synaptic distributions i
when a visual image consists of a figure against a background v cells are physiologically observed to give higher responses to image regions corresponding to the figure relative to their responses to the background the medial axis of the figure also induces relatively higher responses compared to responses to other locations in the figure except for the boundary between the figure and the background since the receptive fields of v cells are very small compared with the global scale of the figure ground and medial axis effects it has been suggested that these effects may be caused by feedback from higher visual areas i show how these effects can be accounted for by v mechanisms when the size of the figure is small or is of a certain scale they are a manifestation of the processes of pre attentive segmentation which detect and highlight the boundaries between homogeneous image regions i
stochastic fluctuations of voltage gated ion channels generate current and voltage noise in neuronal membranes this noise may be a critical determinant of the efficacy of information processing within neural systems using monte carlo simulations we carry out a systematic investigation of the relationship between channel kinetics and the resulting membrane voltage noise using a stochastic markov version of the mainen sejnowski model of dendritic excitability in cortical neurons our simulations show that kinetic parameters which lead to an increase in membrane excitability increasing channel densities decreasing temperature also lead to an increase in the magnitude of the sub threshold voltage noise noise also increases as the membrane is depolarized from rest towards threshold this suggests that channel fluctuations may interfere with a neurons ability to function as an integrator of its synaptic inputs and may limit the reliability and precision of neural information processing
long term potentiation ltp has long been held as a biological substrate for associative learning recently evidence has emerged that long term depression ltd results when the presynaptic cell fires after the postsynaptic cell the computational utility of ltd is explored here synaptic modification kernels for both ltp and ltd have been proposed by other laboratories based studies of one postsynaptic unit here the interaction between time dependent ltp and ltd is studied in small networks
previous biophysical modeling work showed that nonlinear interactions among nearby synapses located on active dendritic trees can provide a large boost in the memory capacity of a cell mel a b the aim of our present work is to quantify this boost by estimating the capacity of a neuron model with passive dendritic integration where inputs are combined linearly across the entire cell followed by a single global threshold and an active dendrite model in which a threshold is applied separately to the output of each branch and the branch subtotals are combined linearly we focus here on the limiting case of binary valued synaptic weights and derive expressions which measure model capacity by estimating the number of distinct input output functions available to both neuron types we show that the application of a fixed nonlinearity to each dendritic compartment substantially increases the models flexibility for a neuron of realistic size the capacity of the nonlinear cell can exceed that of the same sized linear cell by more than an order of magnitude and the largest capacity boost occurs for cells with a relatively large number of dendritic subunits of relatively small size we validated the analysis by empirically measuring memory capacity with randomized two class classification problems where a stochastic delta rule was used to train both linear and nonlinear models we found that large capacity boosts predicted for the nonlinear dendritic model were readily achieved in practice httplncuscedu p poirazi and b mel
neocortical circuits are dominated by massive excitatory feedback more than eighty percent of the synapses made by excitatory cortical neurons are onto other excitatory cortical neurons why is there such massive recurrent excitation in the neocortex and what is its role in cortical computation recent neurophysiological experiments have shown that the plasticity of recurrent neocortical synapses is governed by a temporally asymmetric hebbian learning rule we describe how such a rule may allow the cortex to modify recurrent synapses for prediction of input sequences the goal is to predict the next cortical input from the recent past based on previous experience of similar input sequences we show that a temporal difference learning rule for prediction used in conjunction with dendritic back propagating action potentials reproduces the temporally asymmetric hebbian plasticity observed physiologically biophysical simulations demonstrate that a network of cortical neurons can learn to predict moving stimuli and develop direction selective responses as a consequence of learning the space time response properties of model neurons are shown to be similar to those of direction selective cells in alert monkey v
a very simple model of two reciprocally connected attractor neural networks is studied analytically in situations similar to those encountered in delay match to sample tasks with intervening stimuli and in tasks of memory guided attention the model qualitatively reproduces many of the experimental data on these types of tasks and provides a framework for the understanding of the experimental observations in the context of the attractor neural network scenario
the reliability and accuracy of spike trains have been shown to depend on the nature of the stimulus that the neuron encodes adding ion channel stochasticity to neuronal models results in a macroscopic behavior that replicates the input dependent reliability and precision of real neurons we calculate the amount of information that an ion channel based stochastic hodgkin huxley hh neuron model can encode about a wide set of stimuli we show that both the information rate and the information per spike of the stochastic model are similar to the values reported experimentally moreover the amount of information that the neuron encodes is correlated with the amplitude of fluctuations in the input and less so with the average firing rate of the neuron we also show that for the hh ion channel density the information capacity is robust to changes in the density of ion channels in the membrane whereas changing the ratio between the na and k ion channels has a considerable effect on the information that the neuron can encode finally we suggest that neurons may maximize their information capacity by appropriately balancing the density of the different ion channels that underlie neuronal excitability i
human reaction times during sensory motor tasks vary considerably to begin to understand how this variability arises we examined neuronal populational response time variability at early versus late visual processing stages the conventional view is that precise temporal information is gradually lost as information is passed through a layered network of mean rate units we tested in humans whether neuronal populations at different processing stages behave like mean rate units a blind source separation algorithm was applied to meg signals from sensory motor integration tasks response time latency and variability for multiple visual sources were estimated by detecting single trial stimulus locked events for each source in two subjects tested on four visual reaction time tasks we reliably identified sources belonging to early and late visual processing stages the standard deviation of response latency was smaller for early rather than late processing stages this supports the hypothesis that human populational response time variability increases from early to late visual processing stages i
we study a population decoding paradigm in which the maximum likelihood inference is based on an unfaithful decoding model umli this is usually the case for neural population decoding because the encoding process of the brain is not exactly known or because a simplified decoding model is preferred for saving computational cost we consider an unfaithful decoding model which neglects the pair wise correlation between neuronal activities and prove that umli is asymptotically efficient when the neuronal correlation is uniform or of limited range the performance of umli is compared with that of the maximum likelihood inference based on a faithful model and that of the center of mass decoding method it turns out that umli has advantages of decreasing the computational complexity remarkablely and maintaining a high level decoding accuracy at the same time the effect of correlation on the decoding accuracy is also discussed
we analyze the conditions under which synaptic learning rules based on action potential timing can be approximated by learning rules based on firing rates in particular we consider a form of plasticity in which synapses depress when a presynaptic spike is followed by a postsynaptic spike and potentiate with the opposite temporal ordering such differential anti hebbianplasticity can be approximated under certain conditions by a learning rule that depends on the time derivative of the postsynaptic firing rate such a learning rule acts to stabilize persistent neural activity patterns in recurrent neural networks
this paper presents a novel practical framework for bayesian model averaging and model selection in probabilistic graphical models our approach approximates full posterior distributions over model parameters and structures as well as latent variables in an analytical manner these posteriors fall out of a free form optimization procedure which naturally incorporates conjugate priors unlike in large sample approximations the posteriors are generally nongaussian and no hessian needs to be computed predictive quantities are obtained analytically the resulting algorithm generalizes the standard expectation maximization algorithm and its convergence is guaranteed we demonstrate that this approach can be applied to a large class of models in several domains including mixture models and source separation i
unsupervised learning algorithms are designed to extract structure from data samples reliable and robust inference requires a guarantee that extracted structures are typical for the data source ie similar structures have to be inferred from a second sample set of the same data source the overfitting phenomenon in maximum entropy based annealing algorithms is exemplarily studied for a class of histogram clustering models bernsteins inequality for large deviations is used to determine the maximally achievable approximation quality parameterized by a minimal temperature monte carlo simulations support the proposed model selection criterion by finite temperature annealing i
we give necessary and sufficient conditions for uniqueness of the support vector solution for the problems of pattern recognition and regression estimation for a general class of cost functions we show that if the solution is not unique all support vectors are necessarily at bound and we give some simple examples of non unique solutions we note that uniqueness of the primal dual solution does not necessarily imply uniqueness of the dual primal solution we show how to compute the threshold b when the solution is unique but when all support vectors are at bound in which case the usual method for determining b does not work
new functionals for parameter model selection of support vector machines are introduced based on the concepts of the span of support vectors and rescaling of the feature space it is shown that using these functionals one can both predict the best choice of parameters of the model and the relative quality of performance for any value of parameter
we generalize a recent formalism to describe the dynamics of supervised learning in layered neural networks in the regime where data recycling is inevitable to the case of noisy teachers our theory generates reliable predictions for the evolution in time of trainingand generalization errors and extends the class of mathematically solvable learning processes in large neural networks to those situations where overfitting can occur
we show that the recently proposed variant of the support vector machine svm algorithm known as y svm can be interpreted as a maximal separation between subsets of the convex hulls of the data which we call soft convex hulls the soft convex hulls are controlled by choice of the parameter y if the intersection of the convex hulls is empty the hyperplane is positioned halfway between them such that the distance between convex hulls measured along the normal is maximized and if it is not the hyperplanes normal is similarly determined by the soft convex hulls but its position perpendicular distance from the origin is adjusted to minimize the error sum the proposed geometric interpretation of y svm also leads to necessary and sufficient conditions for the existence of a choice of y for which the y svm solution is nontrivial i
we present three simple approximations for the calculation of the posterior mean in gaussian process classification the first two methods are related to mean field ideas known in statistical physics the third approach is based on bayesian online approach which was motivated by recent results in the statistical mechanics of neural networks we present simulation results showing that the mean field bayesian evidence may be used for hyperparameter tuning and that the online approach may achieve a low training error fast i
recent interpretations of the adaboost algorithm view it as performing a gradient descent on a potential function simply changing the potential function allows one to create new algorithms related to adaboost however these new algorithms are generally not known to have the formal boosting property this paper exmines the question of which potential functions lead to new algorithms that are boosters the two main results are general sets of conditions on the potential one set implies that the resulting algorithm is a booster while the other implies that the algorithm is not these conditions are applied to previously studied potential functions such as those used by logitboost and doom ii i
bayesian predictions are stochastic just like predictions of any other inference scheme that generalize from a finite sample while a simple variational argument shows that bayes averaging is generalization optimal given that the prior matches the teacher parameter distribution the situation is less clear if the teacher distribution is unknown i define a class of averaging procedures the temperated likelihoods including both bayes averaging with a uniform prior and maximum likelihood estimation as special cases i show that bayes is generalization optimal in this family for any teacher distribution for two learning problems that are analytically tractable learning the mean of a gaussian and asymptotics of smooth learners
the performance of regular and irregular gallager type errorcorrecting code is investigated via methods of statistical physics the transmitted codeword comprises products of the original message bits selected by two randomly constructed sparse matrices the number of non zero rowcolumn elements in these matrices constitutes a family of codes we show that shannons channel capacity may be saturated in equilibrium for many of the regular codes while slightly lower performance is obtained for others which may be of higher practical relevance decoding aspects are considered by employing the tap approach which is identical to the commonly used belief propagation based decoding we show that irregular codes may saturate shannons capacity but with improved dynamical properties i
gaussian mixtures or so called radial basis function networks for density estimation provide a natural counterpart to sigmoidal neural networks for function fitting and approximation in both cases it is possible to give simple expressions for the iterative improvement of performance as components of the network are introduced one at a time in particular for mixture density estimation we show that a k component mixture estimated by maximum likelihood or by an iterative likelihood improvement that we introduce achieves log likelihood within order k of the log likelihood achievable by any convex combination consequences for approximation and estimation using kullback leibler risk are also given a minimum description length principle selects the optimal number of components k that minimizes the risk bound i
an important issue in neural computing concerns the description of learning dynamics with macroscopic dynamical variables recent progress on on line learning only addresses the often unrealistic case of an infinite training set we introduce a new framework to model batch learning of restricted sets of examples widely applicable to any learning cost function and fully taking into account the temporal correlations introduced by the recycling of the examples for illustration we analyze the effects of weight decay and early stopping during the learning of teacher generated examples i
everybody knows that neural networks need more than a single layer of nonlinear units to compute interesting functions we show that this is false if one employs winner take all as nonlinear unit any boolean function can be computed by a single k winner takeall unit applied to weighted sums of the input variables any continuous function can be approximated arbitrarily well by a single soft winner take all unit applied to weighted sums of the input variables only positive weights are needed in these linear weighted sums this may be of interest from the point of view of neurophysiology since only of the synapses in the cortex are inhibitory in addition it is widely believed that there are special microcircuits in the cortex that compute winner take all our results support the view that winner take all is a very useful basic computational unit in neural vlsi it is wellknown that winner take all of n input variables can be computed very efficiently with n transistors and a total wire length and area that is linear in n in analog vlsi lazzaro et al we show that winner take all is not just useful for special purpose computations but may serve as the only nonlinear unit for neural circuits with universal computational power we show that any multi layer perceptron needs quadratically in n many gates to compute winner take all for n input variables hence winner take all provides a substantially more powerful computational unit than a perceptron at about the same cost of implementation in analog vlsi complete proofs and further details to these results can be found in maass w maass
it is known that decision tree learning can be viewed as a form of boosting however existing boosting theorems for decision tree learning allow only binary branching trees and the generalization to multi branching trees is not immediate practical decision tree algorithms such as cart and c implement a trade off between the number of branches and the improvement in tree quality as measured by an index function here we give a boosting justification for a particular quantitative trade off curve our main theorem states in essence that if we require an improvement proportional to the log of the number of branches then top down greedy construction of decision trees remains an effective boosting algorithm i
in order to to compare learning algorithms experimental results reported in the machine learning litterature often use statistical tests of significance unfortunately most of these tests do not take into account the variability due to the choice of training set we perform a theoretical investigation of the variance of the cross validation estimate of the generalization error that takes into account the variability due to the choice of training sets this allows us to propose two new ways to estimate this variance we show via simulations that these new statistics perform well relative to the statistics considered by dietterich dietterich
we study here a simple stochastic single neuron model with delayed self feedback capable of generating spike trains simulations show that its spike trains exhibit resonant behavior between noise and delay in order to gain insight into this resonance we simplify the model and study a stochastic binary element whose transition probability depends on its state at a fixed interval in the past with this simplified model we can analytically compute interspike interval histograms and show how the resonance between noise and delay arises the resonance is also observed when such elements are coupled through delayed interaction i
in this article we study the effects of introducing structure in the input distribution of the data to be learnt by a simple perceptron we determine the learning curves within the framework of statistical mechanics stepwise generalization occurs as a function of the number of examples when the distribution of patterns is highly anisotropic although extremely simple the model seems to capture the relevant features of a class of support vector machines which was recently shown to present this behavior
we calculate lower bounds on the size of sigmoidal neural networks that approximate continuous functions in particular we show that for the approximation of polynomials the network size has to grow as log k u where k is the degree of the polynomials this bound is valid for any input dimension ie independently of the number of variables the result is obtained by introducing a new method employing upper bounds on the vapnik chervonenkis dimension for proving lower bounds on the size of networks that approximate continuous functions
in this paper we define a probabilistic computational model which generalizes many noisy neural network models including the recent work of maass and sontag we identify weak ergodicity as the mechanism responsible for restriction of the computational power of probabilistic models to definite languages independent of the characteristics of the noise whether it is discrete or analog or if it depends on the input or not and independent of whether the variables are discrete or continuous we give examples of weakly ergodic models including noisy computational systems with noise depending on the current state and inputs aggregate models and computational systems which update in continuous time i
effective methods of capacity control via uniform convergence bounds for function expansions have been largely limited to support vector machines where good bounds are obtainable by the entropy number approach we extend these methods to systems with expansions in terms of arbitrary parametrized basis functions and a wide range of regularization methods covering the whole range of general linear additive models this is achieved by a data dependent analysis of the eigenvalues of the corresponding design matrix

hierarchical learning machines are non regular and non identifiable statistical models whose true parameter sets are analytic sets with singularities using algebraic analysis we rigorously prove that the stochastic complexity of a non identifiable learning machine is asymptotically equal to allognml loglogn q const where n is the number of training samples moreover we show that the rational number a and the integer ml can be algorithmically calculated using resolution of singularities in algebraic geometry also we obtain inequalities d and i ml d where d is the number of parameters i
in this paper we discuss the semiparametric statistical model for blind deconvolution first we introduce a lie group to the manifold of noncausal fir filters then blind deconvolution problem is formulated in the framework of a semiparametric model and a family of estimating functions is derived for blind deconvolution a natural gradient learning algorithm is developed for training noncausal filters stability of the natural gradient algorithm is also analyzed in this framework
recently sample complexity bounds have been derived for problems involving linear functions such as neural networks and support vector machines in this paper we extend some theoretical results in this area by deriving dimensional independent covering number bounds for regularized linear functions under certain regularization conditions we show that such bounds lead to a class of new methods for training linear classifiers with similar theoretical advantages of the support vector machine furthermore we also present a theoretical analysis for these new methods from the asymptotic statistical point of view this technique provides better description for large sample behaviors of these algorithms
in this paper we propose a full bayesian model for neural networks this model treats the model dimension number of neurons model parameters regularisation parameters and noise parameters as random variables that need to be estimated we then propose a reversible jump markov chain monte carlo mcmc method to perform the necessary computations we find that the results are not only better than the previously reported ones but also appear to be robust with respect to the prior specification moreover we present a geometric convergence theorem for the algorithm i
we present a new technique for time series analysis based on dynamic probabilistic networks in this approach the observed data are modeled in terms of unobserved mutually independent factors as in the recently introduced technique of independent factor analysis ifa however unlike in ifa the factors are not iid each factor has its own temporal statistical characteristics we derive a family of em algorithms that learn the structure of the underlying factors and their relation to the data these algorithms perform source separation and noise reduction in an integrated manner and demonstrate superior performance compared to ifa i
layered sigmoid belief networks are directed graphical models in which the local conditional probabilities are parameterised by weighted sums of parental states learning and inference in such networks are generally intractable and approximations need to be considered progress in learning these networks has been made by using variational procedures we demonstrate however that variational procedures can be inappropriate for the equally important issue of inference that is calculating marginms of the network we introduce an alternative procedure based on assuming that the weighted input to a node is approximately gaussian distributed our approach goes beyond previous gaussian field assumptions in that we take into account correlations between parents of nodes this procedure is specialized for calculating marginals and is significantly faster and simpler than the variational procedure i
samy bengio idiap cp rue du simplon martigny switzerland bengiooidiap ch the curse of dimensionality is severe when modeling high dimensional discrete data the number of possible combinations of the variables explodes exponentially in this paper we propose a new architecture for modeling high dimensional data that requires resources parameters and computations that grow only at most as the square of the number of variables using a multi layer neural network to represent the joint distribution of the variables as the product of conditional distributions the neural network can be interpreted as a graphical model without hidden random variables but in which the conditional distributions are tied through the hidden units the connectivity of the neural network can be pruned by using dependency tests between the variables experiments on modeling the distribution of several discrete data sets show statistically significant improvements over other methods such as naive bayes and comparable bayesian networks and show that significant improvements can be obtained by pruning the network
we replace the commonly used gaussian noise model in nonlinear regression by a more flexible noise model based on the student tdistribution the degrees of freedom of the t distribution can be chosen such that as special cases either the gaussian distribution or the cauchy distribution are realized the latter is commonly used in robust regression since the t distribution can be interpreted as being an infinite mixture of gaussians parameters and hyperparameters such as the degrees of freedom of the t distribution can be learned from the data based on an em leaming algorithm we show that modeling using the t distribution leads to improved predictors on real world data sets in particular if outliers are present the t distribution is superior to the gaussian noise model in effect by adapting the degrees of freedom the system can learn to distinguish between outliers and non outliers especially for online learning tasks one is interested in avoiding inappropriate weight changes due to measurement outliers to maintain stable online learning capability we show experimentally that using the t distribution as a noise model leads to stable online learning algorithms and outperforms state of the art online learning methods like the extended kalman filter algorithm

we introduce an algorithm for estimating the values of a function at a set of test points xtl xtra given a set of training points xl yl xt yt without estimating as an intermediate step the regression function we demonstrate that this direct transducrive way for estimating values of the regression or classification in pattern recognition can be more accurate than the traditional one based on two steps first estimating the function and then calculating the values of this function at the points of interest
the nonnegative boltzmann machine nnbm is a recurrent neural network model that can describe multimodal nonnegative data application of maximum likelihood estimation to this model gives a learning rule that is analogous to the binary boltzmann machine we examine the utility of the mean field approximation for the nnbm and describe how monte carlo sampling techniques can be used to learn its parameters reflective slice sampling is particularly well suited for this distribution and can efficiently be implemented to sample the distribution we illustrate learning of the nnbm on a translationally invariant distribution as well as on a generative model for images of human faces
for many problems the correct behavior of a model depends not only on its input output mapping but also on properties of its jacobian matrix the matrix of partial derivatives of the models outputs with respect to its inputs we introduce the j prop algorithm an efficient general method for computing the exact partial derivatives of a variety of simple functions of the jacobian of a model with respect to its free parameters the algorithm applies to any parametrized feedforward model including nonlinear regression multilayer perceptrons and radial basis function networks

we present an algorithm that infers the model structure of a mixture of factor analysers using an efficient and deterministic variational approximation to full bayesian integration over model parameters this procedure can automatically determine the optimal number of components and the local dimensionality of each component ie the number of factors in each factor analyser alternatively it can be used to infer posterior distributions over number of components and dimensionalities since all parameters are integrated out the method is not prone to overfitting using a stochastic procedure for adding components it is possible to perform the variational optimisation incrementally and to avoid local maxima results show that the method works very well in practice and correctly infers the number and dimensionality of nontrivial synthetic examples by importance sampling from the variational approximation we show how to obtain unbiased estimates of the true evidence the exact predictive density and the kl divergence between the variational posterior and the true posterior not only in this model but for variational approximations in general
transduction is an inference principle that takes a training sample and aims at estimating the values of a function at given points contained in the so called working sample as opposed to the whole of input space for induction transduction provides a confidence measure on single predictions rather than classifiers a feature particularly important for risk sensitive applications the possibly infinite number of functions is reduced to a finite number of equivalence classes on the working sample a rigorous bayesian analysis reveals that for standard classification loss we cannot benefit from considering more than one test point at a time the probability of the label of a given test point is determined as the posterior measure of the corresponding subset of hypothesis space we consider the pac setting of binary classification by linear discriminant functions perceptrons in kernel space such that the probability of labels is determined by the volume ratio in version space we suggest to sample this region by an ergodic billiard experimental results on real world data indicate that bayesian transduction compares fa vourably to the well known support vector machine in particular if the posterior probability of labellings is used as a confidence measure to exclude test points of low confidence i
we describe a class of probabilistic models that we call credibility networks using parse trees as internal representations of images credibility networks are able to perform segmentation and recognition simultaneously removing the need for ad hoc segmentation heuristics promising results in the problem of segmenting handwritten digits were obtained i
we present a general framework for discriminative estimation based on the maximum entropy principle and its extensions all calculations involve distributions over structures andor parameters rather than specific settings and reduce to relative entropy projections this holds even when the data is not separable within the chosen parametric class in the context of anomaly detection rather than classification or when the labels in the training set are uncertain or incomplete support vector machines are naturally subsumed under this class and we provide several extensions we are also able to estimate exactly and efficiently discriminative distributions over tree structures of class conditional models within this flamework preliminary experimental results are indicative of the potential in these techniques i
invariance to topographic transformations such as translation and shearing in an image has been successfully incorporated into feedforward mechanisms eg convolutional neural networks tangent propagation we describe a way to add transformation invariance to a generafive density model by approximating the nonlinear transformation manifold by a discrete set of transformations an em algorithm for the original model can be extended to the new model by computing expectations over the set of transformations we show how to add a discrete transformation variable to gaussian mixture modeling factor analysis and mixtures of factor analysis we give results on filtering microscopy images face and facial pose clustering and handwritten digit modeling and recognition i
a new decomposition algorithm for training regression support vector machines svm is presented the algorithm builds on the basic principles of decomposition proposed by osuna et al and addresses the issue of optimal working set selection the new criteria for testing optimality of a working set are derived based on these criteria the principle of maximal inconsistency is proposed to form approximately optimal working sets experimental results show superior performance of the new algorithm in comparison with traditional training of regression svm without decomposition similar results have been previously reported on decomposition algorithms for pattern recognition svm the new algorithm is also applicable to advanced svm formulations based on regression such as density estimation and integral equation svm i
a latent variable generative model with finite noise is used to describe several different algorithms for independent components analysis ica in particular the fixed point ica algorithm is shown to be equivalent to the expectation maximization algorithm for maximum likelihood under certain constraints allowing the conditions for global convergence to be elucidated the algorithms can also be explained by their generic behavior near a singular point where the size of the optimal generarive bases vanishes an expansion of the likelihood about this singular point indicates the role of higher order correlations in determining the features discovered by ica the application and convergence of these algorithms are demonstrated on a simple illustrative example
we describe a new incremental algorithm for training linear threshold functions the relaxed online maximum margin algorithm or romma romma can be viewed as an approximation to the algorithm that repeatedly chooses the hyperplane that classifies previously seen examples correctly with the maximum margin it is known that such a maximum margin hypothesis can be computed by minimizing the length of the weight vector subject to a number of linear constraints romma works by maintaining a relatively simple relaxation of these constraints that can be efficiently updated we prove a mistake bound for romma that is the same as that proved for the perceptron algorithm our analysis implies that the more computationally intensive maximum margin algorithm also satisfies this mistake bound this is the first worst case performance guarantee for this algorithm we describe some experiments using romma and a variant that updates its hypothesis more aggressively as batch algorithms to recognize handwritten digits the computational complexity and simplicity of these algorithms is similar to that of perceptron algorithm but their generalization is much better we describe a sense in which the performance of romma converges to that of svm in the limit if bias isnt considered
in recent years bayesian networks have become highly successful tool for diagnosis analysis and decision making in real world domains we present an efficient algorithm for learning bayes networks from data our approach constructs bayesian networks by first identifying each nodes markov blankets then connecting nodes in a maximally consistent way in contrast to the majority of work which typically uses hill climbing approaches that may produce dense and causally incorrect nets our approach yields much more compact causal networks by heeding independencies in the data compact causal networks facilitate fast inference and are also easier to understand we prove that under mild assumptions our approach requires time polynomial in the size of the data and the number of nodes a randomized variant also presented here yields comparable results at much higher speeds
we provide an abstract characterization of boosting algorithms as gradient decsent on cost functionals in an inner product function space we prove convergence of these functional gradient descent algorithms under quite weak conditions following previous theoretical results bounding the generalization performance of convex combinations of classifiers in terms of general cost functions of the margin we present a new algorithm doom ii for performing a gradient descent optimization of such cost functions experiments on several data sets from the uc irvine repository demonstrate that doom ii generally outperforms adaboost especially in high noise situations and that the overfitting behaviour of adaboost is predicted by our cost functions i
in this paper we present committee a new multi class learning algorithm related to the winnow family of algorithms committee is an algorithm for combining the predictions of a set of sub experts in the online mistake bounded model of learning a sub expert is a special type of attribute that predicts with a distribution over a finite number of classes committee learns a linear function of sub experts and uses this function to make class predictions we provide bounds for committee that show it performs well when the target can be represented by a few relevant sub experts we also show how committee can be used to solve more traditional problems composed of attributes this leads to a natural extension that learns on multi class problems that contain both traditional attributes and sub experts
we incorporate prior knowledge to construct nonlinear algorithms for invariant feature extraction and discrimination employing a unified framework in terms of a nonlinear variant of the rayleigh coefficient we propose non linear generalizations of fishers discriminant and oriented pca using support vector kernel functions extensive simulations show the utility of our approach i
we present a class of approximate inference algorithms for graphical models of the qmr dt type we give convergence rates for these algorithms and for the jaakkola and jordan algorithm and verify these theoretical predictions empirically we also present empirical results on the difficult qmr dt network problem obtaining performance of the new algorithms roughly comparable to the jaakkola and jordan algorithm
local linear regression performs very well in many low dimensional forecasting problems in high dimensional spaces its performance typically decays due to the well known curse of dimensionality a possible way to approach this problem is by varying the shape of the weighting kernel in this work we suggest a new data driven method to estimating the optimal kernel shape experiments using an artificially generated data set and data from the uc irvine repository show the benefits of kernel shaping i
we present a new learning architecture the decision directed acyclic graph ddag which is used to combine many two class classifiers into a multiclass classifier for an n class problem the ddag contains nn classifiers one for each pair of classes we present a vc analysis of the case when the node classifiers are hyperplanes the resulting bound on the test error depends on n and on the margin achieved at the nodes but not on the dimension of the space this motivates an algorithm dagsvm which operates in a kernel induced feature space and uses two class maximal margin hyperplanes at each decision node of the ddag the dagsvm is substantially faster to train and evaluate than either the standard algorithm or max wins while maintaining comparable accuracy to both of these algorithms
in a bayesian mixture model it is not necessary a priori to limit the number of components to be finite in this paper an infinite gaussian mixture model is presented which neatly sidesteps the difficult problem of finding the right number of mixture components inference in the model is done using an efficient parameter free markov chain that relies entirely on gibbs sampling
adaboost and other ensemble methods have successfully been applied to a number of classification tasks seemingly defying problems of overfitting adaboost performs gradient descent in an error function with respect to the margin asymptotically concentrating on the patterns which are hardest to learn for very noisy problems however this can be disadvantageous indeed theoretical analysis has shown that the margin distribution as opposed to just the minimal margin plays a crucial role in understanding this phenomenon loosely speaking some outliers should be tolerated if this has the benefit of substantially increasing the margin on the remaining points we propose a new boosting algorithm which allows for the possibility of a pre specified fraction of points to lie in the margin area or even on the wrong side of the decision boundary
fishers linear discriminant analysis lda is a classical multivariate technique both for dimension reduction and classification the data vectors are transformed into a low dimensional subspace such that the class centroids are spread out as much as possible in this subspace lda works as a simple prototype classifier with linear decision boundaries however in many applications the linear boundaries do not adequately separate the classes we present a nonlinear generalization of discriminant analysis that uses the kernel trick of representing dot products by kernel functions the presented algorithm allows a simple formulation of the em algorithm in terms of kernel functions which leads to a unique concept for unsupervised mixture analysis supervised discriminant analysis and semi supervised discriminant analysis with partially unlabelled observations in feature spaces i
we provide an analysis of the turbo decoding algorithm tda in a setting involving gaussian densities in this context we are able to show that the algorithm converges and that somewhat surprisingly though the density generated by the tda may differ significantly from the desired posterior density the means of these two densities coincide i
suppose you are given some dataset drawn from an underlying probability distribution p and you want to estimate a simple subset of input space such that the probability that a test point drawn from p lies outside of equals some a priori specified v between and we propose a method to approach this problem by trying to estimate a function f which is positive on and negative on the complement the functional form of f is given by a kernel expansion in terms of a potentially small subset of the training data it is regularized by controlling the length of the weight vector in an associated feature space we provide a theoretical analysis of the statistical performance of our algorithm the algorithm is a natural extension of the support vector algorithm to the case of unlabelled data
this paper describes bidirectional recurrent mixture density networks which can model multi modal distributions of the type pxtly t and pxtlxlxxty t without any explicit assumptions about the use of context these expressions occur frequently in pattern recognition problems with sequential data for example in speech recognition experiments show that the proposed generative models give a higher likelihood on test data compared to a traditional modeling approach indicating that they can summarize the statistical properties of the data better i
i present a simple variation of importance sampling that explicitly searches for important regions in the target distribution i prove that the technique yields unbiased estimates and show empirically it can reduce the variance of standard monte carlo estimators this is achieved by concentrating samples in more significant regions of the sample space
we present a variational bayesian method for model selection over families of kernels classifiers like support vector machines or gaussian processes the algorithm needs no user interaction and is able to adapt a large number of kernel parameters to given data without having to sacrifice training cases for validation this opens the possibility to use sophisticated families of kernels in situations where the small standard kernel classes are clearly inappropriate we relate the method to other work done on gaussian processes and clarify the relation between support vector machines and certain gaussian process models i
we describe an iterative algorithm for building vector machines used in classification tasks the algorithm builds on ideas from support vector machines boosting and generalized additive models the algorithm can be used with various continuously differential functions that bound the discrete classification loss and is very simple to implement we test the proposed algorithm with two different loss functions on synthetic and natural data we also describe a norm penalized version of the algorithm for the exponential loss function used in adaboost the performance of the algorithm on natural data is comparable to support vector machines while typically its running time is shorter than of svm
we introduce a novel distributional clustering algorithm that maximizes the mutual information per cluster between data and given categories this algorithm can be considered as a bottom up hard version of the recently introduced information bottleneck method the algorithm is compared with the top down soft version of the information bottleneck method and a relationship between the hard and soft results is established we demonstrate the algorithm on the newsgroups data set for a subset of two newsgroups we achieve compression by orders of magnitudes loosing only of the original mutual information
in this paper we consider the problem of active learning in trigonometric polynomial networks and give a necessary and sufficient condition of sample points to provide the optimal generalization capability by analyzing the condition from the functional analytic point of view we clarify the mechanism of achieving the optimal generalization capability we also show that a set of training examples satisfying the condition does not only provide the optimal generalization but also reduces the computational complexity and memory required for the calculation of learning results finally examples of sample points satisfying the condition are given and computer simulations are performed to demonstrate the effectiveness of the proposed active learning method
gaussian processes are powerful regression models specified by parametrized mean and covariance functions standard approaches to estimate these parameters known by the name hyperparameters are maximum likelihood ml and maximum aposterior map approaches in this paper we propose and investigate predictive approaches namely maximization of geissers surrogate predictive probability gpp and minimization of mean square error with respect to gpp referred to as geissers predictive mean square error gpe to estimate the hyperparameters we also derive results for the standard cross validation cv error and make a comparison these approaches are tested on a number of problems and experimental results show that these approaches are strongly competitive to existing approaches i
in this paper we will treat input selection for a radial basis function rbf like classifier within a bayesian framework we approximate the a posteriori distribution over both model coefficients and input subsets by samples drawn with gibbs updates and reversible jump moves using some public datasets we compare the classification accuracy of the method with a conventional ard scheme these datasets are also used to infer the a posteriori probabilities of different input subsets
we propose a novel approach for building finite memory predictive models similar in spirit to variable memory length markov models vlmms the models are constructed by first transforming the n block structure of the training sequence into a spatial structure of points in a unit hypercube such that the longer is the common suffix shared by any two n blocks the closer lie their point representations such a transformation embodies a markov assumption n blocks with long common suffixes are likely to produce similar continuations finding a set of prediction contexts is formulated as a resource allocation problem solved by vector quantizing the spatial n block representation we compare our model with both the classical and variable memory length markov models on three data sets with different memory and stochastic components our models have a superior performance yet their construction is fully automatic which is shown to be problematic in the case of vlmms
the support vector machine svm is a state of the art technique for regression and classification combining excellent generalisation properties with a sparse kernel representation however it does suffer from a number of disadvantages notably the absence of probabilistic outputs the requirement to estimate a trade off parameter and the need to utilise mercer kernel functions in this paper we introduce the relevance vector machine rvm a bayesian treatment of a generalised linear model of identical functional form to the svm the rvm suffers from none of the above disadvantages and examples demonstrate that for comparable generalisation performance the rvm requires dramatically fewer kernel functions i
a new method for multivariate density estimation is developed based on the support vector method svm solution of inverse ill posed problems the solution has the form of a mixture of densities this method with gaussian kernels compared favorably to both parzens method and the gaussian mixture model method for synthetic data we achieve more accurate estimates for densities of and dimensions i
dual estimation refers to the problem of simultaneously estimating the state of a dynamic system and the model which gives rise to the dynamics algorithms include expectation maximization em dual kalman filtering and joint kalman methods these methods have recently been explored in the context of nonlinear modeling where a neural network is used as the functional form of the unknown model typically an extended kalman filter ekf or smoother is used for the part of the algorithm that estimates the clean state given the current estimated model an ekf may also be used to estimate the weights of the network this paper points out the flaws in using the ekf and proposes an improvement based on a new approach called the unscented transformation ut a substantial performance gain is achieved with the same order of computational complexity as that of the standard ekf the approach is illustrated on several dual estimation methods
local belief propagation rules of the sort proposed by pearl are guaranteed to converge to the correct posterior probabilities in singly connected graphical models recently a number of researchers have empirically demonstrated good performance of oopy belief propagationusing these same rules on graphs with loops perhaps the most dramatic instance is the near shannon limit performance of turbo codes whose decoding algorithm is equivalent to loopy belief propagation except for the case of graphs with a single loop there has been little theoretical understanding of the performance of oopy propagation here we analyze belief propagation in networks with arbitrary topologies when the nodes in the graph describe jointly gaussian random variables we give an analytical formula relating the true posterior probabilities with those calculated using loopy propagation we give sufficient conditions for convergence and show that when belief propagation converges it gives the correct posterior means for all graph topologies not just networks with a single loop the related max product belief propagation algorithm finds the maximum posterior probability estimate for singly connected networks we show that even for non gaussian probability distributions the convergence points of the max product algorithm in loopy networks are maxima over a particular large local neighborhood of the posterior probability these results help clarify the empirical performance results and motivate using the powerful belief propagation algorithm in a broader class of networks problems involving probabilistic belief propagation arise in a wide variety of applications including error correcting codes speech recognition and medical diagnosis if the graph is singly connected there exist local message passing schemes to calculate the posterior probability of an unobserved variable given the observed variables pearl derived such a scheme for singly connected bayesian networks and showed that this belief propagation algorithm is guaranteed to converge to the correct posterior probabilities or beliefs several groups have recently reported excellent experimental results by running algorithms y weiss and w t freeman equivalent to pearls algorithm on networks with loops perhaps the most dramatic instance of this performance is for turbo code error correcting codes these codes have been described as the most exciting and potentially important development in coding theory in many years and have recently been shown to utilize an algorithm equivalent to belief propagation in a network with loops progress in the analysis of loopy belief propagation has been made for the case of networks with a single loop for these networks it can be shown that unless all the compatabilities are deterministic oopy belief propagation will converge the difference between the loopy beliefs and the true beliefs is related to the convergence rate of the messages the faster the convergence the more exact the approximation and if the hidden nodes are binary then the loopy beliefs and the true beliefs are both maximized by the same assignments although the confidence in that assignment is wrong for the loopy beliefs in this paper we analyze belief propagation in graphs of arbitrary topology for nodes describing jointly gaussian random variables we give an exact formula relating the correct marginal posterior probabilities with the ones calculated using oopy belief propagation we show that if belief propagation converges then it will give the correct posterior means for all graph topologies not just networks with a single loop we show that the covariance estimates will generally be incorrect but present a relationship between the error in the covariance estimates and the convergence speed for gaussian or non gaussian variables we show that the max product algorithm which calculates the map estimate in singly connected networks only converges to points that are maxima over a particular large neighborhood of the posterior probability of loopy networks analysis to simplify the notation we assume the graphical model has been preprocessed into an undirected graphical model with pairwise potentials any graphical model can be converted into this form and running belief propagation on the pairwise graph is equivalent to running belief propagation on the original graph we assume each node zi has a local observation i in each iteration of belief propagation each node zi sends a message to each neighboring zj that is based on the messages it received from the other neighbors its local observation y and the pairwise potentials ij xi x j and ii xi yi we assume the message passing occurs in parallel the idea behind the analysis is to build an unwrapped tree the unwrapped tree is the graphical model which belief propagation is solving exactly when one applies the belief propagation rules in a loopy network it is constructed by maintaining the same local neighborhood structure as the oopy network but nodes are replicated so there are no loops the potentials and the observations are replicated from the loopy graph figure a shows an unwrapped tree for the diamond shaped graph in b by construction the belief at the root node a is identical to that at node zx in the loopy graph after four iterations of belief propagation each node has a shaded observed node attached to it omitted here for clarity because the original network represents jointly gaussian variables so will the unwrapped tree since it is a tree belief propagation is guaranteed to give the correct answer for the unwrapped graph we can thus use gaussian marginalization formulae to calculate the true mean and variances in both the original and the unwrapped networks in this way we calculate the accuracy of belief propagation for gaussian networks of arbitrary topology we assume that the joint mean is zero the means can be added in later the joint districorrectness of belief propagation xl x x figure left a markov network with multiple loops right the unwrapped network corresponding to this structure bution of z is given by pz ae where v it y vvv is straightforward to construct the inverse covariance matrix v of the joint gaussian that describes a given gaussian graphical model writing out the exponent of the joint and completing the square shows that the mean t of x given the observations y is given by vvy and the covariance matrix ci v of x given y is ci v g x we will denote by ciy the ith row of clv so the marginal posterior variance of zi given the data is a i c iv i we will use for unwrapped quantities we scan the tree in breadth first order and denote by the vector of values in the hidden nodes of the tree when so scanned simlarly we denote by the observed nodes scanned in the same order and p pv the inverse covariance matrices since we are scanning in breadth first order the last nodes are the leaf nodes and we denote by l the number of leaf nodes by the nature of unwrapping is the mean of the belief at node zx after t iterations of belief propagation where t is the number of unwrappings similarly xlv is the variance of the belief at node x after t iterations because the data is replicated we can write oy where oi j if i is a replica ofyj and otherwise since the potentials izi yi are replicated we can write pvo over since the i zi z j are also replicated and all non leaf i have the same connectivity as the corresponding zi we can write lzo ovzz e where e is zero in all but the last l rows when these relationships between the loopy and unwrapped inverse covariance matrices are substituted into the loopy and unwrapped versions of equation one obtains the following expression true for any iteration zlve where e is a vector that is zero everywhere but the last l components corresponding to the leaf nodes our choice of the node for the root of the tree is arbitrary so this applies to all nodes of the oopy network this formula relates for any node of a network with loops the means calculated at each iteration by belief propagation with the true posterior means similarly when the relationship between the loopy and unwrapped inverse covariance matrices is substituted into the loopy and unwrapped definitions of ci v we can relate the y weiss and w t freeman node oo figure the conditional correlation between the root node and all other nodes in the unwrapped tree of fig i after eight iterations potentials were chosen randomly nodes are presented in breadth first order so the last elements are the correlations between the root node and the leaf nodes we show that if this correlation goes to zero belief propagation converges and the loopy means are exact symbols plotted with a star denote correlations with nodes that correspond to the node r in the loopy graph the sum of these correlations gives the correct variance of node ca while loopy propagation uses only the first correlation marginalized covariances calculated by belief propagation to the true ones a lvel lve where el is a vector that is zero everywhere but the last l components while e is equal to i for all nodes in the unwrapped tree that are replicas of ca except for ea all other components of e are zero figure shows x iv for the diamond network in fig we generated random potential functions and observations and calculated the conditional correlations in the unwrapped tree note that the conditional correlation decreases with distance in the tree m we are scanning in breadth first order so the last l components correspond to the leaf nodes as the number of iterations of loopy propagation is increased the size of the unwrapped tree increases and the conditional correlation between the leaf nodes and the root node decreases from equations it is clear that if the conditional correlation between the leaf nodes and the root nodes are zero for all sufficiently large unwrappings then belief propagation converges the means are exact and the variances may be incorrect in practice the conditional correlations will not actually be equal to zero for any finite unwrapping in we give a more precise statement if the conditional correlation of the root node and the leaf nodes decreases rapidly enough then belief propagation converges the means are exact and the variances may be incorrect we also show sufficient conditions on the potentials xi xj for the correlation to decrease rapidly enough the rate at which the correlation decreases is determined by the ratio of off diagonal and diagonal components in the quadratic form defining the potentials how wrong will the variances be the term ox lye in equation is simply the sum of many components of o lvfigure shows these components the correct variance is the sum of all the components while the belief propagation variance approximates this sum with the first and dominant term whenever there is a positive correlation between the root node and other replicas of z the loopy variance is strictly less than the true variance m the oopy estimate is overconfident correctness of belief propagation a o o r iterations b figure a x graphical model for simulation the unobserved nodes untilled were connected to their four nearest neighbors and to an observation node filled b the error of the estimates of loopy propagation and successive over relaxation sor as a function of iteration note that belief propagation converges much faster than sor note that when the conditional correlation decreases rapidly to zero two things happen first the convergence is faster because x ite approaches zero faster second the approximation error of the variances is smaller because x ite is smaller thus we have shown as in the single loop case quick convergence is correlated with good approximation simulations we ran belief propagation on the x d grid of fig a the joint probability was px y expe wijxi xj e wiixi yi j where wij if nodes zi zj are not neighbors and otherwise and wii was randomly selected to be or i for all i with probability of set to the observations yi were chosen randomly this problem corresponds to an approximation problem from sparse data where only of the points are visible we found the exact posterior by solving equation we also ran belief propagation and found that when it converged the calculated means were identical to the true means up to machine precision also as predicted by the theory the calculated variances were too small the belief propagation estimate was overconfident in many applications the solution of equation i by matrix inversion is intractable and iterative methods are used figure compares the error in the means as a function of iterations for oopy propagation and successive over relaxation sor considered one of the best relaxation methods note that after essentially five iterations loopy propagation gives the right answer while sor requires many more as expected by the fast convergence the approximation error in the variances was quite small the median error was for comparison the true variances ranged from to with a mean of also the nodes for which the approximation error was worse were indeed the nodes that converged slower y weiss and w t freeman discussion independently two other groups have recently analyzed special cases of gaussian graphical models frey analyzed the graphical model corresponding to factor analysis and gave conditions for the existence of a stable fixed point rusmevichientong and van roy analyzed a graphical model with the topology of turbo decoding but a gaussian joint density for this specific graph they gave sufficient conditions for convergence and showed that the means are exact our main interest in the gaussian case is to understand the performance of belief propagation in general networks with multiple loops we are struck by the similarity of our results for gaussians in arbitrary networks and the results for single loops of arbitrary distributions first in single loop networks with binary nodes loopy belief at a node and the true belief at a node are maximized by the same assignment while the confidence in that assignment is incorrect in gaussian networks with multiple loops the mean at each node is correct but the confidence around that mean may be incorrect second for both singleloop and gaussian networks fast belief propagation convergence correlates with accurate beliefs third in both gaussians and discrete valued single loop networks the statistical dependence between root and leaf nodes governs the convergence rate and accuracy the two models are quite different mean field approximations are exact for gaussian mrfs while they work poorly in sparsely connected discrete networks with a single loop the results for the gaussian and single loop cases lead us to believe that similar results may hold for a larger class of networks can our analysis be extended to non gaussian distributions the basic idea applies to arbitrary graphs and arbitrary potentials belief propagation is performing exact inference on a tree that has the same local neighbor structure as the oopy graph however the linear algebra that we used to calculate exact expressions for the error in belief propagation at any iteration holds only for gaussian variables we have used a similar approach to analyze the related max product belief propagation algorithm on arbitrary graphs with arbitrary distributions both discrete and continuous valued nodes we show that if the max product algorithm converges the max product assignment has greater posterior probability then any assignment in a particular large region around that assignment while this is a weaker condition than a global maximum it is much stronger than a simple local maximum of the posterior probability the sum product and max product belief propagation algorithms are fast and parallelizable due to the well known hardness of probabilistic inference in graphical models belief propagation will obviously not work for arbitrary networks and distributions nevertheless a growing body of empirical evidence shows its success in many networks with loops our results justify applying belief propagation in certain networks with multiple loops this may enable fast approximate probabilistic inference in a range of new applications references sm aji gb horn and rj mceliece on the convergence of iterative decoding on graphs with a single cycle in proc isit c berrou a glavieux and p thitimajshima near shannon limit error correcting coding and decoding turbo codes in proc ieee international communications conference r cowell advanced inference in bayesian networks in mi jordan editor learning in graphical models mit press gd fomey er kschischang and b marcus iterative decoding of tail biting trellisses preprint presented at information theory workshop in san diego correctness of belief propagation w t freeman and y weiss on the fixed points of the max product algorithm technical report merl broadway cambridge ma wt freeman and ec pasztor learning to estimate scenes from images in ms kearns sa solla and da cohn editors adv neural information processing systems i mit press bj frey turbo factor analysis in adv neural information processing systems to appear brendan j frey bayesian networks for pattern classification data compression and channel coding mit press rg gallager low density parity check codes mit press f r kschischang and b j frey iterative decoding of compound codes by probability propagation in graphical models ieee journal on selected areas in communication rj mceliece djc mackkay and jf cheng turbo decoding as as an instance of pearls belief propagation algorithm ieee journal on selected areas in communication rj mceliece e rodereich and jf cheng the turbo decision algorithm in proc rd allerton conference on communications control and computing pages monticello il kp murphy y weiss and mi jordan loopy belief propagation for approximate inference an empirical study in proceedings of uncertainty in ai rusmevichientong p and van roy b an analysis of turbo decoding with gaussian densities in adv neural information processing systems to appear judea pearl probabilistic reasoning in intelligent systems networks ofplausible inference morgan kaufmann gilbert strang
there are many hierarchical clustering algorithms available but these lack a firm statistical basis here we set up a hierarchical probabilistic mixture model where data is generated in a hierarchical tree structured manner markov chain monte carlo mcmc methods are demonstrated which can be used to sample from the posterior distribution over trees containing variable numbers of hidden units
data visualization and feature selection methods are proposed based on the joint mutual information and ica the visualization methods can find many good d projections for high dimensional data interpretation which cannot be easily found by the other existing methods the new variable selection method is found to be better in eliminating redundancy in the inputs than other methods based on simple mutual information the efficacy of the methods is illustrated on a radar signal analysis problem to find d viewing coordinates for data visualization and to select inputs for a neural network classifier keywords feature selection joint mutual information ica visualiz ation classification
we propose a new markov chain monte carlo algorithm which is a generalization of the stochastic dynamics method the algorithm performs exploration of the state space using its intrinsic geometric structure facilitating efficient sampling of complex distributions applied to bayesian learning in neural networks our algorithm was found to perform at least as well as the best state of the art method while consuming considerably less time
imagine that you wish to classify data consisting of tens of thousands of examples residing in a twenty thousand dimensional space how can one apply standard machine learning algorithms we describe the parallel problems server ppserver and matlabp in tandem they allow users of networked computers to work transparently on large data sets from within matlab this work is motivated by the desire to bring the many benefits of scientific computing algorithms and computational power to machine learning researchers we demonstrate the usefulness of the system on a number of tasks for example we perform independent components analysis on very large text corpora consisting of tens of thousands of documents making minimal changes to the original bell and sejnowski matlab source bell and sejnowski applying ml techniques to data previously beyond their reach leads to interesting analyses of both data and algorithms
a system emulating the functionality of a moving eye hence the name oculo motor system has been built and successfully tested it is made of an optical device for shifting the field of view of an image sensor by up to o in any direction four neuromorphic analog vlsi circuits implementing an oculo motor control loop and some off the shelf electronics the custom integrated circuits communicate with each other primarily by non arbitrated address event buses the system implements the behaviors of saliency based saccadic exploration and smooth pursuit of light spots the duration of saccades ranges from ms to ms which is comparable to human eye performance smooth pursuit operates on light sources moving at up to s in the visual field i
i describe a silicon network consisting of a group of excitatory neurons and a global inhibitory neuron the output of the inhibitory neuron is normalized with respect to the input strengths this output models the normalization property of the wide field directionselective cells in the fly visual system this normalizing property is also useful in any system where we wish the output signal to code only the strength of the inputs and not be dependent on the number of inputs the circuitry in each neuron is equivalent to that in lazzaros winner take all wta circuit with one additional transistor and a voltage reference just as in lazzaros circuit the outputs of the excitatory neurons code the neuron with the largest input the difference here is that multiple winners can be chosen by varying the voltage reference of the neuron the network can transition between a soft max behavior and a hard wta behavior i show results from a fabricated chip of neurons in a pm cmos technology
we have developed and tested an analogdigital vlsi system that models the coordination of biological segmental oscillators underlying axial locomotion in animals such as leeches and lampreys in its current form the system consists of a chain of twelve pattern generating circuits that are capable of arbitrary contralateral inhibitory synaptic coupling each pattern generating circuit is implemented with two independent silicon morris lecar neurons with a total of programmable floating gate based inhibitory synapses and an asynchronous address event interconnection element that provides synaptic connectivity and implements axonal delay we describe and analyze the data from a set of experiments exploring the system behavior in terms of synaptic coupling
we have developed a vlsi silicon neuron and a corresponding mathematical model that is a two state variable system we describe the circuit implementation and compare the behaviors observed in the silicon neuron and the mathematical model we also perform bifurcation analysis of the mathematical model by varying the externally applied current and show that the behaviors exhibited by the silicon neuron under corresponding conditions are in good agreement to those predicted by the bifurcation analysis
this paper presents an electronic system that extracts the periodicity of a sound it uses three analogue vlsi building blocks a silicon cochlea two inner hair cell circuits and two spiking neuron chips the silicon cochlea consists of a cascade of filters because of the delay between two outputs from the silicon cochlea spike trains created at these outputs are synchronous only for a narrow range of periodicities in contrast to traditional bandpass filters where an increase in selectivity has to be traded off against a decrease in response time the proposed system responds quickly independent of selectivity i
a neural model is described which uses oscillatory correlation to segregate speech from interfering sound sources the core of the model is a two layer neural oscillator network a sound stream is represented by a synchronized population of oscillators and different streams are represented by desynchronized oscillator populations the model has been evaluated using a corpus of speech mixed with interfering sounds and produces an improvement in signal to noise ratio for every mixture
we present a hidden markov model hmm for inferring the hidden psychological state or neural activity during single trial fmri activation experiments with blocked task paradigms inference is based on bayesian methodology using a combination of analytical and a variety of markov chain monte carlo mcmc sampling techniques the advantage of this method is that detection of short time learning effects between repeated trials is possible since inference is based only on single trial experiments
this paper examines the role of biological constraints in the human auditory localization process a psychophysical and neural system modeling approach was undertaken in which performance comparisons between competing models and a human subject explore the relevant biologically plausible realism constraints the directional acoustical cues upon which sound localization is based were derived from the human subjects head related transfer functions hrtfs sound stimuli were generated by convolving bandpass noise with the hrtfs and were presented to both the subject and the model the input stimuli to the model was processed using the auditory image model of cochlear processing the cochlear data was then analyzed by a time delay neural network which integrated temporal and spectral information to determine the spatial location of the sound source the combined cochlear model and neural network provided a system model of the sound localization process human like localization performance was qualitatively achieved for broadband and bandpass stimuli when the model architecture incorporated frequency division or tonotopicity and was trained using variable bandwidth and center frequency sounds
the differential contribution of the monaural and interaural spectral cues to human sound localization was examined using a combined psychophysical and analytical approach the cues to a sounds location were correlated on an individual basis with the human localization responses to a variety of spectrally manipulated sounds the spectral cues derive from the acoustical filtering of an individuals auditory periphery which is characterized by the measured head related transfer functions hrtfs auditory localization performance was determined in virtual auditory space vas psychoacoustical experiments were conducted in which the amplitude spectra of the sound stimulus was varied independently at each ear while preserving the normal timing cues an impossibility in the free field environment virtual auditory noise stimuli were generated over earphones for a specified target direction such that there was a false flat spectrum at the left eardrum using the subjects hrtfs the sound spectrum at the right eardrum was then adjusted so that either the true right monaural spectral cue or the true interaural spectral cue was preserved all subjects showed systematic mislocalizations in both the true right and true interaural spectral conditions which was absent in their control localization performance the analysis of the different cues along with the subjects localization responses suggests there are significant differences in the use of the monaural and interaural spectral cues and that the auditory systems reliance on the spectral cues varies with the sound condition
n wideband sources recorded using n closely spaced receivers can feasibly be separated based only on second order statistics when using a physical model of the mixing process in this case we show that the parameter estimation problem can be essentially reduced to considering directions of arrival and attenuations of each signal the paper presents two demixing methods operating in the time and frequency domain and experimentally shows that it is always possible to demix signals arriving at different angles moreover one can use spatial cues to solve the channel selection problem and a post processing wiener filter to ameliorate the artifacts caused by demixing

stochastic meta descent smd is a new technique for online adaptation of local learning rates in arbitrary twice differentiable systems like matrix momentum it uses full second order information while retaining on computational complexity by exploiting the efficient computation of hessian vector products here we apply smd to independent component analysis and employ the resulting algorithm for the blind separation of time varying mixtures by matching individual learning rates to the rate of change in each source signals mixture coefficients our technique is capable of simultaneously tracking sources that move at very different a priori unknown speeds
the speech waveform can be modelled as a piecewise stationary linear stochastic state space system and its parameters can be estimated using an expectation maximisation em algorithm one problem is the initialisation of the em algorithm standard initialisation schemes can lead to poor formant trajectories but these trajectories however are important for vowel intelligibility the aim of this paper is to investigate the suitability of subspace identification methods to initialise em the paper compares the subspace state space system identification sid method with the em algorithm the sid and em methods are similar in that they both estimate a state sequence but using kalman filters and kalman smoothers respectively and then estimate parameters but using least squares and maximum likelihood respectively the similarity of sid and em motivates the use of sid to initialise em also sid is non iterative and requires no initialisation whereas em is iterative and requires initialisation however sid is sub optimal compared to em in a probabilistic sense during experiments on real speech sid methods compare favourably with conventional initialisation techniques they produce smoother formant trajectories have greater frequency resolution and produce higher likelihoods work done while in cambridge engineering dept uk speech modelling using subspace and em techniques
in this paper we use mutual information to characterize the distributions of phonetic and speakerchannel information in a timefrequency space the mutual information mi between the phonetic label and one feature and the joint mutual information jmi between the phonetic label and two or three features are estimated the millers bias formulas for entropy and mutual information estimates are extended to include higher order terms the mi and the jmi for speakerchannel recognition are also estimated the results are complementary to those for phonetic classification our results show how the phonetic information is locally spread and how the speakerchannel information is globally spread in time and frequency i
psychophysical and physiological evidence shows that sound localization of acoustic signals is strongly influenced by their synchrony with visual signals this effect known as ventriloquism is at work when sound coming from the side of a tv set feels as if it were coming from the mouth of the actors the ventriloquism effect suggests that there is important information about sound location encoded in the synchrony between the audio and video signals in spite of this evidence audiovisual synchrony is rarely used as a source of information in computer vision tasks in this paper we explore the use of audio visual synchrony to locate sound sources we developed a system that searches for regions of the visual landscape that correlate highly with the acoustic signals and tags them as likely to contain an acoustic source we discuss our experience implementing the system present results on a speaker localization task and discuss potential applications of the approach
the three dimensional motion of humans is underdetermined when the observation is limited to a single camera due to the inherent d ambiguity of d video we present a system that reconstructs the d motion of human subjects from single camera video relying on prior knowledge about human motion learned from training data to resolve those ambiguities after initialization in d the tracking and d reconstruction is automatic we show results for several video sequences the results show the power of treating d body tracking as an inference problem
independent component analysis of natural images leads to emergence of simple cell properties ie linear filters that resemble wavelets or gabor functions in this paper we extend ica to explain further properties of v cells first we decompose natural images into independent subspaces instead of scalar components this model leads to emergence of phase and shift invariant features similar to those in v complex cells second we define a topography between the linear components obtained by ica the topographic distance between two components is defined by their higher order correlations so that two components are close to each other in the topography if they are strongly dependent on each other this leads to simultaneous emergence of both topography and invariances similar to complex cell properties i
in this paper we propose that information maximization can provide a unified framework for understanding saccadic eye movements in this framework the mutual information among the cortical representations of the retinal image the priors constructed from our long term visual experience and a dynamic short term internal representation constructed from recent saccades provides a map for guiding eye navigation by directing the eyes to locations of maximum complexity in neuronal ensemble responses at each step the automatic saccadic eye movement system greedily collects information about the external world while modifying the neural representations in the process this framework attempts to connect several psychological phenomena such as pop out and inhibition of return to long term visual experience and short term working memory it also provides an interesting perspective on contextual computation and formation of neural representation in the visual system
we describe a method for learning an overcomplete set of basis functions for the purpose of modeling sparse structure in images the sparsity of the basis function coefficients is modeled with a mixture of gaussians distribution one gaussian captures nonactive coefficients with a small variance distribution centered at zero while one or more other gaussians capture active coefficients with a large variance distribution we show that when the prior is in such a form there exist efficient methods for learning the basis functions as well as the parameters of the prior the performance of the algorithm is demonstrated on a number of test cases and also on natural images the basis functions learned on natural images are similar to those obtained with other methods but the sparse form of the coefficient distribution is much better described also since the parameters of the prior are adapted to the data no assumption about sparse structure in the images need be made a priori rather it is learned from the data i
we formulate a model for probability distributions on image spaces we show that any distribution of images can be factored exactly into conditional distributions of feature vectors at one resolution pyramid level conditioned on the image information at lower resolutions we would like to factor this over positions in the pyramid levels to make it tractable but such factoring may miss long range dependencies to fix this we introduce hidden class labels at each pixel in the pyramid the result is a hierarchical mixture of conditional probabilities similar to a hidden markov model on a tree the model parameters can be found with maximum likelihood estimation using the em algorithm we have obtained encouraging preliminary results on the problems of detecting various objects in sar images and target recognition in optical aerial images
the statistics of photographic images when represented using multiscale wavelet bases exhibit two striking types of nongaussian behavior first the marginal densities of the coefficients have extended heavy tails second the joint densities exhibit variance dependencies not captured by second order models we examine properties of the class of gaussian scale mixtures and show that these densities can accurately characterize both the marginal and joint distributions of natural image wavelet coefficients this class of model suggests a markov structure in which wavelet coefficients are linked by hidden scaling variables corresponding to local image structure we derive an estimator for these hidden variables and show that a nonlinear normalization procedure can be used to gaussianize the coefficients recent years have witnessed a surge of interest in modeling the statistics of natural images such models are important for applications in image processing and computer vision where many techniques rely either implicitly or explicitly on a prior density a number of empirical studies have demonstrated that the power spectra of natural images follow a if v law in radial frequency where the exponent is typically close to two eg such second order characterization is inadequate however because images usually exhibit highly non gaussian behavior for instance the marginals of wavelet coefficients typically have much heavier tails than a gaussian furthermore despite being approximately decorrelated as suggested by theoretical analysis of if processes orthonormal wavelet coefficients exhibit striking forms of statistical dependency in particular the standard deviation of a wavelet coefficient typically scales with the absolute values of its neighbors a number of researchers have modeled the marginal distributions of wavelet coefficients with generalized laplacians py c exp lyai p eg special cases include the gaussian p and the laplacian p but appropriate exresearch supported by nserc fellowship to mjw and nsf career grant mip to eps m j wainwright and e p sirnoncelli mixing density positive v stable no explicit form gsm density gsm char function symmetrized gamma student a stable generalized laplacian exp yxlp pe t l x no explicit form exp itla a e no explicit form table example densities from the class of gaussian scale mixtures z denotes a positive gamma variable with density pz fyz exp z the characteristic function of a random variable x is defined as optt foo px exp jxt dx ponents for natural images are typically less than one simoncelli has modeled the variance dependencies of pairs of wavelet coefficients romberg et al have modeled wavelet densities using two component mixtures of gaussians huang and mumford have modeled marginal densities and cross sections of joint densities with multi dimensional generalized laplacians in the following sections we explore the semi parametric class of gaussian scale mixtures we show that members of this class satisfy the dual requirements of being heavy tailed and exhibiting multiplicative scaling between coefficients we also show that a particular member of this class in which the multiplier variables are distributed according to a gamma density captures the range of joint statistical behaviors seen in wavelet coefficients of natural images we derive an estimator for the multipliers and show that a nonlinear normalization procedure can be used to gaussianize the wavelet coefficients lastly we form random cascades by linking the multipliers on a multiresolution tree i scale mixtures of gaussians d d a random vector y is a gaussian scale mixture gsm if y zu where denotes equality in distribution z is a scalar random variable u iv q is a gaussian random vector and z and u are independent as a consequence any gsm variable has a density given by an integral yto y pyy oo izqi exp czdz where bz is the probability density of the mixing variable z henceforth the multiplier a special case of a gsm is a finite mixture of gaussians where z is a discrete random variable more generally it is straightforward to provide conditions on either the density or characteristic function of x that ensure it is a gsm but these conditions do not necessarily provide an explicit form of bz nevertheless a number of well known distributions may be written as gaussian scale mixtures for the scalar case a few of these densities along with their associated characteristic functions are listed in table each variable is characterized by a scale parameter a and a tail parameter all of the gsm models listed in table produce heavy tailed marginal and variance scaling joint densities scale mixtures of gaussians and the statistics of natural images baboon boats flower frog k ahh figure gsms dashed lines fitted to empirical histograms solid lines below each plot are the parameter values and the relative entropy between the histogram with bins and the model as a fraction of the histogram entropy modeling natural images as mentioned in the
a novel learning approach for human face detection using a network of linear units is presented the snow learning architecture is a sparse network of linear functions over a pre defined or incrementally learned feature space and is specifically tailored for learning in the presence of a very large number of features a wide range of face images in different poses with different expressions and under different lighting conditions are used as a training set to capture the variations of human faces experimental results on commonly used benchmark data sets of a wide range of face images show that the snow based approach outperforms methods that use neural networks bayesian methods support vector machines and others furthermore learning and evaluation using the snow based method are significantly more efficient than with other methods i

a fundamental problem with the modeling of chaotic time series data is that minimizing short term prediction errors does not guarantee a match between the reconstructed attractors of model and experiments we introduce a modeling paradigm that simultaneously learns to short term predict and to locate the outlines of the attractor by a new way of nonlinear principal component analysis closed loop predictions are constrained to stay within these outlines to prevent divergence from the attractor learning is exceptionally fast parameter estimation for the sample laser data from the santa fe time series competition took less than a minute on a mhz pentium pc
the facial action coding system facs is an objective method for quantifying facial movement in terms of component actions this system is widely used in behavioral investigations of emotion cognitive processes and social interaction the coding is presently performed by highly trained human experts this paper explores and compares techniques for automatically recognizing facial actions in sequences of images these methods include unsupervised learning techniques for finding basis images such as principal component analysis independent component analysis and local feature analysis and supervised learning techniques such as fishers linear discriminants these data driven bases are compared to gabor wavelets in which the basis images are predefined best performances were obtained using the gabor wavelet representation and the independent component representation both of which achieved accuracy for classifying facial actions the ica representation employs orders of magnitude fewer basis images than the gabor representation and takes less cpu time to compute for new images the results provide converging support for using local basis images high spatial frequencies and statistical independence for classifying facial actions i
this paper examines the application of reinforcement learning to a wireless communication problem the problem requires that channel utility be maximized while simultaneously minimizing battery usage we present a solution to this multi criteria problem that is able to significantly reduce power consumption the solution uses a variable discount factor to capture the effects of battery usage
we discuss an information theoretic approach for categorizing and modeling dynamic processes the approach can learn a compact and informative statistic which summarizes past states to predict future observations furthermore the uncertainty of the prediction is characterized nonparametrically by a joint density over the learned statistic and present observation we discuss the application of the technique to both noise driven dynamical systems and random processes sampled from a density which is conditioned on the past in the first case we show results in which both the dynamics of random walk and the statistics of the driving noise are captured in the second case we present results in which a summarizing statistic is learned on noisy random telegraph waves with differing dependencies on past states in both cases the algorithm yields a principled approach for discriminating processes with differing dynamics andor dependencies the method is grounded in ideas from information theory and nonparametric statistics
three contributions to developing an algorithm for assisting engineers in designing analog circuits are provided in this paper first a method for representing highly nonlinear and non continuous analog circuits using kirchoff current law potential functions within the context of a markov field is described second a relatively efficient algorithm for optimizing the markov field objective function is briefly described and the convergence proof is briefly sketched and third empirical results illustrating the strengths and limitations of the approach are provided within the context of a jfet transistor design problem the proposed algorithm generated a set of circuit components for the jfet circuit model that accurately generated the desired characteristic curves analog circuit design using markov random fields markov random field models a markov random field mrf is a generalization of the concept of a markov chain in a markov field one begins with a set of random variables and a neighborhood relation which is represented by a graph each random variable will be assumed in this paper to be a discrete random variable which takes on one of a finite number of possible values each node of the graph indexs a specific random variable a link from the jth node to the ith node indicates that the conditional probability distribution of the ith random variable in the field is functionally dependent upon the jth random variable that is random variable j is a neighbor of random variable i the only restriction upon the definition of a markov field ie the positivity condition is that the probability of every realization of the field is strictly positive the essential idea behind markov field design is that one specifies a potential energy function for every clique in the neighborhood graph such that the subset of random variables associated with that clique obtain their optimal values when that cliques potential function obtains its minimal value for reviews see associate professor at university of texas at dallas wwwutdallas edugolden r m golden markov random field models provide a convenient mechanism for probabilistically representing and optimally combining combinations of local constraints analog circuit design using spice in some mixed signal asic application specific integrated circuit design problems most of the circuit design specifications are well known but the
the project pursued in this paper is to develop from first information geometric principles a general method for learning the similarity between text documents each individual document is modeled as a memoryless information source based on a latent class decomposition of the term document matrix a lowdimensional curved multinomial subfamily is learned from this model a canonical similarity function known as the fisher kernel is derived our approach can be applied for unsupervised and supervised learning problems alike this in particular covers interesting cases where both labeled and unlabeled data are available experiments in automated indexing and text categorization verify the advantages of the proposed method i
the committee approach has been proposed for reducing model uncertainty and improving generalization performance the advantage of committees depends on the performance of individual members and the correlational structure of errors between members this paper presents an input grouping technique for designing a heterogeneous committee with this technique all input variables are first grouped based on their mutual information statistically similar variables are assigned to the same group each members input set is then formed by input variables extracted from different groups our designed committees have less error correlation between its members since each member observes different input variable combinations the individual members feature sets contain less redundant information because highly correlated variables will not be combined together the member feature sets contain almost complete information since each set contains a feature from each information group an empirical study for a noisy and nonstationary economic forecasting problem shows that committees constructed by our proposed technique outperform committees formed using several existing techniques i
we provide preliminary evidence that existing algorithms for inferring small scale gene regulation networks from gene expression data can be adapted to large scale gene expression data coming from hybridization microarrays the essential steps are clustering many genes by their expression time course data into a minimal set of clusters of co expressed genes theoretically modeling the various conditions under which the time courses are measured using a continious time analog recurrent neural network for the cluster mean time courses fitting such a regulatory model to the cluster mean time courses by simulated annealing with weight decay and analysing several such fits for commonalities in the circuit parameter sets including the connection matrices this procedure can be used to assess the adequacy of existing and future gene expression time course data sets for determining transcriptional regulatory relationships such as coregulation

in hyperspectral imagery one pixel typically consists of a mixture of the reflectance spectra of several materials where the mixture coefficients correspond to the abundances of the constituting materials we assume linear combinations of reflectance spectra with some additive normal sensor noise and derive a probabilistic map framework for analyzing hyperspectral data as the material reflectance characteristics are not know a priori we face the problem of unsupervised linear unmixing the incorporation of different prior information eg positivity and normalization of the abundances naturally leads to a family of interesting algorithms for example in the noise free case yielding an algorithm that can be understood as constrained independent component analysis ica simulations underline the usefulness of our theory i
in the analysis of data recorded by optical imaging from intrinsic signals measurement of changes of light reflectance from cortical tissue the removal of noise and artifacts such as blood vessel patterns is a serious problem often bandpass filtering is used but the underlying assumption that a spatial frequency exists which separates the mapping component from other components especially the global signal is questionable here we propose alternative ways of processing optical imaging data using blind source separation techniques based on the spatial decorrelation of the data we first perform benchmarks on artificial data in order to select the way of processing which is most robust with respect to sensor noise we then apply it to recordings of optical imaging experiments from macaque primary visual cortex we show that our bss technique is able to extract ocular dominance and orientation preference maps from single condition stacks for data where standard post processing procedures fail artifacts especially blood vessel patterns can often be completely removed from the maps in summary our method for blind source separation using extended spatial decorrelation is a superior technique for the analysis of optical recording data
recently a number of authors have proposed treating dialogue systems as markov decision processes mdps however the practical application of mdp algorithms to dialogue systems faces a number of severe technical challenges we have built a general software tool rlds for reinforcement learning for dialogue systems based on the mdp framework and have applied it to dialogue corpora gathered from two dialogue systems built at att labs our experiments demonstrate that rlds holds promise as a tool for browsing and understanding correlations in complex temporally dependent dialogue corpora
we propose a new and efficient technique for incorporating contextual information into object classification most of the current techniques face the problem of exponential computation cost in this paper we propose a new general framework that incorporates partial context at a linear cost this technique is applied to microscopic urinalysis image recognition resulting in a significant improvement of recognition rate over the context free approach this gain would have been impossible using conventional context incorporation techniques background recognition in context there are a number of pattern recognition problem domains where the classification of an object should be based on more than simply the appearance of the object itself in remote sensing image classification where each pixel is part of ground cover a pixel is more likely to be a glacier if it is in a mountainous area than if surrounded by pixels of residential areas in text analysis one can expect to find certain letters occurring regularly in particular arrangement with other lettersqu eeest tion etc the information conveyed by the accompanying entities is referred to as contextual information human experts apply contextual information in their decision making it makes sense to design techniques and algorithms to make computers aggregate and utilize a more complete set of information in their decision making the way human experts do in pattern recognition systems however author for correspondence x b song j sill y abu mostafa and h kasdan the primary and often only source of information used to identify an object is the set of measurements or features associated with the object itself augmenting this information by incorporating context into the classification process can yield significant benefits consider a set of n objects ti i n with each object we associate a class label ci that is a member of a label set ft is characterized by a set of measurements xi e r p tor many techniques incorporate context d each object ti which we call a feature vecby conditioning the posterior probability of objects identities on the joint features of all accompanying objects e pcl c cn ix xn and then maximizing it with respect to c c cn it can be shown that pci c civlxl xiv cr pcllxl pcivlxiv vccn given pclpcn certain reasonable assumptions once the context free posterior probabilities pcilxi are known eg through the use of a standard machine learning model such as a neural network computing pcl civlx xiv for all possible cl cv would entail n dv multiplications and finding the maximum has complexity of d iv which is intractable for large n and d another problem with this formulation is the estimation of the high dimensional joint distribution pcl cn which is ill posed and data hungry one way of dealing with these problems is to limit context to local regions with this approach only the pixels in a close neighborhood or letters immediately adjacent are considered such techniques may be ignoring useful information and will not apply to situations where context doesnt have such locality as in the case of microscopic urinalysis image recognition another way is to simplify the problem using specific domain knowledge but this is only possible in certain domains these difficulties motivate the efficient incorporation of partial context as a general framework formulated in section in section we discuss microscopic urinalysis image recognition and address the importance of using context for this application also in section techniques are proposed to identify relevant context empirical results are shown in section followed by discussions in section formulation for incorporation of partial context to avoid the exponential computational cost of using the identities of all accompanying objects directly as context we use partial context denoted by a it is called partial because it is derived from the class labels as opposed to consisting of an explicit labelling of all objects the physical definition of a depends on the problem at hand in our application a represents the presence or absence of certain classes then the posterior probability of an object ti having class label ci conditioned on its feature vector and the relevant context a is pcilxi a pcixia pxilci apci a pxia pxia we assume that the feature distribution of an object depends only on its own class ie pxilc a pxlci this assumption is roughly true for most real world problems then image recognition in context application to microscopic urinalysis pxilcipci a pcia papxi pcixi a pcilxi j pxi a oc pci a pcixippclia pci where pci a pcia is called the context ratio through which context plays its role pc the context sensitive posterior probability pci ixi a is obtained through the context free posterior probability pci xi modified by the context ratio pci a the partial context maximum likelihood decision rule chooses class label i for element i such that ai argmaxpcilxi a i a systematic approach to identify relevant context a is addressed in section the partial context approach treats each element in a set individually but with additional information from the context bearing factor a once pcilxi are known for all i n and the context a is obtained to maximize pcixi a from d possible values that ci can take on and for all i the total number of multiplications is n and the complexity for finding the maximum is nd both are linear in n the density estimation part is also trivial since it is very easy to estimate pca microscopic urinalysis
we describe a bayesian approach to model selection in unsupervised learning that determines both the feature set and the number of clusters we then evaluate this scheme based on marginal likelihood and one based on cross validated likelihood for the bayesian scheme we derive a closed form solution of the marginal likelihood by assuming appropriate forms of the likelihood function and prior extensive experiments compare these approaches and all results are verified by comparison against ground truth in these experiments the bayesian scheme using our objective function gave better results than cross validation
we formulate the problem of retrieving images from visual databases as a problem of bayesian inference this leads to natural and effective solutions for two of the most challenging issues in the design of a retrieval system providing support for region based queries without requiring prior image segmentation and accounting for user feedback during a retrieval session we present a new learning algorithm that relies on belief propagation to account for both positive and negative examples of the users interests
reinforcement learning in nonstationary environments is generally regarded as an important and yet difficult problem this paper partially addresses the problem by formalizing a subclass of nonstationary environments the environment model called hidden mode markov decision process hm mdp assumes that environmental changes are always confined to a small number of hidden modes a mode basically indexes a markov decision process mdp and evolves with time according to a markov chain while hm mdp is a special case of partially observable markov decision processes pomdp modeling an hm mdp environment via the more general pomdp model unnecessarily increases the problem complexity a variant of the baum welch algorithm is developed for model learning requiring less data and time i
many researchers have explored methods for hierarchical reinforcement learning rl with temporal abstractions in which abstract actions are defined that can perform many primitive actions before terminating however little is known about learning with state abstractions in which aspects of the state space are ignored in previous work we developed the maxq method for hierarchical rl in this paper we define five conditions under which state abstraction can be combined with the maxq value function decomposition we prove that the maxq q learning algorithm converges under these conditions and show experimentally that state abstraction is important for the successful application of maxq q learning
we consider the problem of reliably choosing a near best strategy from a restricted class of strategies ii in a partially observable markov decision process pomdp we assume we are given the ability to simulate the pomdp and study what might be called the sample complexity that is the amount of data one must generate in the pomdp in order to choose a good strategy we prove upper bounds on the sample complexity showing that even for infinitely large and arbitrarily complex pomdps the amount of data needed can be finite and depends only linearly on the complexity of the restricted strategy class ii and exponentially on the horizon time this latter dependence can be eased in a variety of ways including the application of gradient and local search algorithms our measure of complexity generalizes the classical supervised learning notion of vc dimension to the settings of reinforcement learning and planning
we propose and analyze a class of actor critic algorithms for simulation based optimization of a markov decision process over a parameterized family of randomized stationary policies these are two time scale algorithms in which the critic uses td learning with a linear approximation architecture and the actor is updated in an approximate gradient direction based on information provided by the critic we show that the features for the critic should span a subspace prescribed by the choice of parameterization of the actor we conclude by discussing convergence properties and some open problems
we consider the problem of learning a grid based map using a robot with noisy sensors and actuators we compare two approaches online em where the map is treated as a fixed parameter and bayesian inference where the map is a matrix valued random variable we show that even on a very simple example online em can get stuck in local minima which causes the robot to get lost and the resulting map to be useless by contrast the bayesian approach by maintaining multiple hypotheses is much more robust we then introduce a method for approximating the bayesian solution called rao blackwellised particle filtering we show that this approximation when coupled with an active learning strategy is fast but accurate i
we propose a new approach to the problem of searching a space of stochastic controllers for a markov decision process mdp or a partially observable markov decision process pomdp following several other authors our approach is based on searching in parameterized families of policies for example via gradient descent to optimize solution quality however rather than trying to estimate the values and derivatives of a policy directly we do so indirectly using estimates for the probability densities that the policy induces on states at the different points in time this enables our algorithms to exploit the many techniques for efficient and robust approximate density propagation in stochastic systems we show how our techniques can be applied both to deterministic propagation schemes where the mdps dynamics are given explicitly in compact form and to stochastic propagation schemes where we have access only to a generative model or simulator of the mdp we present empirical results for both of these variants on complex problems
model predictive control mpc a control algorithm which uses an optimizer to solve for the optimal control moves over a future time horizon based upon a model of the process has become a standard control technique in the process industries over the past two decades in most industrial applications a linear dynamic model developed using empirical data is used even though the process itself is often nonlinear linear models have been used because of the difficulty in developing a generic nonlinear model from empirical data and the computational expense often involved in using nonlinear models in this paper we present a generic neural network based technique for developing nonlinear dynamic models from empirical data and show that these models can be efficiently used in a model predictive control framework this nonlinear mpc based approach has been successfully implemented in a number of industrial applications in the refining petrochemical paper and food industries performance of the controller on a nonlinear industrial process a polyethylene reactor is presented i
the problem of developing good policies for partially observable markov decision problems pomdps remains one of the most challenging areas of research in stochastic planning one line of research in this area involves the use of reinforcement learning with belief states probability distributions over the underlying model states this is a promising method for small problems but its application is limited by the intractability of computing or representing a full belief state for large problems recent work shows that in many settings we can maintain an approximate belief state which is fairly close to the true belief state in particular great success has been shown with approximate belief states that marginalize out correlations between state variables in this paper we investigate two methods of full belief state reinforcement learning and one novel method for reinforcement learning using factored approximate belief states we compare the performance of these algorithms on several well known problem from the literature our results demonstrate the importance of approximate belief state representations for large problems
the problem that we address in this paper is how a mobile robot can plan in order to arrive at its goal with minimum uncertainty traditional motion planning algorithms often assume that a mobile robot can track its position reliably however in real world situations reliable localization may not always be feasible partially observable markov decision processes pomdps provide one way to maximize the certainty of reaching the goal state but at the cost of computational intractability for large state spaces the method we propose explicitly models the uncertainty of the robots position as a state variable and generates trajectories through the augmented pose uncertainty space by minimizing the positional uncertainty at the goal the robot reduces the likelihood it becomes lost we demonstrate experimentally that coastal navigation reduces the uncertainty at the goal especially with degraded localization
the problem of reinforcement learning in a non markov environment is explored using a dynamic bayesian network where conditional independence assumptions between random variables are compactly represented by network parameters the parameters are learned on line and approximations are used to perform inference and to compute the optimal value function the relative effects of inference and value function approximations on the quality of the final policy are investigated by learning to solve a moderately difficult driving task the two value function approximations linear and quadratic were found to perform similarly but the quadratic model was more sensitive to initialization both performed below the level of human performance on the task the dynamic bayesian network performed comparably to a model using a localist hidden state representation while requiring exponentially fewer parameters
function approximation is essential to reinforcement learning but the standard approach of approximating a value function and determining a policy from it has so far proven theoretically intractable in this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator independent of the value function and is updated according to the gradient of expected reward with respect to the policy parameters williamss reinforce method and actor critic methods are examples of this approach our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action value or advantage function using this result we prove for the first time that a version of policy iteration with arbitrary differentiable function approximation is convergent to a locally optimal policy large applications of reinforcement learning rl require the use of generalizing function approximators such neural networks decision trees or instance based methods the dominant approach for the last decade has been the value function approach in which all function approximation effort goes into estimating a value function with the action selection policy represented implicitly as the greedy policy with respect to the estimated values eg as the policy that selects in each state the action with highest estimated value the value function approach has worked well in many applications but has several limitations first it is oriented toward finding deterministic policies whereas the optimal policy is often stochastic selecting different actions with specific probabilities eg see singh jaakkola and jordan second an arbitrarily small change in the estimated value of an action can cause it to be or not be selected such discontinuous changes have been identified as a key obstacle to establishing convergence assurances for algorithms following the value function approach bertsekas and tsitsiklis for example q learning sarsa and dynamic programming methods have all been shown unable to converge to any policy for simple mdps and simple function approximators gordon baird tsitsiklis and van roy bertsekas and tsitsiklis this can occur even if the best approximation is found at each step before changing the policy and whether the notion of best is in the mean squared error sense or the slightly different senses of residual gradient temporal difference and dynamic programming methods in this paper we explore an alternative approach to function approximation in rl r s sutton d mcallester s singh and y mansour rather than approximating a value function and using that to compute a deterministic policy we approximate a stochastic policy directly using an independent function approximator with its own parameters for example the policy might be represented by a neural network whose input is a representation of the state whose output is action selection probabilities and whose weights are the policy parameters let denote the vector of policy parameters and p the performance of the corresponding policy eg the average reward per step then in the policy gradient approach the policy parameters are updated approximately proportional to the gradient op a a where a is a positive definite step size if the above can be achieved then can usually be assured to converge to a locally optimal policy in the performance measure p unlike the value function approach here small changes in can cause only small changes in the policy and in the state visitation distribution in this paper we prove that an unbiased estimate of the gradient can be obtained from experience using an approximate value function satisfying certain properties williamss reinforce algorithm also finds an unbiased estimate of the gradient but without the assistance of a learned value function reinforce learns much more slowly than rl methods using value functions and has received relatively little attention learning a value function and using it to reducethe variance of the gradient estimate appears to be essential for rapid learning jaakkola singh and jordan proved a result very similar to ours for the special case of function approximation corresponding to tabular pomdps our result strengthens theirs and generalizes it to arbitrary differentiable function approximators konda and tsitsiklis in prep independently developed a very simialr result to ours see also baxter and bartlett in prep and marbach and tsitsiklis our result also suggests a way of proving the convergence of a wide variety of algorithms based on actor critic or policy iteration architectures eg barto sutton and anderson sutton kimura and kobayashi in this paper we take the first step in this direction by proving for the first time that a version of policy iteration with general differentiable function approximation is convergent to a locally optimal policy baird and moore obtained a weaker but superficially similar result for their vaps family of methods like policy gradient methods vaps includes separately parameterized policy and value functions updated by gradient methods however vaps methods do not climb the gradient of performance expected long term reward but of a measure combining performance and valuefunction accuracy as a result vaps does not converge to a locally optimal policy except in the case that no weight is put upon value function accuracy in which case vaps degenerates to reinforce similarly gordons fitted value iteration is also convergent and value based but does not find a locally optimal policy i policy gradient theorem we consider the standard reinforcement learning framework see eg sutton and barto in which a learning agent interacts with a markov decision process mdp the state action and reward at each time t are denoted st at a and rt respectively the environments dynamics are characterized by state transition probabilities pa s pr stl s i st s at a and expected rewards z e rtl i st s at a vs s a a the agents decision making procedure at each time is characterized by a policy rs a pr at alst s vs a ji where t for i i is a parameter vector we assume that r is diffentiable with respect to its parameter ie that o exists we also usually write just rsa for rsao policy gradient methods for rl with function approximation with function approximation two ways of formulating the agents objective are useful one is the average reward formulation in which policies are ranked according to their long term expected reward per step pr pr lim erl r rn r z ds z rsa where d s limt pr st sso r is the stationary distribution of states under r which we assume exists and is independent of so for all policies in the average reward formulation the value of a state action pair given a policy is defined as aoar vseaea t the second formulation we cover is that in which there is a designated start state so and we care only about the long term reward obtained from it we will give our results only once but they will apply to this formulation as well under the definitions pr e t lrt sor and qrsa e k lrtk st s at ar tl xkl where is a discount rate i is allowed only in episodic tasks in this formulation we define drs as a discounted weighting of states encountered starting at so and then following r drs o tpr stsso our first result concerns the gradient of the performance metric with respect to the policy parameter theorem i policy gradient for any mdp in either the average reward or start state formulations op or s a oo as oo proof see the appendix this way of expressing the gradient was first discussed for the average reward formulation by marbach and tsitsiklis based on a related expression in terms of the state value function due to jaakkola singh and jordan and cao and chen we extend their results to the start state formulation and provide simpler and more direct proofs williamss theory of reinforce algorithms can also be viewed as implying in any event the key aspect of both expressions for the gradient is that their are no terms of the form oas the effect of policy changes on the distribution of states does not appear this is convenient for approximating the gradient by sampling for example if s was sampled from the distribution aosa would be an unbiased estimate of obtained by following r then a oe e of course q s a is also not normally known and must be estimated one ap proach is to use the actual returns rt oo oo eki ltk pw or re ekl vk iwtk in the start state formulation as an approximation for each qrst at this leads to williamss episodic reinforce algorithm at oc ore the corrects for the oversampling of actions preferred by r which is known to follow in expected value williams policy gradient with approximation now consider the case in which q is approximated by a learned function approximator if the approximation is sufficiently good we might hope to use it in place of q r s sutton d mclester s singh and y mansour in and still point roughly in the direction of the gradient for example jaakkola singh and jordan proved that for the special case of function approximation arising in a tabular pomdp one could assure positive inner product with the gradient which is sufficient to ensure improvement for moving in that direction here we extend their result to general function approximation and prove equality with the gradient let fw x j r be our approximation to qx with parameter w it is natural a rtr s to learn fw by following rr and updating w by a rule such as awt o t at fwao where orst at is some unbiased fstat ostatfst tj o estimator of qrst at perhaps pu when such a process has converged to a local optimum then drs e sa qrsa fusa ofusa ow o s theorem policy gradient with function approximation if fw satisfies and is compatible with the policy parameterization in the sense that i ofusa osa i ow o rsa then op r s a ds ssa proof combining and gives rsa oo which tells us that the error in fus a is orthogonal to the gradient of the policy parameterization because the expression above is zero we can subtract it from the policy gradient theorem to yield op r s a o y drs y o qrsa e drs y osa sws rsa qsa qsa fsa r s a ds oo sa qv application to deriving algorithms and advantages given a policy parameterization theorem can be used to derive an appropriate form for the value function parameterization for example consider a policy that is a gibbs distribution in a linear combination of features eb eotq vs s a tsitsiklis personal communication points out that f being linear in the features given on the righthand side may be the only way to satisfy this condition policy gradient methods for rl with function approximation where each qbsa is an dimensional feature vector characterizing state action pair s a meeting the compatibility condition requires that ofwsa orsa yrsbqbsb w b so that the natural parameterization of fw is in other words fw must be linear in the same features as the policy except normalized to be mean zero for each state other algorithms can easily be derived for a variety of nonlinear policy parameterizations such as multi layer backpropagation networks the careful reader will have noticed that the form given above for f requires that it have zero mean for each state arsafwsa o s in this sense it is better to think of fw as an approximation of the advantage function arsa qrsa vrs much as in baird rather than of qx our convergence requirement is really that fw get the relative value of the actions correct in each state not the absolute value nor the variation from state to state our results can be viewed as a justification for the special status of advantages as the target for value function approximation in rl in fact our and can all be generalized to include an arbitrary function of state added to the value function or its approximation for example can be generalized to o sdrs a o fwsa vs where v r is an arbitrary function o this follows immediately because a a s the choice of v does not affect any of our theorems but can substantially affect the variance of the gradient estimators the issues here are entirely analogous to those in the use of reinforcement baselines in earlier work eg williams dayan sutton in practice v should presumably be set to the best available approximation of v our results establish that that approximation process can proceed without affecting the expected evolution of f and r convergence of policy iteration with function approximation given theorem we can prove for the first time that a form of policy iteration with function approximation is convergent to a locally optimal policy theorem policy iteration with function approximation let rr and fw be any differentiable function approximators for the policy and value function respectively that satisfy the compatibility condition and for which maxij i ooaoj i b oo let rko be any step size sequence such that limkm ak and k ak oo then for any mdp with bounded rewards the sequence prrkko defined by any o rk r ok and wk w such that yds yrksaqsa fsaof a ok k q otk ydw y orka oo converges such that limkm o proof our theorem assures that the k update is in the direction of the gradient a and on the mdps rewards together assure us that the bounds on oo ooj aoao r s sutton d mcallester s singh and y mansour is also bounded these together with the step size requirements are the necessary conditions to apply proposition from page of bertsekas and tsitsildis which assures convergence to a local optimum qed acknowledgements the authors wish to thank martha steenstrup and doina precup for comments and michael kearns for insights into the notion of optimal policy under function approximation references baird l c advantage updating wright lab technical report wl tr baird l c residual algorithms reinforcement learning with function approximation proc of the twelfth int conf on machine learning pp morgan kaufmann baird l c moore a w gradient descent for general reinforcement learning nips mit press barto a g sutton r s anderson c w neuronlike elements that can solve difficult learning control problems ieee trans on systems man and cybernetics baxter j bartlett p in prep direct gradient based reinforcement learning i gradient estimation algorithms bertsekas d p tsitsiklis j n neuro dynamic programming athena scientific cao x r chen h f perturbation realization potentials and sensitivity analysis of markov processes ieee trans on automatic control j dayan p reinforcement comparison in d s touretzky j l elman t j sejnowski and g e hinton eds connectionist models proceedings of the summer school pp morgan kaufmann gordon g j stable function approximation in dynamic programming proceedings of the twelfth int conf on machine learning pp morgan kaufmann gordon g j chattering in sarsaa cmu learning lab technical report jaakkola t singh s p jordan m i reinforcement learning algorithms for partially observable markov decision problems nips pp morgan kaufman kimura h kobayashi s an analysis of actorcritic algorithms using eligibility traces reinforcement learning with imperfect value functions proc icml pp konda v r tsitsiklis j n in prep actor critic algorithms marbach p tsitsiklis j n simulation based optimization of markov reward processes technical report lids p massachusetts institute of technology singh s p jaakkola t jordan m i learning without state estimation in partially observable markovian decision problems proc icml j pp sutton r s temporal credit assignment in reinforcement learning phd thesis university of massachusetts amherst sutton r s barto a g reinforcement learning an
we present a monte carlo algorithm for learning to act in partially observable markov decision processes pomdps with real valued state and action spaces our approach uses importance sampling for representing beliefs and monte carlo approximation for belief propagation a reinforcement learning algorithm value iteration is employed to learn value functions over belief states finally a samplebased version of nearest neighbor is used to generalize across states initial empirical results suggest that our approach works well in practical applications
we introduce a novel algorithm termed ppa performance prediction algorithm that quantitatively measures the contributions of elements of a neural system to the tasks it performs the algorithm identifies the neurons or areas which participate in a cognitive or behavioral task given data about performance decrease in a small set of lesions it also allows the accurate prediction of performances due to multielement lesions the effectiveness of the new algorithm is demonstrated in two models of recurrent neural networks with complex interactions among the ele ments the algorithm is scalable and applicable to the analysis of large neural networks given the recent advances in reversible inactivation techniques it has the potential to significantly contribute to the under standing of the organization of biological nervous systems and to shed light on the longlasting debate about local versus distributed computa tion in the brain
we present an expressive agent design language for reinforcement learn ing that allows the user to constrain the policies considered by the learn ing processthe language includes standard features such as parameter ized subroutines temporary interrupts aborts and memory variables but also allows for unspecified choices in the agent program for learning that which isnt specified we present provably convergent learning algo rithms we demonstrate by example that agent programs written in the language are concise as well as modular this facilitates state abstraction and the transferability of learned skills
we establish a principled framework for adaptive transform coding transform coders are often constructed by concatenating an ad hoc choice of transform with suboptimal bit allocation and quantizer design instead we start from a probabilistic latent variable model in the form of a mixture of constrained gaussian mixtures from this model we derive a transform coding algorithm which is a constrained version of the generalized lloyd algorithm for vector quantizer design a byproduct of our derivation is the
we model hippocampal place cells and headdirection cells by combin ing allothetic visual and idiothetic proprioceptive stimuli visual in put provided by a video camera on a miniature robot is preprocessed by a set of gabor filters on nodes of a logpolar retinotopic graph unsu pervised hebbian learning is employed to incrementally build a popula tion of localized overlapping place fields place cells serve as basis func tions for reinforcement learning experimental results for goaloriented navigation of a mobile robot are presented
this paper presents a unified probabilistic framework for denoising and dereverberation of speech signals the framework transforms the denois ing and dereverberation problems into bayesoptimal signal estimation the key idea is to use a strong speech model that is pretrained on a large data set of clean speech computational efficiency is achieved by using variational em working in the frequency domain and employing conjugate priors the framework covers both single and multiple micro phones we apply this approach to noisy reverberant speech signals and get results substantially better than standard methods
we present an algorithm which compensates for the mismatches between characteristics of real world problems and assumptions of independent component analysis algorithm to provide additional information to the ica network we incorporate top down selective attention an mlp classi er is added to the separated signal channel and the error of the classi er is backpropagated to the ica network this backpropagation process results in estimation of expected ica output signal for the top down attention then the unmixing matrix is retrained according to a new cost function representing the backpropagated error as well as independence it modi es the density of recovered signals to the density appropriate for classi cation for noisy speech signal recorded in real environments the algorithm improved the recognition performance and showed robustness against parametric changes
we present a computational model of the neural mechanisms in the pari etal and temporal lobes that support spatial navigation recall of scenes and imagery of the products of recall long term representations are stored in the hippocampus and are associated with local spatial and objectrelated features in the parahippocampal region viewercentered representations are dynamically generated from long term memory in the parietal part of the model the model thereby simulates recall and im agery of locations and objects in complex environments after parietal damage the model exhibits hemispatial neglect in mental imagery that rotates with the imagined perspective of the observer as in the famous milan square experiment our model makes novel predictions for the neural representations in the parahippocampal and parietal regions and for behavior in healthy volunteers and neuropsychological patients
we develop an approach to object recognition based on matching shapes and using a resulting measure of similarity in a nearest neighbor classi er the key algorithmic problem here is that of nding pointwise correspondences between an image shape and a stored prototype shape we introduce a new shape descriptor the shape context which makes this possible using a simple and robust algorithm the shape context at a point captures the distribution over relative positions of other shape points and thus summarizes global shape in a rich local descriptor we demonstrate that shape contexts greatly simplify recovery of correspondences between points of two given shapes once shapes are aligned shape contexts are used to de ne a robust score for measuring shape similarity we have used this score in a nearest neighbor classi er for recognition of hand written digits as well as d objects using exactly the same distance function on the benchmark mnist dataset of handwritten digits this yields an error rate of outperforming other published techniques
we consider the existence of efficient algorithms for learning the class of halfspaces in n in the agnostic learning model ie mak ing no prior assumptions on the examplegenerating distribution the resulting combinatorial problem finding the best agreement halfspace over an input sample is np hard to approximate to within some constant factor we suggest a way to circumvent this theoretical bound by introducing a new measure of success for such algorithms an algorithm is margin successful if the agreement ratio of the halfspace it outputs is as good as that of any halfspace once training points that are inside the margins of its separating hyperplane are disregarded we prove crisp computational com plexity results with respect to this success measure on one hand for every positive there exist efficient polytime margin suc cessful learning algorithms on the other hand we prove that unless pnp there is no algorithm that runs in time polynomial in the sample size and in that is margin successful for all
we present a novel method for clustering using the support vector ma chine approach data points are mapped to a high dimensional feature space where support vectors are used to define a sphere enclosing them the boundary of the sphere forms in data space a set of closed contours containing the data data points enclosed by each contour are defined as a cluster as the width parameter of the gaussian kernel is decreased these contours fit the data more tightly and splitting of contours occurs the algorithm works by separating clusters according to valleys in the un derlying probability distribution and thus clusters can take on arbitrary geometrical shapes as in other sv algorithms outliers can be dealt with by introducing a soft margin constant leading to smoother cluster bound aries the structure of the data is explored by varying the two parame ters we investigate the dependence of our method on these parameters and apply it to several data sets
a goal of statistical language modeling is to learn the joint probability function of sequences of words this is intrinsically difficult because of the curse of dimensionality we propose to fight it with its own weapons in the proposed approach one learns simultaneously a distributed rep resentation for each word ie a similarity between words along with the probability function for word sequences expressed with these repre sentations generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar to words forming an already seen sentence we report on experiments using neural networks for the probability function showing on two text corpora that the proposed approach very significantly im proves on a stateoftheart trigram model
a variational derivation of plefkas mean eld theory is presented this theory is then applied to sigmoidal belief networks with the aid of further approximations empirical evaluation on small scale networks show that the proposed approximations are quite competitive
many processes in biology from the regulation of gene expression in bacteria to memory in the brain involve switches constructed from networks of biochemical reactions crucial molecules are present in small numbers raising questions about noise and stability analysis of noise in simple reaction schemes indicates that switches stable for years and switchable in milliseconds can be built from fewer than one hundred molecules prospects for direct tests of this prediction as well as implications are discussed
olshausen field demonstrated that a learning algorithm that attempts to generate a sparse code for natural scenes develops a complete family of localised oriented bandpass receptive fields similar to those of simple cells in v this paper describes an algorithm which finds a sparse code for sequences of images that preserves information about the input this algorithm when trained on natural video sequences develops bases representing the movement in particular directions with particular speeds similar to the receptive fields of the movementsensitive cells observed in cortical visual areas furthermore in contrast to previous approaches to learning direction selectivity the timing of neuronal activity encodes the phase of the movement so the precise timing of spikes is crucially important to the information encoding in troduct i on it was suggested by barlow that the goal of early sensory processing is to reduce redundancy in sensory information and the activity of sensory neurons encodes independent features neural modelling can give some insight into how these neural nets may learn and operate atick redlich showed that training a neural network on patches of natural images aiming to remove pairwise correlation between neuronal responses results in neurons having centresurround receptive fields resembling those of retinal ganglion neurons olshausen field demonstrated that a learning algorithm that attempts to generate a sparse code for natural scenes while preserving information about the visual input develops a complete family of localised oriented bandpass receptive fields similar to those of simplecells in v the activities of the neurons implementing this coding signal the presence of edges which are basic components of natural images olshausen field chose their algorithm to create a sparse representation because it possesses a higher degree of statistical independence among its outputs similar receptive fields were also obtained by training a neural net so as to make the responses of neurons as independent as possible other authors have shown that direction selectivity of the simplecells may also emerge from unsupervised learning however there is no agreed way of how the receptive fields of neurons that encode movements are created this paper describes an algorithm which finds a sparse code for sequences of images that preserves the critical information about the input this algorithm trained on natural video images develops bases representing movements in particular directions at particular speeds similar to the receptive fields of the movementsensitive cells observed in early visual areas the activities of the neurons implementing this encoding signal the presence of edges moving with certain speeds in certain directions with each neuron having its preferred speed and direction furthermore in contrast to all the previous approaches the timing of neural activity encodes the movements phase so the precise timing of spikes is crucially important for information coding the proposed algorithm is an extension of the one proposed by olshausen field hence it is a high level algorithm which cannot be directly implemented in a biologically plausible neural network however a plausible neural network performing a similar task can be developed the proposed algorithm is described in section sections and show the methods and the results of simulations finally section discusses how the algorithm differs from the previous approaches and the implications of the presented results descr i p t i on o f the a l gor i thm since the proposed algorithm is an extension of the one described by olshausen field this section starts with a brief

we present a novel way of obtaining pac style bounds on the generalization error of learning algorithms explicitly using their stability properties a stable learner is one for which the learned solution does not change much with small changes in the training set the bounds we obtain do not depend on any measure of the complexity of the hypothesis space eg vc dimension but rather depend on how the learning algorithm searches this space and can thus be applied even when the vc dimension is in nite we demonstrate that regularization networks possess the required stability property and apply our method to obtain new bounds on their generalization performance
we describe an extension of the markov decision process model in which a continuous time dimension is included in the state space this allows for the representation and exact solution of a wide range of problems in which transitions or rewards vary over time we examine problems based on route planning with public transportation and telescope observation scheduling
novelty detection involves modeling the normal behaviour of a sys tem hence enabling detection of any divergence from normality it has potential applications in many areas such as detection of ma chine damage or highlighting abnormal features in medical data one approach is to build a hypothesis estimating the support of the normal data ie constructing a function which is positive in the region where the data is located and negative elsewhere recently kernel methods have been proposed for estimating the support of a distribution and they have performed well in practice training involves solution of a quadratic programming problem in this pa per we propose a simpler kernel method for estimating the support based on linear programming the method is easy to implement and can learn large datasets rapidly we demonstrate the method on medical and fault detection datasets
this paper presents predictive gain scheduling a technique for simplify ing reinforcement learning problems by decomposition link admission control of selfsimilar call traffic is used to demonstrate the technique the control problem is decomposed into online prediction of nearfu ture call arrival rates and precomputation of policies for poisson call ar rival processes at decision time the predictions are used to select among the policies simulations show that this technique results in sig nificantly faster learning without any performance loss compared to a reinforcement learning controller that does not decompose the problem in multiservice communications networks such as asynchronous transfer mode atm networks resource control is of crucial importance for the network operator as well as for the users the objective is to maintain the service quality while maximizing the operators revenue at the call level service quality grade of service is measured in terms of call blocking probabilities and the key resource to be controlled is bandwidth network routing and call admission control cac are two such resource control problems markov decision processes offer a framework for optimal cac and routing by model ling the dynamics of the network with traffic and computing control policies using dynamic programming resource control is optimized a standard assumption in such models is that calls arrive according to poisson processes this makes the models of the dynamics relatively simple although the poisson assumption is valid for most userinitiated requests in communications networks a number of studies indicate that many types of arriv al processes in widearea networks as well as in local area networks are statistically self similar this makes it difficult to find models of the dynamics and the models become large and complex if the number of system states is large straightforward application of dynam ic programming is unfeasible nevertheless the fractal burst structure of selfsimilar traffic should be possible to exploit in the design of efficient resource control methods we have previously presented a method based on temporaldifference td learning for cac of selfsimilar call traffic which yields higher revenue than a tdbased controller assuming poisson call arrival processes however a drawback of this method is the slow convergence of the control policy this paper presents an alternative solution to the above problem called predictive gain scheduling it decomposes the control problem into two parts timeseries prediction of nearfuture call arrival rates and precomputation of a set of control policies for poisson call arrival processes at decision time a policy is selected based on these predictions thus the selfsimilar arrival process is approximated by a qua sistationary poisson process the rate predictions are made by artificial neural networks nns trained online the policies can be computed using dynamic programming or other reinforcement learning techniques this paper concentrates on the link admission control problem however the controllers we describe can be used as building block in optimal routing as shown in and other recent work on reinforcement learning for cac and routing includes where marbach et al show how to extend the use of td learning to network routing and where tong et al apply reinforcement learning to routing subject to quality of service constraints the limitations of the traditional poisson model for network arrival processes have been demonstrated in a number of studies eg which indicate the existence of heavy tailed interarrival time distributions and longterm correlations in the arrival processes selfsimilar fractallike models have been shown to correspond better with this traffic a selfsimilar arrival process has no natural burst length on the contrary its arrival in tensity varies considerably over many time scales this makes the variance of its sample mean decay slowly with the sample size and its autocorrelation function decay slowly with time compared to poisson traffic the complexity of control and prediction of poisson traffic is reduced by the memoryless property of the poisson process its expected future depends on the arrival intensity but not on the process history on the other hand the longrange dependence of selfsimilar traffic makes it possible to improve predictions of the process future by observing the history a compact statistical measure of the degree of selfsimilarity of a stochastic process is the hurst parameter for selfsimilar traffic this parameter takes values in the interval whereas poisson processes have a hurst parameter of in the link admission control lac problem a link with capacity c unitss is offered calls from k different service classes calls belonging to such a class j j k have the same bandwidth requirements b j unitss the perclass call holding times are assumed to be exponentially distributed with mean j s access to the link is controlled by a policy that maps states x x to actions aa x a the set x contains all feasible link states and the action set is a a a k a j j where a is for rejecting a presumptive classj call and for accepting it the set of link states is given by x n h where n is the set of feasible call number tuples and h is the cartesian product of some representations h j of the history of the perclass call arrival processes needed because of the memory of selfsimilar arrival processes n is given by n n n j j jj n j b j c where n j is the number of typej calls accepted on the link we assume uniform call charging which means that the reward rate t at time t is equal to the carried bandwidth t xt j j n j tb time evolves continuously with discrete call arrival and departure events enumerated by k denote by the immediate reward obtained from entering a state x k at time t k until entering the next state at time t k the expectation of this reward is e r k e x k t k t k x k x k x k where x k is the expected sojourn time in state x k under policy by taking optimal actions the policy controls the probabilities of state transitions so as to increase the probability of reaching states that yield high longterm rewards the objective of link admission control is to find a policy that maximizes the average reward per stage r lim n e n n k r k x x x x note that the average reward does not depend on the initial state x as the contribution from this state to the average reward tends to zero as n assuming for example that the probability of reaching any other state y x from every state x x is positive certain states are of special interest for the optimal policy these are the states that are can didates for intelligent blocking the set of such states x ib x is given by x ib n ib h where n ib is the set of call number tuples for which the available bandwidth is a multiple of the bandwidth of a wideband call in the states of x ib the longterm reward may be in creased by rejecting narrowband calls to reserve bandwidth for future expected wideband calls gain scheduling is a control theory technique where the parameters of a controller are changed as a function of operating conditions the approach taken here is to look up policies in a table from predictions of the nearfuture perclass call arrival rates for poisson call arrival processes the optimal policy for the link admission control prob lem does not depend on the history h of the arrival processes due to the memoryless property only the constant perclass arrival rates j j j matter in our gain scheduled control of selfsimilar call arrival processes nearfuture j are predicted from h j the self similar call arrival processes are approximated by quasistationary poisson processes by selecting precomputed polices for poisson arrival processes based on predicted j s one radialbasis function rbf nn per class is trained to predict its nearfuture arrival rate solving the link admission control problem for poisson traffic for poisson call arrival processes dynamic programming offers wellestablished tech niques for solving the lac problem in this paper policy iteration is used it involves two steps value determination and policy improvement the value determination step makes use of the objective function and the concept of relative values the difference vx vy between two relative values under a policy is the expected difference in accumulated reward over an infinite time interval starting in state x instead of state y in this paper the relative values are computed by solving a system of linear equations a method chosen for its fast convergence the dynamics of the system are characterized by state transition probabilities given by the policy the per class call arrival intensities j and mean holding times j the policy improvement step consists of finding the action that maximizes the relative val ue at each state after improving the policy the value determination and policy improve ment steps are iterated until the policy does not change determining the prediction horizon over what future time horizon should we predict the rates used to select policies in this work the prediction horizon is set to an average of estimated mean first passage times from states back to themselves in the following referred to as the mean return time the arrival process is approximated by a quasistationary poisson process within this time interval the motivation for this choice of prediction horizon is that the effects of a decision action in a state x d influence the future probabilities of reaching other states and receiving the as sociated rewards until the state x d is reached the next time when this happens a new deci sion can be made where the previous decision does no longer influence the future expected reward in accordance with the assumption of quasistationarity the mean return time can be estimated for call tuples n instead of the full state descriptor x in case of poisson call arrival processes the mean first passage times e t in from other states to a state n are the unique solution of the linear system of equations e t mn m a ln n e t ln m n n a m the limiting probability q n of occupying state n is determined for all states that are candi dates for intelligent blocking by solving a linear system of equations qb b is a matrix containing the state transition intensities given by j and j the mean return time for the link t l is defined as the average of the individual mean return times of the states of n ib weighted by their limiting probabilities and normalized t l nn ib q n t nn nn ib q n for ease of implementation this time window is expressed as a number of call arrivals the window length l j for class j is computed by multiplying the mean return time by the arrival rate l j t l and rounding off to an integer although the window size varies with this variation is partly compensated by t l decreasing with increasing prediction of future call arrival rates the prediction of future arrival call rates is naturally based on measures of recent arrival rates in this work the following representation of the history of the arrival process is used for all classes j j exponentially weighted running averages h j h j h jm of the in terarrival times are computed on different time scales these history vectors are computed using forgetting factors m taking values in the interval h ji k i t k t j k i h ji k where t j k is the arrival time of the kth call from class j in studies of timeseries prediction nonlinear feedforward nns outperform linear predic tors on time series with long memory we employ rbf nns with symmetric gaussian basis functions the activations of the rbf units are normalized by division by the sum of activations to produce a smooth output function the locations and widths of the rbf units can be determined by inspection of the data sets to cover the region of history vectors the nn is trained with the average interarrival time as target after every new call arrival the prediction error j k is computed j k l j l j i tk i tk i y j k learning is performed online using the least mean squares rule which means that the up dating must be delayed by l call arrivals the predicted perclass arrival rates j k yk are used to select a control policy on the arrival of a call request given the prediction horizon and the arrival rate predictor m can be tuned by linear search to minimize the prediction error on sample traffic traces the performance of the gain scheduled admission controller was evaluated on a simulated link with capacity c unitss that was offered calls from selfsimilar call arrival pro cesses for comparison the simulations were repeated with three other link admission con trollers two tdbased controllers one tablebased and one nn based and a controller us ing complete sharing ie to accept a call if the free capacity on the link is sufficient the nn based td controller uses rbf nns one per n n receiving h h as input each nn has hidden units factorized to units per call class plus a default activation unit its weights were initialized to favor acceptance of all feasible calls in all states the tablebased td controller assumes poisson call arrival processes from this it follows that the call number tuples n n constitute markovian states consequently the value function table stores only one value per n this controller was used for evaluation of the performance loss from incorrectly modelling selfsimilar call traffic by poisson traffic synthesis of call traffic synthetic traffic traces were generated from a gaussian fractional autoregressive inte grated moving average model farima d this results in a statistically selfsimilar arrival process where the hurst parameter is easily tuned we generated traces containing arrivaldeparture pairs from two call classes characterized by bandwidth requirements b narrowband and b wideband unitss and call holding times with mean s a hurst parameter of was used and the call arrival rates were scaled to make the expected longterm arrival rates and for the two classes fulfill b b c the ratio was varied from to gain scheduling for simplicity a constant prediction horizon was used throughout the simulations this was computed according to section by averaging the resulting prediction windows for and a window size l l was obtained the table of policies to be used for gain scheduling was computed for predicted and ranging from to with step size in total policies the two rateprediction nns both had hidden units the nns weights were initialized to numerical results both the td learning controllers and the gain scheduling controller were allowed to adapt to the first simulated call arrivals of the traffic traces the throughput obtained by all four methods was measured on the subsequent call arrivals call arrivals call arrivals tdrbf gsrbf gsrbf gsrbf tdrbf tdtbl cs throughput unitss d throughput versus arrival rate ratio a initial weight evolution in neural predictor b longterm weight evolution in neural predictor c weight evolution in nn based td controller call arrivals figure weight evolution for nn predictor a b nn based tdcontroller c performance d figure a b shows the evolution of the weights of the call arrival rate predictor for class and figure c displays nine weights of the rbf nn corresponding to the call number tuple n n which is a candidate for intelligent blocking these weights corre spond to eight different class center vectors plus the default activation the majority of the weights of the gain scheduling rbf nn seems to converge in a few thousand call arrivals whereas the td learning controller needs about call arrivals to converge this is not surprising since the rbf nns of the td learning controllers split up the set of training data so that a single nn is updated much less frequently than a rate predicting nn in the gain scheduling controller secondly the td learning nns are trained on moving targets due to the temporaldifference learning rule stochastic action selection and a changing policy a few of the weights of the gain scheduling nn change considerably even after long train ing these weights correspond to rbf units that are activated by rare large inputs figure d evaluates performance in terms of throughput versus arrival rate ratio each data point is the averaged throughput for traffic traces gain scheduling gsrbf achieves the same throughput as td learning with rbf nns tdrbf up to compared to tabular td learning tdtbl and up to better than complete sharing cs the difference in throughput between td learning and complete sharing is greatest for low arrival rate ratios since the throughput increase by reserving bandwidth for high rate wideband calls is considerably higher than the loss of throughput from the blocked low rate narrowband traffic we have presented predictive gain scheduling a technique for decomposing reinforcement learning problems link admission control a subproblem of network routing was used to demonstrate the technique by predicting nearfuture call arrival rates from one part of the full state descriptor precomputed policies for poisson call arrival processes computed from the rest of the state descriptor were selected this increased the online convergence rate approximately times compared to a tdbased admission controller getting the full state descriptor as input the decomposition did not result in any performance loss the computational complexity of the controller using predictive gain scheduling may reach a computational bottleneck if the size of the state space is increased the determina tion of optimal policies for poisson traffic by policy iteration this can be overcome by state aggregation or by parametrization the relative value function combined with temporal difference learning it is also possible to significantly reduce the number of relative value functions in we showed that linear interpolation of relative value functions dis tributed by an errordriven algorithm enables the use of less than relative value functions without performance loss further we have successfully employed gain scheduled link ad mission control as a building block of network routing where the performance improve ment compared to conventional methods is larger than for the link admission control prob lem the use of gain scheduling to reduce the complexity of reinforcement learning problems is not limited to link admission control in general the technique should be applicable to problems where parts of the state descriptor can be used directly or after preprocessing to select among policies for instances of a simplified version of the original problem references z dziong atm network resource management mcgrawhill dp bertsekas dynamic programming and optimal control athena scientific belmont mass v paxson and s floyd widearea traffic the failure of poisson modeling ieeeacm trans actions on networking vol pp we leland ms taqqu w willinger and dv wilson on the selfsimilar nature of ethernet traffic extended version ieeeacm transactions on networking vol no pp feb a feldman ac gilbert w willinger and tg kurtz the changing nature of network traffic scaling phenomena computer communication review vol no pp april rs sutton and ag barto reinforcement learning an
the conventional wisdom is that backprop nets with excess hidden units generalize poorly we show that nets with excess capacity generalize well when trained with backprop and early stopping experiments sug gest two reasons for this overfitting can vary significantly in different regions of the model excess capacity allows better fit to regions of high nonlinearity and backprop often avoids overfitting the regions of low nonlinearity regardless of size nets learn task subcomponents in similar sequence big nets pass through stages similar to those learned by smaller nets early stopping can stop training the large net when it generalizes comparably to a smaller net we also show that conjugate gradient can yield worse generalization because it overfits regions of low nonlinearity when learning to fit regions of high nonlinearity
an online recursive algorithm for training support vector machines one vector at a time is presented adiabatic increments retain the kuhn tucker conditions on all previously seen training data in a number of steps each computed analytically the incremental procedure is re versible and decremental unlearning offers an efficient method to ex actly evaluate leaveoneout generalization performance interpretation of decremental unlearning in feature space sheds light on the relationship between generalization and geometry of the data
the vicinal risk minimization principle establishes a bridge between generative models and methods derived from the structural risk mini mization principle such as support vector machines or statistical reg ularization we explain how vrm provides a framework which inte grates a number of existing algorithms such as parzen windows support vector machines ridge regression constrained logistic classifiers and tangentprop we then show how the approach implies new algorithm s for solving problems usually associated with generative models new algorithms are described for dealing with pattern recognition problems with very different pattern distributions and dealing with unlabeled data preliminary empirical results are presented
the paradigm of hebbian learning has recently received a novel interpretation with the discovery of synaptic plasticity that depends on the relative timing of pre and post synaptic spikes this paper derives a temporally dependent learning rule from the basic principle of mutual information maximization and studies its relation to the experimentally observed plasticity we nd that a supervised spike dependent learning rule sharing similar structure with the experimentally observed plasticity increases mutual information to a stable near optimal level moreover the analysis reveals how the temporal structure of time dependent learning rules is determined by the temporal lter applied by neurons over their inputs these results suggest experimental prediction as to the dependency of the learning rule on neuronal biophysical parameters
high dimensional data modeling is difficult mainly because the socalled curse of dimensionality we propose a technique called gaussianiza tion for high dimensional density estimation which alleviates the curse of dimensionality by exploiting the independence structures in the data gaussianization is motivated from recent developments in the statistics literature projection pursuit independent component analysis and gaus sian mixture models with semitied covariances we propose an iter ative gaussianization procedure which converges weakly at each it eration the data is first transformed to the least dependent coordinates and then each coordinate is marginally gaussianized by univariate tech niques gaussianization offers density estimation sharper than traditional kernel methods and radial basis function methods gaussianization can be viewed as efficient solution of nonlinear independent component anal ysis and high dimensional projection pursuit
preliminary work by the authors made use of the so called manhattan world assumption about the scene statistics of city and indoor scenes this assumption stated that such scenes were built on a cartesian grid which led to regularities in the image edge gradient statistics in this paper we explore the general applicability of this assumption and show that surprisingly it holds in a large variety of less structured environments including rural scenes this enables us from a single image to determine the orientation of the viewer relative to the scene structure and also to detect target objects which are not aligned with the grid these inferences are performed using a bayesian model with probability distributions eg on the image gradient statistics learnt from real data
output coding is a general method for solving multiclass problems by reducing them to multiple binary classification problems previous re search on output coding has employed almost solely predefined discrete codes we describe an algorithm that improves the performance of output codes by relaxing them to continuous codes the relaxation procedure is cast as an optimization problem and is reminiscent of the quadratic program for support vector machines we describe experiments with the proposed algorithm comparing it to standard discrete output codes the experimental results indicate that continuous relaxations of output codes often improve the generalization performance especially for short codes
we develop an approach for a sparse representation for gaussian process gp models in order to overcome the limitations of gps caused by large data sets the method is based on a combination of a bayesian online al gorithm together with a sequential construction of a relevant subsample of the data which fully specifies the prediction of the model experi mental results on toy examples and large realworld datasets indicate the efficiency of the approach
hebbian and competitive hebbian algorithms are almost ubiquitous in modeling pattern formation in cortical development we analyse in the oretical detail a particular model adapted from piepenbrock ober mayer for the development of d stripelike patterns which places competitive and interactive cortical influences and free and restricted ini tial arborisation onto a common footing
explaining away has mostly been considered in terms of inference of states in belief networks we show how it can also arise in a bayesian context in inference about the weights governing relationships such as those between stimuli and reinforcers in conditioning experiments such as backward blocking we show how explaining away in weight space can be accounted for using an extension of a kalman filter model pro vide a new approximate way of looking at the kalman gain matrix as a whitener for the correlation matrix of the observation process suggest a network implementation of this whitener using an architecture due to goodall and show that the resulting model exhibits backward blocking
when trying to recover d structure from a set of images the most difficult problem is establishing the correspondence between the measurements most existing approaches assume that features can be tracked across frames whereas methods that exploit rigidity constraints to facilitate matching do so only under restricted cam era motion in this paper we propose a bayesian approach that avoids the brittleness associated with singling out one best cor respondence and instead consider the distribution over all possible correspondences we treat both a fully bayesian approach that yields a posterior distribution and a map approach that makes use of em to maximize this posterior we show how markov chain monte carlo methods can be used to implement these techniques in practice and present experimental results on real data

nearest neighbor classification assumes locally constant class con ditional probabilities this assumption becomes invalid in high dimensions with finite samples due to the curse of dimensionality severe bias can be introduced under these conditions when using the nearest neighbor rule we propose a locally adaptive nearest neighbor classification method to try to minimize bias we use a chisquared distance analysis to compute a flexible metric for pro ducing neighborhoods that are elongated along less relevant feature dimensions and constricted along most influential ones as a result the class conditional probabilities tend to be smoother in the mod ified neighborhoods whereby better classification performance can be achieved the efficacy of our method is validated and compared against other techniques using a variety of real world data
recent work has exploited boundedness of data in the unsupervised learning of new types of generative model for nonnegative data it was recently shown that the maximumentropy generative model is a non negative boltzmann distribution not a gaussian distribution when the model is constrained to match the first and second order statistics of the data learning for practical sized problems is made difficult by the need to compute expectations under the model distribution the computa tional cost of markov chain monte carlo methods and low fidelity of naive mean field techniques has led to increasing interest in advanced mean field theories and variational methods here i present a second order meanfield approximation for the nonnegative boltzmann machine model obtained using a hightemperature expansion the theory is tested on learning a bimodal dimensional model a highdimensional translationally invariant distribution and a generative model for hand written digits
incorporating prior knowledge of a particular task into the architecture of a learning algorithm can greatly improve generalization performance we study here a case where we know that the function to be learned is nondecreasing in two of its arguments and convex in one of them for this purpose we propose a class of functions similar to multilayer neural networks but that has those properties is a universal approximator of continuous functions with these and other properties we apply this new class of functions to the task of modeling the price of call options experiments show improvements on regressing the price of call options using the new types of function classes that incorporate the a priori con straints
a serious problem in learning probabilistic models is the presence of hid den variables these variables are not observed yet interact with several of the observed variables as such they induce seemingly complex de pendencies among the latter in recent years much attention has been devoted to the development of algorithms for learning parameters and in some cases structure in the presence of hidden variables in this pa per we address the related problem of detecting hidden variables that interact with the observed variables this problem is of interest both for improving our understanding of the domain and as a preliminary step that guides the learning procedure towards promising models a very natural approach is to search for structural signatures of hidden variables substructures in the learned network that tend to suggest the presence of a hidden variable we make this basic idea concrete and show how to integrate it with structuresearch algorithms we evaluate this method on several synthetic and reallife datasets and show that it performs surpris ingly well
many neural systems extend their dynamic range by adaptation we ex amine the timescales of adaptation in the context of dynamically mod ulated rapidlyvarying stimuli and demonstrate in the fly visual system that adaptation to the statistical ensemble of the stimulus dynamically maximizes information transmission about the timedependent stimulus further while the rate response has long transients the adaptation takes place on timescales consistent with optimal variance estimation
people can understand complex auditory and visual information often using one to disambiguate the other automated analysis even at a low level faces severe challenges including the lack of accurate statistical models for the signals and their highdimensionality and varied sam pling rates previous approaches assumed simple parametric models for the joint distribution which while tractable cannot capture the com plex signal relationships we learn the joint distribution of the visual and auditory signals using a nonparametric approach first we project the data into a maximally informative lowdimensional subspace suitable for density estimation we then model the complicated stochastic rela tionships between the signals using a nonparametric density estimator these learned densities allow processing across signal modalities we demonstrate on synthetic and real signals localization in video of the face that is speaking in audio and conversely audio enhancement of a particular speaker selected from the video
one way to approximate inference in richly connected graphical models is to apply the sum product algorithm aka probability propagation algorithm while ignoring the fact that the graph has cycles the sum product algorithm can be directly applied in gaussian networks and in graphs for coding but for many conditional probability functions including the sigmoid function direct application of the sum product algorithm is not possible we introduce accumulator networks that have low local complexity but exponential global complexity so the sum product algorithm can be directly applied in an accumulator network the probability of a child given its parents is computed by accumulating the inputs from the parents in a markov chain or more generally a tree after giving expressions for inference and learning in accumulator networks we give results on the bars problem and on the problem of extracting translated overlapping faces from an image
an important class of problems can be cast as inference in noisyor bayesian networks where the binary state of each variable is a logical or of noisy versions of the states of the variables parents for example in medical diagnosis the presence of a symptom can be expressed as a noisy or of the diseases that may cause the symptom on some occasions a disease may fail to activate the symptom inference in richly connected noisy or networks is intractable but approximate methods eg variational techniques are showing increasing promise as practical solutions one problem with most approximations is that they tend to concentrate on a relatively small number of modes in the true posterior ignoring other plausible con gurations of the hidden variables we introduce a new sequential variational method for bipartite noisyor networks that favors including all modes of the true posterior and models the posterior distribution as a tree we compare this method with other approximations using an ensemble of networks with network statistics that are comparable to the qmr dt medical diagnostic network inclusive variational approximations approximate algorithms for probabilistic inference are gaining in popularity and are now even being incorporated into vlsi hardware t richardson personal communication approximate methods include variational techniques ghahramani and jordan saul et al frey and hinton jordan et al local probability propagation gallager pearl frey mackay a freeman and weiss and markov chain monte carlo neal mackay b many algorithms have been proposed in each of these classes one problem that most of the above algorithms su er from is a tendency to concentrate on a relatively small number of modes of the target distribution the distribution being approximated in the case of medical diagnosis di erent modes correspond to di erent explanations of the symptoms markov chain monte carlo methods are usually guaranteed to eventually sample from all the modes but this may take an extremely long time even when tempered transitions neal are a b px px qx qx x x figure we approximate p x by adjusting the mean and variance of a gaussian qx a the result of minimizing dqjjp p x qxlogqxp x as is done for most variational methods b the result of minimizing dp jjq p x p xlogp xqx used preliminary results on local probability propagation in richly connected networks show that it is sometimes able to oscillate between plausible modes murphy et al frey but other results also show that it sometimes diverges or oscillates between implausible con gurations mceliece et al most variational techniques minimize a cost function that favors nding the single most massive mode excluding less probable modes of the target distribution eg saul et al ghahramani and jordan jaakkola and jordan frey and hinton attias more sophisticated variational techniques capture multiple modes using substructures saul and jordan or by leaving part of the original network intact and approximating the remainder jaakkola and jordan however although these methods increase the number of modes that are captured they still exclude modes variational techniques approximate a target distribution p x using a simpler parameterized distribution qx or a parameterized bound for example p disease disease disease n jsymptoms may be approximated by a factorized distribution q disease q disease qn disease n for the current set of observed symptoms the parameters of the q distributions are adjusted to make q as close as possible to p a common approach to variational inference is to minimize a relative entropy dqjjp x x qx log qx p x notice that dqjjp dp jjq often dqjjp can be minimized with respect to the parameters of q using iterative optimization or even exact optimization to see how minimizing dqjjp may exclude modes of the target distribution suppose q is a gaussian and p is bimodal with a region of vanishing density between the two modes as shown in fig if we minimize dqjjp with respect to the mean and variance of q it will cover only one of the two modes as illustrated in fig a we assume the symmetry is broken this is because dqjjp will tend to in nity if q is nonzero in the region where p has vanishing density in contrast if we minimize dp jjq p x p xlogp xqx with respect to the mean and variance of q it will cover all modes since dp jjq will tend to in nity if q vanishes in any region where p is nonzero see fig b for many problems including medical diagnosis it is easy to argue that it is more important that our approximation include all modes than exclude nonplausible con gurations at the cost of excluding other modes the former leads to a low number of false negatives whereas the latter may lead to a large number of false negatives concluding a disease is not present when it is d d d d n d n s s k figure bipartite bayesian network sk s are observed dn s are hidden bipartite noisy or networks fig shows a bipartite noisy or bayesian network with n binary hidden variables d d dn and k binary observed variables s s s k later we present results on medical diagnosis where dn indicates a disease is active dn indicates a disease is inactive s k indicates a symptom is active and s k indicates a symptom is inactive the joint distribution is p d s h k y k p s k jd ih n y n p d n i in the case of medical diagnosis this form assumes the diseases are independent although some diseases probably do depend on other diseases this form is considered to be a worthwhile representation of the problem shwe et al the likelihood for s k takes the noisy or form pearl the probability that symptom s k fails to be activated s k is the product of the probabilities that each active disease fails to activate s k p s k jd p k n y n p dn kn p kn is the probability that an active dn fails to activate s k p k accounts for a leak probability p k is the probability that symptom s k is active when none of the diseases are active exact inference computes the distribution over d given a subset of observed values in s however if s k is not observed the corresponding likelihood node plus edges may be deleted to give a new network that describes the marginal distribution over d and the remaining variables in s so we assume that we are considering a subnetwork where all the variables in s are observed we reorder the variables in s so that the rst j variables are active s k k j and the remaining variables are inactive s k j k k the posterior distribution can then be written p djs p d s h j y k p k n y n p dn kn ih k y kj p k n y n p dn kn ih n y n p d n i taken together the two terms in brackets on the right take a simple product form over the variables in d so the rst step in inference is to absorb the inactive however the diseases are dependent given that some symptoms are present variables in s by modifying the priors p d n as follows p d n np d n k y kj p kn dn where n is a constant that normalizes p d n assuming the inactive symptoms have been absorbed we have p djs h j y k p k n y n p dn kn ih n y n p d n i the term in brackets on the left does not have a product form the entire expression can be multiplied out to give a sum of j product forms and exact quickscore inference can be performed by combining the results of exact inference in each of the j product forms heckerman however this exponential time complexity makes large problems such as qmr dt intractable sequential inference using inclusive variational trees as described above many variational methods minimize dqjjp and nd approximations that exclude some modes of the posterior distribution we present a method that minimizes dp jjq sequentially by absorbing one observation at a time so as to not exclude modes of the posterior also we approximate the posterior distribution with a tree directed and undirected trees are equivalent we use a directed representation where each variable has at most one parent the algorithm absorbs one active symptom at a time producing a new tree by searching for the tree that is closest in the dp jjq sense to the product of the previous tree and the likelihood for the next symptom this search can be performed eciently in on time using probability propagation in two versions of the previous tree to compute weights for edges of a new tree and then applying a minimum weight spanning tree algorithm let t k d be the tree approximation obtained after absorbing the kth symptom s k initially we take t d to be a tree that decouples the variables and has marginals equal to the marginals obtained by absorbing the inactive symptoms as described above interpreting the tree t k d from the previous step as the current prior over the diseases we use the likelihood p s k jd for the next symptom to obtain a new estimate of the posterior p k djs s k t k dp s k jd t k d p k n y n p dn kn t k d t k d where t k d t k d p k q n n p dn kn is a modi ed tree let the new tree be t k d q n t k d n jd k n where k n is the index of the parent of dn in the new tree the parent function k n and the conditional probability tables of t k d are found by minimizing d p k jjt k x d p k djs s k log p k djs s k t k d ignoring constants we have d p k jjt k x d p k djs s k log t k d x d t k d t k d log y n t k d n jd k n x n x d t k d t k d log t k d n jd k n x n x dn x d k n t k d n d k n t k d n d k n log t k d n jd k n for a given structure parent function k n the optimal conditional probability tables are t k d n jd k n n t k d n d k n t k d n d k n where n is a constant that ensures p dn t k d n jd k n this table is easily computed using probability propagation in the two trees to compute the two marginals needed in the di erence the optimal conditional probability table for a variable is independent of the parentchild relationships in the remainder of the network so for the current symptom we compute the optimal conditional probability tables for all nn possible parent child relationships in on time using probability propagation then we use a minimum weight directed spanning tree algorithm bock to search for the best tree once all of the symptoms have been absorbed we use the nal tree distribution t j d to make inferences about d given s the order in which the symptoms are absorbed will generally a ect the quality of the resulting tree jaakkola and jordan but we used a random ordering in the experiments reported below results on qmr dt type networks using the structural and parameter statistics of the qmr dt network given in shwe et al we simulated qmr dt type networks with roughly diseases each there were networks in each of groups with and instantiated active symptoms we chose the number of active symptoms to be small enough that we can compare our approximate method with the exact quickscore method heckerman we also tried two other approximate inference methods local probability propagation murphy et al and a variational upper bound jaakkola and jordan for medical diagnosis an important question is how many most probable diseases n under the approximate posterior must be examined before the most probable n diseases under the exact posterior are found clearly n n n an exact inference algorithm will give n n whereas an approximate algorithm that mistakenly ranks the most probable disease last will give n n for each group of networks and each inference method we averaged the values of n for each value of n the left column of plots in fig shows the average of n versus n for and active symptoms the sequential tree tting method is closest to optimal n n in all cases the right column of plots shows the extra work caused by the excess number of diseases n n that must be examined for the approximate methods n n positive findings exact ub tree pp n n n positive findings ub tree pp n n positive findings exact ub tree pp n n n positive findings ub tree pp n n positive findings exact ub tree pp n n n positive findings ub tree pp figure comparisons of the number of most probable diseases n under the approximate posterior that must be examined before the most probable n diseases under the exact posterior are found approximate methods include the sequential tree tting method presented in this paper tree local probability propagation pp and a variational upper bound ub summary noisy or networks can be used to model a variety of problems including medical diagnosis exact inference in large richly connected noisy or networks is intractable and most approximate inference algorithms tend to concentrate on a small number of most probable con gurations of the hidden variables under the posterior we presented an inclusive variational method for bipartite noisy or networks that favors including all probable con gurations at the cost of including some improbable con gurations the method ts a tree to the posterior distribution sequentially ie one observation at a time results on an ensemble of qmr dt type networks show that the method performs better than local probability propagation and a variational upper bound for ranking most probable diseases acknowledgements we thank dale schuurmans for discussions about this work references h attias independent factor analysis neural computation f bock an algorithm to construct a minimum directed spanning tree in a directed network developments in operations research gordon and breach new york w t freeman and y weiss on the xed points of the max product algorithm to appear in ieee transactions on information theory special issue on codes on graphs and iterative algorithms b j frey graphical models for machine learning and digital communication mit press cambridge ma b j frey filling in scenes by propagating probabilities through layers and into appearance models proceedings of the ieee conference on computer vision and pattern recognition ieee computer society press los alamitos ca b j frey and g e hinton variational learning in non linear gaussian belief networks neural computation r g gallager low density parity check codes mit press cambridge ma z ghahramani and m i jordan factorial hidden markov models machine learning d heckerman a tractable inference algorithm for diagnosing multiple diseases proceedings of the fifth conference on uncertainty in arti cial intelligence t s jaakkola and m i jordan variational probabilistic inference and the qmr dt network journal of arti cial intelligence research m i jordan z ghahramani t s jaakkola and l k saul an
a new form of covariance modelling for gaussian mixture models and hidden markov models is presented this is an extension to an efficient form of covariance modelling used in speech recognition semitied co variance matrices in the standard form of semitied covariance matrices the covariance matrix is decomposed into a highly shared decorrelating transform and a componentspecific diagonal covariance matrix the use of a factored decorrelating transform is presented in this paper this fac toring effectively increases the number of possible transforms without in creasing the number of free parameters maximum likelihood estimation schemes for all the model parameters are presented including the compo nenttransform assignment transform and component parameters this new model form is evaluated on a large vocabulary speech recognition task it is shown that using this factored form of covariance modelling reduces the word error rate
a new incremental learning algorithm is described which approximates the maximal margin hyperplane wrt norm p for a set of linearly separable data our algorithm called alma p approximate large mar gin algorithm wrt norm p takes o p x corrections to sepa rate the data with pnorm margin larger than where is the pnorm margin of the data and x is a bound on the pnorm of the in stances alma p avoids quadratic or higherorder programming meth ods it is very easy to implement and is as fast as online algorithms such as rosenblatts perceptron we report on some experiments comparing alma p to two incremental algorithms perceptron and li and longs romma our algorithm seems to perform quite better than both the accuracy levels achieved by alma p are slightly inferior to those obtained by support vector machines svms on the other hand alma p is quite faster and easier to implement than standard svms training algorithms
variational approximations are becoming a widespread tool for bayesian learning of graphical models we provide some theoretical results for the variational updates in a very general family of conjugate exponential graphical models we show how the belief propagation and the junction tree algorithms can be used in the inference step of variational bayesian learning applying these results to the bayesian analysis of linear gaussian state space models we obtain a learning procedure that exploits the kalman smoothing propagation while integrating over all model parameters we demonstrate how this can be used to infer the hidden state dimensionality of the state space model in a variety of synthetic problems and one real high dimensional data set
many algorithms for approximate reinforcement learning are not known to converge in fact there are counterexamples showing that the adjustable weights in some algorithms may oscillate within a region rather than converging to a point this paper shows that for two popular algorithms such oscillation is the worst that can happen the weights cannot diverge but instead must converge to a bounded region the algorithms are sarsa and v the latter algorithm was used in the well known td gammon program
we present an algorithm that samples the hypothesis space of kernel classiers given a uniform prior over normalised weight vectors and a likelihood based on a model of label noise leads to a piecewise constant posterior that can be sampled by the kernel gibbs sampler kgs the kgs is a markov chain monte carlo method that chooses a random direction in parameter space and samples from the resulting piecewise constant density along the line chosen the kgs can be used as an analytical tool for the exploration of bayesian transduction bayes point machines active learning and evidence based model selection on small data sets that are contaminated with label noise for a simple toy example we demonstrate experimentally how a bayes point machine based on the kgs outperforms an svm that is incapable of taking into account label noise
we present an improvement of novikos perceptron convergence theorem reinterpreting this mistake bound as a margin dependent sparsity guarantee allows us to give a pacstyle generalisation error bound for the classier learned by the perceptron learning algorithm the bound value crucially depends on the margin a support vector machine would achieve on the same data set using the same kernel ironically the bound yields better guarantees than are currently available for the support vector solution itself
we present efficient algorithms for allpointpairs problems or n bodylike problems which are ubiquitous in statistical learning we focus on six examples including nearestneighbor classification kernel density estimation outlier detection and the twopoint correlation these include any problem which abstractly requires a comparison of each of the n points in a dataset with each other point and would naively be solved using n distance computations in practice n is often large enough to make this infeasible we present a suite of new geometric techniques which are applicable in principle to any nbody computation including largescale mixtures of gaussians rbf neural networks and hmms our algorithms exhibit favorable asymptotic scaling and are empirically several orders of magnitude faster than the naive computation even for small datasets we are aware of no exact algorithms for these problems which are more effi cient either empirically or theoretically in addition our framework yields simple and elegant algorithms it also permits two important generalizations beyond the standard allpointpairs problems which are more difficult these are represented by our final examples the multiple twopoint correlation and the notorious npoint correlation
we examine eight di erent techniques for developing visual representations in machine vision tasks in particular we compare di erent versions of principal component and independent component analysis in combination with stepwise regression methods for variable selection we found that local methods based on the statistics of image patches consistently outperformed global methods based on the statistics of entire images this result is consistent with previous work on emotion and facial expression recognition in addition the use of a stepwise regression technique for selecting variables and regions of interest substantially boosted performance

ascribing computational principles to neural feedback circuits is an important problem in theoretical neuroscience we study symmetric threshold linear networks and derive stability results that go beyond the insights that can be gained from lyapunov theory or energy functions by applying linear analysis to subnetworks composed of coactive neurons we determine the stability of potential steady states we nd that stability depends on two types of eigenmodes one type determines global stability and the other type determines whether or not multistability is possible we can prove the equivalence of our stability criteria with criteria taken from quadratic programming also we show that there are permitted sets of neurons that can be coactive at a steady state and forbidden sets that cannot permitted sets are clustered in the sense that subsets of permitted sets are permitted and supersets of forbidden sets are forbidden by viewing permitted sets as memories stored in the synaptic connections we can provide a formulation of longterm memory that is more general than the traditional perspective of xed point attractor networks a lyapunov function can be used to prove that a given set of di erential equations is convergent for example if a neural network possesses a lyapunov function then for almost any initial condition the outputs of the neurons converge to a stable steady state in the past this stability property was used to construct attractor networks that associatively recall memorized patterns lyapunov theory applies mainly to symmetric networks in which neurons have monotonic activation functions here we show that the restriction of activation functions to threshold linear ones is not a mere limitation but can yield new insights into the computational behavior of recurrent networks for completeness see also we present three main theorems about the neural responses to constant inputs the rst theorem provides necessary and sucient conditions on the synaptic weight matrix for the existence of a globally asymptotically stable set of xed points these conditions can be expressed in terms of copositivity a concept from quadratic programming and linear complementarity theory alternatively they can be expressed in terms of certain eigenvalues and eigenvectors of submatrices of the synaptic weight matrix making a connection to linear systems theory the theorem guarantees that the network will produce a steady state response to any constant input we regard this response as the computational output of the network and its characterization is the topic of the second and third theorems in the second theorem we introduce the idea of permitted and forbidden sets under certain conditions on the synaptic weight matrix we show that there exist sets of neurons that are forbidden by the recurrent synaptic connections from being coactivated at a stable steady state no matter what input is applied other sets are permitted in the sense that they can be coactivated for some input the same conditions on the synaptic weight matrix also lead to conditional multistability meaning that there exists an input for which there is more than one stable steady state in other words forbidden sets and conditional multistability are inseparable concepts the existence of permitted and forbidden sets suggests a new way of thinking about memory in neural networks when an input is applied the network must select a set of active neurons and this selection is constrained to be one of the permitted sets therefore the permitted sets can be regarded as memories stored in the synaptic connections our third theorem states that there are constraints on the groups of permitted and forbidden sets that can be stored by a network no matter which learning algorithm is used to store memories active neurons cannot arbitrarily be divided into permitted and forbidden sets because subsets of permitted sets have to be permitted and supersets of forbidden sets have to be forbidden basic de nitions our theory is applicable to the network dynamics dx i dt x i b i x j w ij x j where u maxfu g is a recti cation nonlinearity and the synaptic weight matrix is symmetric w ij w ji the dynamics can also be written in a more compact matrix vector form as x x b wx the state of the network is x an input to the network is an arbitrary vector b an output of the network is a steady state x in response to b the existence of outputs and their relationship to the input are determined by the synaptic weight matrix w a vector v is said to be nonnegative v if all of its components are nonnegative the nonnegative orthant fv v g is the set of all nonnegative vectors it can be shown that any trajectory starting in the nonnegative orthant remains in the nonnegative orthant therefore for simplicity we will consider initial conditions that are con ned to the nonnegative orthant x global asymptotic stability de nition a steady state x is stable if for all initial conditions suciently close to x the state trajectory remains close to x for all later times a steady state is asymptotically stable if for all initial conditions suciently close to x the state trajectory converges to x a set of steady states is globally asymptotically stable if from almost all initial conditions state trajectories converge to one of the steady states exceptions are of measure zero de nition a principal submatrix a of a square matrix b is a square matrix that is constructed by deleting a certain set of rows and the corresponding columns of b the following theorem establishes necessary and sucient conditions on w for global asymptotic stability theorem if w is symmetric then the following conditions are equivalent all nonnegative eigenvectors of all principal submatrices of i w have positive eigenvalues the matrix i w is copositive that is x t i w x for all nonnegative x except x for all b the network has a nonempty set of steady states that are globally asymptotically stable proof sketch let v be the minimum of v t i w v over nonnegative v on the unit sphere if is false the minimum value is less than or equal to zero it follows from lagrange multiplier methods that the nonzero elements of v comprise a nonnegative eigenvector of the corresponding principal submatrix of w with eigenvalue greater than or equal to unity by the copositivity of i w the function l x t i w x b t x is lower bounded and radially unbounded it is also nonincreasing under the network dynamics in the nonnegative orthant and constant only at steady states by the lyapunov stability theorem the stable steady states are globally asymptotically stable in the language of optimization theory the network dynamics converges to a local minimum of l subject to the nonnegativity constraint x suppose that is false then there exists a nonnegative eigenvector of a principal submatrix of w with eigenvalue greater than or equal to unity this can be used to construct an unbounded trajectory of the dynamics the meaning of these stability conditions is best appreciated by comparing with the analogous conditions for the purely linear network obtained by dropping the recti cation from in a linear network all eigenvalues of w would have to be smaller than unity to ensure asymptotic stability here only nonnegative eigenvectors are able to grow without bound due to the recti cation so that only their eigenvalues must be less than unity all principal submatrices of w must be considered because di erent sets of feedback connections are active depending on the set of neurons that are above threshold in a linear network i w would have to be positive de nite to ensure asymptotic stability but because of the recti cation here this condition is replaced by the weaker condition of copositivity the conditions of theorem for global asymptotic stability depend only on w but not on b on the other hand steady states do depend on b the next lemma says that the mapping from input to output is surjective lemma for any nonnegative vector v there exists an input b such that v is a steady state of equation with input b proof de ne c v wv where diag n and i if v i and i if v i choose b i c i for v i and b i wv i for v i this lemma states that any nonnegative vector can be realized as a xed point sometimes this xed point is stable such as in networks subject to theorem in which only a single neuron is active indeed the principal submatrix of i w corresponding to a single active neuron corresponds to a diagonal elements which according to must be positive hence it is always possible to activate only a single neuron at an asymptotically stable xed point however as will become clear from the following theorem not all nonnegative vectors can be realized as an asymptotically stable xed point forbidden and permitted sets the following characterizations of stable steady states are based on the interlacing theorem this theorem says that if a is a n by n principal submatrix of a n by n symmetric matrix b then the eigenvalues of a fall in between the eigenvalues of b in particular the largest eigenvalue of a is always smaller than the largest eigenvalue of b de nition a set of neurons is permitted if the neurons can be coactivated at an asymptotically stable steady state for some input b on the other hand a set of neurons is forbidden if they cannot be coactivated at an asymptotically stable steady state no matter what the input b alternatively we might have de ned a permitted set as a set for which the corresponding square sub matrix of i w has only positive eigenvalues and similarly a forbidden set could be de ned as a set for which there is at least one non positive eigenvalue it follows from theorem that if the matrix i w is copositive then the eigenvectors corresponding to non positive eigenvalues of forbidden sets have to have both positive and non positive components theorem if the matrix i w is copositive then the following statements are equivalent the matrix i w is not positive de nite there exists a forbidden set the network is conditionally multistable that is there exists an input b such that there is more than one stable steady state proof sketch i w is not positive de nite and so there can be no asymptotically stable steady state in which all neurons are active eg the set of all neurons is forbidden denote the forbidden set with k active neurons by without loss of generality assume that the principal submatrix of i w corresponding to has k positive eigenvalues and only one non positive eigenvalue by virtue of the interlacing theorem and the fact that the diagonal elements of i w must be positive there is always a subset of for which this is true by choosing b i for neurons i belonging to and b j for neurons j not belonging to the quadratic lyapunov function l de ned in theorem forms a saddle in the nonnegative quadrant de ned by the saddle point is the point where l restricted to the hyperplane de ned by the k positive eigenvalues reaches its minimum but because neurons can be initialized to lower values of l on either side of the hyperplane and because l is non increasing along trajectories there is no way trajectories can cross the hyperplane in conclusion we have constructed an input b for which the network is multistable suppose that is false then for all b the lyapunov function l is convex and so has only a single local minimum in the convex domain x this local minimum is also the global minimum the dynamics must converge to this minimum if i w is positive de nite then a symmetric threshold linear network has a unique steady state this has been shown previously the next theorem is an expansion of this result stating an equivalent condition using the concept of permitted sets theorem if w is symmetric then the following conditions are equivalent the matrix i w is positive de nite all sets are permitted for all b there is a unique steady state and it is stable proof if i w is positive de nite then it is copositive hence in theorem is false and so in theorem is false eg all set are permitted suppose is false so the set of all neurons active must be forbidden not all sets are permitted see the following theorem characterizes the forbidden and the permitted sets theorem any subset of a permitted set is permitted any superset of a forbidden set is forbidden proof according to the interlacing theorem if the smallest eigenvalue of a symmetric matrix is positive then so are the smallest eigenvalues of all its principal submatrices and if the smallest eigenvalue of a principal submatrix is negative then so is the smallest eigenvalue of the original matrix an example the ring network a symmetric threshold linear network with local excitation and larger range inhibition has been studied in the past as a model for how simple cells in primary visual cortex obtain their orientation tuning to visual stimulation inspired by these results we have recently built an electronic circuit containing a ring network using analog vlsi technology we have argued that the xed tuning width of the neurons in the network arises because active sets consisting of more than a xed number of contiguous neurons are forbidden here we give a more detailed account of this fact and provide a surprising result about the existence of some spurious permitted sets let the synaptic matrix of a neuron ring network be translationally invariant the connection between neurons i and j is given by w ij ij ij ij ij ij where quanti es global inhibition self excitation rst neighbor lateral excitation and second neighbor lateral excitation in figure we have numerically computed the permitted sets of this network with the parameters taken from eg the permitted sets were determined by diagonalising the square sub matrices of i w and by classifying the eigenvalues corresponding to nonnegative eigenvectors the figure shows the resulting parent permitted sets those that have no permitted supersets consistent with the nding that such ring networks can explain contrast invariant tuning of v cells and multiplicative response modulation of parietal cells we found that there are no permitted sets that consist of more than contiguous active neurons however as can be seen there are many non contiguous permitted sets that could in principle be activated by exciting neurons in white and strongly inhibiting neurons in black because the activation of the spurious permitted sets requires highly speci c input inhibition of high spatial frequency it can be argued that the presence of the spurious permitted sets is not relevant for the normal operation of the ring network where inputs are typically tuned and excitatory such as inputs from lgn to primary visual cortex permitted set number neuron number neuron number figure left output of a ring network of neurons to uniform input random initial condition right the parent permitted sets x axis neuron number y axis set number white means that a neurons belongs to a set and black means that it does not left right and translation symmetric parent permitted sets of the ones shown have been excluded the rst parent permitted set rst row from the bottom corresponds to the output on the left discussion we have shown that pattern memorization in threshold linear networks can be viewed in terms of permitted sets of neurons eg sets of neurons that can be coactive at a steady state according to this de nition the memories are stored by the synaptic weights independently of the inputs hence this concept of memory does not su er from input dependence as would be the case for a de nition of memory based on the xed points of the dynamics pattern retrieval is strongly constrained by the input a typical input will not allow for the retrieval of arbitrary stored permitted sets this comes from the fact that multistability is not just dependent on the existence of forbidden sets but also on the input theorem for example in the ring network positive input will always retrieve permitted sets consisting of a group of contiguous neurons but not any of the spurious permitted sets figure generally multistability in the ring network is only possible when more than a single neuron is excited notice that threshold linear networks can behave as traditional attractor networks when the inputs are represented as initial conditions of the dynamics for example by xing b and initializing a copositive network with some input the permitted sets unequivocally determine the stable xed points thus in this case the notion of permitted sets is no di erent from xed point attractors however the hierarchical grouping of permitted sets theorem becomes irrelevant since there can be only one attractive xed point per hierarchical group de ned by a parent permitted set the fact that no permitted set can have a forbidden subset represents a constraint on the possible computations of symmetric networks however this constraint does not have to be viewed as an undesired limitation on the contrary being aware of this constraint may lead to a deeper understanding of learning algorithms and representations for constraint satisfaction problems we are reminded of the history of perceptrons where the insight that they can only solve linearly separable classi cation problems led to the invention of multilayer perceptrons and backpropagation in a similar way grouping problems that do not obey the natural hierarchy inherent in symmetric networks might necessitate the
a system has been developed to extract diagnostic information from jet engine carcass vibration data support vector machines applied to nov elty detection provide a measure of how unusual the shape of a vibra tion signature is by learning a representation of normality we describe a novel method for support vector machines of including information from a second class for novelty detection and give results from the appli cation to jet engine vibration analysis
the concept of averaging over classiers is fundamental to the bayesian analysis of learning based on this viewpoint it has recently been demonstrated for linear classiers that the centre of mass of version space the set of all classiers consistent with the training set also known as the bayes point exhibits excellent generalisation abilities however the billiard algorithm as presented in is restricted to small sample size because it requires o m of memory and o n m computational steps where m is the number of training patterns and n is the number of random draws from the posterior distribution in this paper we present a method based on the simple perceptron learning algorithm which allows to overcome this algorithmic drawback the method is algorithmically simple and is easily extended to the multi class case we present experimental results on the mnist data set of handwritten digits which show that bayes point machines bpms are competitive with the current world champion the support vector machine in addition the computational complexity of bpms can be tuned by varying the number of samples from the posterior finally rejecting test points on the basis of their approximative posterior probability leads to a rapid decrease in generalisation error eg generalisation error for a given rejection rate of
we present a bound on the generalisation error of linear classiers in terms of a rened margin quantity on the training set the result is obtained in a pacbayesian framework and is based on geometrical arguments in the space of linear classiers the new bound constitutes an exponential improvement of the so far tightest margin bound by shawe taylor et al and scales logarithmically in the inverse margin even in the case of less training examples than input dimensions suciently large margins lead to non trivial bound values and for maximum margins to a vanishing complexity term furthermore the classical margin is too coarse a measure for the essential quantity that controls the generalisation error the volume ratio between the whole hypothesis space and the subset of consistent hypotheses the practical relevance of the result lies in the fact that the well known support vector machine is optimal wrt the new bound only if the feature vectors are all of the same length as a consequence we recommend to use svms on normalised feature vectors only a recommendation that is well supported by our numerical experiments on two benchmark data sets
a key challenge for reinforcement learning is scaling up to large partially observable domains in this paper we show how a hierarchy of behaviors can be used to create and select among variable length short term memories appropriate for a task at higher levels in the hierarchy the agent abstracts over lower level details and looks back over a variable number of high level decisions in time we formalize this idea in a framework called hierarchical sux memory hsm hsm uses a memory based smdp learning method to rapidly propagate delayed reward across long decision sequences we describe a detailed experimental study comparing memory vs hierarchy using the hsm framework on a realistic corridor navigation task
the goal of many unsupervised learning procedures is to bring two probability distributions into alignment generative models such as gaussian mixtures and boltzmann machines can be cast in this light as can recoding models such as ica and projection pursuit we propose a novel sample based error measure for these classes of models which applies even in situations where maximum likelihood ml and probability density estimation based formulations cannot be applied eg models that are nonlinear or have intractable posteriors furthermore our sample based error measure avoids the diculties of approximating a density function we prove that with an unconstrained model our approach converges on the correct solution as the number of samples goes to in nity and the expected solution of our approach in the generative framework is the ml solution finally we evaluate our approach via simulations of linear and nonlinear models on mixture of gaussians and ica problems the experiments show the broad applicability and generality of our approach
we propose a general bayesian framework for performing independent component analysis ica which relies on ensemble learning and lin ear response theory known from statistical physics we apply it to both discrete and continuous sources for the continuous source the underde termined overcomplete case is studied the naive meanfield approach fails in this case whereas linear response theory which gives an improved estimate of covariances is very efficient the examples given are for sources without temporal correlations however this derivation can eas ily be extended to treat temporal correlations finally the framework offers a simple way of generating new ica algorithms without needing to define the prior distribution of the sources explicitly

jensens inequality is a powerful mathematical tool and one of the workhorses in statistical learning its applications therein include the em algorithm bayesian estimation and bayesian inference jensen com putes simple lower bounds on otherwise intractable quantities such as products of sums and latent loglikelihoods this simplification then per mits operations like integration and maximization quite often ie in discriminative learning upper bounds are needed as well we derive and prove an efficient analytic inequality that provides such variational upper bounds this inequality holds for latent variable mixtures of exponential family distributions and thus spans a wide range of contemporary statis tical models we also discuss applications of the upper bounds including maximum conditional likelihood large margin discriminative models and conditional bayesian inference convergence efficiency and prediction results are shown
learning a complex task can be significantly facilitated by defining a hierarchy of subtasks an agent can learn to choose between various temporally abstract actions each solving an assigned subtask to accom plish the overall task in this paper we study hierarchical learning using the framework of options we argue that to take full advantage of hier archical structure one should perform optionspecific state abstraction and that if this is to scale to larger tasks state abstraction should be au tomated we adapt mccallums utree algorithm to automatically build optionspecific representations of the state feature space and we illus trate the resulting algorithm using a simple hierarchical task results suggest that automated optionspecific state abstraction is an attractive approach to making hierarchical learning systems more effective
substantial data support a temporal difference td model of dopamine da neuron activity in which the cells provide a global error signal for reinforcement learning however in certain cir cumstances da activity seems anomalous under the td model responding to nonrewarding stimuli we address these anoma lies by suggesting that da cells multiplex information about re ward bonuses including suttons exploration bonuses and ng et als nondistorting shaping bonuses we interpret this additional role for da in terms of the unconditional attentional and psy chomotor effects of dopamine having the computational role of guiding exploration
in this paper we derive a second order mean field theory for directed graphical probability models by using an information theoretic argu ment it is shown how this can be done in the absense of a partition function this method is a direct generalisation of the wellknown tap approximation for boltzmann machines in a numerical example it is shown that the method greatly improves the first order mean field ap proximation for a restricted class of graphical models socalled single overlap graphs the second order method has comparable complexity to the first order method for sigmoid belief networks the method is shown to be particularly fast and effective

in this paper we develop the method of bounding the generalization error of a classifier in terms of its margin distribution which was introduced in the recent papers of bartlett and schapire freund bartlett and lee the theory of gaussian and empirical processes allow us to prove the margin type inequalities for the most general functional classes the complexity of the class being measured via the so called gaussian complexity func tions as a simple application of our results we obtain the bounds of schapire freund bartlett and lee for the generalization error of boost ing we also substantially improve the results of bartlett on bounding the generalization error of neural networks in terms of l norms of the weights of neurons furthermore under additional assumptions on the complexity of the class of hypotheses we provide some tighter bounds which in the case of boosting improve the results of schapire freund bartlett and lee
vapniks result that the expectation of the generalisation error of the opti mal hyperplane is bounded by the expectation of the ratio of the number of support vectors to the number of training examples is extended to a broad class of kernel machines the class includes support vector ma chines for soft margin classification and regression and regularization networks with a variety of kernels and cost functions we show that key inequalities in vapniks result become equalities once the classification error is replaced by the margin error with the latter defined as an in stance with positive cost in particular we show that expectations of the true margin error and the empirical margin error are equal and that the sparse solutions for kernel machines are possible only if the cost function is partially insensitive
condensation a form of likelihoodweighted particle filtering has been successfully used to infer the shapes of highly constrained active con tours in video sequences however when the contours are highly flexible eg for tracking fingers of a hand a computationally burdensome num ber of particles is needed to successfully approximate the contour distri bution we show how the metropolis algorithm can be used to update a particle set representing a distribution over contours at each frame in a video sequence we compare this method to condensation using a video sequence that requires highly flexible contours and show that the new algorithm performs dramatically better that the condensation algorithm we discuss the incorporation of this method into the active contour framework where a shapesubspace is used constrain shape variation
nonnegative matrix factorization nmf has previously been shown to be a useful decomposition for multivariate data two different multi plicative algorithms for nmf are analyzed they differ only slightly in the multiplicative factor used in the update rules one algorithm can be shown to minimize the conventional least squares error while the other minimizes the generalized kullbackleibler divergence the monotonic convergence of both algorithms can be proven using an auxiliary func tion analogous to that used for proving convergence of the expectation maximization algorithm the algorithms can also be interpreted as diag onally rescaled gradient descent where the rescaling factor is optimally chosen to ensure convergence

we introduce total wire length as salient complexity measure for an anal ysis of the circuit complexity of sensory processing in biological neural systems and neuromorphic engineering this new complexity measure is applied to a set of basic computational problems that apparently need to be solved by circuits for translation and scaleinvariant sensory process ing we exhibit new circuit design strategies for these new benchmark functions that can be implemented within realistic complexity bounds in particular with linear or almost linear total wire length
we present a method to bound the partition function of a boltz mann machine neural network with any odd order polynomial this is a direct extension of the mean field bound which is first order we show that the third order bound is strictly better than mean field additionally we show the rough outline how this bound is applicable to sigmoid belief networks numerical experiments in dicate that an error reduction of a factor two is easily reached in the region where expansion based approximations are useful
stimulus arrays are inevitably presented at different positions on the retina in visual tasks even those that nominally require fixation in par ticular this applies to many perceptual learning tasks we show that per ceptual inference or discrimination in the face of positional variance has a structurally different quality from inference about fixed position stimuli involving a particular quadratic nonlinearity rather than a purely lin ear discrimination we show the advantage taking this nonlinearity into account has for discrimination and suggest it as a role for recurrent con nections in area v by demonstrating the superior discrimination perfor mance of a recurrent network we propose that learning the feedforward and recurrent neural connections for these tasks corresponds to the fast and slow components of learning observed in perceptual learning tasks
we introduce a novel kernel for comparing two text documents the kernel is an inner product in the feature space consisting of all subsequences of length k a subsequence is any ordered se quence of k characters occurring in the text though not necessarily contiguously the subsequences are weighted by an exponentially decaying factor of their full length in the text hence emphasising those occurrences which are close to contiguous a direct compu tation of this feature vector would involve a prohibitive amount of computation even for modest values of k since the dimension of the feature space grows exponentially with k the paper describes how despite this fact the inner product can be efficiently evaluated by a dynamic programming technique a preliminary experimental comparison of the performance of the kernel compared with a stan dard word feature space kernel is made showing encouraging results
the paper presents a novel technique of constrained independent component analysis cica to introduce constraints into the classical ica and solve the constrained optimization problem by using lagrange multiplier methods this paper shows that cica can be used to order the resulted independent components in a speci c manner and normalize the demixing matrix in the signal separation procedure it can systematically eliminate the icas indeterminacy on permutation and dilation the experiments demonstrate the use of cica in ordering of independent components while providing normalized demixing processes keywords independent component analysis constrained independent component analysis constrained optimization lagrange multiplier methods
based on a statistical mechanics approach we develop a method for approximately computing average case learning curves for gaussian process regression models the approximation works well in the large sample size limit and for arbitrary dimensionality of the input space we explain how the approximation can be systematically improved and argue that similar techniques can be applied to general likelihood models
an active set strategy is applied to the dual of a simple reformula tion of the standard quadratic program of a linear support vector machine this application generates a fast new dual algorithm that consists of solving a finite number of linear equations with a typically large dimensionality equal to the number of points to be classified however by making novel use of the shermanmorrison woodbury formula a much smaller matrix of the order of the orig inal input space is inverted at each step thus a problem with a dimensional input space and million points required inverting positive definite symmetric matrices of size with a total run ning time of minutes on a mhz pentium ii the algorithm requires no specialized quadratic or linear programming code but merely a linear equation solver which is publicly available
the problem of constructing weak classi ers for boosting algorithms is studied we present an algorithm that produces a linear classi er that is guaranteed to achieve an error better than random guessing for any distribution on the data while this weak learner is not useful for learning in general we show that under reasonable conditions on the distribution it yields an e ective weak learner for one dimensional problems preliminary simulations suggest that similar behavior can be expected in higher dimensions a result which is corroborated by some recent theoretical bounds additionally we provide improved convergence rate bounds for the generalization error in situations where the empirical error can be made small which is exactly the situation that occurs if weak learners with guaranteed performance that is better than random guessing can be established

we present a new view of image segmentation by pairwise similarities we interpret the similarities as edge ows in a markov random walk and study the eigenvalues and eigenvectors of the walks transition matrix this interpretation shows that spectral methods for clustering and segmentation have a probabilistic foundation in particular we prove that the normalized cut method arises naturally from our framework finally the framework provides a principled method for learning the similarity function as a combination of features
in this paper we propose a new particle lter based on sequential importance sampling the algorithm uses a bank of unscented lters to obtain the importance proposal distribution this proposal has two very nice properties firstly it makes ecient use of the latest available information and secondly it can have heavy tails as a result we nd that the algorithm outperforms standard particle ltering and other nonlinear ltering methods very substantially this experimental nding is in agreement with the theoretical convergence proof for the algorithm the algorithm also includes resampling and possibly markov chain monte carlo mcmc steps
we investigate a new kernel based classifier the kernel fisher discrim inant kfd a mathematical programming formulation based on the ob servation that kfd maximizes the average margin permits an interesting modification of the original kfd algorithm yielding the sparse kfd we find that both kfd and the proposed sparse kfd can be understood in an unifying probabilistic context furthermore we show connections to support vector machines and relevance vector machines from this understanding we are able to outline an interesting kernel regression technique based upon the kfd algorithm simulations support the use fulness of our approach
a central issue in principal component analysis pca is choosing the number of principal components to be retained by interpreting pca as density estimation we show how to use bayesian model selection to es timate the true dimensionality of the data the resulting estimate is sim ple to compute yet guaranteed to pick the correct dimensionality given enough data the estimate involves an integral over the steifel manifold of kframes which is difficult to compute exactly but after choosing an appropriate parameterization and applying laplaces method an accu rate and practical estimator is obtained in simulations it is convincingly better than crossvalidation and other proposed algorithms plus it runs much faster
this paper describes a method of dogleg trustregion steps or re stricted levenbergmarquardt steps based on a projection pro cess onto the krylov subspaces for neural networks nonlinear least squares problems in particular the linear conjugate gradient cg method works as the inner iterative algorithm for solving the lin earized gaussnewton normal equation whereas the outer nonlin ear algorithm repeatedly takes socalled krylovdogleg steps re lying only on matrixvector multiplication without explicitly form ing the jacobian matrix or the gaussnewton model hessian that is our iterative dogleg algorithm can reduce both operational counts and memory space by a factor of on the number of pa rameters in comparison with a direct linearequation solver this memoryless property is useful for largescale problems
nonlinear support vector machines svms are investigated for visual sex classification with low resolution thumbnail faces by pixels processed from images from the feret face database the performance of svms is shown to be superior to traditional pattern classifiers linear quadratic fisher linear dis criminant nearestneighbor as well as more modern techniques such as radial basis function rbf classifiers and large ensemble rbf networks furthermore the svm performance error is currently the best result reported in the open literature
this paper proposes a new reinforcement learning rl paradigm that explicitly takes into account input disturbance as well as mod eling errors the use of environmental models in rl is quite pop ular for both offline learning by simulations and for online ac tion planning however the difference between the model and the real environment can lead to unpredictable often unwanted results based on the theory of h control we consider a differential game in which a disturbing agent disturber tries to make the worst possible disturbance while a control agent actor tries to make the best control input the problem is formulated as finding a min max solution of a value function that takes into account the norm of the output deviation and the norm of the disturbance we derive online learning algorithms for estimating the value function and for calculating the worst disturbance and the best control in refer ence to the value function we tested the paradigm which we call robust reinforcement learning rrl in the task of inverted pendulum in the linear domain the policy and the value func tion learned by the online algorithms coincided with those derived analytically by the linear h theory for a fully nonlinear swing up task the control by rrl achieved robust performance against changes in the pendulum weight and friction while a standard rl control could not deal with such environmental changes
this paper explores a framework for recognition of image sequences using partially observable stochastic di erential equation sde models monte carlo importance sampling techniques are used for ecient estimation of sequence likelihoods and sequence likelihood gradients once the network dynamics are learned we apply the sde models to sequence recognition tasks in a manner similar to the way hidden markov models hmms are commonly applied the potential advantage of sdes over hmms is the use of continuous state dynamics we present encouraging results for a video sequence recognition task in which sde models provided excellent performance when compared to hidden markov models
we propose a novel probabilistic framework for semantic video in dexing we define probabilistic multimedia objects multijects to map lowlevel media features to highlevel semantic labels a graphical network of such multijects multinet captures scene con text by discovering intraframe as well as interframe dependency relations between the concepts the main contribution is a novel application of a factor graph framework to model this network we model relations between semantic concepts in terms of their cooccurrence as well as the temporal dependencies between these concepts within video shots using the sumproduct algorithm for approximate or exact inference in these factor graph multinets we attempt to correct errors made during isolated concept detec tion by forcing highlevel constraints this results in a significant improvement in the overall detection performance
experimental data have shown that synapses are heterogeneous different synapses respond with different sequences of amplitudes of postsynaptic responses to the same spike train neither the role of synaptic dynamics itself nor the role of the heterogeneity of synaptic dynamics for com putations in neural circuits is well understood we present in this article methods that make it feasible to compute for a given synapse with known synaptic parameters the spike train that is optimally fitted to the synapse for example in the sense that it produces the largest sum of postsynap tic responses to our surprise we find that most of these optimally fitted spike trains match common firing patterns of specific types of neurons that are discussed in the literature
experimental data show that biological synapses behave quite differently from the symbolic synapses in common artificial neural network models biological synapses are dynamic ie their weight changes on a short time scale by several hundred percent in dependence of the past input to the synapse in this article we explore the consequences that these synaptic dynamics entail for the computational power of feedforward neural networks we show that gradient descent suffices to approximate a given quadratic filter by a rather small neural system with dynamic synapses we also compare our network model to artificial neural net works designed for time series processing our numerical results are complemented by theoretical analysis which show that even with just a single hidden layer such networks can approximate a surprisingly large large class of nonlinear filters all filters that can be characterized by volterra series this result is robust with regard to various changes in the model for synaptic dynamics


in this work we introduce an interactive parts ip model as an alternative to hidden markov models hmms we tested both models on a database of online cursive script we show that im plementations of hmms and the ip model in which all letters are assumed to have the same average width give comparable results however in contrast to hmms the ip model can handle duration modeling without an increase in computational complexity
we show how a wavelet basis may be adapted to best represent natural images in terms of sparse coefficients the wavelet basis which may be either complete or overcomplete is specified by a small number of spatial functions which are repeated across space and combined in a recursive fashion so as to be selfsimilar across scale these functions are adapted to minimize the estimated code length under a model that assumes images are composed of a linear superposition of sparse independent components when adapted to natural images the wavelet bases take on different orientations and they evenly tile the orientation domain in stark contrast to the standard nonoriented wavelet bases used in image compression when the basis set is allowed to be overcomplete it also yields higher coding efficiency than standard wavelet bases
many approaches to reinforcement learning combine neural net works or other parametric function approximators with a form of temporaldifference learning to estimate the value function of a markov decision process a significant disadvantage of those pro cedures is that the resulting learning algorithms are frequently un stable in this work we present a new kernelbased approach to reinforcement learning which overcomes this difficulty and provably converges to a unique solution by contrast to existing algorithms our method can also be shown to be consistent in the sense that its costs converge to the optimal costs asymptotically our focus is on learning in an averagecost framework and on a practical ap plication to the optimal portfolio choice problem
we present methods for learning and tracking human motion in video we estimate a statistical model of typical activities from a large set of d periodic human motion data by segmenting these data automatically into cycles then the mean and the princi pal components of the cycles are computed using a new algorithm that accounts for missing information and enforces smooth tran sitions between cycles the learned temporal model provides a prior probability distribution over human motions that can be used in a bayesian framework for tracking human subjects in complex monocular video sequences and recovering their d motion
we present evidence that several higher order statistical properties of natural images and signals can be explained by a stochastic model which simply varies scale of an otherwise stationary gaussian process we discuss two interesting consequences the rst is that a variety of natural signals can be related through a common model of spherically invariant random processes which have the attractive property that the joint densities can be constructed from the one dimensional marginal the second is that in some cases the non stationarity assumption and only second order methods can be explicitly exploited to nd a linear basis that is equivalent to independent components obtained with higher order methods this is demonstrated on spectro temporal components of speech
lowdimensional representations are key to solving problems in high level vision such as face compression and recognition factorial coding strategies for reducing the redundancy present in natural images on the basis of their secondorder statistics have been successful in account ing for both psychophysical and neurophysiological properties of early vision classspecific representations are presumably formed later at the higherlevel stages of cortical processing here we show that when retinotopic factorial codes are derived for ensembles of natural objects such as human faces not only redundancy but also dimensionality is re duced we also show that objects are built from parts in a nongaussian fashion which allows these localfeature codes to have dimensionalities that are substantially lower than the respective nyquist sampling rates
in this communication we present a new algorithm for solving support vector classiers svc with large training data sets the new algorithm is based on an iterative re weighted least squares procedure which is used to optimize the svc moreover a novel sample selection strategy for the working set is presented which randomly chooses the working set among the training samples that do not fulll the stopping criteria the validity of both proposals the optimization procedure and sample selection strategy is shown by means of computer experiments using well known data sets
we study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints in particular we develop two general approaches for an im portant subproblem identifying phrase structure the first is a marko vian approach that extends standard hmms to allow the use of a rich ob servation structure and of general classifiers to model stateobservation dependencies the second is an extension of constraint satisfaction for malisms we develop efficient combination algorithms under both mod els and study them experimentally in the context of shallow parsing
the bayesian paradigm apparently only sometimes gives rise to occams razor at other times very large models perform well we give simple examples of both kinds of behaviour the two views are reconciled when measuring complexity of functions rather than of the machinery used to implement them we analyze the complexity of functions for some linear in the parameter models that are equivalent to gaussian processes and always find occams razor at work

the problem of reinforcement learning in large factored markov decision processes is explored the qvalue of a stateaction pair is approximated by the free energy of a product of experts network network parameters are learned online using a modified sarsa algorithm which minimizes the inconsistency of the qvalues of consecutive stateaction pairs ac tions are chosen based on the current value estimates by fixing the current state and sampling actions from the network using gibbs sampling the algorithm is tested on a cooperative multiagent task the product of experts model is found to perform comparably to tablebased qlearning for small instances of the task and continues to perform well when the problem becomes too large for a tablebased representation
we consider the problem of designing a linear transformation ir pn of rank p n which projects the features of a classifier x ir n onto y x ir p such as to achieve minimum bayes error or probabil ity of misclassification two avenues will be explored the first is to maximize the average divergence between the class densities and the second is to minimize the union bhattacharyya bound in the range of while both approaches yield similar performance in practice they out perform standard lda features and show a relative improvement in the word error rate over stateoftheart cepstral features on a large vocabulary telephony speech recognition task
we apply to oscillatory networks a class of learning rules in which synaptic weights change proportional to preand post synaptic activity with a kernel a measuring the e ect for a postsynaptic spike a time after the presynaptic one the resulting synaptic matrices have an outer product form in which the oscillating patterns are represented as complex vectors in a simple model the even part of a enhances the resonant response to learned stimulus by reducing the e ective damping while the odd part determines the frequency of oscillation we relate our model to the olfactory cortex and hippocampus and their presumed roles in forming associative memories and input representations
the problem of neural coding is to understand how sequences of action potentials spikes are related to sensory stimuli motor outputs or ultimately thoughts and intentions one clear question is whether the same coding rules are used by di erent neurons or by corresponding neurons in di erent individuals we present a quantitative formulation of this problem using ideas from information theory and apply this approach to the analysis of experiments in the y visual system we nd signi cant individual di erences in the structure of the code particularly in the way that temporal patterns of spikes are used to convey information beyond that available from variations in spike rate on the other hand all the ies in our ensemble exhibit a high coding eciency so that every spike carries the same amount of information in all the individuals thus the neural code has a quanti able mixture of individuality and universality
a method is described which like the kernel trick in support vector ma chines svms lets us generalize distancebased algorithms to operate in feature spaces usually nonlinearly related to the input space this is done by identifying a class of kernels which can be represented as normbased distances in hilbert spaces it turns out that common kernel algorithms such as svms and kernel pca are actually really distance based algorithms and can be run with that class of kernels too as well as providing a useful new insight into how these algorithms work the present work can form the basis for conceiving new algorithms

for many problems which would be natural for reinforcement learning the reward signal is not a single scalar value but has multiple scalar com ponents examples of such problems include agents with multiple goals and agents with multiple users creating a single reward value by com bining the multiple components can throw away vital information and can lead to incorrect solutions we describe the multiple reward source problem and discuss the problems with applying traditional reinforce ment learning we then present an new algorithm for finding a solution and results on simulated environments
the principle of maximizing mutual information is applied to learning overcomplete and recurrent representations the underlying model con sists of a network of input units driving a larger number of output units with recurrent interactions in the limit of zero noise the network is de terministic and the mutual information can be related to the entropy of the output units maximizing this entropy with respect to both the feed forward connections as well as the recurrent interactions results in simple learning rules for both sets of parameters the conventional independent components ica learning algorithm can be recovered as a special case where there is an equal number of output units and no recurrent con nections the application of these new learning rules is illustrated on a simple twodimensional input example

we present a simple sparse greedy technique to approximate the maximum a posteriori estimate of gaussian processes with much improved scaling behaviour in the sample size m in particular computational requirements are on m storage is onm the cost for prediction is on and the cost to compute con dence bounds is onm where n m we show how to compute a stopping criterion give bounds on the approximation error and show applications to large scale problems
in this paper we give necessary and sucient conditions under which kernels of dot product type kx y kx y satisfy mercers condition and thus may be used in support vector machines svm regularization networks rn or gaussian processes gp in particular we show that if the kernel is analytic ie can be expanded in a taylor series all expansion coecients have to be nonnegative we give an explicit functional form for the feature map by calculating its eigenfunctions and eigenvalues
we propose a method of approximate dynamic programming for markov decision processes mdps using algebraic decision diagrams adds we produce nearoptimal value functions and policies with much lower time and space requirements than exact dynamic programming our method reduces the sizes of the intermediate value functions generated during value iteration by replacing the values at the terminals of the add with ranges of values our method is demonstrated on a class of large mdps with up to billion states and we compare the results with the optimal value functions
to control the walking gaits of a fourlegged robot we present a novel neuromorphic vlsi chip that coordinates the relative phasing of the robots legs similar to how spinal central pattern generators are believed to control vertebrate locomotion the chip controls the leg move ments by driving motors with time varying voltages which are the out puts of a small network of coupled oscillators the characteristics of the chips output voltages depend on a set of input parameters the rela tionship between input parameters and output voltages can be computed analytically for an idealized system in practice however this ideal re lationship is only approximately true due to transistor mismatch and off sets fine tuning of the chips input parameters is done automatically by the robotic system using an unsupervised support vector sv learning algorithm introduced recently the learning requires only that the description of the desired output is given the machine learns from un labeled examples how to set the parameters to the chip in order to obtain a desired motor behavior
modern classification applications necessitate supplementing the few available labeled examples with unlabeled examples to improve classi fication performance we present a new tractable algorithm for exploit ing unlabeled examples in discriminative classification this is achieved essentially by expanding the input vectors into longer feature vectors via both labeled and unlabeled examples the resulting classification method can be interpreted as a discriminative kernel density estimate and is read ily trained via the em algorithm which in this case is both discriminative and achieves the optimal solution we provide in addition a purely dis criminative formulation of the estimation problem by appealing to the maximum entropy framework we demonstrate that the proposed ap proach requires very few labeled examples for high classification accu racy
we analyze the bit error probability of multiuser demodulators for direct sequence binary phaseshiftkeying dsbpsk cdma channel with ad ditive gaussian noise the problem of multiuser demodulation is cast into the finitetemperature decoding problem and replica analysis is ap plied to evaluate the performance of the resulting mpm marginal pos terior mode demodulators which include the optimal demodulator and the map demodulator as special cases an approximate implementa tion of demodulators is proposed using analogvalued hopfield model as a naive meanfield approximation to the mpm demodulators and its performance is also evaluated by the replica analysis results of the per formance evaluation shows effectiveness of the optimal demodulator and the meanfield demodulator compared with the conventional one espe cially in the cases of small information bit rate and low noise level
a novel noise suppression scheme for speech signals is proposed which is based on a neurophysiologically motivated estimation of the local signal to noise ratio snr in di erent frequency channels for snr estimation the input signal is transformed into so called amplitude modulation spectrograms ams which represent both spectral and temporal characteristics of the respective analysis frame and which imitate the representation of modulation frequencies in higher stages of the mammalian auditory system a neural network is used to analyse ams patterns generated from noisy speech and estimates the local snr noise suppression is achieved by attenuating frequency channels according to their snr the noise suppression algorithm is evaluated in speakerindependent digit recognition experiments and compared to noise suppression by spectral subtraction
we describe a neurallyinspired unsupervised learning algorithm that builds a nonlinear generative model for pairs of face images from the same individual individuals are then recognized by finding the highest relative probability pair among all pairs that consist of a test image and an image whose identity is known our method compares favorably with other methods in the literature the generative model consists of a single layer of ratecoded nonlinear feature detectors and it has the property that given a data vector the true posterior probability distribution over the feature detector activities can be inferred rapidly without iteration or approximation the weights of the feature detectors are learned by com paring the correlations of pixel intensities and feature activations in two phases when the network is observing real data and when it is observing reconstructions of real data generated from the feature activations
we use graphical models to explore the question of how people learn sim ple causal relationships from data the two leading psychological theo ries can both be seen as estimating the parameters of a fixed graph we argue that a complete account of causal induction should also consider how people learn the underlying causal graph structure and we propose to model this inductive process as a bayesian inference our argument is supported through the discussion of three data sets
kernel principal component analysis pca is an elegant nonlinear generalisation of the popular linear data analysis method where a kernel function implicitly de nes a nonlinear transformation into a feature space wherein standard pca is performed unfortunately the technique is not sparse since the components thus obtained are expressed in terms of kernels associated with every training vector this paper shows that by approximating the covariance matrix in feature space by a reduced number of example vectors using a maximum likelihood approach we may obtain a highly sparse form of kernel pca without loss of e ectiveness
we introduce a new nonparametric and principled distance based clustering method this method combines a pairwise based ap proach with a vectorquantization method which provide a mean ingful interpretation to the resulting clusters the idea is based on turning the distance matrix into a markov process and then examine the decay of mutualinformation during the relaxation of this process the clusters emerge as quasistable structures dur ing this relaxation and then are extracted using the information bottleneck method these clusters capture the information about the initial point of the relaxation in the most effective way the method can cluster data with no geometric or other bias and makes no assumption about the underlying distribution
bayesian networks are graphical representations of probability distributions in virtually all of the work on learning these networks the assumption is that we are presented with a data set consisting of randomly generated instances from the underlying distribution in many situations however we also have the option of active learning where we have the possibility of guiding the sampling process by querying for certain types of samples this paper addresses the problem of estimating the parameters of bayesian networks in an active learning setting we provide a theoretical framework for this problem and an algorithm that chooses which active learning queries to generate based on the model learned so far we present experimental results showing that our active learning algorithm can significantly reduce the need for training data in many situations
we introduce the mixture of gaussian processes mgp model which is useful for applications in which the optimal bandwidth of a map is input dependent the mgp is derived from the mixture of experts model and can also be used for modeling general conditional probability densities we discuss how gaussian processes in particular in form of gaussian process classification the support vector machine and the mgp model can be used for quantifying the dependencies in graphical models
prior knowledge about video structure can be used both as a means to improve the performance of content analysis and to extract features that allow semantic classification we introduce statistical models for two important components of this structure shot duration and activity and demonstrate the usefulness of these models by introducing a bayesian formulation for the shot segmentation problem the new formulations is shown to extend standard thresholding methods in an adaptive and intuitive way leading to improved segmentation accuracy
we analyze gallager codes by employing a simple meanfield approxi mation that distorts the model geometry and preserves important interac tions between sites the method naturally recovers the probability prop agation decoding algorithm as an extremization of a proper freeenergy we find a thermodynamic phase transition that coincides with informa tion theoretical upperbounds and explain the practical code performance in terms of the freeenergy landscape
it has been shown that the receptive fields of simple cells in v can be ex plained by assuming optimal encoding provided that an extra constraint of sparseness is added this finding suggests that there is a reason in dependent of optimal representation for sparseness however this work used an ad hoc model for the noise here i show that if a biologically more plausible noise model describing neurons as poisson processes is used sparseness does not have to be added as a constraint thus i con clude that sparseness is not a feature that evolution has striven for but is simply the result of the evolutionary pressure towards an optimal repre sentation
we present the embedded trees algorithm an iterative technique for estimation of gaussian processes defined on arbitrary graphs by exactly solving a series of modified problems on embedded span ning trees it computes the conditional means with an efficiency comparable to or better than other techniques unlike other meth ods the embedded trees algorithm also computes exact error co variances the error covariance computation is most efficient for graphs in which removing a small number of edges reveals an em bedded tree in this context we demonstrate that sparse loopy graphs can provide a significant increase in modeling power rela tive to trees with only a minor increase in estimation complexity
algebraic geometry is essential to learning theory in hierarchical learning machines such as layered neural networks and gaussian mixtures the asymptotic normality does not hold since fisher in formation matrices are singular in this paper the rigorous asymp totic form of the stochastic complexity is clarified based on resolu tion of singularities and two dierent problems are studied if the prior is positive then the stochastic complexity is far smaller than bic resulting in the smaller generalization error than regular statistical models even when the true distribution is not contained in the parametric model if jereys prior which is coordi nate free and equal to zero at singularities is employed then the stochastic complexity has the same form as bic it is useful for model selection but not for generalization
we introduce a method of feature selection for support vector machines the method is based upon finding those features which minimize bounds on the leaveoneout error this search can be efficiently performed via gradient descent the resulting algorithms are shown to be superior to some standard feature selection algorithms on both toy data and reallife problems of face recognition pedestrian detection and analyzing dna microarray data
in this paper we show that the kernel pca algorithm of scholkopf et al can be interpreted as a form of metric multidimensional scaling mds when the kernel function kx y is isotropic ie it depends only on jjx yjj this leads to a metric mds algorithm where the desired con guration of points is found via the solution of an eigenproblem rather than through the iterative optimization of the stress objective function the question of kernel choice is also discussed

using statistical mechanics results i calculate learning curves average generalization error for gaussian processes gps and bayesian neural networks nns used for regression applying the results to learning a teacher defined by a twolayer network i can directly compare gp and bayesian nn learning i find that a gp in general requires od s training examples to learn input features of order s d is the input dimension whereas a nn can learn the task with order the number of adjustable weights training examples since a gp can be considered as an infinite nn the results show that even in the bayesian approach it is important to limit the complexity of the learning machine the theoretical findings are confirmed in simulations with analytical gp learning and a nn mean field algorithm
we introduce stagewise processing in error correcting codes and image restoration by extracting information from the former stage and using it selectively to improve the performance of the latter one both mean eld analysis using the cavity method and simulations show that it has the advantage of being robust against uncertainties in hyperparameter estimation

belief propagation bp was only supposed to work for treelike networks but works surprisingly well in many applications involving networks with loops including turbo codes however there has been little understanding of the algorithm or the nature of the solutions it finds for general graphs we show that bp can only converge to a stationary point of an approximate free energy known as the bethe free energy in statis tical physics this result characterizes bp fixedpoints and makes connections with variational approaches to approximate inference more importantly our analysis lets us build on the progress made in statistical physics since bethes approximation was introduced in kikuchi and others have shown how to construct more ac curate free energy approximations of which bethes approximation is the simplest exploiting the insights from our analysis we de rive generalized belief propagation gbp versions of these kikuchi approximations these new message passing algorithms can be significantly more accurate than ordinary bp at an adjustable in crease in complexity we illustrate such a new gbp algorithm on a grid markov network and show that it gives much more accurate marginal probabilities than those found using ordinary bp

in theory the winnow multiplicative update has certain advantages over the perceptron additive update when there are many irrelevant attributes recently there has been much effort on enhancing the perceptron algo rithm by using regularization leading to a class of linear classification methods called support vector machines similarly it is also possible to apply the regularization idea to the winnow algorithm which gives meth ods we call regularized winnows we show that the resulting methods compare with the basic winnows in a similar way that a support vector machine compares with the perceptron we investigate algorithmic is sues and learning properties of the derived methods some experimental results will also be provided to illustrate different methods
large margin linear classification methods have been successfully ap plied to many applications for a linearly separable problem it is known that under appropriate assumptions the expected misclassification error of the computed optimal hyperplane approaches zero at a rate propor tional to the inverse training sample size this rate is usually charac terized by the margin and the maximum norm of the input data in this paper we argue that another quantity namely the robustness of the in put data distribution also plays an important role in characterizing the convergence behavior of expected misclassification error based on this concept of robustness we show that for a large margin separable linear classification problem the expected misclassification error may converge exponentially in the number of training sample size
we describe a computer system that provides a realtime musi cal accompaniment for a live soloist in a piece of nonimprovised music for soloist and accompaniment a bayesian network is devel oped that represents the joint distribution on the times at which the solo and accompaniment notes are played relating the two parts through a layer of hidden variables the network is first con structed using the rhythmic information contained in the musical score the network is then trained to capture the musical interpre tations of the soloist and accompanist in an offline rehearsal phase during live accompaniment the learned distribution of the network is combined with a realtime analysis of the soloists acoustic sig nal performed with a hidden markov model to generate a musi cally principled accompaniment that respects all available sources of knowledge a live demonstration will be provided

this paper presents autodj a system for automatically generating mu sic playlists based on one or more seed songs selected by a user autodj uses gaussian process regression to learn a user preference function over songs this function takes music metadata as inputs this paper further introduces kernel metatraining which is a method of learning a gaussian process kernel from a distribution of functions that generates the learned function for playlist generation autodj learns a kernel from a large set of albums this learned kernel is shown to be more effective at predicting users playlists than a reasonable handdesigned kernel
this paper deals with a neural network architecture which establishes a portfolio management system similar to the black litterman approach this allocation scheme distributes funds across various securities or fi nancial markets while simultaneously complying with specific allocation constraints which meet the requirements of an investor the portfolio optimization algorithm is modeled by a feedforward neural network the underlying expected return forecasts are based on error correction neural networks ecnn which utilize the last model error as an auxiliary input to evaluate their own misspecification the portfolio optimization is implemented such that i the allocations comply with investors constraints and that ii the risk of the portfo lio can be controlled we demonstrate the profitability of our approach by constructing internationally diversified portfolios across different financial markets of the g contries it turns out that our approach is superior to a preset benchmark portfolio
with the increasing number of users of mobile computing devices eg personal digital assistants and the advent of third generation mobile phones wireless communications are becoming increasingly important many applications rely on the device maintaining a replica of a data structure which is stored on a server for example news databases calendars and e mail in this paper we explore the question of the optimal strategy for synchronising such replicas we utilise probabilistic models to represent how the data structures evolve and to model user behaviour we then formulate objective functions which can be minimised with respect to the synchronisation timings we demonstrate using two real world data sets that a user can obtain more up to date information using our approach

we introduce a new type of selforganizing map som to navigate in the semantic space of large text collections we propose a hyper bolic som hsom based on a regular tesselation of the hyperbolic plane which is a noneuclidean space characterized by constant negative gaussian curvature the exponentially increasing size of a neighborhood around a point in hyperbolic space provides more freedom to map the complex information space arising from language into spatial relations we describe experiments showing that the hsom can successfully be applied to text categorization tasks and yields results comparable to other stateoftheart methods

we present a probabilistic generative model for timing deviations in expressive music performance the structure of the proposed model is equivalent to a switching state space model we formulate two well known music recognition problems namely tempo tracking and automatic transcription rhythm quantization as ltering and maximum a posteriori map state estimation tasks the inferences are carried out using sequential monte carlo integration particle ltering techniques for this purpose we have derived a novel viterbi algorithm for rao blackwellized particle lters where a subset of the hidden variables is integrated out the resulting model is suitable for realtime tempo tracking and transcription and hence useful in a number of music applications such as adaptive automatic accompaniment and score typesetting
we investigate the following data mining problem from computational chemistry from a large data set of compounds find those that bind to a target molecule in as few iterations of biological testing as possible in each iteration a comparatively small batch of compounds is screened for binding to the target we apply active learning techniques for selecting the successive batches one selection strategy picks unlabeled examples closest to the maximum margin hyperplane another produces many weight vectors by running perceptrons over multiple permutations of the data each weight vector votes with its prediction and we pick the unlabeled examples for which the prediction is most evenly split between and for a third selec tion strategy note that each unlabeled example bisects the version space of consistent weight vectors we estimate the volume on both sides of the split by bouncing a billiard through the version space and select un labeled examples that cause the most even split of the version space we demonstrate that on two data sets provided by dupont pharmaceu ticals that all three selection strategies perform comparably well and are much better than selecting random batches for testing
in packet switches packets queue at switch inputs and contend for out puts the contention arbitration policy directly affects switch perfor mance the best policy depends on the current state of the switch and current traffic patterns this problem is hard because the state space possible transitions and set of actions all grow exponentially with the size of the switch we present a reinforcement learning formulation of the problem that decomposes the value function into many small inde pendent value functions and enables an efficient action selection

estimating the parameters of sparse multinomial distributions is an important component of many statistical learning tasks recent approaches have used uncertainty over the vocabulary of symbols in a multinomial distribution as a means of accounting for sparsity we present a bayesian approach that allows weak prior knowledge in the form of a small set of approximate candidate vocabularies to be used to dramatically improve the resulting estimates we demonstrate these improvements in applications to text compression and estimating distributions over words in newsgroup data

estimating insurance premia from data is a dicult regression problem for several reasons the large number of variables many of which are discrete and the very peculiar shape of the noise distribution asymmetric with fat tails with a large majority zeros and a few unreliable and very large values we compare several machine learning methods for estimating insurance premia and test them on a large data base of car insurance policies we nd that function approximation methods that do not optimize a squared loss like support vector machines regression do not work well in this context compared methods include decision trees and generalized linear models the best results are obtained with a mixture of experts which better identi es the least and most risky contracts and allows to reduce the median premium by charging more to the most risky customers
we report on the use of reinforcement learning with cobot a software agent residing in the wellknown online community lambdamoo our initial work on cobot isbell et al provided him with the ability to collect social statistics and report them to users here we describe an application of rl allowing cobot to take proactive actions in this complex social environment and adapt behavior from multiple sources of human reward after months of training and reward and punishment events from different lambdamoo users cobot learned nontrivial preferences for a number of users modifing his behavior based on his current state here we describe lambdamoo and the state and action spaces of cobot and report the statistical results of the learning experiment
the pagerank algorithm used in the google search engine greatly improves the results of web search by taking into account the link structure of the web pagerank assigns to a page a score propor tional to the number of times a random surfer would visit that page if it surfed indefinitely from page to page following all outlinks from a page with equal probability we propose to improve page rank by using a more intelligent surfer one that is guided by a probabilistic model of the relevance of a page to a query efficient execution of our algorithm at query time is made possible by pre computing at crawl time and thus once for all queries the neces sary terms experiments on two large subsets of the web indicate that our algorithm significantly outperforms pagerank in the hu manrated quality of the pages returned while remaining efficient enough to be used in todays large search engines
we present a principled and efficient planning algorithm for cooperative multia gent dynamic systems a striking feature of our method is that the coordination and communication between the agents is not imposed but derived directly from the system dynamics and function approximation architecture we view the en tire multiagent system as a single large markov decision process mdp which we assume can be represented in a factored way using a dynamic bayesian net work dbn the action space of the resulting mdp is the joint action space of the entire set of agents our approach is based on the use of factored linear value functions as an approximation to the joint value function this factorization of the value function allows the agents to coordinate their actions at runtime using a natural message passing scheme we provide a simple and efficient method for computing such an approximate value function by solving a single linear pro gram whose size is determined by the interaction between the value function structure and the dbn we thereby avoid the exponential blowup in the state and action space we show that our approach compares favorably with approaches based on reward sharing we also show that our algorithm is an efficient alterna tive to more complicated algorithms even in the single agent case
we consider the use of two additive control variate methods to reduce the variance of performance gradient estimates in reinforcement learn ing problems the first approach we consider is the baseline method in which a function of the current state is added to the discounted value estimate we relate the performance of these methods which use sam ple paths to the variance of estimates based on iid data we derive the baseline function that minimizes this variance and we show that the vari ance for any baseline is the sum of the optimal variance and a weighted squared distance to the optimal baseline we show that the widely used average discounted value baseline where the reward is replaced by the difference between the reward and its expectation is suboptimal the second approach we consider is the actorcritic method which uses an approximate value function we give bounds on the expected squared error of its estimates we show that minimizing distance to the true value function is suboptimal in general we provide an example for which the true value function gives an estimate with positive variance but the op timal value function gives an unbiased estimate with zero variance our bounds suggest algorithms to estimate the gradient of the performance of parameterized baseline or value functions we present preliminary exper iments that illustrate the performance improvements on a simple control problem
it is desirable that a complex decision making problem in an uncertain world be adequately modeled by a markov decision process mdp whose structural representation is adaptively designed by a parsimonious resources allocation process resources include time and cost of exploration amount of memory and computational time allowed for the policy or value function representation concerned about making the best use of the available resources we address the problem of eciently estimating where adding extra resources is highly needed in order to improve the expected performance of the resulting policy possible application in reinforcement learning rl when real world exploration is highly costly concerns the detection of those areas of the state space that need primarily to be explored in order to improve the policy another application concerns approximation of continuous state space stochastic control problems using adaptive discretization techniques for which highly ecient grid points allocation is mandatory to survive high dimensionality maybe surprisingly these two problems can be formulated under a common framework for a given resource allocation which denes a belief state over possible mdps nd where adding new resources thus decreasing the uncertainty of some parameters transition probabilities or rewards will most likely increase the expected performance of the new policy to do so we use sampling techniques for estimating the contribution of each parameters probability distribution function pdf to the expected loss of using an approximate policy such as the optimal policy of the most probable mdp instead of the true but unknown policy
we show the convergence of two deterministic variants of qlearning the rst is the widely used optimistic q learning which initializes the q values to large initial values and then follows a greedy policy with respect to the q values we show that setting the initial value suciently large guarantees the converges to an optimal policy the second is a new and novel algorithm incremental q learning which gradually promotes the values of actions that are not taken we show that incremental q learning converges in the limit to the optimal policy our incremental q learning algorithm can be viewed as derandomization of the greedy q learning
this paper presents reinforcement learning with a long short term memory recurrent neural network rllstm modelfree rllstm using advantage learning and directed exploration can solve nonmarkovian tasks with longterm dependencies be tween relevant events this is demonstrated in a tmaze task as well as in a difficult variation of the pole balancing task
we consider the problem of learning to attain multiple goals in a dynamic envi ronment which is initially unknown in addition the environment may contain arbitrarily varying elements related to actions of other agents or to nonstationary moves of nature this problem is modelled as a stochastic markov game between the learning agent and an arbitrary player with a vectorvalued reward function the objective of the learning agent is to have its longterm average reward vector belong to a given target set we devise an algorithm for achieving this task which is based on the theory of approachability for stochastic games this algorithm com bines in an appropriate way a finite set of standard scalarreward learning algo rithms sucient conditions are given for the convergence of the learning algorithm to a general target set the specialization of these results to the singlecontroller markov decision problem are discussed as well
we address the problem of non convergence of online reinforcement learning algorithms eg q learning and sarsa by adopting an incremental batch approach that separates the exploration process from the function tting process our bfbp batch fit to best paths algorithm alternates between an exploration phase during which trajectories are generated to try to nd fragments of the optimal policy and a function tting phase during which a function approximator is t to the best known paths from start states to terminal states an advantage of this approach is that batch value function tting is a global process which allows it to address the tradeo s in function approximation that cannot be handled by local online algorithms this approach was pioneered by boyan and moore with their growsupport and rout algorithms we show how to improve upon their work by applying a better exploration process and by enriching the function tting procedure to incorporate bellman error and advantage error measures into the objective function the results show improved performance on several benchmark problems
we address two open theoretical questions in policy gradient reinforce ment learning the first concerns the efficacy of using function approx imation to represent the state action value function q theory is pre sented showing that linear function approximation representations of q can degrade the rate of convergence of performance gradient estimates by a factor of oml relative to when no function approximation of q is used where m is the number of possible actions and l is the number of basis functions in the function approximation representation the sec ond concerns the use of a bias term in estimating the state action value function theory is presented showing that a nonzero bias term can improve the rate of convergence of performance gradient estimates by o m where m is the number of possible actions experimen tal evidence is presented showing that these theoretical results lead to significant improvement in the convergence properties of policy gradi ent reinforcement learning algorithms

we present three ways of combining linear programming with the kernel trick to nd value function approximations for reinforcement learning one formulation is based on svm regression the second is based on the bellman equation and the third seeks only to ensure that good moves have an advantage over bad moves all formulations attempt to minimize the number of support vectors while tting the data experiments in a dicult synthetic maze problem show that all three formulations give excellent performance but the advantage formulation is much easier to train unlike policy gradient methods the kernel methods described here can easily adjust the complexity of the function approximator to t the complexity of the value function
we provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space although gradient methods cannot make large changes in the values of the parameters we show that the natural gradient is moving toward choosing a greedy optimal action rather than just a better action these greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate compatible value functions as dened by sutton et al we then show drastic performance improvements in simple mdps and in the more challenging mdp of tetris

we propose a new approach to reinforcement learning which combines least squares function approximation with policy iteration our method is modelfree and completely off policy we are motivated by the least squares temporal difference learning algorithm lstd which is known for its efficient use of sample experiences compared to pure temporal difference algorithms lstd is ideal for prediction problems however it heretofore has not had a straightforward application to control problems moreover approximations learned by lstd are strongly influenced by the visitation distribution over states our new algorithm least squares policy iteration lspi addresses these issues the result is an offpolicy method which can use or reuse data collected from any source we have tested lspi on several problems including a bicycle simulator in which it learns to guide the bicycle to a goal efficiently by merely observing a relatively small number of completely random trials
we present a simple approach for computing reasonable policies for factored markov decision processes mdps when the optimal value function can be approximated by a compact linear form our method is based on solving a single linear program that approximates the best linear t to the optimal value function by applying an ecient constraint generation procedure we obtain an iterative solution method that tackles concise linear programs this direct linear programming approach experimentally yields a significant reduction in computation time over approximate valueand policy iteration methods sometimes reducing several hours to a few seconds however the quality of the solutions produced by linear programming is weakerusually about twice the approximation error for the same approximating class nevertheless the speed advantage allows one to use larger approximation classes to achieve similar error in reasonable time
we propose a new classification for multiagent learning algorithms with each league of players characterized by both their possible strategies and possible beliefs using this classification we review the optimality of ex isting algorithms including the case of interleague play we propose an incremental improvement to the existing algorithms that seems to achieve average payoffs that are at least the nash equilibrium payoffs in the long run against fair opponents
the standard reinforcement learning view of the involvement of neuromodulatory systems in instrumental conditioning includes a rather straightforward conception of motivation as prediction of sum future reward competition between actions is based on the motivating characteristics of their consequent states in this sense substantial careful experiments reviewed in dickinson balleine into the neurobiology and psychology of motivation shows that this view is incomplete in many cases animals are faced with the choice not between many different actions at a given state but rather whether a single response is worth executing at all evidence suggests that the motivational process underlying this choice has dierent psychological and neural properties from that underlying action choice we describe and model these motivational systems and consider the way they interact
in this paper we explore two quantitative approaches to the modelling of counterfactual reasoning a linear and a noisyor model based on in formation contained in conceptual dependency networks empirical data is acquired in a study and the fit of the models compared to it we con clude by considering the appropriateness of nonparametric approaches to counterfactual reasoning and examining the prospects for other para metric approaches in the future

we present a neural network model that shows how the prefrontal cortex interacting with the basal ganglia can maintain a sequence of phonological information in activation based working memory ie the phonological loop the primary function of this phonological loop may be to transiently encode arbitrary bindings of information necessary for tasks the combinatorial expressive power of language enables very exible binding of essentially arbitrary pieces of information our model takes advantage of the closed class nature of phonemes which allows di erent neural representations of all possible phonemes at each sequential position to be encoded to make this work we suggest that the basal ganglia provide a region speci c update signal that allocates phonemes to the appropriate sequential coding slot to demonstrate that exible arbitrary binding of novel sequences can be supported by this mechanism we show that the model can generalize to novel sequences after moderate amounts of training
if the promise of computational modeling is to be fully realized in higher level cognitive domains such as language processing principled methods must be developed to construct the semantic representations used in such models in this paper we propose the use of an established formalism from mathematical psychology additive clustering as a means of auto matically constructing binary representations for objects using only pair wise similarity data however existing methods for the unsupervised learning of additive clustering models do not scale well to large prob lems we present a new algorithm for additive clustering based on a novel heuristic technique for combinatorial optimization the algorithm is simpler than previous formulations and makes fewer independence as sumptions extensive empirical tests on both human and synthetic data suggest that it is more effective than previous methods and that it also scales better to larger problems by making additive clustering practical we take a significant step toward scaling connectionist models beyond handcoded examples
unsupervised learning algorithms have been derived for several statistical models of english grammar but their computational complexity makes applying them to large data sets intractable this paper presents a probabilistic model of english grammar that is much simpler than conventional models but which admits an ecient em training algorithm the model is based upon grammatical bigrams ie syntactic relationships between pairs of words we present the results of experiments that quantify the representational adequacy of the grammatical bigram model its ability to generalize from labelled data and its ability to induce syntactic structure from large amounts of raw text

the temporal coding hypothesis of miller and colleagues suggests that animals integrate related temporal patterns of stimuli into single memory representations we formalize this concept using quasi bayes estimation to update the parameters of a constrained hidden markov model this approach allows us to account for some surprising temporal e ects in the second order conditioning experiments of miller et al which other models are unable to explain
a theory of categorization is presented in which knowledge of causal relationships between category features is represented as a bayesian network referred to as causalmodel theory this theory predicts that objects are classified as category members to the extent they are likely to have been produced by a category s causal model on this view people have models of the world that lead them to expect a certain distribution of features in category members eg correlations between feature pairs that are directly connected by causal relationships and consider exemplars good category members when they manifest those expectations these expectations include sensitivity to higherorder feature interactions that emerge from the asymmetries inherent in causal relationships research on the topic of categorization has traditionally focused on the problem of learning new categories given observations of category members in contrast the theorybased view of categories emphasizes the influence of the prior theoretical knowledge that learners often contribute to their representations of categories however in contrast to models accounting for the effects of empirical observations there have been few models developed to account for the effects of prior knowledge the purpose of this article is to present a model of categorization referred to as causalmodel theory or cmt according to cmt peoples knowledge of many categories includes not only features but also an explicit representation of the causal mechanisms that people believe link the features of many categories in this article i apply cmt to the problem of establishing objects category membership in the psychological literature one standard view of categorization is that objects are placed in a category to the extent they have features that have often been observed in members of that category for example an object that has most of the features of birds eg wings fly build nests in trees etc and few features of other categories is thought to be a bird this view of categorization is formalized by prototype models in which classification is a function of the similarity ie number of shared features between a mental representation of a category prototype and a tobeclassified object however a wellknown difficulty with prototype models is that a feature s contribution to category membership is independent of the presence or absence of other features in contrast consideration of a category s theoretical knowledge is likely to influence which combinations of features make for acceptable category members for example people believe that birds have nests in trees because they can fly and in light of this knowledge an animal that doesn t fly and yet still builds nests in trees might be considered a less plausible bird than an animal that builds nests on the ground and doesn t fly eg an ostrich even though the latter animal has fewer features typical of birds to assess whether knowledge in fact influences which feature combinations make for good category members in the following experiment undergraduates were taught novel categories whose four binary features exhibited either a commoncause or a commoneffect schema figure in the commoncause schema one category feature f is described as causing the three other features f f and f in the commoneffect schema one feature f is described as being caused by the three others f f and f cmt assumes that people represent causal knowledge such as that in figure as a kind of bayesian network in which nodes are variables representing binary category features and directed edges are causal relationships representing the presence of probabilistic causal mechanisms between features specifically cmt assumes that when a cause feature is present it enables the operation of a causal mechanism that will with some probability m bring about the presence of the effect feature cmt also allow for the possibility that effect features have potential background causes that are not explicitly represented in the network as represented by parameter b which is the probability that an effect will be present even when its network causes are absent finally each cause node has a parameter c that represents the probability that a cause feature will be present f f f f commoncause schema f f f f commoneffect schema commoncause correlations f f f f f f f f commoneffect correlations figure figure the central prediction of cmt is that an object is considered to be a category member to the extent that its features were likely to have been generated by a categorys causal mechanisms for example table presents the likelihoods that the causal models of figure will generate the sixteen possible combinations of f f f and f each likelihood equation can be derived by the application of simple boolean algebra operations for example the probability of exemplar f f f present f absent being generated by a commoncause model is the probability that f is present c times the probability that f was brought about by f or its background cause m b times the probability that f was brought about by neither f nor its background cause m b times the probability that f was brought about by f or its background cause m b likewise the probability of exemplar f f f present f absent being generated by a commoneffect model is the probability that f and f are present c times the probability that f is absent c times the probability that f was brought about by f f or its background cause m m b note that these likelihoods assume that the causal mechanisms in each model operate independently and with the same probability m restrictions that can be relaxed in other applications this formalization of categorization offered by cmt implies that people s theoretical knowledge leads them to expect a certain distribution of features in category members and that they use this information when assigning category membership thus to gain insight into the categorization performance predicted by cmt we can examine the statistical properties of category features that one can expect to be generated by a causal model for example dotted lines in figure represent the features correlations that are generated from the causal schemas of figure as one would expect pairs of features directly linked by causal relationships are correlated in the commoncause schema f is correlated with its effects and in the commoneffect schema f is correlated with its causes thus cmt predicts that combinations of features serve as evidence for category membership to the extent that they preserve these expected correlations ie both cause and effect present or both absent and against category membership to the extent that they break those correlations one present and the other absent table likelihoods equations and observed and predicted values common cause schema common effect schema control exemplar likelihood observed predicted likelihood observed predicted observed cb c b cb b c b cb b cc m b cb b cc m b cm b cc m b cb b cc m b cb b cc m b cb b c cm b cm b m b cc m b cm b m b c cm b cm b m b c cm b cb c c m b cm b m b c c m b cm b m b c c m b cm b m b c m b c m b c m b note c c m m b b causal networks not only predict pairwise correlations between directly connected features figure indicates that as a result of the asymmetries inherent in causal relationships there is an important disanalogy between the commoncause and commoneffect schemas although the commoncause schema implies that the three effects f f f will be correlated albeit more weakly than directly connected features the commoneffect schema does not imply that the three causes f f f will be correlated this asymmetry between commoncause and commoneffect schemas has been the focus of considerable investigation in the philosophical and psychological literatures use of these schemas in the following experiment enables a test of whether categorizers are sensitive the pattern of correlations between features directlyconnected by causal laws and also those that arise due to the asymmetries inherent in causal relationships shown in figure moreover i will show that cmt predicts and humans exhibit sensitivity to interactions among features of a higherorder than the pairwise interactions shown in figure method six novel categories were used in which the description of causal relationships between features consisted of one sentence indicating the cause and effect feature and then one or two sentences describing the mechanism responsible for the causal relationship for example one of the novel categories lake victoria shrimp was described as having four binary features eg a high quantity of ach neurotransmitter longlasting flight response accelerated sleep cycle etc and causal relationships among those features eg a high quantity of ach neurotransmitter causes a longlasting flight response the duration of the electrical signal to the muscles is longer because of the excess amount of neurotransmitter participants first studied several computer screens of information about their assigned category at their own pace all participants were first presented with the category s four features participants in the commoncause condition were additionally instructed on the commoncause causal relationships f f f f f f and participants in the commoneffect condition were instructed on the commoneffect relationships f f f f f f when ready participants took a multiplechoice test that tested them on the knowledge they had just studied participants were required to retake the test until they committed errors participants then performed a classification task in which they rated on a scale the category membership of exemplars consisting of all possible objects that can be formed from four binary features for example those participants assigned to learn the lake victoria shrimp category were asked to classify a shrimp that possessed high amounts of the ach neurotransmitter a normal flight response accelerated sleep cycle and normal body weight the order of the test exemplars was randomized for each participant one hundred and eight university of illinois undergraduates received course credit for participating in this experiment they were randomly assigned in equal numbers to the three conditions and to one of the six experimental categories resu l t s categorization ratings for the test exemplars averaged over participants in the commoncause commoneffect and control conditions are presented in table the presence of causal knowledge had a large effect on the ratings for instance exemplars and were given lower ratings in the commoncause and commoneffect conditions respectively and than in the control condition and presumably because in these exemplars correlations are broken effect features are present even though their causes are absent in contrast exemplar received a significantly higher rating in the commoncause and commoneffect conditions than in the control condition and vs presumably because in both conditions all correlations are preserved to confirm that causal schemas induced a sensitivity to interactions between features categorization ratings were analyzed by performing a multiple regression for each participant four predictor variables f f f f were coded as if the feature was absent and if it was present an additional six predictor variables were formed from the multiplicative interaction between pairs of features f f f f f and f for those feature pairs connected by a causal relationship the twoway interaction terms represent whether the causal relationship is confirmed cause and effect both present or both absent or violated one present and one absent finally the four threeway interactions f f f and f and the single fourway interaction f were also included as predictors regression weights averaged over participants are presented in figure as a function of causal schema condition figure indicates that the interaction terms corresponding to those feature pairs assigned causal relationships had significantly positive weights in both the commoncause condition f f f and the commoneffect condition f f f that is as predicted figure an exemplar was rated a better category member when it preserved expected correlations cause and effect feature either both present or both absent and a worse member when it broke those correlations one absent and the other present feature weight f f f f f f f f f f f f f f f regression term a common cause vs control cc predict cc predict control cccontrolcc cc predicted control observed cc observed feature weight f f f f f f f f f f f f f f f regression term b common effect vs control cc predict cc predict control cc control cc ce predicted control observed ce observed figure in addition it was shown earlier figure that because of their commoncause the three effect features in a commoncause schema will be correlated albeit more weakly than directlylinked features consistent with this prediction in this condition the three twoway interaction terms between the effect features f f f are greater than those interactions in the control condition in contrast the commoneffect schema does not imply that the three cause features will be correlated and in fact in that condition the interactions between the cause attributes f f f did not differ from those in the control condition figure figure also reveals higherorder interactions among features in the commoneffect condition weights on interaction terms f f f and f and were significantly different from those in the control condition these higherorder interactions arose because a commoneffect schema requires only one cause feature to explain the presence of the common effect figures b presents the logarithm of the ratings in the commoneffect condition for those test exemplars in which the common effect is present as a function of the number of cause features present ratings increased more with the
we present a model of binding of relationship information in a spatial domain eg square above triangle that uses loworder coarsecoded conjunctive representations instead of more popular temporal synchrony mechanisms supporters of temporal synchrony argue that conjunctive representations lack both efficiency ie combinatorial numbers of units are required and systematicity ie the resulting representations are overly specific and thus do not support generalization to novel exem plars to counter these claims we show that our model a uses far fewer hidden units than the number of conjunctions represented by us ing coarsecoded distributed representations where each unit has a broad tuning curve through highdimensional conjunction space and b is ca pable of considerable generalization to novel inputs
tangential hand velocity profiles of rapid human arm movements of ten appear as sequences of several bellshaped accelerationdeceleration phases called submovements or movement units this suggests how the nervous system might efficiently control a motor plant in the presence of noise and feedback delay another critical observation is that stochastic ity in a motor control problem makes the optimal control policy essen tially different from the optimal control policy for the deterministic case we use a simplified dynamic model of an arm and address rapid aimed arm movements we use reinforcement learning as a tool to approximate the optimal policy in the presence of noise and feedback delay using a simplified model we show that multiple submovements emerge as an optimal policy in the presence of noise and feedback delay the optimal policy in this situation is to drive the arms end point close to the target by one fast submovement and then apply a few slow submovements to accu rately drive the arms end point into the target region in our simulations the controller sometimes generates corrective submovements before the initial fast submovement is completed much like the predictive correc tions observed in a number of psychophysical experiments
it has been known that people after being exposed to sentences generated by an arti cial grammar acquire implicit grammatical knowledge and are able to transfer the knowledge to inputs that are generated by a modi ed grammar we show that a second order recurrent neural network is able to transfer grammatical knowledge from one language generated by a finite state machine to another language which di er both in vocabularies and syntax representation of the grammatical knowledge in the network is analyzed using linear discriminant analysis
animal data on delayed reward conditioning experiments shows a striking property the data for di erent time intervals collapses into a single curve when the data is scaled by the time interval this is called the scalar property of interval timing here a simple model of a neural clock is presented and shown to give rise to the scalar property the model is an accumulator consisting of noisy linear spiking neurons it is analytically tractable and contains only three parameters when coupled with reinforcement learning it simulates peak procedure experiments producing both the scalar property and the pattern of single trial covariances
narayanan and jurafsky proposed that human language compre hension can be modeled by treating human comprehenders as bayesian reasoners and modeling the comprehension process with bayesian de cision trees in this paper we extend the narayanan and jurafsky model to make further predictions about reading time given the probability of difference parses or interpretations and test the model against reading time data from a psycholinguistic experiment
partial information can trigger a complete memory at the same time human memory is not perfect a cue can contain enough information to specify an item in memory but fail to trigger that item in the context of word memory we present experiments that demonstrate some basic patterns in human memory errors we use cues that consist of word frag ments we show that short and long cues are completed more accurately than medium length ones and study some of the factors that lead to this behavior we then present a novel computational model that shows some of the flexibility and patterns of errors that occur in human memory this model iterates between bottomup and topdown computations these are tied together using a markov model of words that allows memory to be accessed with a simple feature set and enables a bottomup process to compute a probability distribution of possible completions of word frag ments in a manner similar to models of visual perceptual completion

we describe a programmable multi chip vlsi neuronal system that can be used for exploring spike based information processing models the system consists of a silicon retina a pic microcontroller and a transceiver chip whose integrate and fire neurons are connected in a soft winner take all architecture the circuit on this multi neuron chip approximates a cortical microcircuit the neurons can be configured for different computational properties by the virtual connections of a selected set of pixels on the silicon retina the virtual wiring between the different chips is effected by an event driven communication protocol that uses asynchronous digital pulses similar to spikes in a neuronal system we used the multi chip spike based system to synthesize orientation tuned neurons using both a feedforward model and a feedback model the performance of our analog hardware spiking model matched the experimental observations and digital simulations of continuous valued neurons the multi chip vlsi system has advantages over computer neuronal models in that it is real time and the computational time does not scale with the size of the neuronal network
a mixedsignal paradigm is presented for highresolution parallel inner product computation in very high dimensions suitable for efficient im plementation of kernels in image processing at the core of the externally digital architecture is a highdensity lowpower analog array performing binarybinary partial matrixvector multiplication full digital resolution is maintained even with lowresolution analogtodigital conversion ow ing to random statistics in the analog summation of binary products a random modulation scheme produces nearbernoulli statistics even for highly correlated inputs the approach is validated with real image data and with experimental results from a ciddram analog array prototype in m cmos
this paper describes a clustering algorithm for vector quantizers using a stochastic association model it offers a new simple and powerful soft max adaptation rule the adaptation process is the same as the online kmeans clustering method except for adding random fluctuation in the distortion error evaluation process simulation results demonstrate that the new algorithm can achieve efficient adaptation as high as the neural gas algorithm which is reported as one of the most efficient clustering methods it is a key to add uncorrelated random fluctuation in the simi larity evaluation process for each reference vector for hardware imple mentation of this process we propose a nanostructure whose operation is described by a singleelectron circuit it positively uses fluctuation in quantum mechanical tunneling processes
experimental data has shown that synaptic strength modification in some types of biological neurons depends upon precise spike tim ing differences between presynaptic and postsynaptic spikes sev eral temporallyasymmetric hebbian learning rules motivated by this data have been proposed we argue that such learning rules are suitable to analog vlsi implementation we describe an eas ily tunable circuit to modify the weight of a silicon spiking neuron according to those learning rules test results from the fabrication of the circuit using a m cmos process are given
learning curves for gaussian process regression are well understood when the student model happens to match the teacher true data generation process i derive approximations to the learning curves for the more generic case of mismatched models and find very rich behaviour for large input space dimensionality where the results become exact there are universal studentindependent plateaux in the learning curve with transitions in between that can exhibit arbitrarily many overfitting maxima overfitting can occur even if the student estimates the teacher noise level correctly in lower dimensions plateaux also appear and the learning curve remains dependent on the mismatch between student and teacher even in the asymptotic limit of a large number of training examples learn ing with excessively strong smoothness assumptions can be partic ularly dangerous for example a student with a standard radial basis function covariance function will learn a rougher teacher func tion only logarithmically slowly all predictions are confirmed by simulations
we study online learning in boolean domains using kernels which cap ture feature expansions equivalent to using conjunctions over basic fea tures we demonstrate a tradeoff between the computational efficiency with which these kernels can be computed and the generalization abil ity of the resulting classifier we first describe several kernel functions which capture either limited forms of conjunctions or all conjunctions we show that these kernels can be used to efficiently run the percep tron algorithm over an exponential number of conjunctions however we also prove that using such kernels the perceptron algorithm can make an exponential number of mistakes even when learning simple func tions we also consider an analogous use of kernel functions to run the multiplicativeupdate winnow algorithm over an expanded feature space of exponentially many conjunctions while known upper bounds imply that winnow can learn dnf formulae with a polynomial mistake bound in this setting we prove that it is computationally hard to simulate win nows behavior for learning dnf over such a feature set and thus that such kernel functions for winnow are not efficiently computable
singularities are ubiquitous in the parameter space of hierarchical models such as multilayer perceptrons at singularities the fisher information matrix degenerates and the cramerrao paradigm does no more hold implying that the classical model selection the ory such as aic and mdl cannot be applied it is important to study the relation between the generalization error and the training error at singularities the present paper demonstrates a method of analyzing these errors both for the maximum likelihood estima tor and the bayesian predictive distribution in terms of gaussian random fields by using simple models
we investigate the generalization performance of some learning prob lems in hilbert functional spaces we introduce a notion of convergence of the estimated functional predictor to the best underlying predictor and obtain an estimate on the rate of the convergence this estimate allows us to derive generalization bounds on some learning formulations
the partition function for a boltzmann machine can be bounded from above and below we can use this to bound the means and the correlations for networks with small weights the values of these statistics can be restricted to nontrivial regions ie a subset of gamma experimental results show that reasonable bounding occurs for weight sizes where mean field expansions generally give good results
we report a result of perturbation analysis on decoding error of the belief propagation decoder for gallager codes the analysis is based on infor mation geometry and it shows that the principal term of decoding error at equilibrium comes from the membedding curvature of the loglinear submanifold spanned by the estimated pseudoposteriors one for the full marginal and k for partial posteriors each of which takes a single check into account where k is the number of checks in the gallager code it is then shown that the principal error term vanishes when the paritycheck matrix of the code is so sparse that there are no two columns with overlap greater than
in this paper we show that online algorithms for classification and re gression can be naturally used to obtain hypotheses with good data dependent tail bounds on their risk our results are proven without re quiring complicated concentrationofmeasure arguments and they hold for arbitrary online learning algorithms furthermore when applied to concrete online algorithms our results yield tail bounds that in many cases are comparable or better than the best known bounds
we propose a method for the fast estimation of hyperparameters in large networks based on the linear response relation in the cavity method and an empirical measurement of the greens function simulation results show that it is ecient and precise when compared with cross validation and other techniques which require matrix inversion
we derive an equivalence between adaboost and the dual of a convex optimization problem showing that the only difference between mini mizing the exponential loss used by adaboost and maximum likelihood for exponential models is that the latter requires the model to be normal ized to form a conditional probability distribution over labels in addi tion to establishing a simple and easily understood connection between the two methods this framework enables us to derive new regularization procedures for boosting that directly correspond to penalized maximum likelihood experiments on uci datasets support our theoretical analy sis and give additional insight into the relationship between boosting and logistic regression
recurrent neural networks of analog units are computers for real valued functions we study the time complexity of real computa tion in general recurrent neural networks these have sigmoidal linear and product units of unlimited order as nodes and no re strictions on the weights for networks operating in discrete time we exhibit a family of functions with arbitrarily high complexity and we derive almost tight bounds on the time required to compute these functions thus evidence is given of the computational lim itations that timebounded analog recurrent neural networks are subject to
we consider noisy euclidean traveling salesman problems in the plane which are random combinatorial problems with underlying structure gibbs sampling is used to compute average trajectories which estimate the underlying structure common to all instances this procedure requires identifying the exact relationship between permutations and tours in a learning setting the average trajectory is used as a model to construct solutions to new instances sampled from the same source experimental results show that the average trajectory can in fact estimate the underlying structure and that over tting e ects occur if the trajectory adapts too closely to a single instance
we consider the problem of measuring the eigenvalues of a randomly drawn sample of points we show that these values can be reliably estimated as can the sum of the tail of eigenvalues furthermore the residuals when data is projected into a subspace is shown to be reliably estimated on a random sample experiments are presented that con rm the theoretical results
we give results about the learnability and required complexity of logical formulae to solve classification problems these results are obtained by linking propositional logic with kernel machines in particular we show that decision trees and disjunctive normal forms dnf can be repre sented by the help of a special kernel linking regularized risk to separa tion margin subsequently we derive a number of lower bounds on the required complexity of logic formulae using properties of algorithms for generation of linear estimators such as perceptron and maximal percep tron learning
the rulebased bootstrapping introduced by yarowsky and its co training variant by blum and mitchell have met with considerable em pirical success earlier work on the theory of cotraining has been only loosely related to empirically useful cotraining algorithms here we give a new pacstyle bound on generalization error which justifies both the use of confidences partial rules and partial labeling of the unlabeled data and the use of an agreementbased objective function as sug gested by collins and singer our bounds apply to the multiclass case ie where instances are to be assigned one of k labels for k
we give an unified convergence analysis of ensemble learning meth ods including eg adaboost logistic regression and the leastsquare boost algorithm for regression these methods have in common that they iteratively call a base learning algorithm which returns hypotheses that are then linearly combined we show that these methods are related to the gausssouthwell method known from numerical optimization and state nonasymptotical convergence results for all these methods our analysis includes norm regularized cost functions leading to a clean and general way to regularize ensemble learning
we propose randomized techniques for speeding up kernel principal component analysis on three levels sampling and quantization of the gram matrix in training randomized rounding in evaluating the kernel expansions and random projections in evaluating the kernel itself in all three cases we give sharp bounds on the accuracy of the obtained ap proximations rather intriguingly all three techniques can be viewed as instantiations of the following idea replace the kernel function k by a randomized kernel which behaves like k in expectation
we introduce the notion of kernel alignment a measure of similarity between two kernel functions or between a kernel and a target function this quantity captures the degree of agreement between a kernel and a given learning task and has very natural interpretations in machine learning leading also to simple algorithms for model selection and learning we analyse its theoretical properties proving that it is sharply concentrated around its expected value and we discuss its relation with other standard measures of performance finally we describe some of the algorithms that can be obtained within this framework giving experimental results showing that adapting the kernel to improve alignment on the labelled data signi cantly increases the alignment on the test set giving improved classi cation accuracy hence the approach provides a principled method of performing transduction keywords kernels alignment eigenvectors eigenvalues transduction
in contrast to standard statistical learning theory which studies uniform bounds on the expected error we present a framework that exploits the specic learning algorithm used motivated by the luckiness framework we are also able to exploit the serendipity of the training sample the main dierence to previous approaches lies in the complexity measure rather than covering all hypotheses in a given hypothesis space it is only necessary to cover the functions which could have been learned using the xed learning algorithm we show how the resulting framework relates to the vc luckiness and compression frameworks finally we present an application of this framework to the maximum margin algorithm for linear classiers which results in a bound that exploits both the margin and the distribution of the data in feature space
we study the dynamics of a hebbian ica algorithm extracting a sin gle nongaussian component from a highdimensional gaussian back ground for both online and batch learning we find that a surprisingly large number of examples are required to avoid trapping in a suboptimal state close to the initial conditions to extract a skewed signal at least on examples are required for ndimensional data and on exam ples are required to extract a symmetrical signal with nonzero kurtosis
the mutual information of two random variables and with joint probabilities f ij g is commonly used in learning bayesian nets as well as in many other elds the chances ij are usually estimated by the empirical sampling frequency n ij n leading to a point estimate in ij n for the mutual information to answer questions like is in ij n consistent with zero or what is the probability that the true mutual information is much larger than the point estimate one has to go beyond the point estimate in the bayesian framework one can answer these questions by utilizing a second order prior distribution p comprising prior information about from the prior p one can compute the posterior pjn from which the distribution pi jn of the mutual information can be calculated we derive reliable and quickly computable approximations for pi jn we concentrate on the mean variance skewness and kurtosis and non informative priors for the mean we also give an exact expression numerical issues and the range of validity are discussed
the recent

the cluster variation method is a class of approximation methods containing the bethe and kikuchi approximations as special cases we derive two novel iteration schemes for the cluster variation method one is a xed point iteration scheme which gives a signi cant improvement over loopy bp mean eld and tap methods on directed graphical models the other is a gradient based method that is guaranteed to converge and is shown to give useful results on random graphs with mild frustration we conclude that the methods are of signi cant practical value for large inference problems
using methods of statistical physics we investigate the role of model complexity in learning with support vector machines svms we show the advantages of using svms with kernels of in nite complexity on noisy target rules which in contrast to common theoretical beliefs are found to achieve optimal generalization error although the training error does not converge to the generalization error moreover we nd a universal asymptotics of the learning curves which only depend on the target rule but not on the svm kernel
we combine the replica approach from statistical physics with a varia tional approach to analyze learning curves analytically we apply the method to gaussian process regression as a main result we derive ap proximative relations between empirical error measures the generaliza tion error and the posterior variance
the mystery of belief propagation bp decoder especially of the turbo decoding is studied from information geometrical viewpoint the loopy belief network bn of turbo codes makes it difficult to obtain the true belief by bp and the characteristics of the algorithm and its equilib rium are not clearly understood our study gives an intuitive understand ing of the mechanism and a new framework for the analysis based on the framework we reveal basic properties of the turbo decoding
in this work we introduce an informationtheoretic based correction term to the likelihood ratio classification method for multiple classes under certain conditions the term is sufficient for optimally correcting the dif ference between the true and estimated likelihood ratio and we analyze this in the gaussian case we find that the new correction term signif icantly improves the classification results when tested on medium vo cabulary speech recognition tasks moreover the addition of this term makes the class comparisons analogous to an intransitive game and we therefore use several tournamentlike strategies to deal with this issue we find that further small improvements are obtained by using an appro priate tournament lastly we find that intransitivity appears to be a good measure of classification confidence
an important issue in applying svms to speech recognition is the ability to classify variable length sequences this paper presents extensions to a standard scheme for handling this variable length data the fisher score a more useful mapping is introduced based on the likelihood ratio the score space de ned by this mapping avoids some limitations of the fisher score class conditional generative models are directly incorporated into the de nition of the score space the mapping and appropriate normalisation schemes are evaluated on a speaker independent isolated letter task where the new mapping outperforms both the fisher score and hmms trained to maximise likelihood
logistic units in the rst hidden layer of a feedforward neural network compute the relative probability of a data point under two gaussians this leads us to consider substituting other density models we present an architecture for performing discriminative learning of hidden markov models using a network of many small hmms experiments on speech data show it to be superior to the standard method of discriminatively training hmms
a novel approach for comparing sequences of observations using an explicitexpansion kernel is demonstrated the kernel is derived using the assumption of the independence of the sequence of observations and a meansquared error training criterion the use of an explicit expan sion kernel reduces classifier model size and computation dramatically resulting in model sizes and computation onehundred times smaller in our application the explicit expansion also preserves the computational advantages of an earlier architecture based on meansquared error train ing training using standard support vector machine methodology gives accuracy that significantly exceeds the performance of stateoftheart meansquared error training for a speaker recognition task
a challenging unsolved problem in the speech recognition community is recognizing speech signals that are corrupted by loud highly nonstationary noise one approach to noisy speech recognition is to automatically remove the noise from the cepstrum sequence before feeding it in to a clean speech recognizer in previous work published in eurospeech we showed how a probability model trained on clean speech and a separate probability model trained on noise could be combined for the purpose of estimating the noisefree speech from the noisy speech we showed how an iterative nd order vector taylor series approximation could be used for probabilistic inference in this model in many circumstances it is not possible to obtain examples of noise without speech noise statistics may change signi cantly during an utterance so that speechfree frames are not sucient for estimating the noise model in this paper we show how the noise model can be learned even when the data contains speech in particular the noise model can be learned from the test utterance and then used to denoise the test utterance the approximate inference technique is used as an approximate e step in a generalized em algorithm that learns the parameters of the noise model from a test utterance for both wall street journal data with added noise samples and the aurora benchmark we show that the new noise adaptive technique performs as well as or signi cantly better than the non adaptive algorithm without the need for a separate training set of noise examples
a model of auditory grouping is described in which auditory attention plays a key role the model is based upon an oscillatory correlation framework in which neural oscillators representing a single perceptual stream are synchronised and are desynchronised from oscillators representing other streams the model suggests a mechanism by which attention can be directed to the high or low tones in a repeating sequence of tones with alternating frequencies in addition it simulates the perceptual segregation of a mistuned harmonic from a complex tone
in the missing data approach to improving the robustness of automatic speech recognition to added noise an initial process identifies spectral temporal regions which are dominated by the speech source the remaining regions are considered to be missing in this paper we develop a connectionist approach to the problem of adapting speech recognition to the missing data case using recurrent neural networks in contrast to methods based on hidden markov models rnns allow us to make use of longterm time constraints and to make the problems of classification with incomplete data and imputing missing values interact we report encouraging results on an isolated digit recognition task
when applying unsupervised learning techniques like ica or tem poral decorrelation a key question is whether the discovered pro jections are reliable in other words can we give error bars or can we assess the quality of our separation we use resampling meth ods to tackle these questions and show experimentally that our proposed variance estimations are strongly correlated to the sepa ration error we demonstrate that this reliability estimation can be used to choose the appropriate icamodel to enhance signifi cantly the separation performance and most important to mark the components that have a actual physical meaning application to channeldata from an magnetoencephalography meg ex periment underlines the usefulness of our approach
it is well known that under noisy conditions we can hear speech much more clearly when we read the speakers lips this suggests the utility of audio visual information for the task of speech enhancement we propose a method to exploit audio visual cues to enable speech separation under non stationary noise and with a single microphone we revise and extend hmm based speech enhancement techniques in which signal and noise models are factorially combined to incorporate visual lip information and employ novel signal hmms in which the dynamics of narrow band and wide band components are factorial we avoid the combinatorial explosion in the factorial model by using a simple approximate inference technique to quickly estimate the clean signals in a mixture we present a preliminary evaluation of this approach using a small vocabulary audio visual database showing promising improvements in machine intelligibility for speech enhanced using audio and visual information
we present a sequential monte carlo method applied to additive noise compensation for robust speech recognition in timevarying noise the method generates a set of samples according to the prior distribution given by clean speech models and noise prior evolved from previous estimation an explicit model representing noise ef fects on speech features is used so that an extended kalman filter is constructed for each sample generating the updated continuous state estimate as the estimation of the noise parameter and predic tion likelihood for weighting each sample minimum mean square error mmse inference of the timevarying noise parameter is car ried out over these samples by fusion the estimation of samples ac cording to their weights a residual resampling selection step and a metropolishastings smoothing step are used to improve calcula tion eciency experiments were conducted on speech recognition in simulated nonstationary noises where noise power changed ar tificially and highly nonstationary machinegun noise in all the experiments carried out we observed that the method can have sig nificant recognition performance improvement over that achieved by noise compensation with stationary noise assumption
a cortical model for motion in depth selectivity of complex cells in the visual cortex is proposed the model is based on a time extension of the phase based techniques for disparity estimation we consider the computation of the total temporal derivative of the time varying disparity through the combination of the responses of disparity energy units to take into account the physiological plausibility the model is based on the combinations of binocular cells characterized by di erent ocular dominance indices the resulting cortical units of the model show a sharp selectivity for motion indepth that has been compared with that reported in the literature for real cortical cells
theories of cue combination suggest the possibility of constructing visual stimuli that evoke different patterns of neural activity in sensory areas of the brain but that cannot be distinguished by any behavioral measure of perception such stimuli if they exist would be interesting for two reasons first one could know that none of the differences between the stimuli survive past the computations used to build the percepts second it can be difficult to distinguish stimulus driven components of measured neural activity from top down components such as those due to the interestingness of the stimuli changing the stimulus without changing the percept could be exploited to measure the stimulusdriven activity here we describe stimuli in which vertical and horizontal disparities trade during the construction of percepts of slanted surfaces yielding stimulus equivalence classes equivalence class membership changed after a change of vergence eye posture alone without changes to the retinal images a formal correspondence can be drawn between these perceptual metamers and more familiar sensory metamers such as color metamers
recent work has shown impressive transforminvariant modeling and clustering for sets of images of objects with similar appearance we seek to expand these capabilities to sets of images of an object class that show considerable variation across individual instances eg pedestrian images using a representation based on pixelwise similarities similarity templates because of its invariance to the colors of particular components of an object this representation en ables detection of instances of an object class and enables alignment of those instances further this model implicitly represents the re gions of color regularity in the classspecific image set enabling a decomposition of that object class into component regions
we present new simulation results in which a computational model of interacting visual neurons simultaneously predicts the modulation of spatial vision thresholds by focal visual attention for ve dual task human psychophysics experiments this new study complements our previous ndings that attention activates a winnertake all competition among early visual neurons within one cortical hypercolumn this intensi ed competition hypothesis assumed that attention equally a ects all neurons and yielded two singleunit predictions an increase in gain and a sharpening of tuning with attention while both e ects have been separately observed in electrophysiology no single unit study has yet shown them simultaneously hence we here explore whether our model could still predict our data if attention might only modulate neuronal gain but do so non uniformly across neurons and tasks speci cally we investigate whether modulating the gain of only the neurons that are loudest best tuned or most informative about the stimulus or of all neurons equally but in a task dependent manner may account for the data we nd that none of these hypotheses yields predictions as plausible as the intensi ed competition hypothesis hence providing additional support for our original ndings
with the optimization of pattern discrimination as a goal graph partitioning approaches often lack the capability to integrate prior knowledge to guide grouping in this paper we consider priors from unitary generative models partially labeled data and spatial attention these priors are modelled as constraints in the solution space by imposing uniformity condition on the constraints we restrict the feasible space to one of smooth solutions a subspace projection method is developed to solve this constrained eigenproblem we demonstrate that simple priors can greatly improve image segmentation results
a nonlinear supervised learning model the specialized mappings architecture sma is described and applied to the estimation of human body pose from monocular images the sma consists of several specialized forward mapping functions and an inverse map ping function each specialized function maps certain domains of the input space image features onto the output space body pose parameters the key algorithmic problems faced are those of learning the specialized domains and mapping functions in an op timal way as well as performing inference given inputs and knowl edge of the inverse function solutions to these problems employ the em algorithm and alternating choices of conditional indepen dence assumptions performance of the approach is evaluated with synthetic and real video sequences of human motion
locally linear embedding lle is an elegant nonlinear dimensionality reduction technique recently introduced by roweis and saul it fails when the data is divided into separate groups we study a variant of lle that can simultaneously group the data and calculate local embedding of each group an estimate for the upper bound on the intrinsic dimension of the data set is obtained automatically
this paper develops a new approach for extremely fast detection in do mains where the distribution of positive and negative examples is highly skewed eg face detection or database retrieval in such domains a cascade of simple classifiers each trained to achieve high detection rates and modest false positive rates can yield a final detector with many desir able features including high detection rates very low false positive rates and fast performance achieving extremely high detection rates rather than low error is not a task typically addressed by machine learning al gorithms we propose a new variant of adaboost as a mechanism for training the simple classifiers used in the cascade experimental results in the domain of face detection show the training algorithm yields sig nificant improvements in performance over conventional adaboost the final face detection system can process frames per second achieves over detection and a false positive rate of in a
the most popular algorithms for object detection require the use of exhaustive spatial and scale search procedures in such approaches an object is de ned by means of local features in this paper we show that including contextual information in object detection procedures provides an ecient way of cutting down the need for exhaustive search we present results with real images showing that the proposed scheme is able to accurately predict likely object classes locations and sizes
we describe a neural network which enhances and completes salient closed contours our work is different from all previous work in three important ways first like the input provided to v by lgn the in put to our computation is isotropic that is the input is composed of spots not edges second our network computes a well defined function of the input based on a distribution of closed contours characterized by a random process third even though our computation is implemented in a discrete network its output is invariant to continuous rotations and translations of the input pattern
we describe an algorithm for automatically learning discriminative com ponents of objects with svm classifiers it is based on growing image parts by minimizing theoretical bounds on the error probability of an svm componentbased face classifiers are then combined in a second stage to yield a hierarchical svm classifier experimental results in face classification show considerable robustness against rotations in depth and suggest performance at significantly better level than other face detection systems novel aspects of our approach are a an algorithm to learn componentbased classification experts and their combination b the use of d morphable models for training and c a maximum operation on the output of each component classifier which may be relevant for bio logical models of visual recognition
we describe the g factor which relates probability distributions on image features to distributions on the images themselves the g factor depends only on our choice of features and lattice quantization and is independent of the training image data we illustrate the importance of the g factor by analyzing how the parameters of markov random field ie gibbs or log linear probability models of images are learned from data by maximum likelihood estimation in particular we study homogeneous mrf models which learn image distributions in terms of clique potentials corresponding to feature histogram statistics cf minimax entropy learning mel by zhu wu and mumford we rst use our analysis of the g factor to determine when the clique potentials decouple for di erent features second we show that clique potentials can be computed analytically by approximating the g factor third we demonstrate a connection between this approximation and the generalized iterative scaling algorithm gis due to darroch and ratcli for calculating potentials this connection enables us to use gis to improve our multinomial approximation using bethe kikuchi approximations to simplify the gis procedure we support our analysis by computer simulations
a key question in neuroscience is how to encode sensory stimuli such as images and sounds motivated by studies of response prop erties of neurons in the early cortical areas we propose an encoding scheme that dispenses with absolute measures of signal intensity or contrast and uses instead only local ordinal measures in this scheme the structure of a signal is represented by a set of equalities and inequalities across adjacent regions in this paper we focus on characterizing the fidelity of this representation strategy we develop a regularization approach for image reconstruction from ordinal measures and thereby demonstrate that the ordinal repre sentation scheme can faithfully encode signal structure we also present a neurally plausible implementation of this computation that uses only local update rules the results highlight the robust ness and generalization ability of local ordinal encodings for the task of pattern classification
this paper presents an unsupervised learning algorithm that can derive the probabilistic dependence structure of parts of an object a moving hu man body in our examples automatically from unlabeled data the dis tinguished part of this work is that it is based on unlabeled data ie the training features include both useful foreground parts and background clutter and the correspondence between the parts and detected features are unknown we use decomposable triangulated graphs to depict the probabilistic independence of parts but the unsupervised technique is not limited to this type of graph in the new approach labeling of the data part assignments is taken as hidden variables and the em algo rithm is applied a greedy algorithm is developed to select parts and to search for the optimal structure based on the differential entropy of these variables the success of our algorithm is demonstrated by applying it to generate models of human motion automatically from unlabeled real image sequences
we investigate bayesian alternatives to classical monte carlo methods for evaluating integrals bayesian monte carlo bmc allows the incorporation of prior knowledge such as smoothness of the integrand into the estimation in a simple problem we show that this outperforms any classical importance sampling method we also attempt more challenging multidimensional integrals involved in computing marginal likelihoods of statistical models aka partition functions and model evidences we find that bayesian monte carlo outperformed annealed importance sampling although for very high dimensional problems or problems with massive multimodality bmc may be less adequate one advantage of the bayesian approach to monte carlo is that samples can be drawn from any distribution this allows for the possibility of active design of sample points so as to maximise information gain
we study an explicit parametric model of documents queries and relevancy assessment for information retrieval ir mean field methods are applied to analyze the model and derive efficient practical algorithms to estimate the parameters in the problem the hyperparameters are estimated by a fast approximate leave one out cross validation procedure based on the cavity method the algorithm is further evaluated on several benchmark databases by comparing with standard algorithms in ir
many algorithms rely critically on being given a good metric over their inputs for instance data can often be clustered in many plausible ways and if a clustering algorithm such as k means initially fails to find one that is meaningful to a user the only recourse may be for the user to manually tweak the metric until sufficiently good clusters are found for these and other applications requiring good metrics it is desirable that we provide a more systematic way for users to indicate what they consider similar for instance we may ask them to provide examples in this paper we present an algorithm that given examples of similar and if desired dissimilar pairs of points in learns a distance metric over thatrespectstheserelationships ourmethodisbasedonposingmet ric learning as a convex optimization problem which allows us to give efficient local optima free algorithms we also demonstrate empirically that the learned metrics can be used to significantly improve clustering performance
in this paper we consider formulations of multi class problems based on a generalized notion of a margin and using output coding this includes but is not restricted to standard multi class svm formulations differently from many previous approaches we learn the code as well as the embedding function we illustrate how this can lead to a formulation that allows for solving a wider range of problems with for instance many classes or even missing classes to keep our optimization problems tractable we propose an algorithm capable of solving them using twoclass classifiers similar in spirit to boosting
prior knowledge in the form of multiple polyhedral sets each belonging to one of two categories is introduced into a reformulation of a linear support vector machine classifier the resulting formulation leads to a linear program that can be solved efficiently real world examples from dna sequencing and breast cancer prognosis demonstrate the effectiveness of the proposed method numerical results show improvement in test set accuracy after the incorporation of prior knowledge into ordinary data based linear support vector machine classifiers one experiment also shows that a linear classifier based solely on prior knowledge far outperforms the direct application of prior knowledge rules to classify data keywords use and refinement of prior knowledge support vector machines linear programming
we consider the problem of multi step ahead prediction in time series analysis using the non parametric gaussian process model step ahead forecasting of a discrete time non linear dynamic system can be performed by doing repeated one step ahead predictions for a state space model of the form the prediction of at time per we show how using an analytical gaussian approximation we can formally incorporate the uncertainty about intermediate regressor values thus updating the uncertainty on the current prediction is based on the point estimates of the previous outputs in this pa
the focus of the paper is the problem of learning kernel operators from empirical data we cast the kernel design problem as the construction of an accurate kernel from simple and less accurate base kernels we use the boosting paradigm to perform the kernel construction process to do so we modify the booster so as to accommodate kernel operators we also devise an efficient weak learner for simple kernels that is based on generalized eigen vector decomposition we demonstrate the effectiveness of our approach on synthetic data and on the usps dataset on the usps dataset the performance of the perceptron algorithm with learned kernels is systematically better than a fixed rbf kernel
we introduce a family of classifiers based on a physical analogy to an electrostatic system of charged conductors the family called coulomb classifiers includes the two best known support vector machines svms the svm and the csvm in the electrostatics analogy a training example corresponds to a charged conductor at a given location in space the classification function corresponds to the electrostatic potential function and the training objective function corresponds to the coulomb energy the electrostatic framework provides not only a novel interpretation of existing algorithms and their interrelationships but it suggests a variety of new methods for svms including kernels that bridge the gap between polynomial and radial basis functions objective functions that do not require positive definite kernels regularization techniques that allow for the construction of an optimal classifier in minkowski space based on the framework we propose novel svms and perform simulation studies to show that they are comparable or superior to standard svms the experiments include classification tasks on data which are represented in terms of their pairwise proximities where a coulomb classifier outperformed standard svms
this paper introduces an algorithm for the automatic relevance determination of input variables in kernelized support vector machines relevance is measured by scale factors defining the input space metric and feature selection is performed by assigning zero weights to irrelevant variables the metric is automatically tuned by the minimization of the standard svm empirical risk where scale factors are added to the usual set of parameters defining the classifier feature selection is achieved by constraints encouraging the sparsity of scale factors the resulting algorithm compares favorably to state of the art feature selection procedures and demonstrates its effectiveness on a demanding facial expression recognition problem
in this paper we present a new algorithm suitable for matching discrete objects such as strings and trees in linear time thus obviating dynamic programming with quadratic time complexity furthermore prediction cost in many cases can be reduced to linear cost in the length of the sequence to be classified regardless of the number of support vectors this improvement on the currently available algorithms makes string kernels a viable alternative for the practitioner
we introduce the generalized linear model a statistical estimator which combines features of nonlinear regression and factor analysis a glm approximately decomposes a rectangular matrix x into a simpler representation fgahb here a and b are low rank matrices while f g and h are link functions glms include many useful models as special cases including principal components analysis exponential family pca the infomax formulation of independent components analysis linear regression and generalized linear models they also include new and interesting special cases one of which we describe below we also present an iterative procedure which optimizes the parameters of a glm this procedure reduces to well known algorithms for some of the special cases listed above for other special cases it is new
we propose a framework to incorporate unlabeled data in kernel classi er based on the idea that two points in the same cluster are more likely to have the same label this is achieved by modifying the eigenspectrum of the kernel matrix experimental results assess the validity of this approach
echo state networks esn are a novel approach to recurrent neural network training an esn consists of a large xed recurrent reservoir network from which the desired output is obtained by training suitable output connection weights determination of optimal output weights becomes a linear uniquely solvable task of mse minimization this article reviews the basic ideas and describes an online adaptation scheme based on the rls algorithm known from adaptive linear systems as an example a th order narma system is adaptively identi ed the known bene ts of the rls algorithms carry over from linear systems to nonlinear ones speci cally the convergence rate and misadjustment can be determined at design time
we introduce a general family of kernels based on weighted transducers or rational relations rational kernels that can be used for analysis of variable length sequences or more generally weighted automata in applications such as computational biology or speech recognition we show that rational kernels can be computed efficiently using a general algorithm of composition of weighted transducers and a general single source shortest distance algorithm we also describe several general families of positive definite symmetric rational kernels these general kernels can be combined with support vector machines to form efficient and powerful techniques for spoken dialog classification highly complex kernels become easy to design and implement and lead to substantial improvements in the classification accuracy we also show that the string kernels considered in applications to computational biology are all specific instances of rational kernels
we present a framework for sparse gaussian process gp methods which uses forward selection with criteria based on informationtheoretic principles previously suggested for active learning our goal is not only to learn dsparse predictors which can be evaluated in od rather than on d n n the number of training points but also to perform training under strong restrictions on time and memory requirements the scaling of our method is at we show that it can match prediction performance of the popular most on d and in large real world classification experiments support vector machine svm yet can be significantly faster in training in contrast to the svm our approximation produces estimates of predictive probabilities error bars allows for bayesian model selection and is less complex in implementation
model selection is linked to model assessment which is the problem of comparing different models or model parameters for a specific learning task for supervised learning the standard practical technique is crossvalidation which is not applicable for semi supervised and unsupervised settings in this paper a new model assessment scheme is introduced which is based on a notion of stability the stability measure yields an upper bound to cross validation in the supervised case but extends to semi supervised and unsupervised problems in the experimental part the performance of the stability measure is studied for model order selection in comparison to standard techniques in this area
there exist many approaches to clustering but the important issue of feature selection ie selecting the data attributes that are relevant for clustering is rarely addressed feature selection for clustering is difficult due to the absence of class labels we propose two approaches to feature selection in the context of gaussian mixture based clustering in the first one instead of making hard selections we estimate feature saliencies an expectation maximization em algorithm is derived for this task the second approach extends koller and sahamis mutual informationbased feature relevance criterion to the unsupervised case feature selection is then carried out by a backward search scheme this scheme can be classified as a wrapper since it wraps mixture estimation in an outer layer that performs feature selection experimental results on synthetic and real data show that both methods have promising performance
in this paper we show how the generation of documents can be thought of as a kstage markov process which leads to a fisher ker nel from which the ngram and string kernels can be reconstructed the fisher kernel view gives a more flexible insight into the string kernel and suggests how it can be parametrised in a way that re flects the statistics of the training corpus furthermore the prob abilistic modelling approach suggests extending the markov pro cess to consider subsequences of varying length rather than the standard fixedlength approach used in the string kernel we give a procedure for determining which subsequences are informative features and hence generate a finite state machine model which can again be used to obtain a fisher kernel by adjusting the parametrisation we can also influence the weighting received by the features in this way we are able to obtain a logarithmic weighting in a fisher kernel finally experiments are reported comparing the different kernels using the standard bag of words kernel as a baseline
several authors have suggested viewing boosting as a gradient descent search for a good fit in function space we apply gradient based boosting methodology to the unsupervised learning problem of density estimation we show convergence properties of the algorithm and prove that a strength of weak learnability property applies to this problem as well we illustrate the potential of this approach through experiments with boosting bayesian networks to learn density models
we present a simple direct approach for solving the ica problem using density estimation and maximum likelihood given a candidate orthogonal frame we model each of the coordinates using a semi parametric density estimate based on cubic splines since our estimates have two continuous derivatives we can easily run a second order search for the frame parameters our method performs very favorably when compared to state of the art techniques
the standard representation of text documents as bags of words su ers from well known limitations mostly due to its inability to exploit semantic similarity between terms attempts to incorporate some notion of term similarity include latent semantic indexing the use of semantic networks and probabilistic methods in this paper we propose two methods for inferring such similarity from a corpus the rst one de nes word similarity based on document similarity and viceversa giving rise to a system of equations whose equilibrium point we use to obtain a semantic similarity measure the second method models semantic relations by means of a di usion process on a graph de ned by lexicon and co occurrence information both approaches produce valid kernel functions parametrised by a real number the paper shows how the alignment measure can be used to successfully perform model selection over this parameter combined with the use of support vector machines we obtain positive results
boosting algorithms and successful applications thereof abound for classification and regression learning problems but not for unsupervised learning we propose a sequential approach to adding features to a random field model by training them to improve classification performance between the data and an equal sized sample of negative examples generated from the models current estimate of the data density training in each boosting round proceeds in three stages first we sample negative examples from the models current boltzmann distribution next a feature is trained to improve classification performance between data and negative examples finally a coefficient is learned which determines the importance of this feature relative to ones already in the pool negative examples only need to be generated once to learn each new feature the validity of the approach is demonstrated on binary digits and continuous synthetic data

we propose a new algorithm to estimate the intrinsic dimension of data sets the method is based on geometric properties of the data and requires neither parametric assumptions on the data generating model nor input parameters to set the method is compared to a similar widelyused algorithm from the same family of geometric techniques experiments show that our method is more robust in terms of the data generating distribution and more reliable in the presence of noise
using a markov chain perspective of spectral clustering we present an algorithm to automatically find the number of stable clusters in a dataset the markov chains behaviour is characterized by the spectral properties of the matrix of transition probabilities from which we derive eigenflows along with their halflives an eigenflow describes the flow of probability mass due to the markov chain and it is characterized by its eigenvalue or equivalently by the halflife of its decay as the markov chain is iterated a ideal stable cluster is one with zero eigenflow and infinite half life the key insight in this paper is that bottlenecks between weakly coupled clusters can be identified by computing the sensitivity of the eigenflows halflife to variations in the edge weights we propose a novel eigencuts algorithm to perform clustering that removes these identified bottlenecks in an iterative fashion
a common objective in learning a model from data is to recover its network structure while the model parameters are of minor interest for example we may wish to recover regulatory networks from high throughput data sources in this paper we examine how bayesian regularization using a product of independent dirichlet priors over the model parameters a ects the learned model structure in a domain with discrete variables we show that a small scale parameter often interpreted as equivalent sample size or prior strength leads to a strong regularization of the model structure sparse graph given a suciently large data set in particular the empty graph is obtained in the limit of a vanishing scale parameter this is diametrically opposite to what one may expect in this limit namely the complete graph from an unregularized maximum likelihood estimate since the prior a ects the parameters as expected the scale parameter balances a trade o between regularizing the parameters vs the structure of the model we demonstrate the bene ts of optimizing this trade o in the sense of predictive accuracy
recently proposed algorithms for nonlinear dimensionality reduction fall broadly into two categories which have different advantages and disadvantages global isomap and local locally linear embedding laplacian eigenmaps we present two variants of isomap which combine the advantages of the global approach with what have previously been exclusive advantages of local methods computational sparsity and the ability to invert conformal maps
the application of latenthidden variable dynamic bayesian networks is constrained by the complexity of marginalising over latent variables for this reason either small latent dimensions or gaussian latent conditional tables linearly dependent on past states are typically considered in order that inference is tractable we suggest an alternative approach in which the latent variables are modelled using deterministic conditional probability tables this specialisation has the advantage of tractable inference even for highly complex non linearnon gaussian visible conditional probability tables this approach enables the consideration of highly complex latent dynamics whilst retaining the benefits of a tractable probabilistic model
we propose probabilistic generative models called parametric mixture models pmms for multiclass multi labeled text categorization problem conventionally the binary classification approach has been employed in which whether or not text belongs to a category is judged by the binary classifier for every category in contrast our approach can simultaneously detect multiple categories of text using pmms we derive efficient learning and prediction algorithms for pmms we also empirically show that our method could significantly outperform the conventional binary methods when applied to multi labeled text categorization using real world wide web pages
recently the fisher score or the fisher kernel is increasingly used as a feature extractor for classification problems the fisher score is a vector of parameter derivatives of loglikelihood of a probabilistic model this paper gives a theoretical analysis about how class information is preserved in the space of the fisher score which turns out that the fisher score consists of a few important dimensions with class information and many nuisance dimensions when we perform clustering with the fisher score k means type methods are obviously inappropriate because they make use of all dimensions so we will develop a novel but simple clustering algorithm specialized for the fisher score which can exploit important dimensions this algorithm is successfully tested in experiments with artificial data and real data amino acid sequences
we propose in this paper a probabilistic approach for adaptive inference of generalized nonlinear classification that combines the computational advantage of a parametric solution with the flexibility of sequential sampling techniques we regard the parameters of the classifier as latent states in a first order markov process and propose an algorithm which can be regarded as variational generalization of standard kalman filtering the variational kalman filter is based on two novel lower bounds that enable us to use a non degenerate distribution over the adaptation rate an extensive empirical evaluation demonstrates that the proposed method is capable of infering competitive classifiers both in stationary and non stationary environments although we focus on classification the algorithm is easily extended to other generalized nonlinear models
we introduce a novel learning algorithm for binary classification with hyperplane discriminants based on pairs of training points from opposite classes dyadic hypercuts this algorithm is further extended to nonlinear discriminants using kernel functions satisfying mercers conditions an ensemble of simple dyadic hypercuts is learned incrementally by means of a confidence rated version of adaboost which provides a sound strategy for searching through the finite set of hypercut hypotheses in experiments with real world datasets from the uci repository the generalization performance of the hypercut classifiers was found to be comparable to that of svms and k nn classifiers furthermore the computational cost of classification at run time was found to be similar to or better than that of svm similarly to svms boosted dyadic kernel discriminants tend to maximize the margin via adaboost in contrast to svms however we offer an on line and incremental learning machine for building kernel discriminants whose complexity number of kernel evaluations can be directly controlled traded off for accuracy
greedy importance sampling is an unbiased estimation technique that reduces the variance of standard importance sampling by explicitly searching for modes in the estimation objective previous work has demonstrated the feasibility of implementing this method and proved that the technique is unbiased in both discrete and continuous domains in this paper we present a reformulation of greedy importance sampling that eliminates the free parameters from the original estimator and introduces a new regularization strategy that further reduces variance without compromising unbiasedness the resulting estimator is shown to be effective for difficult estimation problems arising in markov random field inference in particular improvements are achieved over standard mcmc estimators when the distribution has multiple peaked modes
problems in which abnormal or novel situations should be detected can be approached by describing the domain of the class of typical examples these applications come from the areas of machine diagnostics fault detection illness identification or in principle refer to any problem where little knowledge is available outside the typical class in this paper we explain why proximities are natural representations for domain descriptors and we propose a simple one class classifier for dissimilarity representations by the use of linear programming an efficient one class description can be found based on a small number of prototype objects this classifier can be made more robust by transforming the dissimilarities and cheaper to compute by using a reduced representation set finally a comparison to a comparable one class classifier by campbell and bennett is given
we formulate the regression problem as one of maximizing the minimum probability symbolized by that future predicted outputs of the regression model will be within some bound of the true regression function our formulation is unique in that we obtain a direct estimate of this lower probability bound the proposed framework minimax probability machine regression mpmr is based on the recently described minimax probability machine classification algorithm lanckriet et al and uses mercer kernels to obtain nonlinear regression models mpmr is tested on both toy and real world data verifying the accuracy of the bound and the efficacy of the regression models
in recent years variational methods have become a popular tool for approximate inference and learning in a wide variety of probabilistic models for each new application however it is currently necessary first to derive the variational update equations and then to implement them in application specific code each of these steps is both time consuming and error prone in this paper we describe a general purpose inference engine called vibes variational inference for bayesian networks which allows a wide variety of probabilistic models to be implemented and solved variationally without recourse to coding new models are specified either through a simple script or via a graphical interface analogous to a drawing package vibes then automatically generates and solves the variational equations we illustrate the power and flexibility of vibes using examples from bayesian mixture modelling
a new approach to inference in belief networks has been recently proposed which is based on an algebraic representation of belief networks using multilinear functions according to this approach the key computational question is that of representing multilinear functions compactly since inference reduces to a simple process of ev aluating and differentiating such functions w e show here that mainstream inference algorithms based on jointrees are a special case of this approach in a v ery precise sense w e use this result to prov e new properties of jointree algorithms and then discuss some of its practical and theoretical implications
the constraint classification framework captures many flavors of multiclass classification including winner take all multiclass classification multilabel classification and ranking we present a meta algorithm for learning in this framework that learns via a single linear classifier in high dimension we discuss distribution independent as well as margin based generalization bounds and present empirical and theoretical evidence showing that constraint classification benefits over existing methods of multiclass classification
we introduce nashprop an iterative and local message passing algorithm for computing nash equilibria in multi player games represented by arbitrary undirected graphs we provide a formal analysis and experimental evidence demonstrating that nashprop performs well on large graphical games with many loops often converging in just a dozen iterations on graphs with hundreds of nodes nashprop generalizes the tree algorithm of kearns et al and can be viewed as similar in spirit to belief propagation in probabilistic inference and thus complements the recent work of vickrey and koller who explored a junction tree approach thus as for probabilistic inference we have at least two promising general purpose approaches to equilibria computation in graphs
we focus on the problem of efficient learning of dependency trees it is well known that given the pairwise mutual information coefficients a minimum weight spanning tree algorithm solves this problem exactly and in polynomial time however for large data sets it is the construction of the correlation matrix that dominates the running time we have developed a new spanning tree algorithm which is capable of exploiting partial knowledge about edge weights the partial knowledge we maintain is a probabilistic confidence interval on the coefficients which we derive by examining just a small sample of the data the algorithm is able to flag the need to shrink an interval which translates to inspection of more data for the particular attribute pair experimental results show running time that is near constant in the number of records without significant loss in accuracy of the generated trees interestingly our spanning tree algorithm is based solely on tarjans red edge rule which is generally considered a guaranteed recipe for bad performance
we describe a method for computing provably exact maximum a posteriori map estimates for a subclass of problems on graphs with cycles the basic idea is to represent the original problem on the graph with cycles as a convex combination of tree structured problems a convexity argument then guarantees that the optimal value of the original problem ie the log probability of the map assignment is upper bounded by the combined optimal values of the tree problems we prove that this upper bound is met with equality if and only if the tree problems share an optimal configuration in common an important implication is that any such shared configuration must also be the map configuration for the original problem next we develop a tree reweighted max product algorithm for attempting to find convex combinations of tree structured problems that share a common optimum we give necessary and sufficient conditions for a fixed point to yield the exact map estimate an attractive feature of our analysis is that it generalizes naturally to convex combinations of hypertree structured distributions
pairwise data in empirical sciences typically violate metricity either due to noise or due to fallible estimates and therefore are hard to analyze by conventional machine learning technology in this paper we therefore study ways to work around this problem first we present an alternative embedding to multi dimensional scaling mds that allows us to apply a variety of classical machine learning and signal processing algorithms the class of pairwise grouping algorithms which share the shift invariance property is statistically invariant under this embedding procedure leading to identical assignments of objects to clusters based on this new vectorial representation denoising methods are applied in a second step both steps provide a theoretically well controlled setup to translate from pairwise data to the respective denoised metric representation we demonstrate the practical usefulness of our theoretical reasoning by discovering structure in protein sequence data bases visibly improving performance upon existing automatic methods
the similarity between objects is a fundamental element of many learning algorithms most non parametric methods take this similarity to be fixed but much recent work has shown the advantages of learning it in particular to exploit the local invariances in the data or to capture the possibly non linear manifold on which most of the data lies we propose a new non parametric kernel density estimation method which captures the local structure of an underlying manifold through the leading eigenvectors of regularized local covariance matrices experiments in density estimation show significant improvements with respect to parzen density estimators the density estimators can also be used within bayes classifiers yielding classification rates similar to svms and much superior to the parzen classifier
we describe a probabilistic approach to the task of placing objects described by high dimensional vectors or by pairwise dissimilarities in a low dimensional space in a way that preserves neighbor identities a gaussian is centered on each object in the high dimensional space and the densities under this gaussian or the given dissimilarities are used to define a probability distribution over all the potential neighbors of the object the aim of the embedding is to approximate this distribution as well as possible when the same operation is performed on the low dimensional images of the objects a natural cost function is a sum of kullback leibler divergences one per object which leads to a simple gradient for adjusting the positions of the low dimensional images unlike other dimensionality reduction methods this probabilistic framework makes it easy to represent each object by a mixture of widely separated low dimensional images this allows ambiguous objects like the document count vector for the word bank to have versions close to the images of both river and finance without forcing the images of outdoor concepts to be located close to those of corporate concepts
we present an automatic alignment procedure which maps the disparate internal representations learned by several local dimensionality reduction experts into a single coherent global coordinate system for the original data space our algorithm can be applied to any set of experts each of which produces a low dimensional local representation of a highdimensional input unlike recent efforts to coordinate such models by modifying their objective functions our algorithm is invoked after training and applies an efficient eigensolver to post process the trained models the post processing has no local optima and the size of the system it must solve scales with the number of local models rather than the number of original data points making it more efficient than model free algorithms such as isomap or lle
low rank approximation techniques are widespread in pattern recognition research they include latent semantic analysis lsa probabilistic lsa principal components analysus pca the generative aspect model and many forms of bibliometric analysis all make use of a low dimensional manifold onto which data are projected such techniques are generally unsupervised which allows them to model data in the absence of labels or categories with many practical problems however some prior knowledge is available in the form of context in this paper i describe a principled approach to incorporating such information and demonstrate its application to pca based approximations of several data sets
the problem of extracting the relevant aspects of data in face of multiple conflicting structures is inherent to modeling of complex data extracting structure in one random variable that is relevant for another variable has been principally addressed recently via the information bottleneck method however such auxiliary variables often contain more information than is actually required due to structures that are irrelevant for the task in many other cases it is in fact easier to specify what is irrelevant than what is for the task at hand identifying the relevant structures however can thus be considerably improved by also minimizing the information about another irrelevant variable in this paper we give a general formulation of this problem and derive its formal as well as algorithmic solution its operation is demonstrated in a synthetic example and in two real world problems in the context of text categorization and face images while the original information bottleneck problem is related to rate distortion theory with the distortion measure replaced by the relevant information extracting relevant features while removing irrelevant ones is related to rate distortion with side information
we show the existence of critical points as lines for the likelihood function of mixture type models they are given by embedding of a critical point for models with less components a sufficient condition that the critical line gives local maxima or saddle points is also derived based on this fact a component split method is proposed for a mixture of gaussian components and its effectiveness is verified through experiments
we consider the learning problem of nding a dependency between a general class of objects and another possibly di erent general class of objects the objects can be for example vectors images strings trees or graphs such a task is made possible by employing similarity measures in both input and output spaces using kernel functions thus embedding the objects into vector spaces we experimentally validate our approach on several tasks mapping strings to strings pattern recognition and reconstruction from partial images
missing data is common in real world datasets and is a problem for many estimation techniques we have developed a variational bayesian method to perform independent component analysis ica on high dimensional data containing missing entries missing data are handled naturally in the bayesian framework by integrating the generative density model modeling the distributions of the independent sources with mixture of gaussians allows sources to be estimated with different kurtosis and skewness the variational bayesian method automatically determines the dimensionality of the data and yields an accurate density model for the observed data without overfitting problems this allows direct probability estimation of missing values in the high dimensional space and avoids dimension reduction preprocessing which is not feasible with missing data
we investigate the problem of learning a classification task for datasets which are described by matrices rows and columns of these matrices correspond to objects where row and column objects may belong to different sets and the entries in the matrix express the relationships between them we interpret the matrix elements as being produced by an unknown kernel which operates on object pairs and we show that under mild assumptions these kernels correspond to dot products in some unknown feature space minimizing a bound for the generalization error of a linear classifier which has been obtained using covering numbers we derive an objective function for model selection according to the principle of structural risk minimization the new objective function has the advantage that it allows the analysis of matrices which are not positive definite and not even symmetric or square we then consider the case that row objects are interpreted as features we suggest an additional constraint which imposes sparseness on the row objects and show that the method can then be used for feature selection finally we apply this method to data obtained from dna microarrays where column objects correspond to samples row objects correspond to genes and matrix elements correspond to expression levels benchmarks are conducted using standard one gene classification and support vector machines and k nearest neighbors after standard feature selection our new method extracts a sparse set of genes and provides superior classification results
in this paper we study a special kind of learning problem in which each training instance is given a set of or distribution over candidate class labels and only one of the candidate labels is the correct one such a problem can occur eg in an information retrieval setting where a set of words is associated with an image or if classes labels are organized hierarchically we propose a novel discriminative approach for handling the ambiguity of class labels in the training examples the experiments with the proposed approach over five different uci datasets show that our approach is able to find the correct label among the set of candidate labels and actually achieve performance close to the case when each training instance is given a single correct label in contrast nave methods degrade rapidly as more ambiguity is introduced into the labels
in this paper we consider the problem of novelty detection presenting an algorithm that aims to nd a minimal region in input space containing a fraction of the probability mass underlying a data set this algorithmthe single class minimax probability machine mpmis built on a distribution free methodology that minimizes the worst case probability of a data point falling outside of a convex set given only the mean and covariance matrix of the distribution and making no further distributional assumptions we present a robust approach to estimating the mean and covariance matrix within the general two class mpm setting and show how this approach specializes to the single class problem we provide empirical results comparing the single class mpm to the single class svm and a two class svm method
we consider the problem of illusory or artefactual structure from the visualisation of high dimensional structureless data in particular we examine the role of the distance metric in the use of topographic mappings based on the statistical field of multidimensional scaling we show that the use of a squared euclidean metric ie the sstress measure gives rise to an annular structure when the input data is drawn from a highdimensional isotropic distribution and we provide a theoretical justification for this observation
we introduce a new learning algorithm for decision lists to allow features that are constructed from the data and to allow a tradeoff between accuracy and complexity we bound its generalization error in terms of the number of errors and the size of the classifier it finds on the training data we also compare its performance on some natural data sets with the set covering machine and the support vector machine
we consider the general problem of utilizing both labeled and un labeled data to improve classification accuracy under the assump tion that the data lie on a submanifold in a high dimensional space we develop an algorithmic framework to classify a partially labeled data set in a principled manner the central idea of our approach is that classification functions are naturally defined only on the sub manifold in question rather than the total ambient space using the laplace beltrami operator one produces a basis for a hilbert space of square integrable functions on the submanifold to recover such a basis only unlabeled examples are required once a basis is ob tained training can be performed using the labeled data set our algorithm models the manifold using the adjacency graph for the data and approximates the laplace beltrami operator by the graph laplacian practical applications to image and text classification are considered
we discuss the problem of ranking instances with the use of a large margin principle we introduce two main approaches the first is the fixed margin policy in which the margin of the closest neighboring classes is being maximized which turns out to be a direct generalizadifferent margins where the sum of margins is maximized this approach tion of svm to ranking learning the second approach allows for is shown to reduce to svm when the number of classes approaches are optimal in size of both where is the total number of training examples experiments performed on visual classification and collaborative filtering show that both approaches outperform existing ordinal regression algorithms applied for ranking and multi class svm applied to general multi class classification
we describe a new algorithmic framework for learning multiclass categorization problems in this framework a multiclass predictor is composed of a pair of embeddings that map both instances and labels into a common space in this space each instance is assigned the label it is nearest to we outline and analyze an algorithm termed bunching for learning the pair of embeddings from labeled data a key construction in the analysis of the algorithm is the notion of probabilistic output codes a generalization of error correcting output codes ecoc furthermore the method of multiclass categorization using ecoc is shown to be an instance of bunching we demonstrate the advantage of bunching over ecoc by comparing their performance on numerous categorization problems
gaussian process regression allows a simple analytical treatment of exact bayesian inference and has been found to provide good performance yet scales badly with the number of training data in this paper we compare several approaches towards scaling gaussian processes regression to large data sets the subset of representers method the reduced rank approximation online gaussian processes and the bayesian committee machine furthermore we provide theoretical insight into some of our experimental results we found that subset of representers methods can give good and particularly fast predictions for data sets with high and medium noise levels on complex low noise data sets the bayesian committee machine achieves significantly better accuracy yet at a higher computational cost

in this paper we introduce methodology to determine the bifurcation structure of optima for a class of similar cost functions from rate distortion theory deterministic annealing information distortion and the information bottleneck method we also introduce a numerical algorithm which uses the explicit form of the bifurcating branches to find optima at a bifurcation point
this paper investigates a boosting approach to discriminative learning of label sequences based on a sequence rank loss function the proposed method combines many of the advantages of boosting schemes with the eciency of dynamic programming methods and is attractive both conceptually and computationally in addition we also discuss alternative approaches based on the hamming loss for label sequences the sequence boosting algorithm o ers an interesting alternative to methods based on hmms and the more recently proposed conditional random fields applications areas for the presented technique range from natural language processing and information extraction to computational biology we include experiments on named entity recognition and part of speech tagging which demonstrate the validity and competitiveness of our approach
we propose a framework for classifier design based on discriminative densities for representation of the differences of the class conditional distributions in a way that is optimal for classification the densities are selected from a parametrized set by constrained maximization of some objective function which measures the average bounded difference ie the contrast between discriminative densities we show that maximization of the contrast is equivalent to minimization of an approximation of the bayes risk therefore using suitable classes of probability density functions the resulting maximum contrast classifiers mccs can approximate the bayes rule for the general multiclass case in particular for a certain parametrization of the density functions we obtain mccs which have the same functional form as the well known support vector machines svms we show that mcc training in general requires some nonlinear optimization but under certain conditions the problem is concave and can be tackled by a single linear program we indicate the close relation between svmand mcc training and in particular we show that linear programming machines can be viewed as an approximate realization of mccs in the experiments on benchmark data sets the mcc shows a competitive classification performance
adaboost minimizes an upper error bound which is an exponential function of the margin on the training set however the ultimate goal in applications of pattern classification is always minimum error rate on the other hand adaboost needs an effective procedure for learning weak classifiers which by itself is difficult especially for high dimensional data in this paper we present a novel procedure called floatboost for learning a better boosted classifier floatboost uses a backtrack mechanism after each iteration of adaboost to remove weak classifiers which cause higher error rates the resulting float boosted classifier consists of fewer weak classifiers yet achieves lower error rates than adaboost in both training and test we also propose a statistical model for learning weak classifiers based on a stagewise approximation of the posterior using an overcomplete set of scalar features experimental comparisons of floatboost and adaboost are provided through a difficult classification problem face detection where the goal is to learn from training examples a highly nonlinear classifier to differentiate between face and nonface patterns in a high dimensional space the results clearly demonstrate the promises made by floatboost over adaboost
in this paper we consider tippings relevance vector machine rvm and formalize an incremental training strategy as a variant of the expectation maximization em algorithm that we call subspace em ssem working with a subset of active basis functions the sparsity of the rvm solution will ensure that the number of basis functions and thereby the computational complexity is kept low we also introduce a mean field approach to the intractable classification model that is expected to give a very good approximation to exact bayesian inference and contains the laplace approximation as a special case we test the algorithms on two large data sets with o examples the results indicate that bayesian learning of large data sets eg the mnist database is realistic
we present a class of algorithms for learning the structure of graphical models from data the algorithms are based on a measure known as the kernel generalized variance kgv which essentially allows us to treat all variables on an equal footing as gaussians in a feature space obtained from mercer kernels thus we are able to learn hybrid graphs involving discrete and continuous variables of arbitrary type we explore the computational properties of our approach showing how to use the kernel trick to compute the relevant statistics in linear time we illustrate our framework with experiments involving discrete and continuous data
we propose a model that can learn parts based representations of highdimensional data our key assumption is that the dimensions of the data can be separated into several disjoint subsets or factors which take on values independently of each other we assume each factor has a small number of discrete states and model it using a vector quantizer the selected states of each factor represent the multiple causes of the input given a set of training examples our model learns the association of data dimensions with factors as well as the states of each vq inference and learning are carried out efficiently via variational algorithms we present applications of this model to problems in image decomposition collaborative filtering and text classification
classification with partially labeled data requires using a large number of unlabeled examples or an estimated marginal px to further constrain the conditional pyx beyond a few available labeled examples we formulate a regularization approach to linking the marginal and the conditional in a general way the regularization penalty measures the information that is implied about the labels over covering regions no parametric assumptions are required and the approach remains tractable even for continuous marginal densities px we develop algorithms for solving the regularization problem for finite covers establish a limiting differential equation and exemplify the behavior of the new regularization approach in simple cases
gaussian processes provide an approach to nonparametric modelling which allows a straightforward combination of function and derivative observations in an empirical model this is of particular importance in identification of nonlinear dynamic systems from experimental data it allows us to combine derivative information and associated uncertainty with normal function observations into the learning and inference process this derivative information can be in the form of priors specified by an expert or identified from perturbation data close to equilibrium it allows a seamless fusion of multiple local linear models in a consistent manner inferring consistent models and ensuring that integrability constraints are met it improves dramatically the computational efficiency of gaussian process models for dynamic system identification by summarising large quantities of near equilibrium data by a handful of linearisations reducing the training set size traditionally a problem for gaussian process models
we derive multiplicative updates for solving the nonnegative quadratic programming problem in support vector machines svms the updates have a simple closed form and we prove that they converge monotonically to the solution of the maximum margin hyperplane the updates optimize the traditionally proposed objective function for svms they do not involve any heuristics such as choosing a learning rate or deciding which variables to update at each iteration they can be used to adjust all the quadratic programming variables in parallel with a guarantee of improvement at each iteration we analyze the asymptotic convergence of the updates and show that the coefficients of non support vectors decay geometrically to zero at a rate that depends on their margins in practice the updates converge very rapidly to good classifiers
given a set of hidden variables with an a priori markov structure we derive an online algorithm which approximately updates the posterior as pairwise measurements between the hidden variables become available the update is performed using assumed density filtering to incorporate each pairwise measurement we compute the optimal markov structure which represents the true posterior and use it as a prior for incorporating the next measurement we demonstrate the resulting algorithm by calculating globally consistent trajectories of a robot as it navigates along a d trajectory to update a trajectory of length t the update takes ot when all conditional distributions are linear gaussian the algorithm can be thought of as a kalman filter which simplifies the state covariance matrix after incorporating each measurement
particle filters estimate the state of dynamical systems from sensor information in many real time applications of particle filters however sensor information arrives at a significantly higher rate than the update rate of the filter the prevalent approach to dealing with such situations is to update the particle filter as often as possible and to discard sensor information that cannot be processed in time in this paper we present real time particle filters which make use of all sensor information even when the filter update rate is below the update rate of the sensors this is achieved by representing posteriors as mixtures of sample sets where each mixture component integrates one observation arriving during a filter update the weights of the mixture components are set so as to minimize the approximation error introduced by the mixture representation thereby our approach focuses computational resources samples on valuable sensor information experiments using data collected with a mobile robot show that our approach yields strong improvements over other approaches
we present a novel generative model for natural language tree structures in which semantic lexical dependency and syntactic pcfg structures are scored with separate models this factorization provides conceptual simplicity straightforward opportunities for separately improving the component models and a level of performance comparable to similar non factored models most importantly unlike other modern parsing models the factored model admits an extremely effective a parsing algorithm which enables efficient exact inference
we explore the consequences of viewing semantic association as the result of attempting to predict the concepts likely to arise in a particular context we argue that the success of existing accounts of semantic representation comes as a result of indirectly addressing this problem and show that a closer correspondence to human data can be obtained by taking a probabilistic approach that explicitly models the generative structure of language
a standard view of memory consolidation is that episodes are stored temporarily in the hippocampus and are transferred to the neocortex through replay various recent experimental challenges to the idea of transfer particularly for human memory are forcing its re evaluation however although there is independent neurophysiological evidence for replay short of transfer there are few theoretical ideas for what it might be doing we suggest and demonstrate two important computational roles associated with neocortical indices
behavioral goals are achieved reliably and repeatedly with movements rarely reproducible in their detail here we offer an explanation we show that not only are variability and goal achievement compatible but indeed that allowing variability in redundant dimensions is the optimal control strategy in the face of uncertainty the optimal feedback control laws for typical motor tasks obey a minimal intervention principle deviations from the average trajectory are only corrected when they interfere with the task goals the resulting behavior exhibits task constrained variability as well as synergetic coupling among actuators which is another unexplained empirical phenomenon
we present an account of human concept learning that islearning of categories from examples basedon the principle of minimum description length mdlin support of this theorywe testeda wide range of two dimensional concept typesincluding both regular simpleand highly irregular complexstructuresandfoundthe mdl theory to give a goodaccount of subjectsperformancethis suggests that the intrinsiccomplexityof a concept that isits description lengthsystematically influences its learnability the structure ofcategories a number of different principles have been advancedto explain the manner in which humans learn to categorize objectsit has been variously suggestedthat the underlying principle might be the similaritystructureof objects the manipulability of decision boundaries or bayesian inference while many of these theories are mathematically well groundedandhave been successful in explaining a range of experimental findings they have commonly only been testedon a narrow collection of concept types similar to the simple unimodal categories of figure ac figure categories similar to those previously studiedlines represent contours of equal probabilityall except eare unimodal httpruccsrutgersedujacobfeldmanhtml moreover in the scarce research that has ventured to look beyond simple category types the goal has largely been to investigate categorization performance for isolated irregular distributions rather than to present a survey ofperformance across a range ofinteresting distributionsfor example nosofsky has previously examined the criss crosscategory offigure dand a diagonal category similar to concept offigure as well as some othermultimodalcategorieswhiletheseindividualcategorystructuresarenodoubt theoretically important they in no way exhaust the range ofpossible concept structures indeed ifwe view dimensional cartesian space as the canvas upon which a category may be represented then any set ofmanifolds in that space may be considered as a potential category it is therefore natural to ask whether one such category manifold is in principle easier or more difficult to learn than anothersince previous investigations have never considered any reasonably broad range ofcategory structures they have never been in a position to answer this question in this paper we present a theory for human categorization based on the mdlprinciple that is much better equipped to answer questions about the intrinsic learnability of both structurally regular and structurally irregular categoriesin support ofthis theory we briefly present an experiment testing human subjectslearning ofa range ofconcept types defined over a continuous two dimensional feature space including both highly regular and highly irregular structureswe find that our mdl based theory gives a good account ofhuman learning for these concepts and that descriptive complexity accurately predicts the subjective difficulty ofthe various concept types tested previous investigations of category structure the role ofcategory structure in determining learnability has not been overlooked entirely in the literaturein fact the intrinsic structure ofbinary featured categories has been investigated quite thoroughlythe classic work by shepard et alshowed that human performance in learning such boolean categories varies greatly depending on the intrinsic logical structure ofthe conceptmore recently we have shown that this performance is well predicted by the intrinsic booleancomplexityofeach concept given by the length ofthe shortest boolean formula that describes the objects in the category this result suggests that a principle ofsimplicity or parsimony manifested as a minimization of complexity might play an important role in human category learning the details ofboolean complexity analysis do not generalize easily to the type ofcontinuous feature spaces we wish to investigate herethus a newapproach is required similar in general spirit but differing in the mathematicsour goals are therefore to deploy a complexity minimization technique such as mdlto quantify the complexity ofcategories defined over continuous features and to investigate the influence ofcomplexity onhumancategorylearningbytestingarangeofconcepttypesdifferingwidelyinintrinsic complexity experiment while the mdlprinciple that we plan to employ is applicable to concepts ofany dimension for reasons ofconvenience this experiment is limited to category structures that can be formed within a two dimensional feature spacethis feature space is discretized into a gridfromwhichalegitimatecategorycanbespecifiedbytheselectionofanyfourgrid squaresour motivation for discretizing the feature space is to place a constraint on possible category structure that will facilitate the computation ofa complexity measurethis doesnotrestricttherangeofpossiblefeaturevaluesthatcanbeadoptedbystimuliinprinciple feature values are limited only by machine precision but as a matter ofconvenience we restrict features to adopting one of possible values in the range figure abstract concepts used in experiment the particular abstract category structures conceptsexamined in the experiment are shown in figure these concepts were considered to be individually interesting from a cross theoretical perspectiveand jointly representative of the broader range of available concepts the two categories in each concept are referred to as positiveand negative the positive category is represented by the dark shaded regions and the corresponding negative category is its complement note that in many cases the categories are disconnectedor multimodal nevertheless these categories are not in any sense probabilistic or ill defineda given point in feature space is always either positive or negative during the experiment each stimulus is drawn randomly from the feature space and is labeled positiveor negativebased on the region from which it was drawn uniform sampling is used so all categories of figure have the same base rate for positives the experiment itself was clothed as a video game that required subjects to discriminate between two classes of spaceships allyand enemyby destroying enemy ships and quick landing allied ships each subject totalplayed five minute games in which thedistributionofalliesandenemiescorrespondedinrandomordertotheconceptsof figure the physical features of the spaceships in all cases were the height of the tube and the radius of the podas shown in figure these physical features are mapped randomly onto the abstract feature space such that the experimental concepts may be any rigid rotation or reflection of the abstract concepts in figure positive derivation of the mdl principle themdlprincipleislargelyduetorissanenandiseasilyshowntobeaconsequence of optimal bayesian inference while several bayesian algorithms have previously been proposed as models of human concept learning the implications of the mdl principle for human learning have only recently come under scrutiny we briefly review the relevant theory accordingtobayesrulealearneroughttoselectthecategoryhypothesis thatmaximizes figure aaspaceship bdthree possible instantiations of concept from figure the posterior where is the data and takingnegative logarithms ofboth sides we obtain log log log log the problem ofmaximizing is thus identicalto the problem ofminimizing log since log is constant for allhypotheses its value does not enter into the minimizationproblem andwe canstate that the hypothesis ofchoice ought to be such as to minimize the quantity log log ifwe follow rissanenandregardthe quantity log as the descriptionlength of thenequation instructs us to select the hypothesis that minimizes the totaldescriptionlength what this means is that the hypothesis that is optimalfrom the standpoint ofthe bayesian decisionmakeris the same hypothesis thatyieldsthemost compacttwo partcode inequation thusbesidesthemeritsofbrevityforitsownsakeweseethatmaximaldescriptive compactness also corresponds to maximalinferentialpower it is this equivalence between descriptionlengthandinferencethatleadsustoinvestigatetheroleofdescriptivecomplexityinthe domainofconcept learning theory inorder to investigate the complexityofthe concepts offigure equationindicates thatweneedtoanalyzethedescriptionlengthofahypothesisforeachconcept andthedescriptionlengthoftheconceptgiventhehypothesis these insequence the hypothesisdescriptionlength inorder to compute wediscuss we first fixa language withinwhich hypotheses about the categorystructure canbe expressed we choose to use the rectangle languagewhose alphabet table consists ofclassesofsymbols representingthe different sizes of rectangle that canbe compositedwithina grid and each member ofthe class is an or rectangle situatedat a particular positioninthe grid we allowa givenhypothesis to be representedbyup to four distinct rectangles ie four symbols havingspecifiedalanguagetheissueisnowthelengthofthehypothesiscodethederivationabove suggests that a codelength of log be assignedto each symbol which corresponds to the so calledshannoncode we therefore proceedto compute the shannon codelengths for the rectangle alphabet oftable equivalently a modelclass the particular choice oflanguage modelclassis obviouslyanim portantdeterminantoftheultimatehypothesisdescriptionlength wementionthatthemdlanalysis inthis paper might be replacedbyanother theoreticalapproach such as a bayesianframework although we have not pursuedthis possibility we adopt the mdlformulationpartlybecause its emphasis onrepresentation ie descriptionseems apt for a studyofcomplexity the class contains allrectangles ofdimension and we use the noninteger value log rather thanthe integer d log e logs are base table rectangle alphabet the third and fourth columns show the probability that the source generates a given member of the class and the corresponding codelength rectangle class possible locations probability codelength log log log log log log log log log log computing these codelengths requires that we specify the probability mass function of a source it is convenient for this purpose and compatible with the subjects perspectiveto imagine that the concepts in figure are produced by a concept generatoran information source whose parameters are essentially unknown a reasonable assumption is that the source randomly selects a rectangle class with uniform probabilityand then selects an individual member of the chosen class also with uniform probability since there are classesthe assumption regarding class selection places a prior on each rectangle class of moreoverthe assumption of uniform within class sampling means that in order to encode any individual rectanglewe need only consider the cardinality of the class to which it belongs we now recall that the individual rectangles of the class differ only in their positions within the grid thereforethe cardinality of the class is equal tothenumber ofuniqueways from a gridwhere in whichan or rectanglecanbeselected thusthe probability associated with an individual rectangle of class is the corresponding shannon codelengths are shown next to these probabilities in table the description length of a particular hypothesis is the summed codeword lengths for all the rectangles up to fourthat are comprised by the hypothesis the likelihooddescription length the second part of the two part mdlcode is the description of the concept with respect to the selected hypothesiscorresponding to the bayes likelihood there are several possible approaches to computing we recall that a hypothesis we discuss one that is particularly straightforward is composed of up to four rectangular regions computing therefore involves describing that portion of the positive category that falls within each rectangular hypothesis region this is conceptually the same problem that we faced in computing aboveexcept that the region of interest for was fixed table minimum description lengths for the abstract concepts concept mdlcodelength mdlhypothesis mdlconcept bits bits bits bits bits bits bits bits bits bits bits bits at while the regions for can be of anydimension and smaller guided bythis analogywe follow the procedure of the previous section to compute an appropriate probabilitymass function since must capture just the positive squares in the hypothesis region a maximum of four squaresthe onlyrectangle classes needed in the alphabet are those of size four and minimum description lengthsforexperimentalconcepts applying the mdlanalysis above to the concepts in figure requires that we compute the total description length corresponding to all viable hypotheses for each concept the hypothesis corresponding to the shortest total codelength foreach concept is themdlhypothesis the mdlhypothesesfor all concepts are shown in table along with the corresponding minimum codelengths it can be observed that while for some concepts the mdlhypothesis preciselyconforms to the true positive categorymeaning that almost all of the concept information is carried in the hypothesis codefor the majorityof concepts the mdlhypothesis is broaderthan the true categoryregion meaning that the concept information is distributed between the hypothesis and likelihood codes note that the mdlhypothesis is not in general the most compact hypothesisiethe hypoth esis for which is a minimum ratherthe mdlhypothesis is the one for which the sum is minimum results for each game played by the subject ie each concept in figure an overall measure ofperformance is computed figure shows performance for all subjects and all concepts as a function ofthe concept complexities mdlcodelengths in table there is anevidentdecreaseinperformancewithincreasingcomplexitywhicharegressionanalysis that the linear trend in the plot is very unlikely to be a statistical accident thus the mdl complexity predicts the subjective difficulty oflearning across a broad range ofconcepts shows to be highly significant meaning decnam rofrep complexity dlh dldh figure performance vs complexity for all subjects the performance for each concept is indicated by a and the mean for each concept is indicated by an o we mention that the mdlapproach described here can be further modified to make realtimepredictions ofhowsubjects will categorizeeach newstimulus in the most simplistic approach the prediction for each new stimulus is made based on the mdlhypothesis prevailing at the time that stimulus is observed correlation between this mdlprediction and the subjects actual decision is found to be highly significant concept types the pearson statistics are given below concept for each ofthe pearsonr figure illustrates the behavior ofthe real time mdlalgorithm simulations for a variety ofdata sets can be found at httpruccsrutgersedudfassmdlmovieshtml figure real time mdlhypothesis evolution for actual concept data as the size ofthe data set grows beyond there is oscillation between the one rectangle hypothesis shown in step and the two rectangle hypothesis shown in step discriminability gives a measure ofsubjectsintrinsic capacity to discriminate categories ie one that is independent oftheir criterion for responding positive conclusions as discussed above mdl bears a tight relationship with bayesian inference and hence serves as a reasonable basis for a theory of learningthe data presented above suggest that human learners are indeed guided by something very much like rissanens principle when classifying objectswhile it is premature to conclude that humans construct anything precisely corresponding to the two part code of equation it seems likely that they employ some closely related complexity minimization principle and an associated cognitivecodestilltobediscoveredthisfindingisconsistentwithmanyearlierobservations of minimum principles guiding human inference especially in perception eg the gestalt principleof prgnanzmoreover our findings suggest aprincipled approach to predicting the subjective difficulty of concepts defined over continuous featuresas we had previously found with boolean concepts subjective difficulty correlates with intrinsic complexitythatwhichisincompressible isin turnincomprehensiblethe mdl approach is an elegant frameworkinwhichto makethisobservationrigorousandconcrete andonewhich apparently accords well with human performance acknowledgments this research was supported by nsf sbr references nosofsky rm exemplar based accounts of relations between classification recognition and typicalityjournalofexperimentalpsychology learningmemoryandcognition vol no pp ashby fgand alfonso reese la categorization as probability density estimation journalofmathematicalpsychology vol pp andersonjr theadaptivenatureofhumancategorizationpsychologicalreview vol no pp tenenbaum jb bayesian modeling of human concept learningadvancesin neuralinformation processing systems edited by mskearns sasolla and dacohn vol mit press cambridge ma nosofsky rm optimal performanceandexemplar modelsof classificationrationalmodelsofcognition edited by moaksford and nchater chap oxford university press oxford pp nosofskyrmfurthertestsofanexemplar similarityapproachtorelatingidentificationand categorizationperception andpsychophysics vol pp feldman j the structure of perceptual categoriesjournalofmathematicalpsychology vol no pp shepard rn hovland ci and jenkins hm learning and memorization of classificationspsychologicalmonographsgeneralandapplied vol no pp feldman j minimization of boolean complexity in human concept learning nature vol pp rissanen j modeling by shortest data descriptionautomatica vol pp li mand vitnyi p an
people routinely make sophisticated causal inferences unconsciously effortlessly and from very little data often from just one or a few observations we argue that these inferences can be explained as bayesian computations over a hypothesis space of causal graphical models shaped by strong top down prior knowledge in the form of intuitive theories we present two case studies of our approach including quantitative models of human causal judgments and brief comparisons with traditional bottom up models of inference

we argue that human inductive generalization is best explained in a bayesian framework rather than by traditional models based on similarity computations we go beyond previous work on bayesian concept learning by introducing an unsupervised method for constructing flexible hypothesis spaces and we propose a version of the bayesian occams razor that trades off priors and likelihoods to prevent underor over generalization in these flexible spaces we analyze two published data sets on inductive reasoning as well as the results of a new behavioral study that we have carried out

benzodiazepine midazolam the causes dense but temporary anterograde amnesia similar to that produced by hippocampal damage does the action of midazolam on the hippocampus cause less storage or less accurate storage of information in episodic long term memory we used a simple variant of the rem model to fit data collected by hirshman fisher henthorn arndt and passannante on the effects of midazolam study time and normative word frequency on both yes no and remember know recognition memory that a simple strength model fit well was contrary to the expectations of hirshman et al more important within the bayesian based rem modeling framework the data were consistent with the view that midazolam causes less accurate storage rather than less storage of information in episodic memory
current psychological theories of human causal learning and judgment focus primarily on long run predictions two by estimating parameters of a causal bayes nets though for different parameterizations and a third through structural learning this paper focuses on peoples short run behavior by examining dynamical versions of these three theories and comparing their predictions to a real world dataset
we consider the hypothesis that systems learning aspects of visual perception may benefit from the use of suitably designed developmental progressions during training four models were trained to estimate motion velocities in sequences of visual images three of the models were developmental models in the sense that the nature of their input changed during the course of training they received a relatively impoverished visual input early in training and the quality of this input improved as training progressed one model used a coarse to multiscale developmental progression ie it received coarse scale motion features early in training and finer scale features were added to its input as training progressed another model used a fine to multiscale progression and the third model used a random progression the final model was nondevelopmental in the sense that the nature of its input remained the same throughout the training period the simulation results show that the coarse to multiscale model performed best hypotheses are offered to account for this models superior performance we conclude that suitably designed developmental sequences can be useful to systems learning to estimate motion velocities the idea that visual development can aid visual learning is a viable hypothesis in need of further study
according to a series of influential models dopamine da neurons signal reward prediction error using a temporal difference td algorithm we address a problem not convincingly solved in these accounts how to maintain a representation of cues that predict delayed consequences our new model uses a td rule grounded in partially observable semi markov processes a formalism that captures two largely neglected features of da experiments hidden state and temporal variability previous models predicted rewards using a tapped delay line representation of sensory inputs we replace this with a more active process of inference about the underlying state of the world the da system can then learn to map these inferred states to reward predictions using td the new model can explain previously vexing data on the responses of da neurons in the face of temporal variability by combining statistical model based learning with a physiologically grounded td theory it also brings into contact with physiology some insights about behavior that had previously been confined to more abstract psychological models

we consider bayesian mixture approaches where a predictor is constructed by forming a weighted average of hypotheses from some space of functions while such procedures are known to lead to optimal predictors in several cases where sufficiently accurate prior information is available it has not been clear how they perform when some of the prior assumptions are violated in this paper we establish data dependent bounds for such procedures extending previous randomized approaches such as the gibbs algorithm to a fully bayesian setting the finite sample guarantees established in this work enable the utilization of bayesian mixture approaches in agnostic settings where the usual assumptions of the bayesian paradigm fail to hold moreover the bounds derived can be directly applied to non bayesian mixture approaches such as bagging and boosting
we apply the replica method of statistical physics combined with a variational method to the approximate analytical computation of bootstrap averages for estimating the generalization error we demonstrate our approach on regression with gaussian processes and compare our results with averages obtained by monte carlo sampling
the information bottleneck ib method is an information theoretic formulation for clustering problems given a joint distribution this method constructs about maximum likelihood ml of mixture models is a standard statistical a new variable that defines partitions over the values of that are informative approach to clustering problems in this paper we ask how are the two methods related we define a simple mapping between the ib problem and the ml problem for the multinomial mixture model we show that under this mapping the problems are strongly related in fact for uniform input distribution over or for large sample size the problems are mathematically equivalent specifically in these cases every fixed point of the ib functional defines a fixed point of the log likelihood and vice versa moreover the values of the functionals at the fixed points are equal under simple transformations as a result in these cases every algorithm that solves one of the problems induces a solution for the other
we extend recent work on the connection between loopy belief propagation and the bethe free energy constrained minimization of the bethe free energy can be turned into an unconstrained saddle point problem both converging double loop algorithms and standard loopy belief propagation can be interpreted as attempts to solve this saddle point problem stability analysis then leads us to conclude that stable fixed points of loopy belief propagation must be local minima of the bethe free energy perhaps surprisingly the converse need not be the case minima can be unstable fixed points we illustrate this with an example and discuss implications
this paper gives distribution free concentration inequalities for the missing mass and the error rate of histogram rules negative association methods can be used to reduce these concentration problems to concentration questions about independent sums although the sums are independent they are highly heterogeneous such highly heterogeneous independent sums cannot be analyzed using standard concentration inequalities such as hoeffdings inequality the angluin valiant bound bernsteins inequality bennetts inequality or mcdiarmids theorem
classification trees are one of the most popular types of classifiers with ease of implementation and interpretation being among their attractive features despite the widespread use of classification trees theoretical analysis of their performance is scarce in this paper we show that a new family of classification trees called dyadic classification trees dcts are near optimal in a minimax sense for a very broad range of classification problems this demonstrates that other schemes eg neural networks support vector machines cannot perform significantly better than dcts in many cases we also show that this near optimal performance is attained with linear in the number of training data complexity growing and pruning algorithms moreover the performance of dcts on benchmark datasets compares favorably to that of standard cart which is generally more computationally intensive and which does not possess similar near optimality properties our analysis stems from theoretical results on structural risk minimization on which the pruning rule for dcts is based
in this paper we analyze the relationships between the eigenvalues of the mm gram matrix k for a kernel k corresponding to a sample x xm drawn from a density px and the eigenvalues of the corresponding continuous eigenproblem we bound the differences between the two spectra and provide a performance bound on kernel pca
a new family of kernels for statistical learning is introduced that exploits the geometric structure of statistical models based on the heat equation on the riemannian manifold defined by the fisher information metric information diffusion kernels generalize the gaussian kernel of euclidean space and provide a natural way of combining generative statistical modeling with non parametric discriminative learning as a special case the kernels give a new approach to applying kernel based learning algorithms to discrete data bounds on covering numbers for the new kernels are proved using spectral theory in differential geometry and experimental results are presented for text classification
population based incremental learning is shown require very sensitive scaling of its learning rate the learning rate must scale with the system size in a problem dependent way this is shown in two problems the needle in a haystack in which the learning rate must vanish exponentially in the system size and in a smooth function in which the learning rate must vanish like the square root of the system size two methods are proposed for removing this sensitivity a learning dynamics which obeys detailed balance is shown to give consistent performance over the entire range of learning rates an analog of mutation is shown to require a learning rate which scales as the inverse system size but is problem independent
a lot of learning machines with hidden variables used in information science have singularities in their parameter spaces at singularities the fisher information matrix becomes degenerate resulting that the learning theory of regular statistical models does not hold recently it was proven that if the true parameter is contained in singularities then the coefficient of the bayes generalization error is equal to the pole of the zeta function of the kullback information in this paper under the condition that the true parameter is almost but not contained in singularities we show two results if the dimension of the parameter from inputs to hidden units is not larger than three then there exits a region of true parameters where the generalization error is larger than those of regular models however if otherwise then for any true parameter the generalization error is smaller than those of regular models the symmetry of the generalization error and the training error does not hold in singular models in general
we investigate data based procedures for selecting the kernel when learning with support vector machines we provide generalization error bounds by estimating the rademacher complexities of the corresponding function classes in particular we obtain a complexity bound for function classes induced by kernels with given eigenvectors ie we allow to vary the spectrum and keep the eigenvectors fix this bound is only a logarithmic factor bigger than the complexity of the function class induced by a single kernel however optimizing the margin over such classes leads to overfitting we thus propose a suitable way of constraining the class we use an efficient algorithm to solve the resulting optimization problem present preliminary experimental results and compare them to an alignment based approach
we applied statistical mechanics to an inverse problem of linear mapping to investigate the physics of optimal lossy compressions we used the replica symmetry breaking technique with a toy model to demonstrate shannons result the rate distortion function which is widely known as the theoretical limit of the compression with a fidelity criterion is derived numerical study shows that sparse constructions of the model provide suboptimal compressions
a distance based conditional model on the ranking poset is presented for use in classification and ranking the model is an extension of the mallows model and generalizes the classifier combination methods used by several ensemble learning algorithms including error correcting output codes discrete adaboost logistic regression and cranking the algebraic structure of the ranking poset leads to a simple bayesian interpretation of the conditional model and its special cases in addition to a unifying view the framework suggests a probabilistic interpretation for error correcting output codes and an extension beyond the binary coding scheme
we show two related things given a classi er which consists of a weighted sum of features with a large margin we can construct a stochastic classi er with negligibly larger training error rate the stochastic classi er has a future error rate bound that depends on the margin distribution and is independent of the size of the base hypothesis class a new true error bound for classi ers with a margin which is simpler functionally tighter and more data dependent than all previous bounds
we consider loopy belief propagation for approximate inference in probabilistic graphical models a limitation of the standard algorithm is that clique marginals are computed as if there were no loops in the graph to overcome this limitation we introduce fractional belief propagation fractional belief propagation is formulated in terms of a family of approximate free energies which includes the bethe free energy and the naive mean field free as special cases using the linear response correction of the clique marginals the scale parameters can be tuned simulation results illustrate the potential merits of the approach
although the study of clustering is centered around an intuitively compelling goal it has been very difficult to develop a unified framework for reasoning about it at a technical level and profoundly diverse approaches to clustering abound in the research community here we suggest a formal perspective on the difficulty in finding such a unification in the form of an impossibility theorem for a set of three simple properties we show that there is no clustering function satisfying all three relaxations of these properties expose some of the interesting and unavoidable trade offs at work in well studied clustering techniques such as single linkage sum of pairs k means and k median
we investigate the generalization performance of some learning prob lems in hilbert function spaces we introduce a concept of scale sensitive effective data dimension and show that it characterizes the con vergence rate of the underlying learning problem using this concept we can naturally extend results for parametric estimation problems in finite dimensional spaces to nonparametric kernel learning methods we de rive upper bounds on the generalization performance and show that the resulting convergent rates are optimal under various circumstances
prototypes based algorithms are commonly used to reduce the computational complexity of nearest neighbour nn classifiers in this paper we discuss theoretical and algorithmical aspects of such algorithms on the theory side we present margin based generalization bounds that suggest that these kinds of classifiers can be more accurate then the nn rule furthermore we derived a training algorithm that selects a good set of prototypes using large margin principles we also show that the years old learning vector quantization lvq algorithm emerges naturally from our framework
in this work we study an information filtering model where the relevance labels associated to a sequence of feature vectors are realizations of an unknown probabilistic linear function building on the analysis of a restricted version of our model we derive a general filtering rule based on the margin of a ridge regression estimator while our rule may observe the label of a vector only by classfying the vector as relevant experiments on a real world document filtering problem show that the performance of our rule is close to that of the on line classifier which is allowed to observe all labels these empirical results are complemented by a theoretical analysis where we consider a randomized variant of our rule and prove that its expected number of mistakes is never much larger than that of the optimal filtering rule which knows the hidden linear model
we consider the problem of choosing a kernel suitable for estimation using a gaussian process estimator or a support vector machine a novel solution is presented which involves defining a reproducing kernel hilbert space on the space of kernels itself by utilizing an analog of the classical representer theorem the problem of choosing a kernel from a parameterized family of kernels eg of varying width is reduced to a statistical estimation problem akin to the problem of minimizing a regularized risk functional various classical settings for model or kernel selection are special cases of our framework

by comparison to some other sensory cortices the functional properties of cells in the primary auditory cortex are not yet well understood recent attempts to obtain a generalized description of auditory cortical responses have often relied upon characterization of the spectrotemporal receptive field strf which amounts to a model of the stimulusresponse function srf that is linear in the spectrogram of the stimulus how well can such a model account for neural responses at the very first stages of auditory cortical processing to answer this question we develop a novel methodology for evaluating the fraction of stimulus related response power in a population that can be captured by a given type of srf model we use this technique to show that in the thalamo recipient layers of primary auditory cortex strf models account for no more than of the stimulus related power in neural responses
the direct neural control of external devices such as computer displays or prosthetic limbs requires the accurate decoding of neural activity representing continuous movement we develop a real time control system using the spiking activity of approximately neurons recorded with an electrode array implanted in the arm area of primary motor cortex in contrast to previous work we develop a control theoretic approach that explicitly models the motion of the hand and the probabilistic relationship between this motion and the mean firing rates of the cells in bins we focus on a realistic cursor control task in which the subject must move a cursor to hit randomly placed targets on a computer monitor encoding and decoding of the neural data is achieved with a kalman filter which has a number of advantages over previous linear filtering techniques in particular the kalman filter reconstructions of hand trajectories in off line experiments are more accurate than previously reported results and the model provides insights into the nature of the neural coding of movement
inner product operators often referred to as kernels in statistical learning define a mapping from some input space into a feature space the focus of this paper is the construction of biologically motivated kernels for cortical activities the kernels we derive termed spikernels map spike count sequences into an abstract vector space in which we can perform various prediction tasks we discuss in detail the derivation of spikernels and describe an efficient algorithm for computing their value on any two sequences of neural population spike counts we demonstrate the merits of our modeling approach using the spikernel and various standard kernels for the task of predicting hand movement velocities from cortical recordings in all of our experiments all the kernels we tested outperform the standard scalar product used in regression with the spikernel consistently achieving the best performance
how do cortical neurons represent the acoustic environment this question is often addressed by probing with simple stimuli such as clicks or tone pips such stimuli have the advantage of yielding easily interpreted answers but have the disadvantage that they may fail to uncover complex or higher order neuronal response properties here we adopt an alternative approach probing neuronal responses with complex acoustic stimuli including animal vocalizations and music we have used in vivo whole cell methods in the rat auditory cortex to record subthreshold membrane potential fluctuations elicited by these stimuli whole cell recording reveals the total synaptic input to a neuron from all the other neurons in the circuit instead of just its output a sparse binary spike train as in conventional single unit physiological recordings whole cell recording thus provides a much richer source of information about the neurons response many neurons responded robustly and reliably to the complex stimuli in our ensemble here we analyze the linear component the spectrotemporal receptive field strf of the transformation from the sound as represented by its time varying spectrogram to the neurons membrane potential we find that the strf has a rich dynamical structure including excitatory regions positioned in general accord with the prediction of the simple tuning curve we also find that in many cases much of the neurons response although deterministically related to the stimulus cannot be predicted by the linear component indicating the presence of as yet uncharacterized nonlinear response properties
we show that two important properties of the primary visual cortex emerge when the principle of temporal coherence is applied to natural image sequences the properties are simple cell like receptive fields and complex cell like pooling of simple cell outputs which emerge when we apply two different approaches to temporal coherence in the first approach we extract receptive fields whose outputs are as temporally coherent as possible this approach yields simple cell like receptive fields oriented localized multiscale thus temporal coherence is an alternative to sparse coding in modeling the emergence of simple cell receptive fields the second approach is based on a two layer statistical generative model of natural image sequences in addition to modeling the temporal coherence of individual simple cells this model includes inter cell temporal dependencies estimation of this model from natural data yields both simple cell like receptive fields and complex cell like pooling of simple cell outputs in this completely unsupervised learning both layers of the generative model are estimated simultaneously from scratch this is a significant improvement on earlier statistical models of early vision where only one layer has been learned and others have been fixed a priori
we consider a statistical framework for learning in a class of networks of spiking neurons our aim is to show how optimal local learning rules can be readily derived once the neural dynamics and desired functionality of the neural assembly have been specified in contrast to other models which assume sub optimal learning rules within this framework we derive local rules for learning temporal sequences in a model of spiking neurons and demonstrate its superior performance to correlation hebbian based approaches we further show how to include mechanisms such as synaptic depression and outline how the framework is readily extensible to learning in networks of highly complex spiking neurons a stochastic quantal vesicle release mechanism is considered and implications on the complexity of learning discussed
inference and adaptation in noisy and changing rich sensory environments are rife with a variety of specific sorts of variability experimental and theoretical studies suggest that these different forms of variability play different behavioral neural and computational roles and may be reported by different notably neuromodulatory systems here we refine our previous theory of acetylcholines role in cortical inference in the oxymoronic terms of expected uncertainty and advocate a theory for norepinephrine in terms of unexpected uncertainty we suggest that norepinephrine reports the radical divergence of bottom up inputs from prevailing top down interpretations to influence inference and plasticity we illustrate this proposal using an adaptive factor analysis model
single unit activity in the striatum of awake monkeys shows a marked dependence on the expected reward that a behavior will elicit we present a computational model of spiny neurons the principal neurons of the striatum to assess the hypothesis that direct neuromodulatory e ects of dopamine through the activation of d receptors mediate the reward dependency of spiny neuron activity dopamine release results in the ampli cation of key ion currents leading to the emergence of bistability which not only modulates the peak ring rate but also introduces a temporal and state dependence of the models response thus improving the detectability of temporally correlated inputs
we analyze the convergence properties of three spike triggered data analysis techniques all of our results are obtained in the setting of a possibly multidimensional linear nonlinear ln cascade model for stimulus driven neural activity we start by giving exact rate of convergence results for the common spike triggered average sta technique next we analyze a spike triggered covariance method variants of which have been recently exploited successfully by bialek simoncelli and colleagues these first two methods suffer from extraneous conditions on their convergence therefore we introduce an estimator for the ln model parameters which is designed to be consistent under general conditions we provide an algorithm for the computation of this estimator and derive its rate of convergence we close with a brief discussion of the efficiency of these estimators and an application to data recorded from the primary motor cortex of awake behaving primates
what determines the caliber of axonal branches we pursue the hypothesis that the axonal caliber has evolved to minimize signal propagation delays while keeping arbor volume to a minimum we show that for a general cost function the optimal diameters of a branching law d d the derivation relies on the fact that the conduction speed scales with the axon diameter to the power q q for myelinated axons and q for nonmyelinated axons we test the branching law on the available experimental data and find a reasonable agreement

a population of neurons typically exhibits a broad diversity of responses to sensory inputs the intuitive notion of functional classification is that cells can be clustered so that most of the diversity is captured by the identity of the clusters rather than by individuals within clusters we show how this intuition can be made precise using information theory without any need to introduce a metric on the space of stimuli or responses applied to the retinal ganglion cells of the salamander this approach recovers classical results but also provides clear evidence for subclasses beyond those identified previously further we find that each of the ganglion cells is functionally unique and that even within the same subclass only a few spikes are needed to reliably distinguish between cells

a key challenge for neural modeling is to explain how a continuous stream of multi modal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate and fire neurons in real time we propose a new computational model that is based on principles of high dimensional dynamical systems in combination with statistical learning theory it can be implemented on generic evolved or found recurrent circuitry
adaptation is a ubiquitous neural and psychological phenomenon with a wealth of instantiations and implications although a basic form of plasticity it has bar some notable exceptions attracted computational theory of only one main variety in this paper we study adaptation from the perspective of factor analysis a paradigmatic technique of unsupervised learning we use factor analysis to re interpret a standard view of adaptation and apply our new model to some recent data on adaptation in the domain of face discrimination
re mapping patterns in order to equalize their distribution may greatly simplify both the structure and the training of classi ers here the properties of one such map obtained by running a few steps of discrete time dynamical system are explored the system is called digital antennal lobe dal because it is inspired by recent studies of the antennal lobe a structure in the olfactory system of the grasshopper the pattern spreading properties of the dal as well as its average behavior as a function of its few design parameters are analyzed by extending previous results of van vreeswijk and sompolinsky furthermore a technique for adapting the parameters of the initial design in order to obtain opportune noise rejection behavior is suggested our results are demonstrated with a number of simulations
cortical synaptic plasticity depends on the relative timing of preand postsynaptic spikes and also on the temporal pattern of presynaptic spikes and of postsynaptic spikes we study the hypothesis that cortical synaptic plasticity does not associate individual spikes but rather whole firing episodes and depends only on when these episodes start and how long they last but as little as possible on the timing of individual spikes here we present the mathematical background for such a study standard methods from hidden markov models are used to define what firing episodes are estimating the probability of being in such an episode requires not only the knowledge of past spikes but also of future spikes we show how to construct a causal learning rule which depends only on past spikes but associates preand postsynaptic firing episodes as if it also knew future spikes we also show that this learning rule agrees with some features of synaptic plasticity in superficial layers of rat visual cortex froemke and dan nature
a unified biophysically motivated calcium dependent learning model has been shown to account for various rate based and spike time dependent paradigms for inducing synaptic plasticity here we investigate the properties of this model for a multi synapse neuron that receives inputs with different spike train statistics in addition we present a physiological form of metaplasticity an activity driven regulation mechanism that is essential for the robustness of the model a neuron thus implemented develops stable and selective receptive fields given various input statistics

maximally informative dimensions analyzing neural responses to natural signals tatyana sharpee nicole c rust and william bialek sloanswartz center for theoretical neurobiology department of physiology university of california at san francisco san francisco california center for neural science new york university new york ny department of physics princeton university princeton new jersey sharpeephyucsfedu rustcnsnyuedu wbialekprincetonedu we propose a method that allows for a rigorous statistical analysis of neural responses to natural stimuli which are non gaussian and exhibit strong correlations we have in mind a model in which neurons are selective for a small number of stimulus dimensions out of the high dimensional stimulus space but within this subspace the responses can be arbitrarily nonlinear therefore we maximize the mutual information between the sequence of elicited neural responses and an ensemble of stimuli that has been projected on trial directions in the stimulus space the procedure can be done iteratively by increasing the number of directions with respect to which information is maximized those directions that allow the recovery of all of the information between spikes and the full unprojected stimuli describe the relevant subspace if the dimensionality of the relevant subspace indeed is much smaller than that of the overall stimulus space it may become experimentally feasible to map out the neurons input output function even under fully natural stimulus conditions this contrasts with methods based on correlations functions reverse correlation spike triggered covariance which all require simplified stimulus statistics if we are to use them rigorously
if the cortex uses spike timing to compute the timing of the spikes must be robust to perturbations based on a recent framework that provides a simple criterion to determine whether a spike sequence produced by a generic network is sensitive to initial conditions and numerical simulations of a variety of network architectures we argue within the limits set by our model of the neuron that it is unlikely that precise sequences of spike timings are used for computation under conditions typically found in the cortex
the responses of cortical sensory neurons are notoriously variable with the number of spikes evoked by identical stimuli varying significantly from trial to trial this variability is most often interpreted as noise purely detrimental to the sensory system in this paper we propose an alternative view in which the variability is related to the uncertainty about world parameters which is inherent in the sensory stimulus specifically the responses of a population of neurons are interpreted as stochastic samples from the posterior distribution in a latent variable model in addition to giving theoretical arguments supporting such a representational scheme we provide simulations suggesting how some aspects of response variability might be understood in this framework
psychophysical data suggest that temporal modulations of stimulus amplitude envelopes play a prominent role in the perceptual segregation of concurrent sounds in particular the detection of an unmodulated signal can be significantly improved by adding amplitude modulation to the spectral envelope of a competing masking noise this perceptual phenomenon is known as comodulation masking release cmr despite the obvious influence of temporal structure on the perception of complex auditory scenes the physiological mechanisms that contribute to cmr and auditory streaming are not well known a recent physiological study by nelken and colleagues has demonstrated an enhanced cortical representation of auditory signals in modulated noise our study evaluates these cmr like response patterns from the perspective of a hypothetical auditory edge detection neuron it is shown that this simple neural model for the detection of amplitude transients can reproduce not only the physiological data of nelken et al but also in light of previous results a variety of physiological and psychoacoustical phenomena that are related to the perceptual segregation of concurrent sounds
a framework is introduced for assessing the encoding accuracy and the discriminational ability of a population of neurons upon simultaneous presentation of multiple stimuli minimal square estimation errors are obtained from a fisher information analysis in an abstract compound space comprising the features of all stimuli even for the simplest case of linear superposition of responses and gaussian tuning the symmetries in the compound space are very different from those in the case of a single stimulus the analysis allows for a quantitative description of attentional effects and can be extended to include neural nonlinearities such as nonclassical receptive fields
an essential step in understanding the function of sensory nervous systems is to characterize as accurately as possible the stimulus response function srf of the neurons that relay and process sensory information one increasingly common experimental approach is to present a rapidly varying complex stimulus to the animal while recording the responses of one or more neurons and then to directly estimate a functional transformation of the input that accounts for the neuronal firing the estimation techniques usually employed such as wiener filtering or other correlation based estimation of the wiener or volterra kernels are equivalent to maximum likelihood estimation in a gaussian output noise regression model we explore the use of bayesian evidence optimization techniques to condition these estimates we show that by learning hyperparameters that control the smoothness and sparsity of the transfer function it is possible to improve dramatically the quality of srf estimates as measured by their success in predicting responses to novel input
we present a method to distinguish direct connections between two neurons from common input originating from other unmeasured neurons the distinction is computed from the spike times of the two neurons in response to a white noise stimulus although the method is based on a highly idealized linear nonlinear approximation of neural response we demonstrate via simulation that the approach can work with a more realistic integrate and fire neuron model we propose that the approach exemplified by this analysis may yield viable tools for reconstructing stimulus driven neural networks from data gathered in neurophysiology experiments
certain simple images are known to trigger a percept of transparency the input image i is perceived as the sum of two images ix y ix y ix y this percept is puzzling first why do we choose the more complicated description with two images rather than the simpler explanation ixy ixy second given the infinite number of ways to express i as a sum of two images how do we compute the best decomposition here we suggest that transparency is the rational percept of a system that is adapted to the statistics of natural scenes we present a probabilistic model of images based on the qualitative statistics of derivative filters and corner detectors in natural scenes and use this model to find the most probable decomposition of a novel image the optimization is performed using loopy belief propagation we show that our model computes perceptually correct decompositions on synthetic images and discuss its application to real images
the goal of this work is to accurately detect and localize boundaries in natural scenes using local image measurements we formulate features that respond to characteristic changes in brightness and texture associated with natural boundaries in order to combine the information from these features in an optimal way a classifier is trained using human labeled images as ground truth we present precision recall curves showing that the resulting detector outperforms existing approaches
dimensionality reduction techniques such as principal component analysis and factor analysis are used to discover a linear mapping between high dimensional data samples and points in a lower dimensional subspace in jojic and frey introduced mixture of transformation invariant component analyzers mtca that can account for global transformations such as translations and rotations perform clustering and learn local appearance deformations by dimensionality reduction however due to enormous computational requirements of the em algorithm for learning the model o where is the dimensionality of a data sample mtca was not practical for most applications in this paper we demonstrate how fast fourier transforms can reduce the computation to the order of log with this speedup we show the effectiveness of mtca in various applications tracking video textures clustering video sequences object recognition and object detection in images
we present ongoing work on a project for automatic recognition of spontaneous facial actions spontaneous facial expressions differ substantially from posed expressions similar to how continuous spontaneous speech differs from isolated words produced on command previous methods for automatic facial expression recognition assumed images were collected in controlled environments in which the subjects deliberately faced the camera since people often nod or turn their heads automatic recognition of spontaneous facial behavior requires methods for handling out of image plane head rotations here we explore an approach based on d warping of images into canonical views we evaluated the performance of the approach as a front end for a spontaneous expression recognition system using support vector machines and hidden markov models this system employed general purpose learning mechanisms that can be applied to recognition of any facial movement the system was tested for recognition of a set of facial actions defined by the facial action coding system facs we showed that d tracking and warping followed by machine learning techniques directly applied to the warped images is a viable and promising technology for automatic facial expression recognition one exciting aspect of the approach presented here is that information about movement dynamics emerged out of filters which were derived from the statistics of images
the extraction of a single high quality image from a set of lowresolution images is an important problem which arises in elds such as remote sensing surveillance medical imaging and the extraction of still images from video typical approaches are based on the use of cross correlation to register the images followed by the inversion of the transformation from the unknown high resolution image to the observed low resolution images using regularization to resolve the ill posed nature of the inversion process in this paper we develop a bayesian treatment of the super resolution problem in which the likelihood function for the image registration parameters is based on a marginalization over the unknown high resolution image this approach allows us to estimate the unknown point spread function and is rendered tractable through the
recent algorithms for sparse coding and independent component analysis ica have demonstrated how localized features can be learned from natural images however these approaches do not take image transformations into account as a result they produce image codes that are redundant because the same feature is learned at multiple locations we describe an algorithm for sparse coding based on a bilinear generative model of images by explicitly modeling the interaction between image features and their transformations the bilinear approach helps reduce redundancy in the image code and provides a basis for transformationinvariant vision we present results demonstrating bilinear sparse coding of natural images we also explore an extension of the model that can capture spatial relationships between the independent features of an object thereby providing a new framework for parts based object recognition
the problem of super resolution involves generating feasible higher resolution images which are pleasing to the eye and realistic from a given low resolution image this might be attempted by using simple filters for smoothing out the high resolution blocks or through applications where substantial prior information is used to imply the textures and shapes which will occur in the images in this paper we describe an approach which lies between the two extremes it is a generic unsupervised method which is usable in all domains but goes beyond simple smoothing methods in what it achieves we use a dynamic tree like architecture to model the high resolution data approximate conditioning on the low resolution image is achieved through a mean field approach
in we introduced a linear statistical model of joint color changes in images due to variation in lighting and certain non geometric camera parameters we did this by measuring the mappings of colors in one image of a scene to colors in another image of the same scene under different lighting conditions here we increase the flexibility of this color flow model by allowing flow coefficients to vary according to a low order polynomial over the image this allows us to better fit smoothly varying lighting conditions as well as curved surfaces without endowing our model with too much capacity we show results on image matching and shadow removal and detection
accurate representation of articulated motion is a challenging problem for machine perception several successful tracking algorithms have been developed that model human body as an articulated tree we propose a learning based method for creating such articulated models from observations of multiple rigid motions this paper is concerned with recovering topology of the articulated model when the rigid motion of constituent segments is known our approach is based on finding the maximum likelihood tree shaped factorization of the joint probability density function pdf of rigid segment motions the topology of graphical model formed from this factorization corresponds to topology of the underlying articulated body we demonstrate the performance of our algorithm on both synthetic and real motion capture data
the tangential neurons in the fly brain are sensitive to the typical optic flow patterns generated during self motion in this study we examine whether a simplified linear model of these neurons can be used to estimate self motion from the optic flow we present a theory for the construction of an estimator consisting of a linear combination of optic flow vectors that incorporates prior knowledge both about the distance distribution of the environment and about the noise and self motion statistics of the sensor the estimator is tested on a gantry carrying an omnidirectional vision sensor the experiments show that the proposed approach leads to accurate and robust estimates of rotation rates whereas translation estimates turn out to be less reliable
we describe a method for learning sparse multiscale image representations using a sparse prior distribution over the basis function coefficients the prior consists of a mixture of a gaussian and a dirac delta function and thus encourages coefficients to have exact zero values coefficients for an image are computed by sampling from the resulting posterior distribution with a gibbs sampler the learned basis is similar to the steerable pyramid basis and yields slightly higher snr for the same number of active coefficients denoising using the learned image model is demonstrated for some standard test images with results that compare favorably with other denoising methods
the goal of low level vision is to estimate an underlying scene given an observed image real world scenes eg albedos or shapes can be very complex conventionallyrequiring high dimensional representations which are hard to estimate and store we propose a low dimensional representation called a scene recipe that relies on the image itself to describe the complex scene configurations shape recipes are an example these are the regression coefficients that predict the bandpassed shape from image data we describe the benefits of this representation and show two uses illustrating their properties we improve stereo shape estimates by learning shape recipes at low resolution and applying them at full resolution shape recipes implicitly contain information about lighting and materials and we use them for material segmentation
we present an algorithm that uses multiple cues to recover shading and reflectance intrinsic images from a single image using both color information and a classifier trained to recognize gray scale patterns each image derivative is classified as being caused by shading or a change in the surfaces reflectance generalized belief propagation is then used to propagate information from areas where the correct classification is clear to areas where it is ambiguous we also show results on real images
we address the question of feature selection in the context of visual recognition it is shown that besides efficient from a computational standpoint the infomax principle is nearly optimal in the minimum bayes error sense the concept of marginal diversity is introduced leading to a generic principle for feature selection the principle of maximum marginal diversity of extreme computational simplicity the relationships between infomax and the maximization of marginal diversity are identified uncovering the existence of a family of classification procedures for which near optimal in the bayes error sense feature selection does not require combinatorial search examination of this family in light of recent studies on the statistics of natural images suggests that visual recognition problems are a subset of it
we propose a model for natural images in which the probability of an image is proportional to the product of the probabilities of some filter outputs we encourage the system to find sparse features by using a studentt distribution to model each filter output if the t distribution is used to model the combined outputs of sets of neurally adjacent filters the system learns a topographic map in which the orientation spatial frequency and location of the filters change smoothly across the map even though maximum likelihood learning is intractable in our model the product form allows a relatively efficient learning procedure that works well even for highly overcomplete sets of filters once the model has been learned it can be used as a prior to derive the iterated wiener filter for the purpose of denoising images
we present a hierarchical bayesian model for learning efficient codes of higher order structure in natural images the model a non linear generalization of independent component analysis replaces the standard assumption of independence for the joint distribution of coefficients with a distribution that is adapted to the variance structure of the coefficients of an efficient image basis this offers a novel description of higherorder image structure and provides a way to learn coarse coded sparsedistributed representations of abstract image properties such as object location scale and texture
this paper presents a kernel method that allows to combine color and shape information for appearance based object recognition it doesnt require to de ne a new common representation but use the power of kernels to combine di erent representations together in an e ective manner these results are achieved using results of statistical mechanics of spin glasses combined with markov random elds via kernel functions experiments show an increase in recognition rate up to with respect to conventional strategies

we consider data which are images containing views of multiple objects our task is to learn about each of the objects present in the images this task can be approached as a factorial learning problem where each image must be explained by instantiating a model for each of the objects present with the correct instantiation parameters a major problem with learning a factorial model is that as the number of objects increases there is a combinatorial explosion of the number of configurations that need to be considered we develop a method to extract object models sequentially from the data by making use of a robust statistical method thus avoiding the combinatorial explosion and present results showing successful extraction of objects from real images
the problem of approximating the product of several gaussian mixture distributions arises in a number of contexts including the nonparametric belief propagation nbp inference algorithm and the training of product of experts models this paper develops two multiscale algorithms for sampling from a product of gaussian mixtures and compares their performance to existing methods the first is a multiscale variant of previously proposed monte carlo techniques with comparable theoretical guarantees but improved empirical convergence rates the second makes use of approximate kernel density evaluation methods to construct a fast approximate sampler which is guaranteed to sample points to within a tunable parameter of their true probability we compare both multiscale samplers on a set of computational examples motivated by nbp demonstrating significant improvements over existing methods
to provide a compact generative representation of the sequential activity of a number of individuals within a group there is a tradeoff between the definition of individual specific and global models this paper proposes a linear time distributed model for finite state symbolic sequences representing traces of individual user activity by making the assumption that heterogeneous user behavior may be explained by a relatively small number of common structurally simple behavioral patterns which may interleave randomly in a user specific proportion the results of an empirical study on three different sources of user traces indicates that this modelling approach provides an efficient representation scheme reflected by improved prediction performance as well as providing lowcomplexity and intuitively interpretable representations
we address the problem of learning topic hierarchies from data the model selection problem in this domain is daunting which of the large collection of possible trees to use we take a bayesian approach generating an appropriate prior via a distribution on partitions that we refer to as the nested chinese restaurant process this nonparametric prior allows arbitrarily large branching factors and readily accommodates growing data collections we build a hierarchical topic model by combining this prior with a likelihood that is based on a hierarchical variant of latent dirichlet allocation we illustrate our approach on simulated data and with an application to the modeling of nips abstracts
in typical classification tasks we seek a function which assigns a label to a single object kernel based approaches such as support vector machines svms which maximize the margin of confidence of the classifier are the method of choice for many such tasks their popularity stems both from the ability to use high dimensional feature spaces and from their strong theoretical guarantees however many real world tasks involve sequential spatial or structured data where multiple labels must be assigned existing kernel based methods ignore structure in the problem assigning labels independently to each object losing much useful information conversely probabilistic graphical models such as markov networks can represent correlations between labels by exploiting problem structure but cannot handle high dimensional feature spaces and lack strong theoretical generalization guarantees in this paper we present a new framework that combines the advantages of both approaches maximum margin markov m networks incorporate both kernels which efficiently deal with high dimensional features and the ability to capture correlations in structured data we present an efficient algorithm for learning m networks based on a compact quadratic program formulation we provide a new theoretical bound for generalization in structured domains experiments on the task of handwritten character recognition and collective hypertext classification demonstrate very significant gains over previous approaches
knowledge about local invariances with respect to given pattern transformations can greatly improve the accuracy of classification previous approaches are either based on regularisation or on the generation of virtual transformed examples we develop a new framework for learning linear classifiers under known transformations based on semidefinite programming we present a new learning algorithm the semidefinite programming machine sdpm which is able to find a maximum margin hyperplane when the training examples are polynomial trajectories instead of single points the solution is found to be sparse in dual variables and allows to identify those points on the trajectory with minimal real valued output as virtual support vectors extensions to segments of trajectories to more than one transformation parameter and to learning with kernels are discussed in experiments we use a taylor expansion to locally approximate rotational invariance in pixel images from usps and find improvements over known methods
this paper presents a method for learning a distance metric from relative comparison such as a is closer to b than a is to c taking a support vector machine svm approach we develop an algorithm that provides a flexible way of describing qualitative training data as a set of constraints we show that such constraints lead to a convex quadratic programming problem that can be solved by adapting standard methods for svm training we empirically evaluate the performance and the modelling flexibility of the algorithm on a collection of text documents
the standard norm svm is known for its good performance in twoclass classication in this paper we consider the norm svm we argue that the norm svm may have some advantage over the standard norm svm especially when there are redundant noise features we also propose an efcient algorithm that computes the whole solution path of the norm svm hence facilitates adaptive selection of the tuning parameter for the norm svm
a common way of image denoising is to project a noisy image to the subspace of admissible images made for instance by pca however a major drawback of this method is that all pixels are updated by the projection even when only a few pixels are corrupted by noise or occlusion we proand update the identified pixels only the identification and updating of noisy pixels are formulated as one linear program which can be solved efficiently especially one can apply the trick to directly specify the fraction of pixels to be reconstructed moreover we extend the linear program to be able to exploit prior knowledge that occlusions often appear in contiguous blocks eg sunglasses on faces the basic idea is to penalize boundary points and interior points of the occluded area differently we are able to show the property also for this extended lp leading a method which is easy to use experimental results impressively demonstrate the power of our approach
learning from ambiguous training data is highly relevant in many applications we present a new learning algorithm for classification problems where labels are associated with sets of pattern instead of individual patterns this encompasses multiple instance learning as a special case our approach is based on a generalization of linear programming boosting and uses results from disjunctive programming to generate successively stronger linear relaxations of a discrete non convex problem
the class transduction problem as formulated by vapnik involves finding a separating hyperplane for a labelled data set that is also maximally distant from a given set of unlabelled test points in this form the problem has exponential computational complexity in the size of the working set so far it has been attacked by means of integer programming techniques that do not scale to reasonable problem sizes or by local search procedures in this paper we present a relaxation of this task based on semidefinite programming sdp resulting in a convex optimization problem that has polynomial complexity in the size of the data set the results are very encouraging for mid sized data sets however the cost is still too high for large scale problems due to the high dimensional search space to this end we restrict the feasible region by introducing an approximation based on solving an eigenproblem with this approximation the computational cost of the algorithm is such that problems with more than points can be treated
we propose a novel method of dimensionality reduction for supervised learning given a regression or classification problem in which we wish to predict a variable y from an explanatory vector x we treat the problem of dimensionality reduction as that of finding a low dimensional effective subspace of x which retains the statistical relationship between x and y we show that this problem can be formulated in terms of conditional independence to turn this formulation into an optimization problem we characterize the notion of conditional independence using covariance operators on reproducing kernel hilbert spaces this allows us to derive a contrast function for estimation of the effective subspace unlike many conventional methods the proposed method requires neither assumptions on the marginal distribution of x nor a parametric model of the conditional distribution of y
clustering aims at extracting hidden structure in dataset while the problem of finding compact clusters has been widely studied in the literature extracting arbitrarily formed elongated structures is considered a much harder problem in this paper we present a novel clustering algorithm which tackles the problem by a two step procedure first the data are transformed in such a way that elongated structures become compact ones in a second step these new objects are clustered by optimizing a compactness based criterion the advantages of the method over related approaches are threefold i robustness properties of compactness based criteria naturally transfer to the problem of extracting elongated structures leading to a model which is highly robust against outlier objects ii the transformed distances induce a mercer kernel which allows us hard clustering problem iii the new method does not contain free kernel to formulate a polynomial approximation scheme to the generally npparameters in contrast to methods like spectral clustering or mean shift clustering
a new feature extraction criterion maximum margin criterion mmc is proposed in this paper this new criterion is general in the sense that when combined with a suitable constraint it can actually give rise to the most popular feature extractor in the literature linear discriminate analysis lda we derive a new feature extractor based on mmc using a different constraint that does not depend on the nonsingularity of the within class scatter matrix sw such a dependence is a major drawback of lda especially when the sample size is small the kernelized nonlinear counterpart of this linear feature extractor is also established in this paper our preliminary experimental results on face images demonstrate that the new feature extractors are efficient and stable
the minimax probability machine classification mpmc framework lanckriet et al builds classifiers by minimizing the maximum probability of misclassification and gives direct estimates of the probabilistic accuracy bound the only assumptions that mpmc makes is that good estimates of means and covariance matrices of the classes exist however as with support vector machines mpmc is computationally expensive and requires extensive cross validation experiments to choose kernels and kernel parameters that give good performance in this paper we address the computational cost of mpmc by proposing an algorithm that constructs nonlinear sparse mpmc smpmc models by incrementally adding basis functions ie kernels one at a time greedily selecting the next one that maximizes the accuracy bound smpmc automatically chooses both kernel parameters and feature weights without using computationally expensive cross validation therefore the smpmc algorithm simultaneously addresses the problem of kernel selection and feature selection ie feature weighting based solely on maximizing the accuracy bound experimental results indicate that we can obtain reliable bounds as well as test set accuracies that are comparable to state of the art classification algorithms
we propose a method for sequential bayesian kernel regression as is the case for the popular relevance vector machine rvm the method automatically identifies the number and locations of the kernels our algorithm overcomes some of the computational difficulties related to batch methods for kernel regression it is non iterative and requires only a single pass over the data it is thus applicable to truly sequential data sets and batch data sets alike the algorithm is based on a generalisation of importance sampling which allows the design of intuitively simple and efficient proposal distributions for the model parameters comparative results on two standard data sets show our algorithm to compare favourably with existing batch estimation strategies
new feature selection algorithms for linear threshold functions are described which combine backward elimination with an adaptive regularization method this makes them particularly suitable to the classification of microarray expression data where the goal is to obtain accurate rules depending on few genes only our algorithms are fast and easy to implement since they center on an incremental large margin algorithm which allows us to avoid linear quadratic or higher order programming methods we report on preliminary experiments with five known dna microarray datasets these experiments suggest that multiplicative large margin algorithms tend to outperform additive algorithms such as svm on feature selection tasks
we consider the question of predicting nonlinear time series kernel dynamical modeling kdm a new method based on kernels is proposed as an extension to linear dynamical models the kernel trick is used twice first to learn the parameters of the model and second to compute preimages of the time series predicted in the feature space by means of support vector regression our model shows strong connection with the classic kalman filter model with the kernel feature space as hidden state space kernel dynamical modeling is tested against two benchmark time series and achieves high quality predictions
principal components analysis pca is one of the most widely used techniques in machine learning and data mining minor components analysis mca is less well known but can also play an important role in the presence of constraints on the data distribution in this paper we present a probabilistic model for extreme components analysis xca which at the maximum likelihood solution extracts an optimal combination of principal and minor components for a given number of components the log likelihood of the xca model is guaranteed to be larger or equal than that of the probabilistic models for pca and mca we describe an efficient algorithm to solve for the globally optimal solution for log convex spectra we prove that the solution consists of principal components only while for log concave spectra the solution consists of minor components in general the solution admits a combination of both in experiments we explore the properties of xca on some synthetic and real world datasets
we formulate linear dimensionality reduction as a semi parametric estimation problem enabling us to study its asymptotic behavior we generalize the problem beyond additive gaussian noise to unknown nongaussian additive noise and to unbiased non additive models
many problems in information processing involve some form of dimensionality reduction in this paper we introduce locality preserving projections lpp these are linear projective maps that arise by solving a variational problem that optimally preserves the neighborhood structure of the data set lpp should be seen as an alternative to principal component analysis pca a classical linear technique that projects the data along the directions of maximal variance when the high dimensional data lies on a low dimensional manifold embedded in the ambient space the locality preserving projections are obtained by finding the optimal linear approximations to the eigenfunctions of the laplace beltrami operator on the manifold as a result lpp shares many of the data representation properties of nonlinear techniques such as laplacian eigenmaps or locally linear embedding yet lpp is linear and more crucially is defined everywhere in ambient space rather than just on the training data points this is borne out by illustrative examples on some high dimensional data sets

the google search engine has enjoyed huge success with its web page ranking algorithm which exploits global rather than local hyperlink structure of the web using random walks here we propose a simple universal ranking algorithm for data lying in the euclidean space such as text or image data the core idea of our method is to rank the data with respect to the intrinsic manifold structure collectively revealed by a great amount of data encouraging experimental results from synthetic image and text data illustrate the validity of our method
several unsupervised learning algorithms based on an eigendecomposition provide either an embedding or a clustering only for given training points with no straightforward extension for out of sample examples short of recomputing eigenvectors this paper provides a unified framework for extending local linear embedding lle isomap laplacian eigenmaps multi dimensional scaling for dimensionality reduction as well as for spectral clustering this framework is based on seeing these algorithms as learning eigenfunctions of a data dependent kernel numerical experiments show that the generalizations performed have a level of error comparable to the variability of the embedding algorithms due to the choice of training data
significant progress in clustering has been achieved by algorithms that are based on pairwise affinities between the datapoints in particular spectral clustering methods have the advantage of being able to divide arbitrarily shaped clusters and are based on efficient eigenvector calculations however spectral methods lack a straightforward probabilistic interpretation which makes it difficult to automatically set parameters using training data in this paper we use the previously proposed typical cut framework for pairwise clustering we show an equivalence between calculating the typical cut and inference in an undirected graphical model we show that for clustering problems with hundreds of datapoints exact inference may still be possible for more complicated datasets we show that loopy belief propagation bp and generalized belief propagation gbp can give excellent results on challenging clustering problems we also use graphical models to derive a learning algorithm for affinity matrices based on labeled data
approximation structure plays an important role in inference on loopy graphs as a tractable structure tree approximations have been utilized in the variational method of ghahramani jordan and the sequential projection method of frey et al however belief propagation represents each factor of the graph with a product of single node messages in this paper belief propagation is extended to represent factors with tree approximations by way of the expectation propagation framework that is each factor sends a message to all pairs of nodes in a tree structure the result is more accurate inferences and more frequent convergence than ordinary belief propagation at a lower cost than variational trees or double loop algorithms
the maximisation of information transmission over noisy channels is a common albeit generally computationally difficult problem we approach the difficulty of computing the mutual information for noisy channels by using a variational approximation the resulting im algorithm is analagous to the em algorithm yet maximises mutual information as opposed to likelihood we apply the method to several practical examples including linear compression population encoding and cdma
the online incremental gradient or backpropagation algorithm is widely considered to be the fastest method for solving large scale neural network nn learning problems in contrast we show that an appropriately implemented iterative batch mode or block mode learning method can be much faster for example it is three times faster in the uci letter classification problem outputs data items parameters with a two hidden layer multilayer perceptron and times faster in a nonlinear regression problem arising in color recipe prediction outputs data items parameters with a neuro fuzzy modular network the three principal innovative ingredients in our algorithm are the following first we use scaled trust region regularization with inner outer iteration to solve the associated overdetermined nonlinear least squares problem where the inner iteration performs a truncated or inexact newton method second we employ pearlmutters implicit sparse hessian matrix vector multiply algorithm to construct the krylov subspaces used to solve for the truncated newton update third we exploit sparsity for preconditioning in the matrices resulting from the nns having many outputs
we consider situations where training data is abundant and computing resources are comparatively scarce we argue that suitably designed online learning algorithms asymptotically outperform any batch learning algorithm both theoretical and experimental evidences are presented
online algorithms for classification often require vast amounts of memory and computation time when employed in conjunction with kernel functions in this paper we describe and analyze a simple approach for an on the fly reduction of the number of past examples used for prediction experiments performed with real datasets show that using the proposed algorithmic approach with a single epoch is competitive with the support vector machine svm although the latter being a batch algorithm accesses each training example multiple times
this work presents an architecture based on perceptrons to recognize phrase structures and an online learning algorithm to train the perceptrons together and dependently the recognition strategy applies learning in two layers a filtering layer which reduces the search space by identifying plausible phrase candidates and a ranking layer which recursively builds the optimal phrase structure we provide a recognition based feedback rule which reflects to each local function its committed errors from a global point of view and allows to train them together online as perceptrons experimentation on a syntactic parsing problem the recognition of clause hierarchies improves state of the art results and evinces the advantages of our global training method over optimizing each function locally and independently
in this paper sparse representation factorization of a data matrix is first discussed an overcomplete basis matrix is estimated by using the k means method we have proved that for the estimated overcomplete basis matrix the sparse solution coefficient matrix with minimum l norm is unique with probability of one which can be obtained using a linear programming algorithm the comparisons of the l norm solution and the l norm solution are also presented which can be used in recoverability analysis of blind source separation bss next we apply the sparse matrix factorization approach to bss in the overcomplete case generally if the sources are not sufficiently sparse we perform blind separation in the time frequency domain after preprocessing the observed data using the wavelet packets transformation third an eeg experimental data analysis example is presented to illustrate the usefulness of the proposed approach and demonstrate its performance two almost independent components obtained by the sparse representation method are selected for phase synchronization analysis and their periods of significant phase synchronization are found which are related to tasks finally concluding remarks review the approach and state areas that require further study
recently relevance vector machines rvm have been fashioned from a sparse bayesian learning sbl framework to perform supervised learning using a weight prior that encourages sparsity of representation the methodology incorporates an additional set of hyperparameters governing the prior one for each weight and then adopts a specific approximation to the full marginalization over all weights and hyperparameters despite its empirical success however no rigorous motivation for this particular approximation is currently available to address this issue we demonstrate that sbl can be recast as the application of a rigorous variational approximation to the full model by expressing the prior in a dual form this formulation obviates the necessity of assuming any hyperpriors and leads to natural intuitive explanations of why sparsity is achieved in practice
we describe a nonparametric bayesian approach to generalizing from few labeled examples guided by a larger set of unlabeled objects and the assumption of a latent tree structure to the domain the tree or a distribution over trees may be inferred using the unlabeled data a prior over concepts generated by a mutation process on the inferred trees allows efficient computation of the optimal bayesian classification function from the labeled examples we test our approach on eight real world datasets
this paper is about non approximate acceleration of high dimensional nonparametric operations such as k nearest neighbor classifiers and the prediction phase of support vector machine classifiers we attempt to exploit the fact that even if we want exact answers to nonparametric queries we usually do not need to explicitly find the datapoints close to the query but merely need to ask questions about the properties about that set of datapoints this offers a small amount of computational leeway and we investigate how much that leeway can be exploited for clarity this paper concentrates on pure k nn classification and the prediction phase of svms we introduce new ball tree algorithms that on real world datasets give accelerations of fold up to fold compared against highly optimized traditional ball tree based k nn these results include datasets with up to dimensions and records and show non trivial speedups while giving exact answers
we introduce a class of nonstationary covariance functions for gaussian process gp regression nonstationary covariance functions allow the model to adapt to functions whose smoothness varies with the inputs the class includes a nonstationary version of the matrn stationary covariance in which the differentiability of the regression function is controlled by a parameter freeing one from fixing the differentiability in advance in experiments the nonstationary gp regression model performs well when the input space is two or three dimensions outperforming a neural network model and bayesian free knot spline models and competitive with a bayesian neural network but is outperformed in one dimension by a state of the art bayesian free knot spline model the model readily generalizes to non gaussian data use of computational methods for speeding gp fitting may allow for implementation of the method on larger datasets
when clustering a dataset the right number k of clusters to use is often not obvious and choosing k automatically is a hard algorithmic problem in this paper we present an improved algorithm for learning k while clustering the g means algorithm is based on a statistical test for the hypothesis that a subset of data follows a gaussian distribution g means runs k means with increasing k in a hierarchical fashion until the test accepts the hypothesis that the data assigned to each k means center are gaussian two key advantages are that the hypothesis test does not limit the covariance of the data and does not compute a full covariance matrix additionally g means only requires one intuitive parameter the standard statistical significance level we present results from experiments showing that the algorithm works well and better than a recent method based on the bic penalty for model complexity in these experiments we show that the bic is ineffective as a scoring function since it does not penalize strongly enough the models complexity
loopy belief propagation bp has been successfully used in a number of difficult graphical models to find the most probable configuration of the hidden variables in applications ranging from protein folding to image analysis one would like to find not just the best configuration but rather the top m while this problem has been solved using the junction tree formalism in many real world problems the clique size in the junction tree is prohibitively large in this work we address the problem of finding the m best configurations when exact inference is impossible we start by developing a new exact inference algorithm for calculating the best configurations that uses only max marginals for approximate inference we replace the max marginals with the beliefs calculated using max product bp and generalized bp we show empirically that the algorithm can accurately and rapidly approximate the m best configurations in graphs with hundreds of variables
we propose a non linear canonical correlation analysis cca method which works by coordinating or aligning mixtures of linear models in the same way that cca extends the idea of pca our work extends recent methods for non linear dimensionality reduction to the case where multiple embeddings of the same underlying low dimensional coordinates are observed each lying on a different high dimensional manifold we also show that a special case of our method when applied to only a single manifold reduces to the laplacian eigenmaps algorithm as with previous alignment schemes once the mixture models have been estimated all of the parameters of our model can be estimated in closed form without local optima in the learning experimental results illustrate the viability of the approach as a non linear extension of cca
spectral clustering refers to a class of techniques which rely on the eigenstructure of a similarity matrix to partition points into disjoint clusters with points in the same cluster having high similarity and points in different clusters having low similarity in this paper we derive a new cost function for spectral clustering based on a measure of error between a given partition and a solution of the spectral relaxation of a minimum normalized cut problem minimizing this cost function with respect to the partition leads to a new spectral clustering algorithm minimizing with respect to the similarity matrix leads to an algorithm for learning the similarity matrix we develop a tractable approximation of our cost function that is based on the power method of computing eigenvectors

we consider the general problem of learning from labeled and unlabeled data which is often called semi supervised learning or transductive inference a principled approach to semi supervised learning is to design a classifying function which is sufficiently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled points we present a simple algorithm to obtain such a smooth solution our method yields encouraging experimental results on a number of classification problems and demonstrates effective use of unlabeled data
in this paper we introduce a new underlying probabilistic model for principal component analysis pca our formulation interprets pca as a particular gaussian process prior on a mapping from a latent space to the observed data space we show that if the priors covariance function constrains the mappings to be linear the model is equivalent to pca we then extend the model by considering less restrictive covariance functions which allow non linear mappings this more general gaussian process latent variable model gplvm is then evaluated as an approach to the visualisation of high dimensional data for three different data sets additionally our non linear algorithm can be further kernelised leading to twin kernel pca in which a mapping between feature spaces occurs
we generalise the gaussian process gp framework for regression by learning a nonlinear transformation of the gp outputs this allows for non gaussian processes and non gaussian noise the learning algorithm chooses a nonlinear transformation such that transformed data is well modelled by a gp this can be seen as including a preprocessing transformation as an integral part of the probabilistic modelling problem rather than as an ad hoc step we demonstrate on several real regression problems that learning the transformation can lead to significantly better performance than using a regular gp or a gp with a fixed transformation
a novel algorithm for actively trading stocks is presented while traditional universal algorithms and technical trading heuristics attempt to predict winners or trends our approach relies on predictable statistical relations between all pairs of stocks in the market our empirical results on historical markets provide strong evidence that this type of technical trading can beat the market and moreover can beat the best stock in the market in doing so we utilize a new idea for smoothing critical parameters in the context of expert learning
we discuss the integration of the expectation maximization em algorithm for maximum likelihood learning of bayesian networks with belief propagation algorithms for approximate inference speci cally we propose to combine the outer loop step of convergent belief propagation algorithms with the m step of the em algorithm this then yields an approximate em algorithm that is essentially still double loop with the important advantage of an inner loop that is guaranteed to converge simulations illustrate the merits of such an approach
belief propagation on cyclic graphs is an efficient algorithm for computing approximate marginal probability distributions over single nodes and neighboring nodes in the graph in this paper we propose two new algorithms for approximating joint probabilities of arbitrary pairs of nodes and prove a number of desirable properties that these estimates fulfill the first algorithm is a propagation algorithm which is shown to converge if belief propagation converges to a stable fixed point the second algorithm is based on matrix inversion experiments compare a number of competing methods
we present a new method for calculating approximate marginals for probability distributions defined by graphs with cycles based on a gaussian entropy bound combined with a semidefinite outer bound on the marginal polytope this combination leads to a log determinant maximization problem that can be solved by efficient interior point methods as with the bethe approximation and its generalizations the optimizing arguments of this problem can be taken as approximations to the exact marginals in contrast to bethekikuchi approaches our variational problem is strictly convex and so has a unique global optimum an additional desirable feature is that the value of the optimal solution is guaranteed to provide an upper bound on the log partition function in experimental trials the performance of the log determinant relaxation is comparable to or better than the sum product algorithm and by a substantial margin for certain problem classes finally the zero temperature limit of our log determinant relaxation recovers a class of well known semidefinite relaxations for integer programming eg
we consider the question of how well a given distribution can be approximated with probabilistic graphical models we introduce a new parameter effective treewidth that captures the degree of approximability as a tradeoff between the accuracy and the complexity of approximation we present a simple approach to analyzing achievable tradeoffs that exploits the threshold behavior of monotone graph properties and provide experimental results that support the approach
this paper addresses the problem of untangling hidden graphs from a set of noisy detections of undirected edges we present a model of the generation of the observed graph that includes degree based structure priors on the hidden graphs exact inference in the model is intractable we present an efficient approximate inference algorithm to compute edge appearance posteriors we evaluate our model and algorithm on a biological graph inference problem
we present an analysis of concentration of expectation phenomena in layered bayesian networks that use generalized linear models as the local conditional probabilities this framework encompasses a wide variety of probability distributions including both discrete and continuous random variables we utilize ideas from large deviation analysis and the delta method to devise and evaluate a class of approximate inference algorithms for layered bayesian networks that have superior asymptotic error bounds and very fast computation time
we describe a markov chain method for sampling from the distribution of the hidden state sequence in a non linear dynamical system given a sequence of observations this method updates all states in the sequence simultaneously using an embedded hidden markov model hmm an update begins with the creation of pools of candidate states at each time we then define an embedded hmm whose states are indexes within these pools using a forward backward dynamic programming algorithm we can efficiently choose a state sequence with the appropriate probabilities from the exponentially large number of state sequences that pass through states in these pools we illustrate the method in a simple one dimensional example and in an example showing how an embedded hmm can be used to in effect discretize the state space without any discretization error we also compare the embedded hmm to a particle smoother on a more substantial problem of inferring human motion from d traces of markers
in applying hidden markov models to the analysis of massive data streams it is often necessary to use an artificially reduced set of states this is due in large part to the fact that the basic hmm estimation algorithms have a quadratic dependence on the size of the state set we present algorithms that reduce this computational bottleneck to linear or near linear time when the states can be embedded in an underlying grid of parameters this type of state representation arises in many domains in particular we show an application to traffic analysis at a high volume web site
in models that define probabilities via energies maximum likelihood learning typically involves using markov chain monte carlo to sample from the models distribution if the markov chain is started at the data distribution learning often works well even if the chain is only run for a few time steps but if the data distribution contains modes separated by regions of very low density brief mcmc will not ensure that different modes have the correct relative energies because it cannot move particles from one mode to another we show how to improve brief mcmc by allowing long range moves that are suggested by the data distribution if the model is approximately correct these long range moves have a reasonable acceptance rate
raoblackwellization is an approximation technique for probabilistic inference that flexibly combines exact inference with sampling it is useful in models where conditioning on some of the variables leaves a simpler inference problem that can be solved tractably this paper presents sample propagation an efficient implementation of raoblackwellized approximate inference for a large class of models sample propagation tightly integrates sampling with message passing in a junction tree and is named for its simple appealing structure it walks the clusters of a junction tree sampling some of the current clusters variables and then passing a message to one of its neighbors we discuss the application of sample propagation to conditional gaussian inference problems such as switching linear dynamical systems
discrete fourier transforms and other related fourier methods have been practically implementable due to the fast fourier transform fft however there are many situations where doing fast fourier transforms without complete data would be desirable in this paper it is recognised that formulating the fft algorithm as a belief network allows suitable priors to be set for the fourier coefficients furthermore efficient generalised belief propagation methods between clusters of four nodes enable the fourier coefficients to be inferred and the missing data to be estimated in near to on log n time where n is the total of the given and missing data points this method is compared with a number of common approaches such as setting missing data to zero or to interpolation it is tested on generated data and for a fourier analysis of a damaged audio signal
we present a novel method for approximate inference in bayesian models and regularized risk functionals it is based on the propagation of mean and variance derived from the laplace approximation of conditional probabilities in factorizing distributions much akin to minkas expectation propagation in the jointly normal case it coincides with the latter and belief propagation whereas in the general case it provides an optimization strategy containing support vector chunking the bayes committee machine and gaussian process chunking as special cases
we consider the problem of reconstructing patterns from a feature map learning algorithms using kernels to operate in a reproducing kernel hilbert space rkhs express their solutions in terms of input points mapped into the rkhs we introduce a technique based on kernel principal component analysis and regression to reconstruct corresponding patterns in the input space aka pre images and review its performance in several applications requiring the construction of pre images the introduced technique avoids difficult andor unstable numerical optimization is easy to implement and unlike previous methods permits the computation of pre images in discrete input spaces
we present a modified version of the perceptron learning algorithm pla which solves semidefinite programs sdps in polynomial time the algorithm is based on the following three observations i semidefinite programs are linear programs with infinitely many linear constraints ii every linear program can be solved by a sequence of constraint satisfaction problems with linear constraints iii in general the perceptron learning algorithm solves a constraint satisfaction problem with linear constraints in finitely many updates combining the pla with a probabilistic rescaling algorithm which on average increases the size of the feasable region results in a probabilistic algorithm for solving sdps that runs in polynomial time we present preliminary results which demonstrate that the algorithm works but is not competitive with state of the art interior point methods
density estimation with gaussian mixture models is a popular generative technique used also for clustering we develop a framework to incorporate side information in the form of equivalence constraints into the model estimation procedure equivalence constraints are defined on pairs of data points indicating whether the points arise from the same source positive constraints or from different sources negative constraints such constraints can be gathered automatically in some learning problems and are a natural form of supervision in others for the estimation of model parameters we present a closed form em procedure which handles positive constraints and a generalized em procedure using a markov net which handles negative constraints using publicly available data sets we demonstrate that such side information can lead to considerable improvement in clustering tasks and that our algorithm is preferable to two other suggested methods using the same type of side information
a novel approach to combining clustering and feature selection is presented it implements a wrapper strategy for feature selection in the sense that the features are directly selected by optimizing the discriminative power of the used partitioning algorithm on the technical side we present an efficient optimization algorithm with guaranteed local convergence property the only free parameter of this method is selected by a resampling based stability analysis experiments with real world datasets demonstrate that our method is able to infer both meaningful partitions and meaningful subsets of features
we describe a procedure which finds a hierarchical clustering by hillclimbing the cost function we use is a hierarchical extension of the k means cost our local moves are tree restructurings and node reorderings we show these can be accomplished efficiently by exploiting special properties of squared euclidean distances and by using techniques from scheduling algorithms
we propose an information theoretic clustering approach that incorporates a pre known partition of the data aiming to identify common clusters that cut across the given partition in the standard clustering setting the formation of clusters is guided by a single source of feature information the newly utilized pre partition factor introduces an additional bias that counterbalances the impact of the features whenever they become correlated with this known partition the resulting algorithmic framework was applied successfully to synthetic data as well as to identifying text based cross religion correspondences
label ranking is the task of inferring a total order over a predefined set of labels for each given instance we present a general framework for batch learning of label ranking functions from supervised data we assume that each instance in the training data is associated with a list of preferences over the label set however we do not assume that this list is either complete or consistent this enables us to accommodate a variety of ranking problems in contrast to the general form of the supervision our goal is to learn a ranking function that induces a total order over the entire set of labels special cases of our setting are multilabel categorization and hierarchical classification we present a general boosting based learning algorithm for the label ranking problem and prove a lower bound on the progress of each boosting iteration the applicability of our approach is demonstrated with a set of experiments on a large scale text corpus

most machine learning researchers perform quantitative experiments to estimate generalization error and compare algorithm performances in order to draw statistically convincing conclusions it is important to estimate the uncertainty of such estimates this paper studies the estimation of uncertainty around the k fold cross validation estimator the main theorem shows that there exists no universal unbiased estimator of the variance of k fold cross validation an analysis based on the eigendecomposition of the covariance matrix of errors helps to better understand the nature of the problem and shows that naive estimators may grossly underestimate variance as conrmed by numerical experiments
the bootstrap has become a popular method for exploring model structure uncertainty our experiments with artificial and realworld data demonstrate that the graphs learned from bootstrap samples can be severely biased towards too complex graphical models accounting for this bias is hence essential eg when exploring model uncertainty we find that this bias is intimately tied to well known spurious dependences induced by the bootstrap the leading order bias correction equals one half of akaikes penalty for model complexity we demonstrate the effect of this simple bias correction in our experiments we also relate this bias to the bias of the plug in estimator for entropy as well as to the difference between the expected test and training errors of a graphical model which asymptotically equals akaikes penalty rather than one half
pairwise coupling is a popular multi class classification method that combines together all pairwise comparisons for each pair of classes this paper presents two approaches for obtaining class probabilities both methods can be reduced to linear systems and are easy to implement we show conceptually and experimentally that the proposed approaches are more stable than two existing popular methods voting and
in pattern classification tasks errors are introduced because of differences between the true model and the one obtained via model estimation using likelihood ratio based classification it is possible to correct for this discrepancy by finding class pair specific terms to adjust the likelihood ratio directly and that can make class pair preference relationships intransitive in this work we introduce new methodology that makes necessary corrections to the likelihood ratio specifically those that are necessary to achieve perfect classification but not perfect likelihood ratio correction which can be overkill the new corrections while weaker than previously reported such adjustments are analytically challenging since they involve discontinuous functions therefore requiring several approximations we test a number of these new schemes on an isolatedword speech recognition task as well as on the uci machine learning data sets results show that by using the bias terms calculated in this new way classification accuracy can substantially improve over both the baseline and over our previous results
although discriminatively trained classifiers are usually more accurate when labeled training data is abundant previous work has shown that when training data is limited generative classifiers can out perform them this paper describes a hybrid model in which a high dimensional subset of the parameters are trained to maximize generative likelihood and another small subset of parameters are discriminatively trained to maximize conditional likelihood we give a sample complexity bound showing that in order to fit the discriminative parameters well the number of training examples required depends only on the logarithm of the number of feature occurrences and feature set size experimental results show that hybrid models can provide lower test error and can produce better accuracycoverage curves than either their purely generative or purely discriminative counterparts we also discuss several advantages of hybrid models and advocate further work in this area
we propose an approach to learning the semantics of images which allows us to automatically annotate an image with keywords and to retrieve images based on text queries we do this using a formalism that models the generation of annotated images we assume that every image is divided into regions each described by a continuous valued feature vector given a training set of images with annotations we compute a joint probabilistic model of image features and words which allow us to predict the probability of generating a word given the image regions this may be used to automatically annotate and retrieve images given a word as a query experiments show that our model significantly outperforms the best of the previously reported results on the tasks of automatic image annotation and retrieval

this paper applies fast sparse multidimensional scaling mds to a large graph of music similarity with k vertices that represent artists albums and tracks and m edges that represent similarity between those entities once vertices are assigned locations in a euclidean space the locations can be used to browse music and to generate playlists mds on very large sparse graphs can be effectively performed by a family of algorithms called rectangular dijsktra rd mds algorithms these rd algorithms operate on a dense rectangular slice of the distance matrix created by calling dijsktra a constant number of times two rd algorithms are compared landmark mds which uses the nystrm approximation to perform mds and a new algorithm called fast sparse embedding which uses fastmap these algorithms compare favorably to laplacian eigenmaps both in terms of speed and embedding quality
in this article we present a novel approach to solving the localization problem in cellular networks the goal is to estimate a mobile users position based on measurements of the signal strengths received from network base stations our solution works by building gaussian process models for the distribution of signal strengths as obtained in a series of calibration measurements in the localization stage the users position can be estimated by maximizing the likelihood of received signal strengths with respect to the position we investigate the accuracy of the proposed approach on data obtained within a large indoor cellular network
we present the software architecture of a robotic system for mapping abandoned mines the software is capable of acquiring consistent d maps of large mines with many cycles represented as markov random elds d c space maps are acquired from local d range scans which are used to identify navigable paths using a search our system has been deployed in three abandoned mines two of which inaccessible to people where it has acquired maps of unprecedented detail and accuracy
a key issue in supervised protein classification is the representation of input sequences of amino acids recent work using string kernels for protein data has achieved state of the art classification performance however such representations are based only on labeled data examples with known d structures organized into structural classes while in practice unlabeled data is far more plentiful in this work we develop simple and scalable cluster kernel techniques for incorporating unlabeled data into the representation of protein sequences we show that our methods greatly improve the classification performance of string kernels and outperform standard approaches for using unlabeled data such as adding close homologs of the positive examples to the training data we achieve equal or superior performance to previously presented cluster kernel methods while achieving far greater computational efficiency
we present a novel strategy for automatically debugging programs given sampled data from thousands of actual user runs our goal is to pinpoint those features that are most correlated with crashes this is accomplished by maximizing an appropriately defined utility function it has analogies with intuitive debugging heuristics and as we demonstrate is able to deal with various types of bugs that occur in real programs
we examine the use of hidden markov and hidden semi markov models for automatically segmenting an electrocardiogram waveform into its constituent waveform features an undecimated wavelet transform is used to generate an overcomplete representation of the signal that is more appropriate for subsequent modelling we show that the state durations implicit in a standard hidden markov model are ill suited to those of real ecg features and we investigate the use of hidden semi markov models for improved state duration modelling
as part of an environmental observation and forecasting system sensors deployed in the columbia river estuary corie gather information on physical dynamics and changes in estuary habitat of these salinity sensors are particularly susceptible to biofouling which gradually degrades sensor response and corrupts critical data automatic fault detectors have the capability to identify bio fouling early and minimize data loss complicating the development of discriminatory classifiers is the scarcity of bio fouling onset examples and the variability of the bio fouling signature to solve these problems we take a novelty detection approach that incorporates a parameterized bio fouling model these detectors identify the occurrence of bio fouling and its onset time as reliably as human experts real time detectors installed during the summer of produced no false alarms yet detected all episodes of sensor degradation before the field staff scheduled these sensors for cleaning from this initial deployment through february our bio fouling detectors have essentially doubled the amount of useful data coming from the corie sensors
in this paper we present a generative latent variable model for rating based collaborative filtering called the user rating profile model urp the generative process which underlies urp is designed to produce complete user rating profiles an assignment of one rating to each item for each user our model represents each user as a mixture of user attitudes and the mixing proportions are distributed according to a dirichlet random variable the rating for each item is generated by selecting a user attitude for the item and then selecting a rating according to the preference pattern associated with that attitude urp is related to several models including a multinomial mixture model the aspect model and lda but has clear advantages over each
this article addresses the issues of colour classification and collision detection as they occur in the legged league robot soccer environment of robocup we show how the method of one class classification with support vector machines svms can be applied to solve these tasks satisfactorily using the limited hardware capacity of the prescribed sony aibo quadruped robots the experimental evaluation shows an improvement over our previous methods of ellipse fitting for colour classification and the statistical approach used for collision detection
this paper devises a novel kernel function for structured natural language data in the field of natural language processing feature extraction consists of the following two steps syntactically and semantically analyzing raw data ie character strings then representing the results as discrete structures such as parse trees and dependency graphs with part of speech tags creating possibly high dimensional numerical feature vectors from the discrete structures the new kernels called hierarchical directed acyclic graph hdag kernels directly accept dags whose nodes can contain dags hdag data structures are needed to fully reflect the syntactic and semantic structures that natural language data inherently have in this paper we define the kernel function and show how it permits efficient calculation experiments demonstrate that the proposed kernels are superior to existing kernel functions eg sequence kernels tree kernels and bag of words kernels
given an n n grid of squares where each square has a count and an underlying population our goal is to find the square region with the highest density and to calculate its significance by randomization any density measure d dependent on the total count and total population of a region can be used for example if each count represents the number of disease cases occurring in that square we can use kulldorffs spatial scan statistic dk to find the most significant spatial disease cluster a naive approach to finding the maximum density region requires on time and is generally computationally infeasible we present a novel algorithm which partitions the grid into overlapping regions bounds the maximum score of subregions contained in each region and prunes regions which cannot contain the maximum density region for sufficiently dense regions this method finds the maximum density region in optimal on time in practice resulting in significant x speedups
many real world domains are relational in nature consisting of a set of objects related to each other in complex ways this paper focuses on predicting the existence and the type of links between entities in such domains we apply the relational markov network framework of taskar et al to define a joint probabilistic model over the entire link graph entity attributes and links the application of the rmn algorithm to this task requires the definition of probabilistic patterns over subgraph structures we apply this method to two new relational datasets one involving university webpages and the other a social network we show that the collective classification approach of rmns and the
accurate spectral decomposition is essential for the analysis and diagnosis of histologically stained tissue sections in this paper we present the first automated system for performing this decomposition we compare the performance of our system with ground truth data and report favorable results
we propose an unsupervised methodology using independent component analysis ica to cluster genes from dna microarray data based on an ica mixture model of genomic expression patterns linear and nonlinear ica finds components that are specific to certain biological processes genes that exhibit significant up regulation or down regulation within each component are grouped into clusters we test the statistical significance of enrichment of gene annotations within each cluster ica based clustering outperformed other leading methods in constructing functionally coherent clusters on various datasets this result supports our model of genomic expression data as composite effect of independent biological processes comparison of clustering performance among various ica algorithms including a kernel based nonlinear ica algorithm shows that nonlinear ica performed the best for small datasets and natural gradient maximization likelihood worked well for all the datasets
we propose a functional mixture model for simultaneous clustering and alignment of sets of curves measured on a discrete time grid the model is specifically tailored to gene expression time course data each functional cluster center is a nonlinear combination of solutions of a simple linear differential equation that describes the change of individual mrna levels when the synthesis and decay rates are constant the mixture of continuous time parametric functional forms allows one to a account for the heterogeneity in the observed profiles b align the profiles in time by estimating real valued time shifts c capture the synthesis and decay of mrna in the course of an experiment and d regularize noisy profiles by enforcing smoothness in the mean curves we derive an em algorithm for estimating the parameters of the model and apply the proposed approach to the set of cycling genes in yeast the experiments show consistent improvement in predictive power and within cluster variance compared to regular gaussian mixtures
existing source location and recovery algorithms used in magnetoencephalographic imaging generally assume that the source activity at different brain locations is independent or that the correlation structure is known however electrophysiological recordings of local field potentials show strong correlations in aggregate activity over significant distances indeed it seems very likely that stimulus evoked activity would follow strongly correlated time courses in different brain areas here we present and validate through simulations a new approach to source reconstruction in which the correlation between sources is modelled and estimated explicitly by variational bayesian methods facilitating accurate recovery of source locations and the time courses of their activation
to understand the brain mechanisms involved in reward prediction on different time scales we developed a markov decision task that requires prediction of both immediate and future rewards and analyzed subjects brain activities using functional mri we estimated the time course of reward prediction and reward prediction error on different time scales from subjects performance data and used them as the explanatory variables for spm analysis we found topographic maps of different time scales in medial frontal cortex and striatum the result suggests that different cortico basal ganglia loops are specialized for reward prediction on different time scales
we consider learning to classify cognitive states of human subjects based on their brain activity observed via functional magnetic resonance imaging fmri this problem is important because such classifiers constitute virtual sensors of hidden cognitive states which may be useful in cognitive science research and clinical applications in recent work mitchell et al have demonstrated the feasibility of training such classifiers for individual human subjects eg to distinguish whether the subject is reading an ambiguous or unambiguous sentence or whether they are reading a noun or a verb here we extend that line of research exploring how to train classifiers that can be applied across multiple human subjects including subjects who were not involved in training the classifier we describe the design of several machine learning approaches to training multiple subject classifiers and report experimental results demonstrating the success of these methods in learning cross subject classifiers for two different fmri data sets
nonlinear filtering can solve very complex problems but typically involve very time consuming calculations here we show that for filters that are constructed as a rbf network with gaussian basis functions a decomposition into linear filters exists which can be computed efficiently in the frequency domain yielding dramatic improvement in speed we present an application of this idea to image processing in electron micrograph images of photoreceptor terminals of the fruit fly drosophila synaptic vesicles containing neurotransmitter should be detected and labeled automatically we use hand labels provided by human experts to learn a rbf filter using support vector regression with gaussian kernels we will show that the resulting nonlinear filter solves the task to a degree of accuracy which is close to what can be achieved by human experts this allows the very time consuming task of data evaluation to be done efficiently
this paper presents an energy normalization transform as a method to reduce system errors in the lf asd brain computer interface the energy normalization transform has two major benefits to the system performance first it can increase class separation between the active and idle eeg data second it can desensitize the system to the signal amplitude variability for four subjects in the study the benefits resulted in the performance improvement of the lf asd in the range from to while for the fifth subject who had the highest non normalized accuracy of the performance did not change notably with normalization
brain computer interfaces bci are an interesting emerging technology that is driven by the motivation to develop an effective communication interface translating human intentions into a control signal for devices like computers or neuroprostheses if this can be done bypassing the usual human output pathways like peripheral nerves and muscles it can ultimately become a valuable tool for paralyzed patients most activity in bci research is devoted to finding suitable features and algorithms to increase information transfer rates itrs the present paper studies the implications of using more classes eg left vs right hand vs foot for operating a bci we contribute by a theoretical study showing under some mild assumptions that it is practically not useful to employ more than three or four classes two extensions of the common spatial pattern csp algorithm one interestingly based on simultaneous diagonalization and controlled eeg experiments that underline our theoretical findings and show excellent improved itrs
we describe a system that localizes a single dipole to reasonable accuracy from noisy magnetoencephalographic meg measurements in real time at its core is a multilayer perceptron mlp trained to map sensor signals and head position to dipole location including head position overcomes the previous need to retrain the mlp for each subject and session the training dataset was generated by mapping randomly chosen dipoles and head positions through an analytic model and adding noise from real meg recordings after training a localization took ms with an average error of cm a few iterations of a levenberg marquardt routine using the mlps output as its initial guess took ms and improved the accuracy to cm only slightly above the statistical limits on accuracy imposed by the noise we applied these methods to localize single dipole sources from meg components isolated by blind source separation and compared the estimated locations to those generated by standard manually assisted commercial software
we exploit some useful properties of gaussian process gp regression models for reinforcement learning in continuous state spaces and discrete time we demonstrate how the gp model allows evaluation of the value function in closed form the resulting policy iteration algorithm is demonstrated on a simple problem with a two dimensional state space further we speculate that the intrinsic ability of gp models to characterise distributions of functions would allow the method to capture entire distributionsoverfuturevalues instead of merelytheir expectationwhich has traditionally been the focus of much of reinforcement learning
recent developments in grid based and point based approximation algorithms for pomdps have greatly improved the tractability of pomdp planning these approaches operate on sets of belief points by individually learning a value function for each point in reality belief points exist in a highly structured metric simplex but current pomdp algorithms do not exploit this property this paper presents a new metric tree algorithm which can be used in the context of pomdp planning to sort belief points spatially and then perform fast value function updates over groups of points we present results showing that this approach can reduce computation in point based pomdp algorithms for a wide range of problems
in real world planning problems time for deliberation is often limited anytime planners are well suited for these problems they find a feasible solution quickly and then continually work on improving it until time runs out in this paper we propose an anytime heuristic search ara which tunes its performance bound based on available search time it starts by finding a suboptimal solution quickly using a loose bound then tightens the bound progressively as time allows given enough time it finds a provably optimal solution while improving its bound ara reuses previous search efforts and as a result is significantly more efficient than other anytime search methods in addition to our theoretical analysis we demonstrate the practical utility of ara with experiments on a simulated robot kinematic arm and a dynamic path planning problem for an outdoor rover
recent research has demonstrated that useful pomdp solutions do not require consideration of the entire belief space we extend this idea with the notion of temporal abstraction we present and explore a new reinforcement learning algorithm over grid points in belief space which uses macro actions and monte carlo updates of the q values we apply the algorithm to a large scale robot navigation task and demonstrate that with temporal abstraction we can consider an even smaller part of the belief space we can learn pomdp policies faster and we can do information gathering more efficiently
a mobile robot acting in the world is faced with a large amount of sensory data and uncertainty in its action outcomes indeed almost all interesting sequential decision making domains involve large state spaces and large stochastic action sets we investigate a way to act intelligently as quickly as possible in domains where finding a complete policy would take a hopelessly long time this approach relational envelopebased planning rebp tackles large noisy problems along two axes first describing a domain as a relational mdp instead of as an atomic or propositionally factored mdp allows problem structure and dynamics to be captured compactly with a small set of probabilistic relational rules second an envelope based approach to planning lets an agent begin acting quickly within a restricted part of the full state space and to judiciously expand its envelope as resources permit
online mechanism design md considers the problem of providing incentives to implement desired system wide outcomes in systems with self interested agents that arrive and depart dynamically agents can choose to misrepresent their arrival and departure times in addition to information about their value for different outcomes we consider the problem of maximizing the total longterm value of the system despite the self interest of agents the online md problem induces a markov decision process mdp which when solved can be used to implement optimal policies in a truth revealing bayesian nash equilibrium
autonomous helicopter flight represents a challenging control problem with complex noisy dynamics in this paper we describe a successful application of reinforcement learning to autonomous helicopter flight we first fit a stochastic nonlinear model of the helicopter dynamics we then use the model to learn to hover in place and to fly a number of maneuvers taken from an rc helicopter competition
in large multiagent games partial observability coordination and credit assignment persistently plague attempts to design good learning algorithms we provide a simple and efficient algorithm that in part uses a linear system to model the world from a single agents limited perspective and takes advantage of kalman filtering to allow an agent to construct a good training signal and learn an effective policy
the so called experts algorithms constitute a methodology for choosing actions repeatedly when the rewards depend both on the choice of action and on the unknown current state of the environment an experts algorithm has access to a set of strategies experts each of which may recommend which action to choose the algorithm learns how to combine the recommendations of individual experts so that in the long run for any fixed sequence of states of the environment it does as well as the best expert would have done relative to the same sequence this methodology may not be suitable for situations where the evolution of states of the environment depends on past chosen actions as is usually the case for example in a repeated non zero sum game a new experts algorithm is presented and analyzed in the context of repeated games it is shown that asymptotically under certain conditions it performs as well as the best available expert this algorithm is quite different from previously proposed experts algorithms it represents a shift from the paradigms of regret minimization and myopic optimization to consideration of the long term effect of a players actions on the opponents actions or the environment the importance of this shift is demonstrated by the fact that this algorithm is capable of inducing cooperation in the repeated prisoners dilemma game whereas previous experts algorithms converge to the suboptimal non cooperative play
we describe a new approximation algorithm for solving partially observable mdps our bounded policy iteration approach searches through the space of bounded size stochastic finite state controllers combining several advantages of gradient ascent efficiency search through restricted controller space and policy iteration less vulnerability to local optima
we consider the policy search approach to reinforcement learning we show that if a baseline distribution is given indicating roughly how often we expect a good policy to visit each state then we can derive a policy search algorithm that terminates in a finite number of steps and for which we can provide non trivial performance guarantees we also demonstrate this algorithm on several grid world pomdps a planar biped walking robot and a double pole balancing problem
optimal solutions to markov decision problems mdps are very sensitive with respect to the state transition probabilities in many practical problems the estimation of those probabilities is far from accurate hence estimation errors are limiting factors in applying mdps to realworld problems we propose an algorithm for solving finite state and finite action mdps where the solution is guaranteed to be robust with respect to estimation errors on the state transition probabilities our algorithm involves a statistically accurate yet numerically efficient representation of uncertainty via kullback leibler divergence bounds the worst case complexity of the robust algorithm is the same as the original bellman recursion hence robustness can be added at practically no extra computing cost
we explore approximate policy iteration replacing the usual costfunction learning step with a learning step in policy space we give policy language biases that enable solution of very large relational markov decision processes mdps that no previous technique can solve in particular we induce high quality domain specific planners for classical planning domains both deterministic and stochastic variants by solving such domains as extremely large mdps
predictive state representations psrs use predictions of a set of tests to represent the state of controlled dynamical systems one reason why this representation is exciting as an alternative to partially observable markov decision processes pomdps is that psr models of dynamical systems may be much more compact than pomdp models empirical work on psrs to date has focused on linear psrs which have not allowed for compression relative to pomdps we introduce a new notion of tests which allows us to define a new type of psr that is nonlinear in general and allows for exponential compression in some deterministic dynamical systems these new tests called e tests are related to the tests used by rivest and schapire in their work with the diversity representation but our psr avoids some of the pitfalls of their representation in particular its potential to be exponentially larger than the equivalent pomdp
we study how to learn to play a pareto optimal strict nash equilibrium when there exist multiple equilibria and agents may have different preferences among the equilibria we focus on repeated coordination games of non identical interest where agents do not know the game structure up front and receive noisy payoffs we design efficient near optimal algorithms for both the perfect monitoring and the imperfect monitoring settingwhere the agents only observe their own payoffs and the joint actions
recent multi agent extensions of q learning require knowledge of other agents payoffs and q functions and assume game theoretic play at all times by all other agents this paper proposes a fundamentally different approach dubbed hyper q learning in which values of mixed strategies rather than base actions are learned and in which other agents strategies are estimated from observed actions via bayesian inference hyper q may be effective against many different types of adaptive agents even if they are persistently dynamic against certain broad categories of adaptation it is argued that hyper q may converge to exact optimal time varying policies in tests using rock paper scissors hyper q learns to significantly exploit an infinitesimal gradient ascent iga player as well as a policy hill climber phc player preliminary analysis of hyper q against itself is also presented
the design of cooperative multi robot systems is a highly active research area in robotics two lines of research in particular have generated interest the solution of large weakly coupled mdps and the design and implementation of market architectures we propose a new algorithm which joins together these two lines of research for a class of coupled mdps our algorithm automatically designs a market architecture which causes a decentralized multi robot system to converge to a consistent policy we can show that this policy is the same as the one which would be produced by a particular centralized planning algorithm we demonstrate the new algorithm on three simulation examples multi robot towing multi robot path planning with a limited fuel resource and coordinating behaviors in a game of paint ball
we develop a protocol for optimizing dynamic behavior of a network of simple electronic components such as a sensor network an ad hoc network of mobile devices or a network of communication switches this protocol requires only local communication and simple computations which are distributed among devices the protocol is scalable to large networks as a motivating example we discuss a problem involving optimization of power consumption delay and buffer overflow in a sensor network our approach builds on policy gradient methods for optimization of markov decision processes the protocol can be viewed as an extension of policy gradient methods to a context involving a team of agents optimizing aggregate performance through asynchronous distributed communication and computation we establish that the dynamics of the protocol approximate the solution to an ordinary differential equation that follows the gradient of the performance objective
approximate linear programming alp has emerged recently as one of the most promising methods for solving complex factored mdps with finite state spaces in this work we show that alp solutions are not limited only to mdps with finite state spaces but that they can also be applied successfully to factored continuous state mdps cmdps we show how one can build an alp based approximation for such a model and contrast it to existing solution methods we argue that this approach offers a robust alternative for solving high dimensional continuous state space problems the point is supported by experiments on three cmdp problems with continuous state factors
we attempt to understand visual classification in humans using both psychophysical and machine learning techniques frontal views of human faces were used for a gender classification task human subjects classified the faces and their gender judgment reaction time and confidence rating were recorded several hyperplane learning algorithms were used on the same classification task using the principal components of the texture and shape representation of the faces the classification performance of the learning algorithms was estimated using the face database with the true gender of the faces as labels and also with the gender estimated by the subjects we then correlated the human responses to the distance of the stimuli to the separating hyperplane of the learning algorithms our results suggest that human classification can be modeled by some hyperplane algorithms in the feature space we used for classification the brain needs more processing for stimuli close to that hyperplane than for those further away
why are sensory modalities segregated the way they are in this paper we show that sensory modalities are well designed for self supervised cross modal learning using the minimizing disagreement algorithm on an unsupervised speech categorization task with visual moving lips and auditory sound signal inputs we show that very informative auditory dimensions actually harm performance when moved to the visual side of the network it is better to throw them away than to consider them part of the visual input we explain this finding in terms of the statistical structure in sensory inputs
we show that temporal logic and combinations of temporal logics and modal logics of knowledge can be effectively represented in artificial neural networks we present a translation algorithm from temporal rules to neural networks and show that the networks compute a fixed point semantics of the rules we also apply the translation to the muddy children puzzle which has been used as a testbed for distributed multi agent systems we provide a complete solution to the puzzle with the use of simple neural networks capable of reasoning about time and of knowledge acquisition through inductive learning
we present a connectionist architecture that can learn a model of the relations between perceptions and actions and use this model for behavior planning state representations are learned with a growing selforganizing layer which is directly coupled to a perception and a motor layer knowledge about possible state transitions is encoded in the lateral connectivity motor signals modulate this lateral connectivity and a dynamic field on the layer organizes a planning process all mechanisms are local and adaptation is based on hebbian ideas the model is continuous in the action perception and time domain
despite the popularity of connectionist models in cognitive science their performance can often be difficult to evaluate inspired by the geometric approach to statistical model selection we introduce a conceptually similar method to examine the global behavior of a connectionist model by counting the number and types of response patterns it can simulate the markov chain monte carlo based algorithm that we constructed nds these patterns efficiently we demonstrate the approach using two localist network models of speech perception
is there a way for an algorithm linked to an unknown body to infer by itself information about this body and the world it is in taking the case of space for example is there a way for this algorithm to realize that its body is in a three dimensional world is it possible for this algorithm to discover how to move in a straight line and more basically do these questions make any sense at all given that the algorithm only has access to the very high dimensional data consisting of its sensory inputs and motor outputs we demonstrate in this article how these questions can be given a positive answer we show that it is possible to make an algorithm that by analyzing the law that links its motor outputs to its sensory inputs discovers information about the structure of the world regardless of the devices constituting the body it is linked to we present results from simulations demonstrating a way to issue motor orders resulting in fundamental movements of the body as regards the structure of the physical world
we explore the phenomena of subjective randomness as a case study in understanding how people discover structure embedded in noise we present a rational account of randomness perception based on the statistical problem of model selection given a stimulus inferring whether the process that generated it was random or regular inspired by the mathematical definition of randomness given by kolmogorov complexity we characterize regularity in terms of a hierarchy of automata that augment a finite controller with different forms of memory we find that the regularities detected in binary sequences depend upon presentation format and that the kinds of automata that can identify these regularities are informative about the cognitive processes engaged by different formats
we describe a pattern acquisition algorithm that learns in an unsupervised fashion a streamlined representation of linguistic structures from a plain natural language corpus this paper addresses the issues of learning structured knowledge from a large scale natural language data set and of generalization to unseen text the implemented algorithm represents sentences as paths on a graph whose vertices are words or parts of words significant patterns determined by recursive context sensitive statistical inference form new vertices linguistic constructions are represented by trees composed of significant patterns and their associated equivalence classes an input module allows the algorithm to be subjected to a standard test of english as a second language esl proficiency the results are encouraging the model attains a level of performance considered to be intermediate for th grade students despite having been trained on a corpus childes containing transcribed speech of parents directed to small children
we present a novel connectionist model for acquiring the semantics of a simple language through the behavioral experiences of a real robot we focus on the compositionality of semantics a fundamental characteristic of human language which is the ability to understand the meaning of a sentence as a combination of the meanings of words we also pay much attention to the embodiment of a robot which means that the robot should acquire semantics which matches its body or sensory motor system the essential claim is that an embodied compositional semantic representation can be self organized from generalized correspondences between sentences and behavioral patterns this claim is examined and confirmed through simple experiments in which a robot generates corresponding behaviors from unlearned sentences by analogy with the correspondences between learned sentences and behaviors
we develop a framework based on bayesian model averaging to explain how animals cope with uncertainty about contingencies in classical conditioning experiments traditional accounts of conditioning fit parameters within a fixed generative model of reinforcer delivery uncertainty over the model structure is not considered we apply the theory to explain the puzzling relationship between second order conditioning and conditioned inhibition two similar conditioning regimes that nonetheless result in strongly divergent behavioral outcomes according to the theory second order conditioning results when limited experience leads animals to prefer a simpler world model that produces spurious correlations conditioned inhibition results when a more complex model is justified by additional experience
we have designed and tested a single chip analog vlsi sensor that detects imminent collisions by measuring radially expansive optic flow the design of the chip is based on a model proposed to explain leg extension behavior in flies during landing approaches a new elementary motion detector emd circuit was developed to measure optic flow this emd circuit models the bandpass nature of large monopolar cells lmcs immediately postsynaptic to photoreceptors in the fly visual system a array of d motion detectors was fabricated on a mm mm die in a standard m cmos process the chip consumes w of power from a v supply with the addition of wide angle optics the sensor is able to detect collisions around ms before impact in complex real world scenes

synapses are a critical element of biologically realistic spike based neural computation serving the role of communication computation and modification many different circuit implementations of synapse function exist with different computational goals in mind in this paper we describe a new cmos synapse design that separately controls quiescent leak current synaptic gain and time constant of decay this circuit implements part of a commonly used kinetic model of synaptic conductance we show a theoretical analysis and experimental data for prototypes fabricated in a commercially available m cmos process
this paper presents vlsi circuits with continuous valued probabilistic behaviour realized by injecting noise into each computing unitneuron interconnecting the noisy neurons forms a continuous restricted boltzmann machine crbm which has shown promising performance in modelling and classifying noisy biomedical data the minimising contrastive divergence learning algorithm for crbm is also implemented in mixed mode vlsi to adapt the noisy neurons parameters on chip
most proposals for quantum neural networks have skipped over the problem of how to train the networks the mechanics of quantum computing are different enough from classical computing that the issue of training should be treated in detail we propose a simple quantum neural network and a training method for it it can be shown that this algorithm works in quantum systems results on several real world data sets show that this algorithm can train the proposed quantum neural networks and that it has some advantages over classical learning algorithms
we present test results from spike timing correlation learning experiments carried out with silicon neurons with stdp spike timing dependent plasticity synapses the weight change scheme of the stdp synapses can be set to either weight independent or weight dependent mode we present results that characterise the learning window implemented for both modes of operation when presented with spike trains with different types of synchronisation the neurons develop bimodal weight distributions we also show that a layered network of silicon spiking neurons with stdp synapses can perform hierarchical synchrony detection
a mixed signal image filtering vlsi has been developed aiming at real time generation of edge based image vectors for robust image recognition a four stage asynchronous median detection architecture based on analog digital mixed signal circuits has been introduced to determine the threshold value of edge detection the key processing parameter in vector generation as a result a fully seamless pipeline processing from threshold detection to edge feature map generation has been established a prototype chip was designed in a m double polysilicon three metal layer cmos technology and the concept was verified by the fabricated chip the chip generates a dimension feature vector from a x pixel gray scale image every sec this is about times faster than the software computation making a real time image recognition system feasible
we have constructed a second generation cpg chip capable of generating the necessary timing to control the leg of a walking machine we demonstrate improvements over a previous chip by moving toward a significantly more versatile device this includes a larger number of silicon neurons more sophisticated neurons including voltage dependent charging and relative and absolute refractory periods and enhanced programmability of neural networks this chip builds on the basic results achieved on a previous chip and expands its versatility to get closer to a self contained locomotion controller for walking robots
the relative depth of objects causes small shifts in the left and right retinal positions of these objects called binocular disparity here we describe a neuromorphic implementation of a disparity selective complex cell using the binocular energy model which has been proposed to model the response of disparity selective cells in the visual cortex our system consists of two silicon chips containing spiking neurons with monocular gabor type spatial receptive fields rf and circuits that combine the spike outputs to compute a disparity selective complex cell response the disparity selectivity of the cell can be adjusted by both position and phase shifts between the monocular rf profiles which are both used in biology our neuromorphic system performs better with phase encoding because the relative responses of neurons tuned to different disparities by phase shifts are better matched than the responses of neurons tuned by position shifts
the decision functions constructed by support vector machines svms usually depend only on a subset of the training set the so called support vectors we derive asymptotically sharp lower and upper bounds on the number of support vectors for several standard types of svms in particular we show for the gaussian rbf kernel that the fraction of support vectors tends to twice the bayes risk for the l svm to the probability of noise for the l svm and to for the ls svm
the purpose of this paper is to investigate infinity sample properties of risk minimization based multi category classification methods these methods can be considered as natural extensions to binary large margin classification we establish conditions that guarantee the infinity sample consistency of classifiers obtained in the risk minimization framework examples are provided for two specific forms of the general formulation which extend a number of known methods using these examples we show that some risk minimization formulations can also be used to obtain conditional probability estimates for the underlying problem such conditional probability information will be useful for statistical inferencing tasks beyond classification motivation consider a binary classification problem where we want to predict label y based on observation x one of the most significant achievements for binary classification in machine learning is the invention of large margin methods which include support vector machines and boosting algorithms based on a set of observations xy xnyn a large margin classification algorithm produces a decision function fn by empirically minimizing a loss function that is often a convex upper bound of the binary classification error function given fn the binary decision rule is to predict y if fnx and to predict y otherwise the decision rule at fnx is not important in the literature the following form of large margin binary classification is often encountered we minimize the empirical risk associated with a convex function in a pre chosen function class cn fn arg min fxiyi n fcn n i originally such a scheme was regarded as a compromise to avoid computational difficulties associated with direct classification error minimization which often leads to an np hard problem the current view in the statistical literature interprets such methods as algorithms to obtain conditional probability estimates for example see for some related studies this point of view allows people to show the consistency of various large margin methods that is in the large sample limit the obtained classifiers achieve the optimal bayes error rate for example see the consistency of a learning method is certainly a very desirable property and one may argue that a good classification method should be consistent in the large sample limit although statistical properties of binary classification algorithms based on the risk minimization formulation are quite well understood due to many recent works such as those mentioned above there are much fewer studies on risk minimization based multicategory problems which generalizes the binary large margin method the complexity of possible generalizations may be one reason another reason may be that one can always estimate the conditional probability for a multi category problem using the binary classification formulation for each category and then pick the category with the highest estimated conditional probability or score however it is still useful to understand whether there are more natural alternatives and what kind of risk minimization formulation which generalizes can be used to yield consistent classifiers in the large sample limit an important step toward this direction has recently been taken in where the authors proposed a multi category extension of the support vector machine that is bayes consistent note that there were a number of earlier proposals that were not consistent the purpose of this paper is to generalize their investigation so as to include a much wider class of risk minimization formulations that can lead to consistent classifiers in the infinity sample limit we shall see that there is a rich structure in risk minimization based multi category classification formulations multi category large margin methods have started to draw more attention recently for example in learning bounds for some multi category convex risk minimization methods were obtained although the authors did not study possible choices of bayes consistent formulations multi category classification we consider the following k class classification problem we would like to predict the label y k of an input vector x in this paper we only consider the simplest scenario with classification loss we have a loss of for correct prediction and loss of for incorrect prediction in binary classification the class label can be determined using the sign of a decision function this can be generalized to k class classification problem as follows we consider k decision functions fcx where c k and we predict the label y of x as tfx arg max fcx where we denote by fx the vector function fx fx fkx note that if two or more components of f achieve the same maximum value then we may choose any of them as tf in this framework fcx is often regarded as a scoring function for category c that is correlated with how likely x belongs to category c compared with the remaining k categories the classification error is given by f expy txx note that only the relative strength of fc compared with the alternatives is important in particular the decision rule given in does not change when we add the same numerical quantity to each component of fx this allows us to impose one constraint on the vector fx which decreases the degree of freedom k of the k component vector fx to k ck this approach is often called one versus all or ranking in machine learning another main ap proach is to encode a multi category classification problem into binary classification sub problems the consistency of such encoding schemes can be difficult to analyze and we shall not discuss them for example in the binary classification case we can enforce fxfx and hence fx can be represented as fx fx the decision rule in which compares fx fx is equivalent to fx this leads to the binary classification rule mentioned in the
this paper is concerned with transductive learning although transduction appears to be an easier task than induction there have not been many provably useful algorithms and bounds for transduction we present explicit error bounds for transduction and derive a general technique for devising bounds within this setting the technique is applied to derive error bounds for compression schemes such as transductive svms and for transduction algorithms based on clustering
we consider an online learning scenario in which the learner can make predictions on the basis of a fixed set of experts we derive upper and lower relative loss bounds for a class of universal learning algorithms involving a switching dynamics over the choice of the experts on the basis of the performance bounds we provide the optimal a priori discretization for learning the parameter that governs the switching dynamics we demonstrate the new algorithm in the context of wireless networks
in order to understand adaboosts dynamics especially its ability to maximize margins we derive an associated simplified nonlinear iterated map and analyze its behavior in low dimensional cases we find stable cycles for these cases which can explicitly be used to solve for adaboosts output by considering adaboost as a dynamical system we are able to prove ratsch and warmuths conjecture that adaboost may fail to converge to a maximal margin combined classifier when given a nonoptimal weak learning algorithm adaboost is known to be a coordinate descent method but other known algorithms that explicitly aim to maximize the margin such as adaboost and arc gv are not we consider a differentiable function for which coordinate ascent will yield a maximum margin solution we then make a simple approximation to derive a new boosting algorithm whose updates are slightly more aggressive than those of arc gv
we investigate improvements of adaboost that can exploit the fact that the weak hypotheses are one sided ie either all its positive or negative predictions are correct in particular for any set of m labeled examples consistent with a disjunction of k literals which are one sided in this case adaboost constructs a consistent hypothesis by using ok log m iterations on the other hand a greedy set covering algorithm finds a consistent hypothesis of size ok log m our primary question is whether there is a simple boosting algorithm that performs as well as the greedy set covering we first show that infoboost a modification of adaboost proposed by aslam for a different purpose does perform as well as the greedy set covering algorithm we then show that adaboost requires k log m iterations for learning k literal disjunctions we achieve this with an adversary construction and as well as in simple experiments based on artificial data further we give a variant called semiboost that can handle the degenerate case when the given examples all have the same label we conclude by showing that semiboost can be used to produce small conjunctions as well
this paper reports on a family of computationally practical classifiers that converge to the bayes error at near minimax optimal rates for a variety of distributions the classifiers are based on dyadic classification trees dcts which involve adaptively pruned partitions of the feature space a key aspect of dcts is their spatial adaptivity which enables local rather than global fitting of the decision boundary our risk analysis involves a spatial decomposition of the usual concentration inequalities leading to a spatially adaptive data dependent pruning criterion for any distribution on x y whose bayes decision boundary behaves locally like a lipschitz smooth function we show that the dct error converges to the bayes error at a rate within a logarithmic factor of the minimax optimal rate we also study dcts equipped with polynomial classification rules at each leaf and show that as the smoothness of the boundary increases their errors converge to the bayes error at a rate approaching n the parametric rate we are not aware of any other practical classi fiers that provide similar rate of convergence guarantees fast algorithms for tree pruning are discussed
there exist many different generalization error bounds for classification each of these bounds contains an improvement over the others for certain situations our goal is to combine these different improvements into a single bound in particular we combine the pac bayes approach introduced by mcallester which is interesting for averaging classifiers with the optimal union bound provided by the generic chaining technique developed by fernique and talagrand this combination is quite natural since the generic chaining is based on the notion of majorizing measures which can be considered as priors on the set of classifiers and such priors also arise in the pac bayesian setting
in the problem of probability forecasting the learners goal is to output given a training set and a new object a suitable probability measure on the possible values of the new objects label an on line algorithm for probability forecasting is said to be well calibrated if the probabilities it outputs agree with the observed frequencies we give a natural nonasymptotic formalization of the notion of well calibratedness which we then study under the assumption of randomness the objectlabel pairs are independent and identically distributed it turns out that although no probability forecasting algorithm is automatically well calibrated in our sense there exists a wide class of algorithms for multiprobability forecasting such algorithms are allowed to output a set ideally very narrow of probability measures which satisfy this property we call the algorithms in this class venn probability machines our experimental results demonstrate that a nearest neighbor venn probability machine performs reasonably well on a standard benchmark data set and one of our theoretical results asserts that a simple venn probability machine asymptotically approaches the true conditional probabilities regardless and without knowledge of the true probability measure generating the examples
we interpret non negative matrix factorization geometrically as the problem of finding a simplicial cone which contains a cloud of data points and which is contained in the positive orthant we show that under certain conditions basically requiring that some of the data are spread across the faces of the positive orthant there is a unique such simplicial cone we give examples of synthetic image articulation databases which obey these conditions these require separated support and factorial sampling for such databases there is a generative model in terms of parts and nmf correctly identifies the parts we show that our theoretical results are predictive of the performance of published nmf code by running the published algorithms on one of our synthetic image articulation databases
in this paper we obtain convergence bounds for the concentration of bayesian posterior distributions around the true distribution using a novel method that simplifies and enhances previous results based on the analysis we also introduce a generalized family of bayesian posteriors and show that the convergence behavior of these generalized posteriors is completely determined by the local prior structure around the true distribution this important and surprising robustness property does not hold for the standard bayesian posterior in that it may not concentrate when there exist bad prior structures even at places far away from the true distribution
a general linear response method for deriving improved estimates of correlations in the variational bayes framework is presented three applications are given and it is discussed how to use linear response as a general principle for improving mean field approximations
we argue that kmeans and deterministic annealing algorithms for geometric clustering can be derived from the more general information bottleneck approach if we cluster the identities of data points to preserve information about their location the set of optimal solutions is massively degenerate but if we treat the equations that define the optimal solution as an iterative algorithm then a set of smooth initial conditions selects solutions with the desired geometrical properties in addition to conceptual unification we argue that this approach can be more efficient and robust than classic algorithms
many classification algorithms including the support vector machine boosting and logistic regression can be viewed as minimum contrast methods that minimize a convex surrogate of the loss function we characterize the statistical consequences of using such a surrogate by providing a general quantitative relationship between the risk as assessed using the loss and the risk as assessed using any nonnegative surrogate loss function we show that this relationship gives nontrivial bounds under the weakest possible condition on the loss function that it satisfy a pointwise form of fisher consistency for classification the relationship is based on a variational transformation of the loss function that is easy to compute in many applications we also present a refined version of this result in the case of low noise finally we present applications of our results to the estimation of convergence rates in the general setting of function classes that are scaled hulls of a finite dimensional base class
we derive the limiting form of the eigenvalue spectrum for sample covariance matrices produced from non isotropic data for the analysis of standard pca we study the case where the data has increased variance along a small number of symmetry breaking directions the spectrum depends on the strength of the symmetry breaking signals and on a parameter which is the ratio of sample size to data dimension results are derived in the limit of large data dimension while keeping fixed as increases there are transitions in which delta functions emerge from the upper end of the bulk spectrum corresponding to the symmetry breaking directions in the data and we calculate the bias in the corresponding eigenvalues for kernel pca the covariance matrix in feature space may contain symmetry breaking structure even when the data components are independently distributed with equal variance we show examples of phase transition behaviour analogous to the pca results in this case
we compute approximate analytical bootstrap averages for support vector classification using a combination of the replica method of statistical physics and the tap approach for approximate inference we test our method on a few datasets and compare it with exact averages obtained by extensive monte carlo sampling
gradient following learning methods can encounter problems of implementation in many applications and stochastic variants are frequently used to overcome these difficulties we derive quantitative learning curves for three online training methods used with a linear perceptron direct gradient descent node perturbation and weight perturbation the maximum learning rate for the stochastic methods scales inversely with the first power of the dimensionality of the noise injected into the system with sufficiently small learning rate all three methods give identical learning curves these results suggest guidelines for when these stochastic methods will be limited in their utility and considerations for architectures in which they will be effective
what happens to the optimal interpretation of noisy data when there exists more than one equally plausible interpretation of the data in a bayesian model learning framework the answer depends on the prior expectations of the dynamics of the model parameter that is to be inferred from the data local time constraints on the priors are insufficient to pick one interpretation over another on the other hand nonlocal time constraints induced by a f noise spectrum of the priors is shown to permit learning of a specific model parameter even when there are infinitely many equally plausible interpretations of the data this transition is inferred by a remarkable mapping of the model estimation problem to a dissipative physical system allowing the use of powerful statistical mechanical methods to uncover the transition from indeterminate to determinate model learning
the problem of extracting the relevant aspects of data was addressed through the information bottleneck ib method by soft clustering one variable while preserving information about another relevance variable an interesting question addressed in the current work is the extension of these ideas to obtain continuous representations that preserve relevant information rather than discrete clusters we give a formal definition of the general continuous ib problem and obtain an analytic solution for the optimal representation for the important case of multivariate gaussian variables the obtained optimal representation is a noisy linear projection to eigenvectors of the normalized correlation matrix xy which x is also the basis obtained in canonical correlation analysis however in gaussian ib the compression tradeoff parameter uniquely determines the dimension as well as the scale of each eigenvector this introduces a novel interpretation where solutions of different ranks lie on a continuum parametrized by the compression level our analysis also provides an analytic expression for the optimal tradeoff the information curve in terms of the eigenvalue spectrum
we address in this paper the question of how the knowledge of the marginal distribution p x can be incorporated in a learning algorithm we suggest three theoretical methods for taking into account this distribution for regularization and provide links to existing graph based semi supervised learning algorithms we also propose practical implementations
we present a unified view for online classification regression and uniclass problems this view leads to a single algorithmic framework for the three problems we prove worst case loss bounds for various algorithms for both the realizable case and the non realizable case a conversion of our main online algorithm to the setting of batch learning is also discussed the end result is new algorithms and accompanying loss bounds for the hinge loss
margin maximizing properties play an important role in the analysis of classication models such as boosting and support vector machines margin maximization is theoretically interesting because it facilitates generalization error analysis and practically interesting because it presents a clear geometric interpretation of the models being built we formulate and prove a sufcient condition for the solutions of regularized loss functions to converge to margin maximizing separators as the regularization vanishes this condition covers the hinge loss of svm the exponential loss of adaboost and logistic regression loss we also generalize it to multi class classication problems and present margin maximizing multiclass versions of logistic regression and support vector machines
a balanced network leads to contradictory constraints on memory models as exemplified in previous work on accommodation of synfire chains here we show that these constraints can be overcome by introducing a shadow inhibitory pattern for each excitatory pattern of the model this is interpreted as a doublebalance principle whereby there exists both global balance between average excitatory and inhibitory currents and local balance between the currents carrying coherent activity at any given time frame this principle can be applied to networks with hebbian cell assemblies leading to a high capacity of the associative memory the number of possible patterns is limited by a combinatorial constraint that turns out to be pn within the specific model that we employ this limit is reached by the hebbian cell assembly network to the best of our knowledge this is the first time that such high memory capacities are demonstrated in the asynchronous state of models of spiking neurons
we employ an efficient method using bayesian and linear classifiers for analyzing the dynamics of information in high dimensional states of generic cortical microcircuit models it is shown that such recurrent circuits of spiking neurons have an inherent capability to carry out rapid computations on complex spike patterns merging information contained in the order of spike arrival with previously acquired context information
signal transduction networks biochemical are the biological information processing systems by which individual cells from neurons to amoebae perceive and respond to their chemical environments we introduce a simplified model of a single biochemical relay and analyse its capacity as a communications channel a diffusible ligand is released by a sending cell and received by binding to a transmembrane receptor protein on a receiving cell this receptor ligand interaction creates a nonlinear communications channel with non gaussian noise we model this channel numerically and study its response to input signals of different frequencies in order to estimate its channel capacity stochastic effects introduced in both the diffusion process and the receptor ligand interaction give the channel low pass characteristics we estimate the channel capacity using a water filling formula adapted from the additive white noise gaussian channel
dopamine exerts two classes of effect on the sustained neural activity in prefrontal cortex that underlies working memory direct release in the cortex increases the contrast of prefrontal neurons enhancing the robustness of storage release of dopamine in the striatum is associated with salient stimuli and makes medium spiny neurons bistable this modulation of the output of spiny neurons affects prefrontal cortex so as to indirectly gate access to working memory and additionally damp sensitivity to noise existing models have treated dopamine in one or other structure or have addressed basal ganglia gating of working memory exclusive of dopamine effects in this paper we combine these mechanisms and explore their joint effect we model a memory guided saccade task to illustrate how dopamines actions lead to working memory that is selective for salient input and has increased robustness to distraction
the connectivity of the nervous system of the nematode caenorhabditis elegans has been described completely but the analysis of the neuronal basis of behavior in this system is just beginning here we used an optimization algorithm to search for patterns of connectivity sufficient to compute the sensorimotor transformation underlying c elegans chemotaxis a simple form of spatial orientation behavior in which turning probability is modulated by the rate of change of chemical concentration optimization produced differentiator networks with inhibitory feedback among all neurons further analysis showed that feedback regulates the latency between sensory input and behavior common patterns of connectivity between the model and biological networks suggest new functions for previously identified connections in the c elegans nervous system
significant plasticity in sensory cortical representations can be driven in mature animals either by behavioural tasks that pair sensory stimuli with reinforcement or by electrophysiological experiments that pair sensory input with direct stimulation of neuromodulatory nuclei but usually not by sensory stimuli presented alone biologically motivated theories of representational learning however have tended to focus on unsupervised mechanisms which may play a significant role on evolutionary or developmental timescales but which neglect this essential role of reinforcement in adult plasticity by contrast theoretical reinforcement learning has generally dealt with the acquisition of optimal policies for action in an uncertain world rather than with the concurrent shaping of sensory representations this paper develops a framework for representational learning which builds on the relative success of unsupervised generativemodelling accounts of cortical encodings to incorporate the effects of reinforcement in a biologically plausible way
this paper proposes neural mechanisms of transcranial magnetic stimulation tms tms can stimulate the brain non invasively through a brief magnetic pulse delivered by a coil placed on the scalp interfering with specific cortical functions with a high temporal resolution due to these advantages tms has been a popular experimental tool in various neuroscience fields however the neural mechanisms underlying tmsinduced interference are still unknown a theoretical basis for tms has not been developed this paper provides computational evidence that inhibitory interactions in a neural population not an isolated single neuron play a critical role in yielding the neural interference induced by tms

we discuss an idea for collecting data in a relatively efficient manner our point of view is bayesian and information theoretic on any given trial we want to adaptively choose the input in such a way that the mutual information between the unknown state of the system and the stochastic output is maximal given any prior information including data collected on any previous trials we prove a theorem that quantifies the effectiveness of this strategy and give a few illustrative examples comparing the performance of this adaptive technique to that of the more usual nonadaptive experimental design for example we are able to explicitly calculate the asymptotic relative efficiency of the staircase method widely employed in psychophysics research and to demonstrate the dependence of this efficiency on the form of the psychometric function underlying the output responses
when we learn a new motor skill we have to contend with both the variability inherent in our sensors and the task the sensory uncertainty can be reduced by using information about the distribution of previously experienced tasks here we impose a distribution on a novel sensorimotor task and manipulate the variability of the sensory feedback we show that subjects internally represent both the distribution of the task as well as their sensory uncertainty moreover they combine these two sources of information in a way that is qualitatively predicted by optimal bayesian processing we further analyze if the subjects can represent multimodal distributions such as mixtures of gaussians the results show that the cns employs probabilistic models during sensorimotor learning even when the priors are multimodal
when we model a higher order functions such as learning and memory we face a difficulty of comparing neural activities with hidden variables that depend on the history of sensory and motor signals and the dynamics of the network here we propose novel method for estimating hidden variables of a learning agent such as connection weights from sequences of observable variables bayesian estimation is a method to estimate the posterior probability of hidden variables from observable data sequence using a dynamic model of hidden and observable variables in this paper we apply particle filter for estimating internal parameters and metaparameters of a reinforcement learning model we verified the effectiveness of the method using both artificial data and real animal behavioral data
spike timing plasticity stdp is a special form of synaptic plasticity where the relative timing of postand presynaptic activity determines the change of the synaptic weight on the postsynaptic side active backpropagating spikes in dendrites seem to play a crucial role in the induction of spike timing dependent plasticity we argue that postsynaptically the temporal change of the membrane potential determines the weight change coming from the presynaptic side induction of stdp is closely related to the activation of nmda channels therefore we will calculate analytically the change of the synaptic weight by correlating the derivative of the membrane potential with the activity of the nmda channel thus for this calculation we utilise biophysical variables of the physiological cell the final result shows a weight change curve which conforms with measurements from biology the positive part of the weight change curve is determined by the nmda activation the negative part of the weight change curve is determined by the membrane potential change therefore the weight change curve should change its shape depending on the distance from the soma of the postsynaptic cell we find temporally asymmetric weight change close to the soma and temporally symmetric weight change in the distal dendrite
the barn owl is a nocturnal hunter capable of capturing prey using auditory information alone the neural basis for this localization behavior is the existence of auditory neurons with spatial receptive fields we provide a mathematical description of the operations performed on auditory input signals by the barn owl that facilitate the creation of a representation of auditory space to develop our model we first formulate the sound localization problem solved by the barn owl as a statistical estimation problem the implementation of the solution is constrained by the known neurobiology
decoding is a strategy that allows us to assess the amount of information neurons can provide about certain aspects of the visual scene in this study we develop a method based on bayesian sequential updating and the particle filtering algorithm to decode the activity of v neurons in awake monkeys a distinction in our method is the use of volterra kernels to filter the particles which live in a high dimensional space this parametric bayesian decoding scheme is compared to the optimal linear decoder and is shown to work consistently better than the linear optimal decoder interestingly our results suggest that for decoding in real time spike trains of as few as independent but similar neurons would be sufficient for decoding a critical scene variable in a particular class of visual stimuli the reconstructed variable can predict the neural activity about as well as the actual signal with respect to the volterra kernels
we report and compare the performance of different learning algorithms based on data from cortical recordings the task is to predict the orientation of visual stimuli from the activity of a population of simultaneously recorded neurons we compare several ways of improving the coding of the input ie the spike data as well as of the output ie the orientation and report the results obtained using different kernel algorithms
a recent area of significant progress in speaker recognition is the use of high level features idiolect phonetic relations prosody discourse structure etc a speaker not only has a distinctive acoustic sound but uses language in a characteristic manner large corpora of speech data available in recent years allow experimentation with long term statistics of phone patterns word patterns etc of an individual we propose the use of support vector machines and term frequency analysis of phone sequences to model a given speaker to this end we explore techniques for text categorization applied to the problem we derive a new kernel based upon a linearization of likelihood ratio scoring we introduce a new phone based svm speaker recognition approach that halves the error rate of conventional phone based approaches
over the last years significant efforts have been made to develop kernels that can be applied to sequence data such as dna text speech video and images the fisher kernel and similar variants have been suggested as good ways to combine an underlying generative model in the feature space and discriminant classifiers such as svms in this paper we suggest an alternative procedure to the fisher kernel for systematically finding kernel functions that naturally handle variable length sequence data in multimedia domains in particular for domains such as speech and images we explore the use of kernel functions that take full advantage of well known probabilistic models such as gaussian mixtures and single full covariance gaussian models we derive a kernel distance based on the kullback leibler kl divergence between generative models in effect our approach combines the best of both generative and discriminative methods and replaces the standard svm kernels we perform experiments on speaker identificationverification and image classification tasks and show that these new kernels have the best performance in speaker verification and mostly outperform the fisher kernel based svms and the generative classifiers in speaker identification and image classification
many techniques for complex speech processing such as denoising and deconvolution timefrequency warping multiple speaker separation and multiple microphone analysis operate on sequences of short time power spectra spectrograms a representation which is often well suited to these tasks however a significant problem with algorithms that manipulate spectrograms is that the output spectrogram does not include a phase component which is needed to create a time domain signal that has good perceptual quality here we describe a generative model of time domain speech signals and their spectrograms and show how an efficient optimizer can be used to find the maximum a posteriori speech signal given the spectrogram in contrast to techniques that alternate between estimating the phase and a spectrally consistent signal our technique directly infers the speech signal thus jointly optimizing the phase and a spectrally consistent signal we compare our technique with a standard method using signal to noise ratios but we also provide audio files on the web for the purpose of demonstrating the improvement in perceptual quality that our technique offers
eigenvoice speaker adaptation has been shown to be effective when only a small amount of adaptation data is available at the heart of the method is principal component analysis pca employed to find the most important eigenvoices in this paper we postulate that nonlinear pca in particular kernel pca may be even more effective one major challenge is to map the feature space eigenvoices back to the observation space so that the state observation likelihoods can be computed during the estimation of eigenvoice weights and subsequent decoding our solution is to compute kernel pca using composite kernels and we will call our new method kernel eigenvoice speaker adaptation on the tidigits corpus we found that compared with a speaker independent model our kernel eigenvoice adaptation method can reduce the word error rate by while the standard eigenvoice approach can only match the performance of the speaker independent model
a major issue in evaluating speech enhancement and hearing compensation algorithms is to come up with a suitable metric that predicts intelligibility as judged by a human listener previous methods such as the widely used speech transmission index sti fail to account for masking effects that arise from the highly nonlinear cochlear transfer function we therefore propose a neural articulation index nai that estimates speech intelligibility from the instantaneous neural spike rate over time produced when a signal is processed by an auditory neural model by using a well developed model of the auditory periphery and detection theory we show that human perceptual discrimination closely matches the modeled distortion in the instantaneous spike rates of the auditory nerve in highly rippled frequency transfer conditions the nais prediction error is versus the stis prediction error of
speech dereverberation is desirable with a view to achieving for example robust speech recognition in the real world however it is still a challenging problem especially when using a single microphone although blind equalization techniques have been exploited they cannot deal with speech signals appropriately because their assumptions are not satisfied by speech signals we propose a new dereverberation principle based on an inherent property of speech signals namely quasi periodicity the present methods learn the dereverberation filter from a lot of speech data with no prior knowledge of the data and can achieve high quality speech dereverberation especially when the reverberation time is long
at a cocktail party a listener can selectively attend to a single voice and filter out other acoustical interferences how to simulate this perceptual ability remains a great challenge this paper describes a novel supervised learning approach to speech segregation in which a target speech signal is separated from interfering sounds using spatial location cues interaural time differences itd and interaural intensity differences iid motivated by the auditory masking effect we employ the notion of an ideal time frequency binary mask which selects the target if it is stronger than the interference in a local time frequency unit within a narrow frequency band modifications to the relative strength of the target source with respect to the interference trigger systematic changes for estimated itd and iid for a given spatial configuration this interaction produces characteristic clustering in the binaural feature space consequently we perform pattern classification in order to estimate ideal binary masks a systematic evaluation in terms of signal to noise ratio as well as automatic speech recognition performance shows that the resulting system produces masks very close to ideal binary ones a quantitative comparison shows that our model yields significant improvement in performance over an existing approach furthermore under certain conditions the model produces large speech intelligibility improvements with normal listeners
local phase coherence and the perception of blur zhou wang and eero p simoncelli howard hughes medical institute center for neural science and courant institute of mathematical sciences new york university new york ny zhouwangieeeorg eerosimoncellinyuedu humans are able to detect blurring of visual images but the mechanism by which they do so is not known a traditional view is that a blurred image looks unnatural because of the reduction in energy at high frequencies we argue that the disruption of local phase is a more important factor for detecting blur we first demonstrate that a sharp image with its high frequency energy reduced but local phase preserved appears much sharper than a blurred image with its high frequency energy corrected but local phase uncorrected we show that precisely localized features such as step edges result in strong local phase coherence structures across scale and space in the complex wavelet transform domain and blurring causes loss of such phase coherence we propose a technique for coarseto fine phase prediction of wavelet coefficients and observe that such predictions are highly effective in natural images phase coherence increases with the strength of image features and blurring disrupts the phase coherence relationship in images we thus lay the groundwork for a new theory of perceptual blur estimation as well as a variety of algorithms for restoration and manipulation of photographic images
according to a widely held view neurons in lateral geniculate nucleus lgn operate on visual stimuli in a linear fashion there is ample evidence however that lgn responses are not entirely linear to account for nonlinearities we propose a model that synthesizes more than years of research in the field model neurons have a linear receptive field and a nonlinear divisive suppressive field the suppressive field computes local root meansquare contrast to test this model we recorded responses from lgn of anesthetized paralyzed cats we estimate model parameters from a basic set of measurements and show that the model can accurately predict responses to novel stimuli the model might serve as the new standard model of lgn responses it specifies how visual processing in lgn involves both linear filtering and divisive gain control
psychophysical studies suggest the existence of specialized detectors for component motion patterns radial circular and spiral that are consistent with the visual motion properties of cells in the dorsal medial superior temporal area mstd of non human primates here we use a biologically constrained model of visual motion processing in mstd in conjunction with psychophysical performance on two motion pattern tasks to elucidate the computational mechanisms associated with the processing of widefield motion patterns encountered during self motion in both tasks discrimination thresholds varied significantly with the type of motion pattern presented suggesting perceptual correlates to the preferred motion bias reported in mstd through the model we demonstrate that while independently responding motion pattern units are capable of encoding information relevant to the visual motion tasks equivalent psychophysical performance can only be achieved using interconnected neural populations that systematically inhibit non responsive units these results suggest the cyclic trends in psychophysical performance may be mediated in part by recurrent connections within motion pattern responsive areas whose structure is a function of the similarity in preferred motion patterns and receptive field locations between units
this paper compares the ability of human observers to detect target image curves with that of an ideal observer the target curves are sampled from a generative model which specifies probabilistically the geometry and local intensity properties of the curve the ideal observer performs bayesian inference on the generative model using map estimation varying the probability model for the curve geometry enables us investigate whether human performance is best for target curves that obey specific shape statistics in particular those observed on natural shapes experiments are performed with data on both rectangular and hexagonal lattices our results show that human observers performance approaches that of the ideal observer and are in general closest to the ideal for conditions where the target curve tends to be straight or similar to natural statistics on curves this suggests a bias of human observers towards straight curves and natural statistics
recent eye tracking studies in natural tasks suggest that there is a tight link between eye movements and goal directed motor actions however most existing models of human eye movements provide a bottom up account that relates visual attention to attributes of the visual scene the purpose of this paper is to introduce a new model of human eye movements that directly ties eye movements to the ongoing demands of behavior the basic idea is that eye movements serve to reduce uncertainty about environmental variables that are task relevant a value is assigned to an eye movement by estimating the expected cost of the uncertainty that will result if the movement is not made if there are several candidate eye movements the one with the highest expected value is chosen the model is illustrated using a humanoid graphic figure that navigates on a sidewalk in a virtual urban environment simulations show our protocol is superior to a simple round robin scheduling mechanism
even under perfect fixation the human eye is under steady motion tremor microsaccades slow drift the dynamic theory of vision states that eye movements can improve hyperacuity according to this theory eye movements are thought to create variable spatial excitation patterns on the photoreceptor grid which will allow for better spatiotemporal summation at later stages we reexamine this theory using a realistic model of the vertebrate retina by comparing responses of a resting and a moving eye the performance of simulated ganglion cells in a hyperacuity task is evaluated by ideal observer analysis we find that in the central retina eye micromovements have no effect on the performance here optical blurring limits vernier acuity in the retinal periphery however eye micromovements clearly improve performance based on roc analysis our predictions are quantitatively testable in electrophysiological and psychophysical experiments
one current explanation of the view independent representation of space by the place cells of the hippocampus is that they arise out of the summation of view dependent gaussians this proposal assumes that visual representations show bounded invariance here we investigate whether a recently proposed visual encoding scheme called the temporal population code can provide such representations our analysis is based on the behavior of a simulated robot in a virtual environment containing specific visual cues our results show that the temporal population code provides a representational substrate that can naturally account for the formation of place fields
we present and empirically test a novel approach for categorizing d free form object shapes represented by range data in contrast to traditional surface signature based systems that use alignment to match specific objects we adapted the newly introduced symbolic signature representation to classify deformable shapes our approach constructs an abstract description of shape classes using an ensemble of classifiers that learn object class parts and their corresponding geometrical relationships from a set of numeric and symbolic descriptors we used our classification engine in a series of large scale discrimination experiments on two well defined classes that share many common distinctive features the experimental results suggest that our method outperforms traditional numeric signature based methodologies
standard approaches to object detection focus on local patches of the image and try to classify them as background or not we propose to use the scene context image as a whole as an extra source of global information to help resolve local ambiguities we present a conditional random field for jointly solving the tasks of object detection and scene classification
the problem of structure from motion is a central problem in vision given the d locations of certain points we wish to recover the camera motion and the d coordinates of the points under simplified camera models the problem reduces to factorizing a measurement matrix into the product of two low rank matrices each element of the measurement matrix contains the position of a point in a particular image when all elements are observed the problem can be solved trivially using svd but in any realistic situation many elements of the matrix are missing and the ones that are observed have a different directional uncertainty under these conditions most existing factorization algorithms fail while human perception is relatively unchanged in this paper we use the well known em algorithm for factor analysis to perform factorization this allows us to easily handle missing data and measurement uncertainty and more importantly allows us to place a prior on the temporal trajectory of the latent variables the camera position we show that incorporating this prior gives a significant improvement in performance in challenging image sequences
mutual boosting is a method aimed at incorporating contextual information to augment object detection when multiple detectors of objects and parts are trained in parallel using adaboost object detectors might use the remaining intermediate detectors to enrich the weak learner set this method generalizes the efficient features suggested by viola and jones thus enabling information inference between parts and objects in a compositional hierarchy in our experiments eye nose mouthand face detectors are trained using the mutual boosting framework results show that the method outperforms applications overlooking contextual information we suggest that achieving contextual integration is a step toward human like detection capabilities
face detection is a canonical example of a rare event detection problem in which target patterns occur with much lower frequency than nontargets out of millions of face sized windows in an input image for example only a few will typically contain a face viola and jones recently proposed a cascade architecture for face detection which successfully addresses the rare event nature of the task a central part of their method is a feature selection algorithm based on adaboost we present a novel cascade learning algorithm based on forward feature selection which is two orders of magnitude faster than the viola jones approach and yields classifiers of equivalent quality this faster method could be used for more demanding classification tasks such as on line learning
in this paper we present discriminative random fields drf a discriminative framework for the classification of natural image regions by incorporating neighborhood spatial dependencies in the labels as well as the observed data the proposed model exploits local discriminative models and allows to relax the assumption of conditional independence of the observed data given the labels commonly used in the markov random field mrf framework the parameters of the drf model are learned using penalized maximum pseudo likelihood method furthermore the form of the drf model allows the map inference for binary classification problems using the graph min cut algorithms the performance of the model was verified on the synthetic as well as the real world images the drf model outperforms the mrf model in the experiments
the detection and pose estimation of people in images and video is made challenging by the variability of human appearance the complexity of natural scenes and the high dimensionality of articulated body models to cope with these problems we represent the d human body as a graphical model in which the relationships between the body parts are represented by conditional probability distributions we formulate the pose estimation problem as one of probabilistic inference over a graphical model where the random variables correspond to the individual limb parameters position and orientation because the limbs are described by dimensional vectors encoding pose in space discretization is impractical and the random variables in our model must be continuousvalued to approximate belief propagation in such a graph we exploit a recently introduced generalization of the particle filter this framework facilitates the automatic initialization of the body model from low level cues and is robust to occlusion of body parts and scene clutter
this paper describes a system that can annotate a video sequence with a description of the appearance of each actor when the actor is in view and a representation of the actors activity while in view the system does not require a fixed background and is automatic the system works by tracking people in d and then using an annotated motion capture dataset synthesizing an annotated d motion sequence matching the d tracks the d motion capture data is manually annotated off line using a class structure that describes everyday motions and allows motion annotations to be composed one may jump while running for example descriptions computed from video of real motions show that the method is accurate
this paper presents an algorithm for learning the time varying shape of a non rigid d object from uncalibrated d tracking data we model shape motion as a rigid component rotation and translation combined with a non rigid deformation reconstruction is ill posed if arbitrary deformations are allowed we constrain the problem by assuming that the object shape at each time instant is drawn from a gaussian distribution based on this assumption the algorithm simultaneously estimates d shape and motion for each time frame learns the parameters of the gaussian and robustly fills in missing data points we then extend the algorithm to model temporal smoothness in object shape thus allowing it to handle severe cases of missing data
computer animated agents and robots bring a social dimension to human computer interaction and force us to think in new ways about how computers could be used in daily life face to face communication is a real time process operating at a time scale of less than a second in this paper we present progress on a perceptual primitive to automatically detect frontal faces in the video stream and code them with respect to dimensions in real time neutral anger disgust fear joy sadness surprise the face finder employs a cascade of feature detectors trained with boosting techniques the expression recognizer employs a novel combination of adaboost and svms the generalization performance to new subjects for a way forced choice was and correct on two publicly available datasets the outputs of the classifier change smoothly as a function of time providing a potentially valuable representation to code facial expression dynamics in a fully automatic and unobtrusive manner the system was deployed and evaluated for measuring spontaneous facial expressions in the field in an application for automatic assessment of human robot interaction
this paper presents a novel graph theoretic approach named ratio contour to extract perceptually salient boundaries from a set of noisy boundary fragments detected in real images the boundary saliency is defined using the gestalt laws of closure proximity and continuity this paper first constructs an undirected graph with two different sets of edges solid edges and dashed edges the weights of solid and dashed edges measure the local saliency in and between boundary fragments respectively then the most salient boundary is detected by searching for an optimal cycle in this graph with minimum average weight the proposed approach guarantees the global optimality without introducing any biases related to region area or boundary length we collect a variety of images for testing the proposed approach with encouraging results
we present a geometric approach to statistical shape analysis of closed curves in images the basic idea is to specify a space of closed curves satisfying given constraints and exploit the differential geometry of this space to solve optimization and inference problems we demonstrate this approach by i defining and computing statistics of observed shapes ii defining and learning a parametric probability model on shape space and iii designing a binary hypothesis test on this space
super resolution aims to produce a high resolution image from a set of one or more low resolution images by recovering or inventing plausible high frequency image content typical approaches try to reconstruct a high resolution image using the sub pixel displacements of several lowresolution images usually regularized by a generic smoothness prior over the high resolution image space other methods use training data to learn low to high resolution matches and have been highly successful even in the single input image case here we present a domain specific image prior in the form of a pdf based upon sampled images and show that for certain types of super resolution problems this sample based prior gives a significant improvement over other common multiple image super resolution techniques
we present a bayesian approach to color constancy which utilizes a nongaussian probabilistic model of the image formation process the parameters of this model are estimated directly from an uncalibrated image set and a small number of additional algorithmic parameters are chosen using cross validation the algorithm is empirically shown to exhibit rms error lower than other color constancy algorithms based on the lambertian surface reflectance model when estimating the illuminants of a set of test images this is demonstrated via a direct performance comparison utilizing a publicly available set of real world test images and code base
consider a number of moving points where each point is attached to a joint of the human body and projected onto an image plane johannson showed that humans can effortlessly detect and recognize the presence of other humans from such displays this is true even when some of the body points are missing eg because of occlusion and unrelated clutter points are added to the display we are interested in replicating this ability in a machine to this end we present a labelling and detection scheme in a probabilistic framework our method is based on representing the joint probability density of positions and velocities of body points with a graphical model and using loopy belief propagation to calculate a likely interpretation of the scene furthermore we introduce a global variable representing the bodys centroid experiments on one motion captured sequence suggest that our scheme improves on the accuracy of a previous approach based on triangulated graphical models especially when very few parts are visible the improvement is due both to the more general graph structure we use and more significantly to the
learning in a multiagent system is a challenging problem due to two key factors first if other agents are simultaneously learning then the environment is no longer stationary thus undermining convergence guarantees second learning is often susceptible to deception where the other agents may be able to exploit a learners particular dynamics in the worst case this could result in poorer performance than if the agent was not learning at all these challenges are identifiable in the two most common evaluation criteria for multiagent learning algorithms convergence and regret algorithms focusing on convergence or regret in isolation are numerous in this paper we seek to address both criteria in a single algorithm by introducing giga wolf a learning algorithm for normalform games we prove the algorithm guarantees at most zero average regret while demonstrating the algorithm converges in many situations of self play we prove convergence in a limited setting and give empirical results in a wider variety of situations these results also suggest a third new learning criterion combining convergence and regret which we call negative non convergence regret nnr
the formation of disulphide bridges among cysteines is an important feature of protein structures here we develop new methods for the prediction of disulphide bond connectivity we first build a large curated data set of proteins containing disulphide bridges and then use dimensional recursive neural networks to predict bonding probabilities between cysteine pairs these probabilities in turn lead to a weighted graph matching problem that can be addressed efficiently we show how the method consistently achieves better results than previous approaches on the same validation data in addition the method can easily cope with chains with arbitrary numbers of bonded cysteines therefore it overcomes one of the major limitations of previous approaches restricting predictions to chains containing no more than oxidized cysteines the method can be applied both to situations where the bonded state of each cysteine is known or unknown in which case bonded state can be predicted with precision and recall the method also yields an estimate for the total number of disulphide bridges in each chain
in this paper we propose an efficient algorithm for reducing a large mixture of gaussians into a smaller mixture while still preserving the component structure of the original model this is achieved by clustering grouping the components the method minimizes a new easily computed distance measure between two gaussian mixtures that can be motivated from a suitable stochastic model and the iterations of the algorithm use only the model parameters avoiding the need for explicit resampling of datapoints we demonstrate the method by performing hierarchical clustering of scenery images and handwritten digits
a reactive environment is one that responds to the actions of an agent rather than evolving obliviously in reactive environments experts algorithms must balance exploration and exploitation of experts more carefully than in oblivious ones in addition a more subtle definition of a learnable value of an expert is required a general exploration exploitation experts method is presented along with a proper definition of value the method is shown to asymptotically perform as well as the best available expert several variants are analyzed from the viewpoint of the exploration exploitation tradeoff including explore then exploit polynomially vanishing exploration constant frequency exploration and constant size exploration phases complexity and performance bounds are proven
we consider the problem of deriving class size independent generalization bounds for some regularized discriminative multi category classification methods in particular we obtain an expected generalization bound for a standard formulation of multi category support vector machines based on the theoretical result we argue that the formulation over penalizes misclassification error which in theory may lead to poor generalization performance a remedy based on a generalization of multi category logistic regression conditional maximum entropy is then proposed and its theoretical properties are examined
we propose a new method for estimating intrinsic dimension of a dataset derived by applying the principle of maximum likelihood to the distances between close neighbors we derive the estimator by a poisson process approximation assess its bias and variance theoretically and by simulations and apply it to a number of simulated and real datasets we also show it has the best overall performance compared with two other intrinsic dimension estimators
belief propagation bp is an increasingly popular method of performing approximate inference on arbitrary graphical models at times even further approximations are required whether from quantization or other simplified message representations or from stochastic approximation methods introducing such errors into the bp message computations has the potential to adversely affect the solution obtained we analyze this effect with respect to a particular measure of message error and show bounds on the accumulation of errors in the system this leads both to convergence conditions and error bounds in traditional and approximate bp message passing
we consider the problem of recovering an underwater image distorted by surface waves a large amount of video data of the distorted image is acquired the problem is posed in terms of finding an undistorted image patch at each spatial location this challenging reconstruction task can be formulated as a manifold learning problem such that the center of the manifold is the image of the undistorted patch to compute the center we present a new technique to estimate global distances on the manifold our technique achieves robustness through convex flow computations and solves the leakage problem inherent in recent manifold embedding techniques
assume a uniform multidimensional grid of bivariate data where each cell of the grid has a count ci and a baseline bi our goal is to find spatial regions d dimensional rectangles where the ci are significantly higher than expected given bi we focus on two applications detection of clusters of disease cases from epidemiological data emergency department visits over the counter drug sales and discovery of regions of increased brain activity corresponding to given cognitive tasks from fmri data each of these problems can be solved using a spatial scan statistic kulldorff where we compute the maximum of a likelihood ratio statistic over all spatial regions and find the significance of this region by randomization however computing the scan statistic for all spatial regions is generally computationally infeasible so we introduce a novel fast spatial scan algorithm generalizing the d scan algorithm of neill and moore to arbitrary dimensions our new multidimensional multiresolution algorithm allows us to find spatial clusters up to x faster than the naive spatial scan without any loss of accuracy
this paper presents a general family of algebraic positive definite similarity functions over spaces of matrices with varying column rank the columns can represent local regions in an image whereby images have varying number of local parts images of an image sequence motion trajectories in a multibody motion and so forth the family of set kernels we derive is based on a group invariant tensor product lifting with parameters that can be naturally tuned to provide a cook book of sorts covering the possible wish lists from similarity measures over sets of varying cardinality we highlight the strengths of our approach by demonstrating the set kernels for visual recognition of pedestrians using local parts representations
in this paper we propose a novel method for learning a mahalanobis distance measure to be used in the knn classification algorithm the algorithm directly maximizes a stochastic variant of the leave one out knn score on the training set it can also learn a low dimensional linear embedding of labeled data that can be used for data visualization and fast classification unlike other methods our classification model is non parametric making no assumptions about the shape of the class distributions or the boundaries between them the performance of the method is demonstrated on several data sets both for metric learning and linear dimensionality reduction
we study the problem of hierarchical classification when labels corresponding to partial andor multiple paths in the underlying taxonomy are allowed we introduce a new hierarchical loss function the h loss implementing the simple intuition that additional mistakes in the subtree of a mistaken class should not be charged for based on a probabilistic data model introduced in earlier work we derive the bayes optimal classifier for the h loss we then empirically compare two incremental approximations of the bayes optimal classifier with a flat svm classifier and with classifiers obtained by using hierarchical versions of the perceptron and svm algorithms the experiments show that our simplest incremental approximation of the bayes optimal classifier performs after just one training epoch nearly as well as the hierarchical svm classifier which performs best for the same incremental algorithm we also derive an h loss bound showing when data are generated by our probabilistic data model exponentially fast convergence to the h loss of the hierarchical classifier based on the true model parameters
agents learning to act in a partially observable domain may need to overcome the problem of perceptual aliasing ie different states that appear similar but require different responses this problem is exacerbated when the agents sensors are noisy ie sensors may produce different observations in the same state we show that many well known reinforcement learning methods designed to deal with perceptual aliasing such as utile suffix memory finite size history windows eligibility traces and memory bits do not handle noisy sensors well we suggest a new algorithm noisy utile suffix memory nusm based on usm that uses a weighted classification of observed trajectories we compare nusm to the above methods and show it to be more robust to noise
we propose a soft greedy learning algorithm for building small conjunctions of simple threshold functions called rays defined on single real valued attributes we also propose a pac bayes risk bound which is minimized for classifiers achieving a non trivial tradeoff between sparsity the number of rays used and the magnitude of the separating margin of each ray finally we test the soft greedy algorithm on four dna micro array data sets
we devise and experiment with a dynamical kernel based system for tracking hand movements from neural activity the state of the system corresponds to the hand location velocity and acceleration while the systems input are the instantaneous spike rates the systems state dynamics is defined as a combination of a linear mapping from the previous estimated state and a kernel based mapping tailored for modeling neural activities in contrast to generative models the activity to state mapping is learned using discriminative methods by minimizing a noise robust loss function we use this approach to predict hand trajectories on the basis of neural activity in motor cortex of behaving monkeys and find that the proposed approach is more accurate than both a static approach based on support vector regression and the kalman filter
we present an extension to the jojic and frey layered sprite model which allows for layers to undergo affine transformations this extension allows for affine object pose to be inferred whilst simultaneously learning the object shape and appearance learning is carried out by applying an augmented variational inference algorithm which includes a global search over a discretised transform space followed by a local optimisation to aid correct convergence we use bottom up cues to restrict the space of possible affine transformations we present results on a number of video sequences and show how the model can be extended to track an object whose appearance changes throughout the sequence
we present a semi parametric latent variable model based technique for density modelling dimensionality reduction and visualization unlike previous methods we estimate the latent distribution non parametrically which enables us to model data generated by an underlying low dimensional multimodal distribution in addition we allow the components of latent variable models to be drawn from the exponential family which makes the method suitable for special data types for example binary or count data simulations on real valued binary and count data show favorable comparison to other related schemes both in terms of separating different populations and generalization to unseen samples
we show that anomaly detection can be interpreted as a binary classification problem using this interpretation we propose a support vector machine svm for anomaly detection we then present some theoretical results which include consistency and learning rates finally we experimentally compare our svm with the standard one class svm
we establish learning rates to the bayes risk for support vector machines svms with hinge loss in particular for svms with gaussian rbf kernels we propose a geometric condition for distributions which can be used to determine approximation properties of these kernels finally we compare our methods with a recent paper of g blanchard et al
this paper concerns approximate nearest neighbor searching algorithms which have become increasingly important especially in high dimensional perception areas such as computer vision with dozens of publications in recent years much of this enthusiasm is due to a successful new approximate nearest neighbor approach called locality sensitive hashing lsh in this paper we ask the question can earlier spatial data structure approaches to exact nearest neighbor such as metric trees be altered to provide approximate answers to proximity queries and if so how we introduce a new kind of metric tree that allows overlap certain datapoints may appear in both the children of a parent we also introduce new approximate k nn search algorithms on this structure we show why these structures should be able to exploit the same randomprojection based approximations that lsh enjoys but with a simpler algorithm and perhaps with greater efficiency we then provide a detailed empirical evaluation on five large high dimensional datasets which show up to fold accelerations over lsh this result holds true throughout the spectrum of approximation levels
we describe an algorithm for support vector machines svm that can be parallelized efficiently and scales to very large problems with hundreds of thousands of training vectors instead of analyzing the whole training set in one optimization step the data are split into subsets and optimized separately with multiple svms the partial results are combined and filtered again in a cascade of svms until the global optimum is reached the cascade svm can be spread over multiple processors with minimal communication overhead and requires far less memory since the kernel matrices are much smaller than for a regular svm convergence to the global optimum is guaranteed with multiple passes through the cascade but already a single pass provides good generalization a single pass is x x faster than a regular svm for problems of vectors when implemented on a single processor parallel implementations on a cluster of processors were tested with over million vectors class problems converging in a day or two while a regular svm never converged in over a week
the nips workshops included a feature selection competition organized by the authors we provided participants with five datasets from different application domains and called for classification results using a minimal number of features the competition took place over a period of weeks and attracted research groups participants were asked to make on line submissions on the validation and test sets with performance on the validation set being presented immediately to the participant and performance on the test set presented to the participants at the workshop in total entries were made on the validation sets during the development period and entries on all test sets for the final competition the winners used a combination of bayesian neural networks with ard priors and dirichlet diffusion trees other top entries used a variety of methods for feature selection which combined filters andor wrapper or embedded methods using random forests kernel methods or neural networks as a classification engine the results of the benchmark including the predictions made by the participants and the features they selected and the scoring software are publicly available the benchmark is available at wwwnipsfscecssotonacuk for post challenge submissions to stimulate further research
this paper investigates a new learning model in which the input data is corrupted with noise we present a general statistical framework to tackle this problem based on the statistical reasoning we propose a novel formulation of support vector classification which allows uncertainty in input data we derive an intuitive geometric interpretation of the proposed formulation and develop algorithms to efficiently solve it empirical results are included to show that the newly formed method is superior to the standard svm for problems with noisy input
saliency mechanisms play an important role when visual recognition must be performed in cluttered scenes we propose a computational definition of saliency that deviates from existing models by equating saliency to discrimination in particular the salient attributes of a given visual class are defined as the features that enable best discrimination between that class and all other classes of recognition interest it is shown that this definition leads to saliency algorithms of low complexity that are scalable to large recognition problems and is compatible with existing models of early biological vision experimental results demonstrating success in the context of challenging recognition problems are also presented

a graph based prior is proposed for parametric semi supervised classification the prior utilizes both labelled and unlabelled data it also integrates features from multiple views of a given sample eg multiple sensors thus implementing a bayesian form of co training an em algorithm for training the classifier automatically adjusts the tradeoff between the contributions of a the labelled data b the unlabelled data and c the co training information active label query selection is performed using a mutual information based criterion that explicitly uses the unlabelled data and the co training information encouraging results are presented on public benchmarks and on measured data from single and multiple sensors
linear discriminant analysis lda is a well known scheme for feature extraction and dimension reduction it has been used widely in many applications involving high dimensional data such as face recognition and image retrieval an intrinsic limitation of classical lda is the so called singularity problem that is it fails when all scatter matrices are singular a well known approach to deal with the singularity problem is to apply an intermediate dimension reduction stage using principal component analysis pca before lda the algorithm called pcalda is used widely in face recognition however pcalda has high costs in time and space due to the need for an eigen decomposition involving the scatter matrices in this paper we propose a novel lda algorithm namely dlda which stands for dimensional linear discriminant analysis dlda overcomes the singularity problem implicitly while achieving efficiency the key difference between dlda and classical lda lies in the model for data representation classical lda works with vectorized representations of data while the dlda algorithm works with data in matrix representation to further reduce the dimension by dlda the combination of dlda and classical lda namely dldalda is studied where lda is preceded by dlda the proposed algorithms are applied on face recognition and compared with pcalda experiments show that dlda and dldalda achieve competitive recognition accuracy while being much more efficient
linear discriminant analysis lda is a well known method for feature extraction and dimension reduction it has been used widely in many applications such as face recognition recently a novel lda algorithm based on qr decomposition namely ldaqr has been proposed which is competitive in terms of classification accuracy with other lda algorithms but it has much lower costs in time and space however ldaqr is based on linear projection which may not be suitable for data with nonlinear structure this paper first proposes an algorithm called kdaqr which extends the ldaqr algorithm to deal with nonlinear data by using the kernel operator then an efficient approximation of kdaqr called akdaqr is proposed experiments on face image data show that the classification accuracy of both kdaqr and akdaqr are competitive with generalized discriminant analysis gda a general kernel discriminant analysis algorithm while akdaqr has much lower time and space costs
gaussian processes are usually parameterised in terms of their covariance functions however this makes it difficult to deal with multiple outputs because ensuring that the covariance matrix is positive definite is problematic an alternative formulation is to treat gaussian processes as white noise sources convolved with smoothing kernels and to parameterise the kernel instead using this we extend gaussian processes to handle multiple coupled outputs
two neural networks that are trained on their mutual output synchronize to an identical time dependant weight vector this novel phenomenon can be used for creation of a secure cryptographic secret key using a public channel several models for this cryptographic system have been suggested and have been tested for their security under different sophisticated attack strategies the most promising models are networks that involve chaos synchronization the synchronization process of mutual learning is described analytically using statistical physics methods
we address the problem of identifying specific instances of a class cars from a set of images all belonging to that class although we cannot build a model for any particular instance as we may be provided with only one training example of it we can use information extracted from observing other members of the class we pose this task as a learning problem in which the learner is given image pairs labeled as matching or not and must discover which image features are most consistent for matching instances and discriminative for mismatches we explore a patch based representation where we model the distributions of similarity measurements defined on the patches finally we describe an algorithm that selects the most salient patches based on a mutual information criterion this algorithm performs identification well for our challenging dataset of car images after matching only a few well chosen patches
in this paper we propose a probabilistic model for online document clustering we use non parametric dirichlet process prior to model the growing number of clusters and use a prior of general english language model as the base distribution to handle the generation of novel clusters furthermore cluster uncertainty is modeled with a bayesian dirichletmultinomial distribution we use empirical bayes method to estimate hyperparameters based on a historical dataset our probabilistic model is applied to the novelty detection task in topic detection and tracking tdt and compared with existing approaches in the literature
experimental studies have observed synaptic potentiation when a presynaptic neuron fires shortly before a postsynaptic neuron and synaptic depression when the presynaptic neuron fires shortly after the dependence of synaptic modulation on the precise timing of the two action potentials is known as spike timing dependent plasticity or stdp we derive stdp from a simple computational principle synapses adapt so as to minimize the postsynaptic neurons variability to a given presynaptic input causing the neurons output to become more reliable in the face of noise using an entropy minimization objective function and the biophysically realistic spike response model of gerstner we simulate neurophysiological experiments and obtain the characteristic stdp curve along with other phenomena including the reduction in synaptic plasticity as synaptic efficacy increases we compare our account to other efforts to derive stdp from computational principles and argue that our account provides the most comprehensive coverage of the phenomena thus reliability of neural response in the face of noise may be a key goal of cortical adaptation
in this paper we address the problem of statistical learning for multitopic text categorization mtc whose goal is to choose all relevant topics a label from a given set of topics the proposed algorithm maximal margin labeling mml treats all possible labels as independent classes and learns a multi class classifier on the induced multi class categorization problem to cope with the data sparseness caused by the huge number of possible labels mml combines some prior knowledge about label prototypes and a maximal margin criterion in a novel way experiments with multi topic web pages show that mml outperforms existing learning algorithms including support vector machines multi topic text categorization mtc this paper addresses the problem of learning for multi topic text categorization mtc whose goal is to select all topics relevant to a text from a given set of topics in mtc multiple topics may be relevant to a single text we thus call a set of topics label and say that a text is assigned a label not a topic in almost all previous text categorization studies eg the label is predicted by judging each topics relevance to the text in this decomposition approach the features specific to a topic not a label are regarded as important features however the approach may result in inefficient learning as we will explain in the following example imagine an mtc problem of scientific papers where quantum computing papers are assigned multi topic label quantum physics qp computer science cs qp and cs are topics in this example since there are some words specific to quantum computing such as qbit one can say that efficient mtc learners should use such words to assign label qp cs however the decomposition approach is likely to ignore these words since they are only specific to a small portion of the whole qp or cs papers there are many more qp and cs papers than quantum computing papers and therefore are not discriminative features for either topic qp or cs qbit is a unit of quantum information and frequently appears in real quantum computing literatures but rarely seen in other literatures fisymbol x rd t t tl t l t lj t xi li m i meaning a document vector topics the set of all topics a label the binary representation of l if tj l and otherwise the set of all possible labels training samples table notation parametric mixture model pmm adopts another approach to mtc it is assumed in pmm that multi topic texts are generated from a mixture of topic specific word distributions its decision on labeling is done at once not separately for each topic however pmm also has a problem with multi topic specific features such as qbit since it is impossible for texts to have such features given pmms mixture process these problems with multi topic specific features are caused by dependency assumptions between labels which are explicitly or implicitly made in existing methods to solve these problems we propose maximal margin labeling which treats labels as independent classes and learns a multi class classifier on the induced multi class problem in this paper we first discuss why multi class classifiers cannot be directly applied to mtc in section we then propose mml in section and address implementation issues in section in section mml is experimentally compared with existing methods using a collection of multi topic web pages we summarize this paper in section solving mtc as a multi class categorization to discuss why existing multi class classifiers do not work in mtc we start from the multi class classifier proposed in hereafter we use the notation given in table the multi class classifier in categorizes an object into the class whose prototype vector is the closest to the objects feature vector by substituting label for class the classifier can be written as follows f x arg max x m x where x is the the inner product of rd and m rd is the prototype vector of label following the similar argument as in the prototype vectors are learned by solving the following maximal margin problem min m m c im li x i st xi mli where m is the prototype matrix whose columns are the prototype vectors and m is the frobenius matrix norm of m note that eq and eq cover only training samples labels but also all possible labels this is because the labels unseen in training samples may be relevant to test samples in in eq we penalize all violation of the margin constraints on the other hand crammer and singer penalize only the largest violation of the margin constraint for each training sample we chose the penalize all approach since it leads to an optimization problem without equality constraints see eq which is much easier to solve than the one in xi m x i for i m li fiusual multi class problems such unseen labels seldom exist in mtc however the number of labels is generally very large eg one of our datasets has labels table and unseen labels often exist thus it is necessary to consider all possible labels in eq and eq since it is impossible to know which unseen labels are present in the test samples there are two problems with eq and eq the first problem is that they involve the prototype vectors of seldom or never seen labels without the help of prior knowledge about where the prototype vectors should be it is impossible to obtain appropriate prototype vectors for such labels the second problem is that these equations are computationally too demanding since they involve combinatorial maximization and summation over all possible labels whose number can be quite large for example the number is around in the datasets used in our experiments we will address the first problem in section and the second problem in section maximal margin labeling in this section we incorporate some prior knowledge about the location of prototype vectors into eq and eq and propose a novel mtc learning algorithm maximal margin labeling mml as prior knowledge we simply assume that the prototype vectors of similar labels should be placed close to each other based on this assumption we first rewrite eq to yield f x arg max m t x e l where l is the inner product of r and e is the orthonormal basis of r the classifier of eq can be interpreted as a two step process the first step is to map the vector x into r by m t and the second step is to find the closest e to image m t x then we replace e with generally non orthogonal vectors whose geometrical configuration reflects label similarity more formally speaking we use vectors that satisfy the condition s s for where s is an inner product of the vector space spanned by and s is a mercer kernel on and is a similarity measure between labels we call the vector space spanned by vs with this replacement mmls classifier is written as follows f x arg max w x s where w is a linear map from rd to vs w is the solution of the following problem min w w m c i li i li i i for i m li st w xi li note that if is replaced by e eq becomes identical to eq except for a scale factor thus eq and eq are natural extensions of the multi class classifier in we call the mtc classifier of eq and eq maximal margin labeling mml figure explains the margin the inner product in eq in mml the margin represents the distance from the image of the training sample xi to the boundary between the correct label li and wrong label mml optimizes the linear map w so that the smallest margin between all training samples and all possible labels becomes maximal along with a penalty c for the case that samples penetrate into the margin fifigure maximal margin labeling dual form for numerical computation the following wolfe dual form of eq is more convenient we omit its derivation due to space limits sli li sli s li s i max i i xi xi sli sli i i i i st i c for i m li m where we denote i li by i and i are the dual variables corresponding to the first inequality constraints in eq note that eq does not contain all the computations involving can be done through the label similarity s additionally xi only appears in the inner products and therefore can be replaced by any kernel of x using the solution i of eq the mmls classifier in eq can be written as follows sli l s l f x arg max i xxi sli l i label similarity as examples of label similarity we use two similarity measures dice measure and cosine measure l j l j dice measure sd sc j j l j j l j l j j l j cosine measure j j j j efficient implementation approximation in learning the following discussion is easily extended to include the case that both and are empty although we do not discuss the case due to space limits eq contains the sum over all possible labels as the number of topics l increases this summation rapidly becomes intractable since grows exponentially as l to circumvent fithis problem we approximate the sum over all possible labels in eq by the partial sum over i of a b c ac b and set all the other i to zero this approximation reduces the burden of the summation quite a lot the number of summands is reduced from l to l which is a huge reduction especially when many topics exist to understand the rationale behind the approximation first note that i is the dual variable corresponding to the first inequality constraint the margin constraint in eq thus i is non zero if and only if w xi falls in the margin between li and we assume that this margin violation mainly occurs when is close to li ie a b c ac b if this assumption holds well the proposed approximation of the sum will lead to a good approximation of the exact solution polynomial time algorithms for classification the classification of mml eq involves the combinatorial maximization over all possible labels so it can be a computationally demanding process however efficient classification algorithms are available when either the cosine measure or dice measure is used as label similarity eq can be divided into the subproblems by the number of topics in a label f x arg max gx l ln where gx is l ll l ll arg max gx l lln gx l j cn jlj i i cn j xxi i sd li xxi i sc li li j li n j n n if sd is used if sc is used li j j li n here n l the computational cost of eq for all j is on l n is the number of non zero and that of eq is ol log l thus the total cost of the classification by eq is on l l log l on the other hand n is oml under the approximation described above therefore the classification can be done within oml computational steps which is a significant reduction from the case that the brute force search is used in eq experiments in this section we report experiments that compared mml to pmm svm and boostexter using a collection of web pages we used a normalized linear kernel kx x x x x x in mml and svm as for boostexter real abstaining adaboostmh was used as the weak learner experimental setup the datasets used in our experiment represent the web page collection used in table the web pages were collected through the hyperlinks from yahoos top directory for each topic an svm classifier is trained to predict whether the topic is relevant positive or irrelevant negative to input doucments fidataset name abbrev arts humanities ar business economy bu computers internet co education ed entertainment en health he recreation rc reference rf science si social science ss society culture sc text voc tpc lbl label size frequency table a summary of the web page datasets text is the number of texts in the dataset voc the number of vocabularies ie features tpc the number of topics lbl the number of labels and label size frequency is the relative frequency of each label size label size is the number of topics in a label method mml pmm svm boost feature type tf tfidf tf tf tfidf binary parameter c model model c r table candidate feature types and learning parameters r is the number of weak hypotheses the underlined fetures and parameters were selected for the evaluation with the test data wwwyahoocom and then divided into datasets by yahoos top category each page is labeled with the yahoos second level sub categories from which the page is hyperlinked thus the sub categories are topics in our term see for more details about the collection then the web pages were converted into three types of feature vectors a binary vectors where each feature indicates the presence absence of a term by b tf vectors where each feature is the number of appearances of a term term frequency and c tfidf vectors where each feature is the product of term frequency and inverse document frequency to select the best combinations of feature types and learning parameters such as the penalty c for mml the learners were trained on web pages with all combinations of feature and parameter listed in table and then were evaluated by labeling f measure on independently drawn development data the combinations which achieve the best labeling f measures underlined in table were used in the following experiments evaluation measures we used three measures to evaluate labeling performance labeling f measure exact match ratio and retrieval f measure in the following definitions lpred n and ltrue n i i i i mean the predicted labels and the true labels respectively labeling f measure labeling f measure fl evaluates the average labeling performance while taking partial match into account fl n n i lpred ltrue i i lpred ltrue i i n n i l pred jltrue j i j li l lpred j ltrue j i i j fidataset ar bu co ed en he rc rf si ss sc avg md labeling f measure mc pm sv bo md exact match ratio mc pm sv bo md retrieval f measure mc pm sv bo table the performance comparison by labeling f measure left exact match ratio middle and retrieval f measure right the bold figures are the best ones among the five methods and the underlined figures the second best ones md mc pm sv and bo represent mml with sd mml with sc pmm svm and boostexter respectively exact match ratio exact match ratio ex counts only exact matches between the predicted label and the true label ex n n true ilpred li i i where is is if the statement s is true and otherwise retrieval f measure for real tasks it is also important to evaluate retrieval performance ie how accurately classifiers can find relevant texts for a given topic retrieval f measure fr measures the average retrieval performance over all topics fr l l j n pred jltrue j i i li n lpred j ltrue j i i i results first we trained the classifiers with randomly chosen samples we then calculated the three evaluation measures for other randomly chosen samples this process was repeated five times and the resulting averaged values are shown in table table shows that the mmls with dice measure outperform other methods in labeling f measure and exact match ratio the mmls also show the best performance with regard to retrieval fmeasure although the margins to the other methods are not as large as observed in labeling f measure and exact match ratio note that no classifier except mml with dice measure achieves good labeling on all the three measures for example pmm shows high labeling f measures but its performance is rather poor when evaluated in retrieval f measure as the second experiment we evaluated the classifiers trained with training samples on the same test samples figure shows each measure averaged over all datasets it is observed that the mmls show high generalization even when training data is small an interesting point is that mml with cosine measure achieves rather high labeling f measures and retrieval f measure with training data of smaller size such high performace however does not continue when trained on larger data fr is called the macro average of f measures in the text categorization community fifigure the learning curve of labeling f measure left exact match ratio middle and retrieval f measure right md mc pm sv bo mean the same as in table conclusion in this paper we proposed a novel learning algorithm for multi topic text categorization the algorithm maximal margin labeling embeds labels sets of topics into a similarityinduced vector space and learns a large margin classifier in the space to overcome the demanding computational cost of mml we provide an approximation method in learning and efficient classification algorithms in experiments on a collection of web pages mml outperformed other methods including svm and showed better generalization acknowledgement the authors would like to thank naonori ueda kazumi saito and yuji kaneda of nippon telegraph and telephone corporation for providing pmms codes and the datasets references thorsten joachims text categorization with support vector machines learning with many relevant features in claire n dellec and c line rouveirol editors proc of the e e th european conference on machine learning number pages robert e schapire and yoram singer boostexter a boosting based system for text categorization machine learning naonori ueda and kazumi saito parametoric mixture models for multi topic text in advances in neural information processing systems pages koby crammer and yoram singer on the algorithmic implementation of multiclass kernel based vector machines journal of machine learning research klaus robert m ller sebastian mika gunnar r tsch koji tsuda and bernhard u a sch lkopf an
spike sorting involves clustering spike trains recorded by a microelectrode according to the source neuron it is a complicated problem which requires a lot of human labor partly due to the non stationary nature of the data we propose an automated technique for the clustering of non stationary gaussian sources in a bayesian framework at a first search stage data is divided into short time frames and candidate descriptions of the data as a mixture of gaussians are computed for each frame at a second stage transition probabilities between candidate mixtures are computed and a globally optimal clustering is found as the map solution of the resulting probabilistic model transition probabilities are computed using local stationarity assumptions and are based on a gaussian version of the jensen shannon divergence the method was applied to several recordings the performance appeared almost indistinguishable from humans in a wide range of scenarios including movement merges and splits of clusters
the structural similarity of neural networks and genetic regulatory networks to digital circuits and hence to each other was noted from the very beginning of their study in this work we propose a simple biochemical system whose architecture mimics that of genetic regulation and whose components allow for in vitro implementation of arbitrary circuits we use only two enzymes in addition to dna and rna molecules rna polymerase rnap and ribonuclease rnase we develop a rate equation for in vitro transcriptional networks and derive a correspondence with general neural network rate equations as proof of principle demonstrations an associative memory task and a feedforward network computation are shown by simulation a difference between the neural network and biochemical models is also highlighted global coupling of rate equations through enzyme saturation can lead to global feedback regulation thus allowing a simple network without explicit mutual inhibition to perform the winner take all computation thus the full complexity of the cell is not necessary for biochemical computation a wide range of functional behaviors can be achieved with a small set of biochemical components
the standard approach to the classification of objects is to consider the examples as independent and identically distributed iid in many real world settings however this assumption is not valid because a topographical relationship exists between the objects in this contribution we consider the special case of image segmentation where the objects are pixels and where the underlying topography is a d regular rectangular grid we introduce a classification method which not only uses measured vectorial feature information but also the label configuration within a topographic neighborhood due to the resulting dependence between the labels of neighboring pixels a collective classification of a set of pixels becomes necessary we propose a new method called topographic support vector machine tsvm which is based on a topographic kernel and a self consistent solution to the label assignment shown to be equivalent to a recurrent neural network the performance of the algorithm is compared to a conventional svm on a cell image segmentation task
we present a probabilistic approach to learning a gaussian process classifier in the presence of unlabeled data our approach involves a null category noise model ncnm inspired by ordered categorical noise models the noise model reflects an assumption that the data density is lower between the class conditional densities we illustrate our approach on a toy problem and present comparative results for the semi supervised classification of handwritten digits
this paper analyzes generalization of the classic rescorla wagner rw learning algorithm and studies their relationship to maximum likelihood estimation of causal parameters we prove that the parameters of two popular causal models p and p c can be learnt by the same generalized linear rescorla wagner glrw algorithm provided genericity conditions apply we characterize the fixed points of these glrw algorithms and calculate the fluctuations about them assuming that the input is a set of iid samples from a fixed unknown distribution we describe how to determine convergence conditions and calculate convergence rates for the glrw algorithms under these conditions
this paper analyses the contrastive divergence algorithm for learning statistical parameters we relate the algorithm to the stochastic approximation literature this enables us to specify conditions under which the algorithm is guaranteed to converge to the optimal solution with probability this includes necessary and sufficient conditions for the solution to be unbiased
we propose a convex optimization based strategy to deal with uncertainty in the observations of a classification problem we assume that instead of a sample xi yi a distribution over xi yi is specified in particular we derive a robust formulation when the distribution is given by a normal distribution it leads to second order cone programming formulation our method is applied to the problem of missing data where it outperforms direct imputation
we describe how we used a data set of chorale harmonisations composed by johann sebastian bach to train hidden markov models using a probabilistic framework allows us to create a harmonisation system which learns from examples and which can compose new harmonisations we make a quantitative comparison of our systems harmonisation performance against simpler models and provide example harmonisations
in this paper linear multilayer ica lmica is proposed for extracting independent components from quite high dimensional observed signals such as large size natural scenes there are two phases in each layer of lmica one is the mapping phase where a one dimensional mapping is formed by a stochastic gradient algorithm which makes more highlycorrelated non independent signals be nearer incrementally another is the local ica phase where each neighbor namely highly correlated pair of signals in the mapping is separated by the maxkurt algorithm because lmica separates only the highly correlated pairs instead of all ones it can extract independent components quite efficiently from appropriate observed signals in addition it is proved that lmica always converges some numerical experiments verify that lmica is quite efficient and effective in large size natural image processing
prediction suffix trees pst provide a popular and effective tool for tasks such as compression classification and language modeling in this paper we take a decision theoretic view of psts for the task of sequence prediction generalizing the notion of margin to psts we present an online pst learning algorithm and derive a loss bound for it the depth of the pst generated by this algorithm scales linearly with the length of the input we then describe a self bounded enhancement of our learning algorithm which automatically grows a bounded depth pst we also prove an analogous mistake bound for the self bounded algorithm the result is an efficient algorithm that neither relies on a priori assumptions on the shape or maximal depth of the target pst nor does it require any parameters to our knowledge this is the first provably correct pst learning algorithm which generates a bounded depth pst while being competitive with any fixed pst determined in hindsight
an important aspect of clustering algorithms is whether the partitions constructed on finite samples converge to a useful clustering of the whole data space as the sample size increases this paper investigates this question for normalized and unnormalized versions of the popular spectral clustering algorithm surprisingly the convergence of unnormalized spectral clustering is more difficult to handle than the normalized case even though recently some first results on the convergence of normalized spectral clustering have been obtained for the unnormalized case we have to develop a completely new approach combining tools from numerical integration spectral and perturbation theory and probability it turns out that while in the normalized case spectral clustering usually converges to a nice partition of the data space in the unnormalized case the same only holds under strong additional assumptions which are not always satisfied we conclude that our analysis gives strong evidence for the superiority of normalized spectral clustering it also provides a basis for future exploration of other laplacian based methods
computation without stable states is a computing paradigm different from turings and has been demonstrated for various types of simulated neural networks this publication transfers this to a hardware implemented neural network results of a software implementation are reproduced showing that the performance peaks when the network exhibits dynamics at the edge of chaos the liquid computing approach seems well suited for operating analog computing devices such as the used vlsi neural network

we consider the problem of geometrical surface reconstruction from one or several images using learned shape models while humans can effortlessly retrieve d shape information this inverse problem has turned out to be difficult to perform automatically we introduce a framework based on level set surface reconstruction and shape models for achieving this goal through this merging we obtain an efficient and robust method for reconstructing surfaces of an object category of interest the shape model includes surface cues such as point curve and silhouette features based on ideas from active shape models we show how both the geometry and the appearance of these features can be modelled consistently in a multi view context the complete surface is obtained by evolving a level set driven by a pde which tries to fit the surface to the inferred d features in addition an a priori d surface model is used to regularize the solution in particular where surface features are sparse experiments are demonstrated on a database of real face images
in this paper we show that it is possible to model sensory impressions of consumers about beef meat this is not a straightforward task the reason is that when we are aiming to induce a function that maps object descriptions into ratings we must consider that consumers ratings are just a way to express their preferences about the products presented in the same testing session therefore we had to use a special purpose svm polynomial kernel the training data set used collects the ratings of panels of experts and consumers the meat was provided by bovines of spanish breeds with different carcass weights and aging periods additionally to gain insight into consumer preferences we used feature subset selection tools the result is that aging is the most important trait for improving consumers appreciation of beef meat
the problem of detecting atypical objects or outliers is one of the classical topics in robust statistics recently it has been proposed to address this problem by means of one class svm classifiers the main conceptual shortcoming of most one class approaches however is that in a strict sense they are unable to detect outliers since the expected fraction of outliers has to be specified in advance the method presented in this paper overcomes this problem by relating kernelized one class classification to gaussian density estimation in the induced feature space having established this relation it is possible to identify atypical objects by quantifying their deviations from the gaussian model for rbf kernels it is shown that the gaussian model is rich enough in the sense that it asymptotically provides an unbiased estimator for the true density in order to overcome the inherent model selection problem a cross validated likelihood criterion for selecting all free model parameters is applied
we establish a mistake bound for an ensemble method for classification based on maximizing the entropy of voting weights subject to margin constraints the bound is the same as a general bound proved for the weighted majority algorithm and similar to bounds for other variants of winnow we prove a more refined bound that leads to a nearly optimal algorithm for learning disjunctions again based on the maximum entropy principle we describe a simplification of the on line maximum entropy method in which after each iteration the margin constraints are replaced with a single linear inequality the simplified algorithm which takes a similar form to winnow achieves the same mistake bounds
we provide a worst case analysis of selective sampling algorithms for learning linear threshold functions the algorithms considered in this paper are perceptron like algorithms ie algorithms which can be efficiently run in any reproducing kernel hilbert space our algorithms exploit a simple margin based randomized rule to decide whether to query the current label we obtain selective sampling algorithms achieving on average the same bounds as those proven for their deterministic counterparts but using much fewer labels we complement our theoretical findings with an empirical comparison on two text categorization tasks the outcome of these experiments is largely predicted by our theoretical results our selective sampling algorithms tend to perform as good as the algorithms receiving the true label after each classification while observing in practice substantially fewer labels
we describe a framework for learning an object classifier from a single example this goal is achieved by emphasizing the relevant dimensions for classification using available examples of related classes learning to accurately classify objects from a single training example is often unfeasible due to overfitting effects however if the instance representation provides that the distance between each two instances of the same class is smaller than the distance between any two instances from different classes then a nearest neighbor classifier could achieve perfect performance with a single training example we therefore suggest a two stage strategy first learn a metric over the instances that achieves the distance criterion mentioned above from available examples of other related classes then using the single examples define a nearest neighbor classifier where distance is evaluated by the learned class relevance metric finding a metric that emphasizes the relevant dimensions for classification might not be possible when restricted to linear projections we therefore make use of a kernel based metric learning algorithm our setting encodes object instances as sets of locality based descriptors and adopts an appropriate image kernel for the class relevance metric learning the proposed framework for learning from a single example is demonstrated in a synthetic setting and on a character classification task
what makes a neural microcircuit computationally powerful or more precisely which measurable quantities could explain why one microcircuit c is better suited for a particular family of computational tasks than another microcircuit c we propose in this article quantitative measures for evaluating the computational power and generalization capability of a neural microcircuit and apply them to generic neural microcircuit models drawn from different distributions we validate the proposed measures by comparing their prediction with direct evaluations of the computational performance of these microcircuit models this procedure is applied first to microcircuit models that differ with regard to the spatial range of synaptic connections and with regard to the scale of synaptic efficacies in the circuit and then to microcircuit models that differ with regard to the level of background input currents and the level of noise on the membrane potential of neurons in this case the proposed method allows us to quantify differences in the computational power and generalization capability of circuits in different dynamic regimes upand down states that have been demonstrated through intracellular recordings in vivo
in this paper we present a framework for using multi layer perceptron mlp networks in nonlinear generative models trained by variational bayesian learning the nonlinearity is handled by linearizing it using a gauss hermite quadrature at the hidden neurons this yields an accurate approximation for cases of large posterior variance the method can be used to derive nonlinear counterparts for linear algorithms such as factor analysis independent componentfactor analysis and state space models this is demonstrated with a nonlinear factor analysis experiment in which even sources can be estimated from a real world speech data set
an auditory scene composed of overlapping acoustic sources can be viewed as a complex object whose constituent parts are the individual sources pitch is known to be an important cue for auditory scene analysis in this paper with the goal of building agents that operate in human environments we describe a real time system to identify the presence of one or more voices and compute their pitch the signal processing in the front end is based on instantaneous frequency estimation a method for tracking the partials of voiced speech while the pattern matching in the back end is based on nonnegative matrix factorization an unsupervised algorithm for learning the parts of complex objects while supporting a framework to analyze complicated auditory scenes our system maintains real time operability and state of the art performance in clean speech
log concavity is an important property in the context of optimization laplace approximation and sampling bayesian methods based on gaussian process priors have become quite popular recently for classification regression density estimation and point process intensity estimation here we prove that the predictive densities corresponding to each of these applications are log concave given any observed data we also prove that the likelihood is log concave in the hyperparameters controlling the mean function of the gaussian prior in the density and point process intensity estimation cases and the mean covariance and observation noise parameters in the classification and regression cases this result leads to a useful parameterization of these hyperparameters indicating a suitably large class of priors for which the corresponding maximum a posteriori problem is log concave
we present an algorithm based on convex optimization for constructing kernels for semi supervised learning the kernel matrices are derived from the spectral decomposition of graph laplacians and combine labeled and unlabeled data in a systematic fashion unlike previous work using diffusion kernels and gaussian random field kernels a nonparametric kernel approach is presented that incorporates order constraints during optimization this results in flexible kernels and avoids the need to choose among different parametric forms our approach relies on a quadratically constrained quadratic program qcqp and is computationally feasible for large datasets we evaluate the kernels on real datasets using support vector machines with encouraging results
there has been substantial progress in the past decade in the development of object classifiers for images for example of faces humans and vehicles here we address the problem of contaminations eg occlusion shadows in test images which have not explicitly been encountered in training data the variational ising classifier vic algorithm models contamination as a mask a field of binary variables with a strong spatial coherence prior variational inference is used to marginalize over contamination and obtain robust classification in this way the vic approach can turn a kernel classifier for clean data into one that can tolerate contamination without any specific training on contaminated positives
the bradley terry model for paired comparison has been popular in many areas we propose a generalized version in which paired individual comparisons are extended to paired team comparisons we introduce a simple algorithm with convergence proofs to solve the model and obtain individual skill a useful application to multi class probability estimates using error correcting codes is demonstrated

regularization plays a central role in the analysis of modern data where non regularized fitting is likely to lead to over fitted models useless for both prediction and interpretation we consider the design of incremental algorithms which follow paths of regularized solutions as the regularization varies these approaches often result in methods which are both efficient and highly flexible we suggest a general path following algorithm based on second order approximations prove that under mild conditions it remains very close to the path of optimal solutions and illustrate it with examples
we present an algorithm to overcome the local maxima problem in estimating the parameters of mixture models it combines existing approaches from both em and a robust fitting algorithm ransac to give a data driven stochastic learning scheme minimal subsets of data points sufficient to constrain the parameters of the model are drawn from proposal densities to discover new regions of high likelihood the proposal densities are learnt using em and bias the sampling toward promising solutions the algorithm is computationally efficient as well as effective at escaping from local maxima we compare it with alternative methods including em and ransac on both challenging synthetic data and the computer vision problem of alpha matting
the computation of classical higher order statistics such as higher order moments or spectra is difficult for images due to the huge number of terms to be estimated and interpreted we propose an alternative approach in which multiplicative pixel interactions are described by a series of wiener functionals since the functionals are estimated implicitly via polynomial kernels the combinatorial explosion associated with the classical higher order statistics is avoided first results show that image structures such as lines or corners can be predicted correctly and that pixel interactions up to the order of five play an important role in natural images most of the interesting structure in a natural image is characterized by its higher order statistics arbitrarily oriented lines and edges for instance cannot be described by the usual pairwise statistics such as the power spectrum or the autocorrelation function from knowing the intensity of one point on a line alone we cannot predict its neighbouring intensities this would require knowledge of a second point on the line ie we have to consider some third order statistics which describe the interactions between triplets of points analogously the prediction of a corner neighbourhood needs at least fourth order statistics and so on in terms of fourier analysis higher order image structures such as edges or corners are described by phase alignments ie phase correlations between several fourier components of the image classically harmonic phase interactions are measured by higher order spectra unfortunately the estimation of these spectra for high dimensional signals such as images involves the estimation and interpretation of a huge number of terms for instance a sixth order spectrum of a sized image contains roughly coefficients about of which would have to be estimated independently if all symmetries in the spectrum are considered first attempts at estimating the higher order structure of natural images were therefore restricted to global measures such as skewness or kurtosis or to submanifolds of fourth order spectra here we propose an alternative approach that models the interactions of image points in a series of wiener functionals a wiener functional of order n captures those image components that can be predicted from the multiplicative interaction of n image points in contrast to higher order spectra or moments the estimation of a wiener model does not require the estimation of an excessive number of terms since it can be computed implicitly fivia polynomial kernels this allows us to decompose an image into components that are characterized by interactions of a given order in the next section we introduce the wiener expansion and discuss its capability of modeling higher order pixel interactions the implicit estimation method is described in sect followed by some examples of use in sect we conclude in sect by briefly discussing the results and possible improvements modeling pixel interactions with wiener functionals for our analysis we adopt a prediction framework given a d d neighbourhood of an image pixel we want to predict its gray value from the gray values of the neighbours we are particularly interested to which extent interactions of different orders contribute to the overall prediction our basic assumption is that the dependency of the central pixel value y on its neighbours xi i m d can be modeled as a series y h x h x h x hn x of discrete volterra functionals h x h const and hn x m i m in hi in xi xin n here we have stacked the grayvalues of the neighbourhood into the vector x x xm rm the discrete nth order volterra functional is accordingly a linear combination of all ordered nth order monomials of the elements of x with mn coefficients n hi in volterra functionals provide a controlled way of introducing multiplicative interactions of image points since a functional of order n contains all products of the input of order n in terms of higher order statistics this means that we can control the order of the statistics used since an nth order volterra series leads to dependencies between maximally n pixels unfortunately volterra functionals are not orthogonal to each other ie depending on the input distribution a functional of order n generally leads to additional lower order interactions as a result the output of the functional will contain components that are proportional to that of some lower order monomials for instance the output of a second order volterra functional for gaussian input generally has a mean different from zero if one wants to estimate the zeroeth order component of an image ie the constant component created without pixel interactions the constant component created by the second order interactions needs to be subtracted for general volterra series this correction can be achieved by decomposing it into a new series y g x g x gn x of functionals gn x that are uncorrelated ie orthogonal with respect to the input the resulting wiener functionals gn x are linear combinations of volterra functionals up to order n they are computed from the original volterra series by a procedure akin to gram schmidt orthogonalization it can be shown that any wiener expansion of finite degree minimizes the mean squared error between the true system output and its volterra series model the orthogonality condition ensures that a wiener functional of order n captures only the component of the image created by the multiplicative interaction of n pixels in contrast to general volterra functionals a wiener functional is orthogonal to all monomials of lower order so far we have not gained anything compared to classical estimation of higher order moments or spectra an nth order volterra functional contains the same number of terms as strictly speaking the term wiener functional is reserved for orthogonal volterra functionals with respect to gaussian input here the term will be used for orthogonalized volterra functionals with arbitrary input distributions fithe corresponding n order spectrum and a wiener functional of the same order has an even higher number of coefficients as it consists also of lower order volterra functionals in the next section we will introduce an implicit representation of the wiener series using polynomial kernels which allows for an efficient computation of the wiener functionals estimating wiener series by regression in rkhs volterra series as linear functionals in rkhs the nth order volterra functional is a weighted sum of all nth order monomials of the input vector x we can interpret the evaluation of this functional for a given input x as a map n defined for n as x and n n n x xn x x x x xn xn m n such that n maps the input x rm into a vector n x fn rm containing all mn ordered monomials of degree n using n we can write the nth order volterra functional in eq as a scalar product in fn hn x n n x n n n with the coefficients stacked into the vector n h h h fn the same idea can be applied to the entire pth order volterra series by stacking the maps n into a single map p x x x p x one obtains a mapping from p p rm into fp r rm rm rm rm with dimensionality m m the m entire pth order volterra series can be written as a scalar product in fp p n hn x p p x with p fp below we will show how we can express p as an expansion in terms of the training points this will dramatically reduce the number of parameters we have to estimate this procedure works because the space fn of nth order monomials has a very special property it has the structure of a reproducing kernel hilbert space rkhs as a consequence the dot product in fn can be computed by evaluating a positive definite kernel function kn x x for monomials one can easily show that eg n x n x x x n kn x x since fp is generated as a direct sum of the single spaces fn the associated scalar product is simply the sum of the scalar products in the fn p x p x p n x x n k p x x thus we have shown that the discretized volterra series can be expressed as a linear functional in a rkhs linear regression in rkhs for our prediction problem the rkhs property of the volterra series leads to an efficient solution which is in part due to the so called representer theorem eg it states the following suppose we are given n observations p kinh x x a similar approach has been taken by using the inhomogeneous polynomial kernel x x p this kernel implies a map inh into the same space of monomials but it weights the degrees of the monomials differently as can be seen by applying the binomial theorem fix y xn yn of the function and an arbitrary cost function c is a nondecreasing function on r and f is the norm of the rkhs associated with the kernel k if we minimize an objective function cx y f x xn yn f xn f f over all functions in the rkhs then an optimal solution can be expressed as f x n j aj kx xj aj r in other words although we optimized over the entire rkhs including functions which are defined for arbitrary input points it turns out that we can always express the solution in terms of the observations xj only hence the optimization problem over the extremely large number of coefficients p in eq is transformed into one over n variables aj let us consider the special case where the cost function is the mean squared error n cx y f x xn yn f xn n j f xj yj and the regularizer is zero the solution for a a an is readily computed by setting the derivative of with respect to the vector a equal to zero it takes the form a k y with the gram matrix defined as kij kxi xj hence y f x a zx y k zx where zx kx x kx x kx xn r implicit wiener series estimation as we stated above the pth degree wiener expansion is the pth order volterra series that minimizes the squared error this can be put into the regression framework since any finite volterra series can be represented as a linear functional in the corresponding rkhs we can find the pth order volterra series that minimizes the squared error by linear regression this by definition must be the pth degree wiener series since no other volterra series has this property from eqn we obtain the following expressions for the implicit wiener series p p g x y gn x hn x y kp zp x n n n p where the gram matrix kp and the coefficient vector z x are computed using the kernel from eq and rn note that the wiener series is represented only implicitly since we are using the rkhs representation as a sum of scalar products with the training points thus we can avoid the curse of dimensionality ie there is no need to compute the possibly large number of coefficients explicitly the explicit volterra and wiener expansions can be recovered from eq by collecting all terms containing monomials of the desired order and summing them up the individual nth order volterra functionals in a wiener series of degree p are given implicitly by hn x y kp zn x n n n n with zn x x x x x xn x for p the only term is the constant zero order volterra functional h x g x the coefficient vector n n n n h h h of the explicit volterra functional is obtained as n n kp y for conditions on uniqueness of the solution see note that this is different from the regularized approach used by if is not zero the resulting volterra series are different from the wiener series since they are not orthogonal with respect to the input if k is not invertible k denotes the pseudo inverse of k assuming symmetrized volterra kernels which can be obtained from any volterra expanson fiusing the design matrix n n x n x n x the individual wiener functionals can only be recovered by applying the regression procedure twice if we are interested in the nth degree wiener functional we have to compute the solution for the kernels k n x x and k n x x the wiener functional for n is then obtained from the difference of the two results as gn x n i gi x n i gi x y kn zn x kn zn x the corresponding ith order volterra functionals of the nth degree wiener functional are computed analogously to eqns and orthogonality the resulting wiener functionals must fulfill the orthogonality condition which in its strictest form states that a pth degree wiener functional must be orthogonal to all monomials in the input of lower order formally we will prove the following theorem the functionals obtained from eq fulfill the orthogonality condition e mxgp x where e denotes the expectation over the input distribution and mx an arbitrary ithorder monomial with i p we will show that this a consequence of the least squares fit of any linear expansion in a set m of basis functions of the form y j j j x in the case of the wiener and volterra expansions the basis functions j x are monomials of the components of x we denote the error of the expansion as ex y j j j xi the minimum of the expected quadratic loss l with respect to the expansion coefficient k is given by l e ex k k m e k xex this means that for an expansion in a set of basis functions minimizing the squared error the error is orthogonal to all basis functions used in the expansion now let us assume we know the wiener series expansion which minimizes the mean squared error of a system up to degree p the approximation error is given by the sum of the higher order wiener functionals ex np gn x so gp x is part of the error as a consequence of the linearity of the expectation eq implies np e k xgn x and np e k xgn x for any k of order less than p the difference of both equations yields e k xgp x so that gp x must be orthogonal to any of the lower order basis functions namely to all monomials with order smaller than p experiments toy examples in our first experiment we check whether our intuitions about higher order statistics described in the
a new distance measure between probability density functions pdfs is introduced which we refer to as the laplacian pdf distance the laplacian pdf distance exhibits a remarkable connection to mercer kernel based learning theory via the parzen window technique for density estimation in a kernel feature space defined by the eigenspectrum of the laplacian data matrix this pdf distance is shown to measure the cosine of the angle between cluster mean vectors the laplacian data matrix and hence its eigenspectrum can be obtained automatically based on the data at hand by optimal parzen window selection we show that the laplacian pdf distance has an interesting interpretation as a risk function connected to the probability of error

many interesting multiclass problems can be cast in the general framework of label ranking defined on a given set of classes the evaluation for such a ranking is generally given in terms of the number of violated order constraints between classes in this paper we propose the preference learning model as a unifying framework to model and solve a large class of multiclass problems in a large margin perspective in addition an original kernel based method is proposed and evaluated on a ranking dataset with state of the art results
this paper presents an application of boosting for classifying labeled graphs general structures for modeling a number of real world data such as chemical compounds natural language texts and bio sequences the proposal consists of i decision stumps that use subgraph as features and ii a boosting algorithm in which subgraph based decision stumps are used as weak learners we also discuss the relation between our algorithm and svms with convolution kernels two experiments using natural language data and chemical compounds show that our method achieves comparable or even better performance than svms with convolution kernels as well as improves the testing efficiency
dominant sets are a new graph theoretic concept that has proven to be relevant in pairwise data clustering problems such as image segmentation they generalize the notion of a maximal clique to edgeweighted graphs and have intriguing non trivial connections to continuous quadratic optimization and spectral based grouping we address the problem of grouping out of sample examples after the clustering process has taken place this may serve either to drastically reduce the computational burden associated to the processing of very large data sets or to efficiently deal with dynamic situations whereby data sets need to be updated continually we show that the very notion of a dominant set offers a simple and efficient way of doing this numerical experiments on various grouping problems show the effectiveness of the approach
de novo sequencing of peptides is a challenging task in proteome research while there exist reliable dna sequencing methods the highthroughput de novo sequencing of proteins by mass spectrometry is still an open problem current approaches suffer from a lack in precision to detect mass peaks in the spectrograms in this paper we present a novel method for de novo peptide sequencing based on a hidden markov model experiments effectively demonstrate that this new method significantly outperforms standard approaches in matching quality
in this paper we analyze the relationship between the computational capabilities of randomly connected networks of threshold gates in the timeseries domain and their dynamical properties in particular we propose a complexity measure which we find to assume its highest values near the edge of chaos ie the transition from ordered to chaotic dynamics furthermore we show that the proposed complexity measure predicts the computational capabilities very well only near the edge of chaos are such networks able to perform complex computations on time series additionally a simple synaptic scaling rule for self organized criticality is presented and analyzed
we propose to selectively remove examples from the training set using probabilistic estimates related to editing algorithms devijver and kittler this heuristic procedure aims at creating a separable distribution of training examples with minimal impact on the position of the decision boundary it breaks the linear dependency between the number of svs and the number of training examples and sharply reduces the complexity of svms during both the training and prediction stages
we prove generalization error bounds for predicting entries in a partially observed matrix by fitting the observed entries with a low rank matrix in justifying the analysis approach we take to obtain the bounds we present an example of a class of functions of finite pseudodimension such that the sums of functions from this class have unbounded pseudodimension
co training is a method for combining labeled and unlabeled data when examples can be thought of as containing two distinct sets of features it has had a number of practical successes yet previous theoretical analyses have needed very strong assumptions on the data that are unlikely to be satisfied in practice in this paper we propose a much weaker expansion assumption on the underlying data distribution that we prove is sufficient for iterative cotraining to succeed given appropriately strong pac learning algorithms on each feature set and that to some extent is necessary as well this expansion assumption in fact motivates the iterative nature of the original co training algorithm unlike stronger assumptions such as independence given the label that allow a simpler one shot co training to succeed we also heuristically analyze the effect on performance of noise in the data predicted behavior is qualitatively matched in synthetic experiments on expander graphs

learning algorithms have enjoyed numerous successes in robotic control tasks in problems with time varying dynamics online learning methods have also proved to be a powerful tool for automatically tracking andor adapting to the changing circumstances however for safety critical applications such as airplane flight the adoption of these algorithms has been significantly hampered by their lack of safety such as stability guarantees rather than trying to show difficult a priori stability guarantees for specific learning methods in this paper we propose a method for monitoring the controllers suggested by the learning algorithm online and rejecting controllers leading to instability we prove that even if an arbitrary online learning method is used with our algorithm to control a linear dynamical system the resulting system is stable
we propose a new set of criteria for learning algorithms in multi agent systems one that is more stringent and we argue better justified than previous proposed criteria our criteria which apply most straightforwardly in repeated games with average rewards consist of three requirements a against a specified class of opponents this class is a parameter of the criterion the algorithm yield a payoff that approaches the payoff of the best response b against other opponents the algorithms payoff at least approach and possibly exceed the security level payoff or maximin value and c subject to these requirements the algorithm achieve a close to optimal payoff in self play we furthermore require that these average payoffs be achieved quickly we then present a novel algorithm and show that it meets these new criteria for a particular parameter class the class of stationary opponents finally we show that the algorithm is effective not only in theory but also empirically using a recently introduced comprehensive game theoretic test suite we show that the algorithm almost universally outperforms previous learning algorithms

we describe semi markov conditional random fields semi crfs a conditionally trained version of semi markov chains intuitively a semicrf on an input sequence x outputs a segmentation of x in which labels are assigned to segments ie subsequences of x rather than to individual elements xi of x importantly features for semi crfs can measure properties of segments and transitions within a segment can be non markovian in spite of this additional power exact learning and inference algorithms for semi crfs are polynomial time often only a small constant factor slower than conventional crfs in experiments on five named entity recognition problems semi crfs generally outperform conventional crfs
we give a fast rejection scheme that is based on image segments and demonstrate it on the canonical example of face detection however instead of focusing on the detection step we focus on the rejection step and show that our method is simple and fast to be learned thus making it an excellent pre processing step to accelerate standard machine learning classifiers such as neural networks bayes classifiers or svm we decompose a collection of face images into regions of pixels with similar behavior over the image set the relationships between the mean and variance of image segments are used to form a cascade of rejectors that can reject over of image patches thus only a small fraction of the image patches must be passed to a full scale classifier moreover the training time for our method is much less than an hour on a standard pc the shape of the features ie image segments we use is data driven they are very cheap to compute and they form a very low dimensional feature space in which exhaustive search for the best features is tractable
this paper proposes a method for computing fast approximations to support vector decision functions in the field of object detection in the present approach we are building on an existing algorithm where the set of support vectors is replaced by a smaller so called reduced set of synthesized input space points in contrast to the existing method that finds the reduced set via unconstrained optimization we impose a structural constraint on the synthetic points such that the resulting approximations can be evaluated via separable filters for applications that require scanning large images this decreases the computational complexity by a significant amount experimental results show that in face detection rank deficient approximations are to times faster than unconstrained reduced set systems
we introduce a novel active learning scenario in which a user wants to work with a learning algorithm to identify useful anomalies these are distinguished from the traditional statistical definition of anomalies as outliers or merely ill modeled points our distinction is that the usefulness of anomalies is categorized subjectively by the user we make two additional assumptions first there exist extremely few useful anomalies to be hunted down within a massive dataset second both useful and useless anomalies may sometimes exist within tiny classes of similar anomalies the challenge is thus to identify rare category records in an unlabeled noisy set with help in the form of class labels from a human expert who has a small budget of datapoints that they are prepared to categorize we propose a technique to meet this challenge which assumes a mixture model fit to the data but otherwise makes no assumptions on the particular form of the mixture components this property promises wide applicability in real life scenarios and for various statistical models we give an overview of several alternative methods highlighting their strengths and weaknesses and conclude with a detailed empirical analysis we show that our method can quickly zoom in on an anomaly set containing a few tens of points in a dataset of hundreds of thousands
the computation and memory required for kernel machines with n training samples is at least on such a complexity is significant even for moderate size problems and is prohibitive for large datasets we present an approximation technique based on the improved fast gauss transform to reduce the computation to on we also give an error bound for the approximation and provide experimental results on the uci datasets
a novel linear feature selection algorithm is presented based on the global minimization of a data dependent generalization error bound feature selection and scaling algorithms often lead to non convex optimization problems which in many previous approaches were addressed through gradient descent procedures that can only guarantee convergence to a local minimum we propose an alternative approach whereby the global solution of the non convex optimization problem is derived via an equivalent optimization problem moreover the convex optimization task is reduced to a conic quadratic programming problem for which efficient solvers are available highly competitive numerical results on both artificial and real world data sets are reported
during the last ten years there has been growing interest in the development of brain computer interfaces bcis the field has mainly been driven by the needs of completely paralyzed patients to communicate with a few exceptions most human bcis are based on extracranial electroencephalography eeg however reported bit rates are still low one reason for this is the low signal to noise ratio of the eeg we are currently investigating if bcis based on electrocorticography ecog are a viable alternative in this paper we present the method and examples of intracranial eeg recordings of three epilepsy patients with electrode grids placed on the motor cortex the patients were asked to repeatedly imagine movements of two kinds eg tongue or finger movements we analyze the classifiability of the data using support vector machines svms and recursive channel elimination rce
we describe methods for computing an implicit model of a hypersurface that is given only by a finite sampling the methods work by mapping the sample points into a reproducing kernel hilbert space and then determining regions in terms of hyperplanes
we develop a family of upper and lower bounds on the worst case expected kl loss for estimating a discrete distribution on a finite number m of points given n iid samples our upper bounds are approximationtheoretic similar to recent bounds for estimating discrete entropy the lower bounds are bayesian based on averages of the kl loss under dirichlet distributions the upper bounds are convex in their parameters and thus can be minimized by descent methods to provide estimators with low worst case error the lower bounds are indexed by a one dimensional parameter and are thus easily maximized asymptotic analysis of the bounds demonstrates the uniform kl consistency of a wide class of estimators as c nm no matter how slowly and shows that no estimator is consistent for c bounded in contrast to entropy estimation moreover the bounds are asymptotically tight as c or and are shown numerically to be tight within a factor of two for all c finally in the sparse data limit c we find that the dirichlet bayes add constant estimator with parameter scaling like c logc optimizes both the upper and lower bounds suggesting an optimal choice of the add constant parameter in this regime
in this paper we propose to combine two powerful ideas boosting and manifold learning on the one hand we improve adaboost by incorporating knowledge on the structure of the data into base classifier design and selection on the other hand we use adaboosts efficient learning mechanism to significantly improve supervised and semi supervised algorithms proposed in the context of manifold learning beside the specific manifold based penalization the resulting algorithm also accommodates the boosting of a large family of regularized learning algorithms
we propose a gossip based distributed algorithm for gaussian mixture learning newscast em the algorithm operates on network topologies where each node observes a local quantity and can communicate with other nodes in an arbitrary point to point fashion the main difference between newscast em and the standard em algorithm is that the m step in our case is implemented in a decentralized manner random pairs of nodes repeatedly exchange their local parameter estimates and combine them by weighted averaging we provide theoretical evidence and demonstrate experimentally that under this protocol nodes converge exponentially fast to the correct estimates in each m step of the em algorithm
amino acid profiles which capture position specific mutation probabilities are a richer encoding of biological sequences than the individual sequences themselves however profile comparisons are much more computationally expensive than discrete symbol comparisons making profiles impractical for many large datasets furthermore because they are such a rich representation profiles can be difficult to visualize to overcome these problems we propose a discretization for profiles using an expanded alphabet representing not just individual amino acids but common profiles by using an extension of information bottleneck ib incorporating constraints and priors on the class distributions we find an informationally optimal alphabet this discretization yields a concise informative textual representation for profile sequences also alignments between these sequences while nearly as accurate as the full profileprofile alignments can be computed almost as quickly as those between individual or consensus sequences a full pairwise alignment of swissprot would take years using profiles but less than days using a discrete ib encoding illustrating how discrete encoding can expand the range of sequence problems to which profile information can be applied
we formulate the problem of graph inference where part of the graph is known as a supervised learning problem and propose an algorithm to solve it the method involves the learning of a mapping of the vertices to a euclidean space where the graph is easy to infer and can be formulated as an optimization problem in a reproducing kernel hilbert space we report encouraging results on the problem of metabolic network reconstruction from genomic data
in this paper we explore the use of random forests rfs in the structured language model slm which uses rich syntactic information in predicting the next word based on words already seen the goal in this work is to construct rfs by randomly growing decision trees dts using syntactic information and investigate the performance of the slm modeled by the rfs in automatic speech recognition rfs which were originally developed as classifiers are a combination of decision tree classifiers each tree is grown based on random training data sampled independently and with the same distribution for all trees in the forest and a random selection of possible questions at each node of the decision tree our approach extends the original idea of rfs to deal with the data sparseness problem encountered in language modeling rfs have been studied in the context of n gram language modeling and have been shown to generalize well to unseen data we show in this paper that rfs using syntactic information can also achieve better performance in both perplexity ppl and word error rate wer in a large vocabulary speech recognition system compared to a baseline that uses kneser ney smoothing

visual action recognition is an important problem in computer vision in this paper we propose a new method to probabilistically model and recognize actions of articulated objects such as hand or body gestures in image sequences our method consists of three levels of representation at the low level we first extract a feature vector invariant to scale and in plane rotation by using the fourier transform of a circular spatial histogram then spectral partitioning is utilized to obtain an initial clustering this clustering is then refined using a temporal smoothness constraint gaussian mixture model gmm based clustering and density estimation in the subspace of linear discriminant analysis lda are then applied to thousands of image feature vectors to obtain an intermediate level representation finally at the high level we build a temporal multiresolution histogram model for each action by aggregating the clustering weights of sampled images belonging to that action we discuss how this high level representation can be extended to achieve temporal scaling invariance and to include bi gram or multi gram transition information both image clustering and action recognitionsegmentation results are given to show the validity of our three tiered representation
high retrieval precision in content based image retrieval can be attained by adopting relevance feedback mechanisms these mechanisms require that the user judges the quality of the results of the query by marking all the retrieved images as being either relevant or not then the search engine exploits this information to adapt the search to better meet users needs at present the vast majority of proposed relevance feedback mechanisms are formulated in terms of search model that has to be optimized such an optimization involves the modification of some search parameters so that the nearest neighbor of the query vector contains the largest number of relevant images in this paper a different approach to relevance feedback is proposed after the user provides the first feedback following retrievals are not based on knn search but on the computation of a relevance score for each image of the database this score is computed as a function of two distances namely the distance from the nearest non relevant image and the distance from the nearest relevant one images are then ranked according to this score and the top k images are displayed reported results on three image data sets show that the proposed mechanism outperforms other state of the art relevance feedback mechanisms in t rod u ct i on a large number of content based image retrieval cbir systems rely on the vector representation of images in a multidimensional feature space representing low level image characteristics eg color texture shape etc content based queries are often expressed by visual examples in order to retrieve from the database the images that are similar to the examples this kind of retrieval is often referred to as k nearest neighbor retrieval it is easy to see that the effectiveness of content based image retrieval systems cbir strongly depends on the choice of the set of visual features on the choice of the metric used to model the users perception of image similarity and on the choice of the image used to query the database typically if we allow different users to mark the images retrieved with a given query as firelevant or non relevant different subsets of images will be marked as relevant accordingly the need for mechanisms to adapt the cbir system response based on some feedback from the user is widely recognized it is interesting to note that while relevance feedback mechanisms have been first introduced in the information retrieval field they are receiving more attention in the cbir field huang the vast majority of relevance feedback techniques proposed in the literature is based on modifying the values of the search parameters as to better represent the concept the user bears in mind to this end search parameters are computed as a function of the relevance values assigned by the user to all the images retrieved so far as an example relevance feedback is often formulated in terms of the modification of the query vector andor in terms of adaptive similarity metrics recently pattern classification paradigms such as svms have been proposed feedback is thus used to model the concept of relevant images and adjust the search consequently concept modeling may be difficult on account of the distribution of relevant images in the selected feature space narrow domain image databases allows extracting good features so that images bearing similar concepts belong to compact clusters on the other hand broad domain databases such as image collection used by graphic professionals or those made up of images from the internet are more difficult to subdivide in cluster because of the high variability of concepts in these cases it is worth extracting only low level non specialized features and image retrieval is better formulated in terms of a search problem rather then concept modeling the present paper aims at offering an original contribution in this direction rather then modeling the concept of relevance the user bears in mind feedback is used to assign each image of the database a relevance score such a score depends only from two dissimilarities distances computed against the images already marked by the user the dissimilarity from the set of relevant images and the dissimilarity from the set of non relevant images despite its computational simplicity this mechanism allows outperforming state of the art relevance feedback mechanisms both on narrow domain databases and on broad domain databases this paper is organized as follows section illustrates the idea behind the proposed mechanism and provides the basic assumptions section details the proposed relevance feedback mechanism results on three image data sets are presented in section where performances of other relevance feedback mechanisms are compared conclusions are drawn in section in st a n ceb a sed rel eva n ce est i m a t i on the proposed mechanism has been inspired by classification techniques based on the nearest case nearest case theory provided the mechanism to compute the dissimilarity of each image from the sets of relevant and non relevant images the ratio between the nearest relevant image and the nearest non relevant image has been used to compute the degree of relevance of each image of the database the present section illustrates the rationale behind the use of the nearest case paradigm let us assume that each image of the database has been represented by a number of low level features and that a dissimilarity measure has been defined so that the proximity between pairs of images represents some kind of conceptual similarity in other words the chosen feature space and similarity metric is meaningful at least for a restricted number of users fia search in image databases is usually performed by retrieving the k most similar images with respect to a given query the dimension of k is usually small to avoid displaying a large number of images at a time typical values for k are between and however as the relevant images that the user wishes to retrieve may not fit perfectly with the similarity metric designed for the search engine the user may be interested in exploring other regions of the feature space to this end the user marks the subset of relevant images out of the k retrieved usually such relevance feedback is used to perform a new k nn search by modifying some search parameters ie the position of the query point the similarity metric and other tuning parameters recent works proposed the use of support vector machine to learn the distribution of relevant images these techniques require some assumption about the general form of the distribution of relevant images in the feature space as it is difficult to make any assumption about such a distribution for broad domain databases we propose to exploit the information about the relevance of the images retrieved so far in a nearest neighbor fashion nearest neighbor techniques as used in statistical pattern recognition case based reasoning or instance based learning are effective in all applications where it is difficult to produce a high level generalization of a class of objects relevance learning in content base image retrieval may well fit into this definition as it is difficult to provide a general model that can be adapted to represent different concepts of similarity in addition the number of available cases may be too small to estimate the optimal set of parameters for such a general model on the other hand it can be more effective to use each relevant image as well as each non relevant image as cases or instances against which the images of the database should be compared consequently we assume that an image is as much as relevant as much as its dissimilarity from the nearest relevant image is small analogously an image is as much as non relevant as much as its dissimilarity from the nearest non relevant image is small rel evan ce s core com p u t a ti on according to previous section each image of the database can be thus characterized by a degree of relevance and a degree of non relevance according to the dissimilarities from the nearest relevant image and from the nearest non relevant image respectively however it should be noted that these degrees should be treated differently because only relevant images represent a concept in the users mind while non relevant images may represent a number of other concepts different from users interest in other words while it is meaningful to treat the degree of relevance as a degree of membership to the class of relevant images the same does not apply to the degree of non relevance for this reason we propose to use the degree of non relevance to weight the degree of relevance let us denote with r the subset of indexes j k related to the set of relevant images retrieved so far and the original query that is relevant by default and with nr the subset of indexes j k related to the set of non relevant images retrieved so far for each image i of the database according to the nearest neighbor rule let us compute the dissimilarity from the nearest image in r and the dissimilarity from the nearest image in nr let us denote these dissimilarities as dri and dnri respectively the value of dri can be clearly used to measure the degree of relevance of image i assuming that small values of dri are related to very relevant images on the other hand the hypothesis that image i is relevant to the users query can be supported by a high value of dnri accordingly we defined the relevance score fi dr i relevance i dn i this formulation of the score can be easily explained in terms of a distanceweighted nn estimation of the posterior probability that image i is relevant the nearest neighbors are made up of the nearest relevant image and the nearest nonrelevant image while the weights are computed as the inverse of the distance from the nearest neighbors the relevance score computed according to equation is then used to rank the images and the first k are presented to the user exp eri m en t a l resu l t s in order to test the proposed method and compare it with other methods described in the literature three image databases have been used the mit database a database contained in the uci repository and a subset of the corel database these databases are currently used for assessing and comparing relevance feedback techniques the mit database was collected by the mit media lab ftpwhitechapelmediamitedupubvistex this database contains texture images that have been manually classified into fifteen classes each of these images has been subdivided into sixteen non overlapping images obtaining a data set with images sixteen gabor filters were used to characterise these images so that each image is represented by a dimensional feature vector the database extracted from the uci repository httpwwwcsuciedumlearnmlrepositoryhtml consists of outdoor images the images are subdivided into seven data classes brickface sky foliage cement window path and grass nineteen colour and spatial features characterise each image details are reported in the uci web site the database extracted from the corel collection is available at the kdd uci repository httpkddicsuciedudatabasescorelfeaturescorelfeaturesdatahtml we used a subset made up of images manually subdivided into classes for each image four sets of features were available at the web site in this paper we report the results related to the color moments features and the co occurrence texture features feature sets for each dataset the euclidean distance metric has been used a linear normalisation procedure has been performed so that each feature takes values in the range between and for the first two databases each image is used as a query while for the corel database images have been randomly extracted and used as query so that all the classes are represented at each retrieval iteration twenty images are returned relevance feedback is performed by marking images belonging to the same class of the query as relevant and all other images as non relevant the users query itself is included in the set of relevant images this experimental set up affords an objective comparison among different methods and is currently used by many researchers results are evaluated in term of the retrieval precision averaged over all the considered queries the precision is measured as the fraction of relevant images contained in the top retrieved images fias the first two databases are of the narrow domain type while the third is of the broad domain type this experimental set up allowed a thorough testing of the proposed technique for the sake of comparison retrieval performances obtained with two methods recently described in the literature are also reported mindreader which modifies the query vector and the similarity metric on account of features relevance and bayes qs bayesian query shifting which is based on query reformulation these two methods have been selected because they can be easily implemented and their performances can be compared to those provided by a large number of relevance feedback techniques proposed in the cbir literature see for example results presented in it is worth noting that results presented in different papers cannot be directly compared to each other because they are not related to a common experimental set up however as they are related to the same data sets with similar experimental set up a qualitative comparisons let us conclude that the performance of the two above techniques are quite close to other results in the literature experiments w ith th e mi t database this database can be considered of the narrow domain type as it contains only images of textures of different types in addition the selected feature space is very suited to measure texture similarity figure show the performances of the proposed relevance feedback mechanism and those of the two techniques used for comparison precision relevance score bayes qs mindreader rf rf rf rf rf rf iter rel feedback rf rf rf figure retrieval performances for the mit database in terms of average percentage retrieval precision after the first feedback iteration rf in the graph each relevance feedback mechanism is able to improve the average precision attained in the first retrieval by more than the proposed mechanism performing slightly better than mindreader this is a desired behaviour as a user typically allows few iterations however if the user aims to better refine the search by additional feedback iteration mindreader and bayes qs are not able to exploit the additional information as they provide no improvements after the second feedback iteration on the other hand the proposed mechanism provides further improvement in precision by increasing the number of iteration these improvements are very small fibecause the first feedback already provides a high precision value near to experiments w ith th e uc i database this database too can be considered of the narrow domain type as the images clearly belong to one of the seven data classes and features have been extracted accordingly precision relevance score bayes qs mindreader rf rf rf rf rf iter rel feedback rf rf rf rf figure retrieval performances for the uci data set in terms of average percentage retrieval precision figure show the performances attained on the uci database retrieval precision is very high after the first extraction with no feedback nonetheless each of the considered mechanism is able to exploit relevance feedback mindreader and bayes qs providing a improvement while the proposed mechanism attains a improvement this example clearly shows the superiority of the proposed technique as it attains a precision of after the second iteration further iterations allow attaining a precision on the other hand bayes qs also exploits further feedback iteration attaining a precision of after iterations while mindreader does not improve the precision attained after the first iteration as the user typically allows very few feedback iterations the proposed mechanism proved to be very suited for narrow domain databases as it allows attaining a precision close to experiments w ith th e co rel databas e figures and show the performances attained on two feature sets extracted from the corel database this database is of the broad domain type as images represent a very large number of concepts and the selected feature sets represent conceptual similarity between pairs of images only partly reported results clearly show the superiority of the proposed mechanism let us note that the retrieval precision after the first k nn search rf in the graphs is quite small this is a consequence of the difficulty of selecting a good feature space to represent conceptual similarity between pairs of images in a broad domain database this difficulty is partially overcome by using mindreader or bayes qs as they allow improving the retrieval precision by to according to the number of iteration allowed and according to the selected feature space let us recall that both mindreader and bayes qs perform a query movement in order to perform a k nn fiquery on a more promising region of the feature space on the other hand the proposed mechanism based on ranking all the images of the database according to a relevance score not only provided higher precision after the first feedback but also allow to improve significantly the retrieval precision as the number of iteration is increased as the initial precision is quite small a user may have more willingness to perform further iterations as the proposed mechanism allows retrieving new relevant images figure retrieval performances for the corel data set color moments feature set in terms of average percentage retrieval precision figure retrieval performances for the corel data set co occurrence texture feature set in terms of average percentage retrieval precision con cl u si on s in this paper we proposed a novel relevance feedback technique for content based image retrieval while the vast majority of relevance feedback mechanisms aims at modeling users concept of relevance based on the available labeled samples the proposed mechanism is based on ranking the images according to a relevance score depending on the dissimilarity from the nearest relevant and non relevant images fithe rationale behind our choice is the same of case based reasoning instance based learning and nearest neighbor pattern classification these techniques provide good performances when the number of available training samples is too small to use statistical techniques this is the case of relevance feedback in cbir where the use of classification models should require a suitable formulation in order to avoid socalled small sample problems reported results clearly showed the superiority of the proposed mechanism especially when large databases made up of images related to many different concepts are searched in addition while many relevance feedback techniques require the tuning of some parameters and exhibit high computational complexity the proposed mechanism does not require any parameter tuning and exhibit a low computational complexity as a number of techniques are available to speed up distance computations references smeulders awm worring m santini s gupta a jain r content based image retrieval at the end of the early years ieee trans on pattern analysis and machine intelligence g salton and mj mcgill
motivated by the particular problems involved in communicating with locked in paralysed patients we aim to develop a braincomputer interface that uses auditory stimuli we describe a paradigm that allows a user to make a binary decision by focusing attention on one of two concurrent auditory stimulus sequences using support vector machine classification and recursive channel elimination on the independent components of averaged eventrelated potentials we show that an untrained users eeg data can be classified with an encouragingly high level of accuracy this suggests that it is possible for users to modulate eeg signals in a single trial by the conscious direction of attention well enough to be useful in bci
statistical language models estimate the probability of a word occurring in a given context the most common language models rely on a discrete enumeration of predictive contexts eg n grams and consequently fail to capture and exploit statistical regularities across these contexts in this paper we show how to learn hierarchical distributed representations of word contexts that maximize the predictive value of a statistical language model the representations are initialized by unsupervised algorithms for linear and nonlinear dimensionality reduction then fed as input into a hierarchical mixture of experts where each expert is a multinomial distribution over predicted words while the distributed representations in our model are inspired by the neural probabilistic language model of bengio et al our particular architecture enables us to work with significantly larger vocabularies and training corpora for example on a large scale bigram modeling task involving a sixty thousand word vocabulary and a training corpus of three million sentences we demonstrate consistent improvement over class based bigram models we also discuss extensions of our approach to longer multiword contexts
we discuss an identification framework for noisy speech mixtures a block based generative model is formulated that explicitly incorporates the time varying harmonic plus noise hn model for a number of latent sources observed through noisy convolutive mixtures all parameters including the pitches of the source signals the amplitudes and phases of the sources the mixing filters and the noise statistics are estimated by maximum likelihood using an em algorithm exact averaging over the hidden sources is obtained using the kalman smoother we show that pitch estimation and source separation can be performed simultaneously the pitch estimates are compared to laryngograph egg measurements artificial and real room mixtures are used to demonstrate the viability of the approach intelligible speech signals are re synthesized from the estimated hn models
we provide a method for mass meta analysis in a neuroinformatics database containing stereotaxic talairach coordinates from neuroimaging experiments database labels are used to group the individual experiments eg according to cognitive function and the consistent pattern of the experiments within the groups are determined the method voxelizes each group of experiments via a kernel density estimation forming probability density volumes the values in the probability density volumes are compared to null hypothesis distributions generated by resamplings from the entire unlabeled set of experiments and the distances to the nullhypotheses are used to sort the voxels across groups of experiments this allows for mass meta analysis with the construction of a list with the most prominent associations between brain areas and group labels furthermore the method can be used for functional labeling of voxels
in this paper we propose a new method parametric embedding pe for visualizing the posteriors estimated over a mixture model pe simultaneously embeds both objects and their classes in a low dimensional space pe takes as input a set of class posterior vectors for given data points and tries to preserve the posterior structure in an embedding space by minimizing a sum of kullback leibler divergences under the assumption that samples are generated by a gaussian mixture with equal covariances in the embedding space pe has many potential uses depending on the source of the input data providing insight into the classifiers behavior in supervised semi supervised and unsupervised settings the pe algorithm has a computational advantage over conventional embedding methods based on pairwise object relations since its complexity scales with the product of the number of objects and the number of classes we demonstrate pe by visualizing supervised categorization of web pages semi supervised categorization of digits and the relations of words and latent topics found by an unsupervised algorithm latent dirichlet allocation
we abstract out the core search problem of active learning schemes to better understand the extent to which adaptive labeling can improve sample complexity we give various upper and lower bounds on the number of labels which need to be queried and we prove that a popular greedy active learning rule is approximately as good as any other strategy for minimizing this number of labels
a typical neuron in visual cortex receives most inputs from other cortical neurons with a roughly similar stimulus preference does this arrangement of inputs allow efficient readout of sensory information by the target cortical neuron we address this issue by using simple modelling of neuronal population activity and information theoretic tools we find that efficient synaptic information transmission requires that the tuning curve of the afferent neurons is approximately as wide as the spread of stimulus preferences of the afferent neurons reaching the target neuron by meta analysis of neurophysiological data we found that this is the case for cortico cortical inputs to neurons in visual cortex we suggest that the organization of v cortico cortical synaptic inputs allows optimal information transmission
we consider the semi supervised learning problem where a decision rule is to be learned from labeled and unlabeled data in this framework we motivate minimum entropy regularization which enables to incorporate unlabeled data in the standard supervised learning our approach includes other approaches to the semi supervised problem as particular or limiting cases a series of experiments illustrates that the proposed solution benefits from unlabeled data the method challenges mixture models when the data are sampled from the distribution class spanned by the generative model the performances are definitely in favor of minimum entropy regularization when generative models are misspecified and the weighting of unlabeled data provides robustness to the violation of the cluster assumption finally we also illustrate that the method can also be far superior to manifold learning in high dimension spaces
alternative splicing as is an important and frequent step in mammalian gene expression that allows a single gene to specify multiple products and is crucial for the regulation of fundamental biological processes the extent of as regulation and the mechanisms involved are not well understood we have developed a custom dna microarray platform for surveying as levels on a large scale we present here a generative model for the as array platform genasap and demonstrate its utility for quantifying as levels in different mouse tissues learning is performed using a variational expectation maximization algorithm and the parameters are shown to correctly capture expected as trends a comparison of the results obtained with a well established but low through put experimental method demonstrate that as levels obtained from genasap are highly predictive of as levels in mammalian tissues biological diversity through alternative splicing current estimates place the number of genes in the human genome at approximately which is a surprisingly small number when one considers that the genome of yeast a singlecelled organism has genes the number of genes alone cannot account for the complexity and cell specialization exhibited by higher eukaryotes ie mammals plants etc some of that added complexity can be achieved through the use of alternative splicing whereby a single gene can be used to code for a multitude of products genes are segments of the double stranded dna that contain the information required by the cell for protein synthesis that information is coded using an alphabet of a c g and t corresponding to the four nucleotides that make up the dna in what is known as the central dogma of molecular biology dna is transcribed to rna which in turn is translated into proteins messenger rna mrna is synthesized in the nucleus of the cell and carries the genomic information to the ribosome in eukaryotes genes are generally comprised of both exons which contain the information needed by the cell to synthesize proteins and introns sometimes referred to as spacer dna which are spliced out of the pre mrna to create mature mrna an estimated of human genes can be fic a c c c a b c d c a c c a c c a c c a a c c c c c c a c c a c c c c a a c c c c c figure four types of as boxes represent exons and lines represent introns with the possible splicing alternatives indicated by the connectors a single cassette exon inclusionexclusion c and c are constitutive exons exons that are included in all isoforms and flank a single alternative exon a the alternative exon is included in one isoform and excluded in the other b alternative or donor and alternative acceptor splicing sites both exons are constitutive but may contain alternative donor andor acceptor splicing sites c mutually exclusive exons one of the two alternative exons a and a may be included in the isoform but not both d intron inclusion an intron may be included in the mature mrna strand spliced to yield different combinations of exons called isoforms a phenomenon referred to as alternative splicing as there are four major types of as as shown in figure many multi exon genes may undergo more than one alternative splicing event resulting in many possible isoforms from a single gene in addition to adding to the genetic repertoire of an organism by enabling a single gene to code for more than one protein as has been shown to be critical for gene regulation contributing to tissue specificity and facilitating evolutionary processes despite the evident importance of as its regulation and impact on specific genes remains poorly understood the work presented here is concerned with the inference of single cassette exon as levels figure a based on data obtained from rna expression arrays also known as microarrays an exon microarray data set that probes alternative splicing events although it is possible to directly analyze the proteins synthesized by a cell it is easier and often more informative to instead measure the abundance of mrna present traditionally gene expression abundance of mrna has been studied using low throughput techniques such as rt pcr or northern blots limited to studying a few sequences at a time and making large scale analysis nearly impossible in the early s microarray technology emerged as a method capable of measuring the expression of thousands of dna sequences simultaneously sequences of interest are deposited on a substrate the size of a small microscope slide to form probes the mrna is extracted from the cell and reverse transcribed back into dna which is labelled with red and green fluorescent dye molecules cy and cy respectively when the sample of tagged dna is washed over the slide complementary strands of dna from the sample hybridize to the probes on the array forming a t and c g pairings the slide is then scanned and the fluorescent intensity is measured at each probe it is generally assumed that the intensity measure at the probe is linearly related to the abundance of mrna in the cell over a wide dynamic range despite significant improvements in microarray technologies in recent years microarray data still presents some difficulties in analysis low measurements tend to have extremely low signal to noise ratio snr and probes often bind to sequences that are very similar but not identical to the one for which they were designed a process referred to as cros fic c a a c c body probes inclusion junction probes exclusion junction probe ca c a ac c cc c c figure each alternative splicing event is studied using six probes probes were chosen to measure the expression levels of each of the three exons involved in the event additionally probes are used that target the junctions that are formed by each of the two isoforms the inclusion isoform would express the junctions formed by c and a and a and c while the exclusion isoform would express the junction formed by c and c hybridization additionally probes exhibit somewhat varying hybridization efficiency and sequences exhibit varying labelling efficiency to design our data sets we mined public sequence databases and identified exons that were strong candidates for exhibiting as the details of that analysis are provided elsewhere of the candidates potential as events in unique mouse genes were selected for the design of agilent custom oligonucleotide microarray the arrays were hybridized with unamplified mrna samples extracted from wild type mouse tissues brain heart intestine kidney liver lung salivary gland skeletal muscle spleen and testis each as event has six target probes on the arrays chosen from regions of the c exon c exon a exon c a splice junction ac splice junction and c c splice junction as shown in figure unsupervised discovery of alternative splicing with the exception of the probe measuring the alternative exon a figure all probes measure sequences that occur in both isoforms for example while the sequence of the probe measuring the junction ac is designed to measure the inclusion isoform half of it corresponds to a sequence that is found in the exclusion isoform we can therefore safely assume that the measured intensity at each probe is a result of a certain amount of both isoforms binding to the probe due to the generally assumed linear relationship between the abundance of mrna hybridized at a probe and the fluorescent intensity measured we model the measured intensity as a weighted sum of the overall abundance of the two isoforms a stronger assumption is that of a single consistent hybridization profile for both isoforms across all probes and all slides ideally one would prefer to estimate an individual hybridization profile for each as event studied across all slides however in our current setup the number of tissues is small resulting in two difficulties first the number of parameters is very large when compared to the number of data point using this model and second a portion of the events do not exhibit tissue specific alternative splicing within our small set of tissues while the first hurdle could be accounted for using baysian parameter estimation the second cannot genasap a generative model for alternative splicing array platform using the setup described above the expression vector x containing the six microarray measurements as real numbers can be decomposed as a linear combination of the abundance of the two splice isoforms represented by the real vector s with some added noise x s noise where is a weight matrix containing the hybridization profiles for fis xc xc xa xc a s xac xcc r xc xc xa xc a xac xcc occ oc oc oa oca oac n figure graphical model for alternative splicing each measurement in the observed expression profile x is generated by either using a scale factor r on a linear combination of the isoforms s or drawing randomly from an outlier model for a detailed description of the model see text the two isoforms across the six probes note that we may not have a negative amount of a given isoform nor can the presence of an isoform deduct from the measured expression and so both s and are constrained to be positive expression levels measured by microarrays have previously been modelled as having expression dependent noise to address this we rewrite the above formulation as x rs where r is a scale factor and is a zero mean normally distributed random variable with a diagonal covariance matrix denoted as p n the prior distribution for the abundance of the splice isoforms is given by a truncated normal distribution denoted as ps n s is where is an indicator function such that s if i si and s otherwise lastly there is a need to account for aberrant observations eg due to faulty probes flakes of dust etc with an outlier model the complete genasap model shown in figure accounts for the observations as the outcome of either applying equation or an outlier model to avoid degenerate cases and ensure meaningful and interpretable results the number of faulty probes considered for each as event may not exceed two as indicated by the filled in square constraint node in figure the distribution of x conditional on the latent variables s r and o is pxs r o i n xi ri s r i oi n xi ei vi oi where oi is a bernoulli random variable indicating if the measurement at probe xi is the result of the as model or the outlier model parameterized by poi i the parameters of the outlier model e and v are not optimized and are set to the mean and variance of the data fi variational learning in the genasap model to infer the posterior distribution over the splice isoform abundances while at the same time learning the model parameters we use a variational expectation maximization algorithm em em maximizes the log likelihood of the data by iteratively estimating the posterior distribution of the model given the data in the expectation e step and maximizing the log likelihood with respect to the parameters while keeping the posterior fixed in the maximization m step variational em is used when as in the case of genasap the exact posterior is intractable variational em minimizes the free energy of the model defined as the kl divergence between the joint distribution of the latent and observed variables and the approximation to the posterior under the model parameters we approximate the true posterior using the q distribution given by t qst ot rt t qrt qot rt i qsi oi rt d d t t t t z t t t n st t t st ro ro where z is a normalization constant the superscript d indicates that is constrained to be diagonal and there are t iid as events for computational efficiency r is selected from a finite set r r r rc with uniform probability the variational free energy is given by qst ot rt p st ot rt xt s r o variational em minimizes the free energy by iteratively updating the q distributions varitd td ational parameters t t ro and ro in the e step and the model parameters r r rc and in the m step the resulting updates are too long to be shown in the context of this paper and are discussed in detail elsewhere a few particular points regarding the e step are worth covering in detail here fq p qst ot rt log if the prior on s was a full normal distribution there would be no need for a variational approach and exact em is possible for a truncated normal distribution however the mixing proportions qrqor cannot be calculated analytically except for the case where s is scalar necessitating the diagonality constraint note that if was allowed to be a full covariance matrix equation would be the true posterior and we could find the sufficient statistics of qst ot rt t i t i ot t i ot t i ot t xt rt ro t ro i t i ot t i ot where o is a diagonal matrix with elements oii oi furthermore it can be easily shown that the optimal settings for d and d approximating a normal distribution with full covariance and mean is d optimal d optimal diag in the truncated case equation is still true equation does not hold though and d optimal cannot be found analytically in our experiments we found that using equation still decreases the free energy every e step and it is significantly more efficient than using for example a gradient decent method to compute the optimal d fiintuitive weigh matrix inclusion isoform exclusion isoform optimal weight matrix inclusion isoform exclusion isoform a b figure a an intuitive set of weights based on the biological background one would expect to see the inclusion isoform hybridize to the probes measuring c c a c a and ac while the exclusion isoform hybridizes to c c and c c b the learned set of weights closely agrees with the intuition and captures cross hybridization between the probes contribution of exclusion isoform contribution of inclusion isoform as model original data rt pcr as model measurement prediction exclusion exclusion a b outliers c figure three examples of data cases and their predictions a the data does not follow our notion of single cassette exon as but the as level is predicted accurately by the modelb the probe c a is marked as outlier allowing the model to predict the other probes accurately c two probes are marked as outliers and the model is still successful in predicting the as levels making biological predictions about alternative splicing the results presented in this paper were obtained using two stages of learning in the first step the weight matrix is learned on a subset of the data that is selected for quality two selection criteria were used a sequencing data was used to select those cases for which with high confidence no other as event is present figure and b probe sets were selected for high expression as determined by a set of negative controls the second selection criterion is motivated by the common assumption that low intensity measurements are of lesser quality see section in the second step is kept fixed and we introduce the additional constraint that the noise is isotropic i and learn on the entire data set the constraint on the noise is introduced to prevent the model from using only a subset of the six probes for making the final set of predictions we show a typical learned set of weights in figure the weights fit well with our intuition of what they should be to capture the presence of the two isoforms moreover the learned weights account for the specific trends in the data examples of model prediction based on the microarray data are shown in figure due to the nature of the microarray data we do not expect all the inferred abundances to be equally good and we devised a scoring criterion that ranks each as event based on its fit to the model intuitively given two input vectors that are equivalent up to a scale factor with inferred map estimations that are equal up to the same scale factor we would like their scores to be identical the scoring criterion used therefore is k xk rk s xk firank pearsons correlation coefficient false positive rate table model performance evaluated at various ranks using rt pcr measurements we are able to predict the models performance at various ranks two evaluation criteria are used pearsons correlation coefficient between the models predictions and the rt pcr measurements and false positive rate where a prediction is considered to be false positive if it is more than away from the rt pcr measurement rk s where the map estimations for r and s are used this scoring criterion can be viewed as proportional to the sum of noise to signal ratios as estimated using the two values given by the observation and the models best prediction of that observation since it is the relative amount of the isoforms that is of most interest we need to use the inferred distribution of the isoform abundances to obtain an estimate for the relative levels of as it is not immediately clear how this should be done we do however have rtpcr measurements for as events to guide us see figure for details using the top ranked rt pcr measurement we fit three parameters a a a such that the s proportion of excluded isoform present p is given by p a s a s a where s is the map estimation of the abundance of the inclusion isoform s is the map estimation of the abundance of the exclusion isoform and the rt pcr measurement are used for target p the parameters are fitted using gradient descent on a least squared error lse evaluation criterion we used two criteria to evaluate the quality of the as model predictions pearsons correlation coefficient pcc is used to evaluate the overall ability of the model to correctly estimate trends in the data pcc is invariant to affine transformation and so is independent of the transformation parameters a and a discussed above while the parameter a was found to effect pcc very little the pcc stays above for the top two thirds ranked predictions the second evaluation criterion used is the false positive rate where a prediction is considered to be false positive if it is more than away from the rt pcr measurement this allows us to say for example that if a prediction is within the top we are confident that it is within of the actual levels of as summary we designed a novel as model for the inference of the relative abundance of two alternatively spliced isoforms from six measurements unsupervised learning in the model is performed using a structured variational em algorithm which correctly captures the underlying structure of the data as suggested by its biological nature the as model though presented here for a cassette exon as events can be used to learn any type of as and with a simple adjustment multiple types the predictions obtained from the as model are currently being used to verify various claims about the role of as in evolution and functional genomics and to help identify sequences that affect the regulation of as fi exclusion isoform rt pcr measurement vs as model predictions int es te tine sti kid s n sa ey liva br ry ain sp le liv en er mu sc lu le ng as model prediction rt pcr measurement rt pcr measurements as model prediction a b figure a sample rt pcr rna extracted from the cell is reverse transcribed to dna amplified and labelled with radioactive or fluorescent molecules the sample is pulled through a viscous gel in an electric field dna being an acid is positively charged shorter strands travel further through the gel than longer ones resulting in two distinct bands corresponding to the two isoforms when exposed to a photosensitive or x ray film b a scatter plot showing the rt pcr measurements as compared to the as model predictions the plot shows all available rt pcr measurements with a rank of or better the as model presented assumes a single weight matrix for all data cases this is an oversimplified view of the data and current work is being carried out in identifying probe specific expression profiles however due to the low dimensionality of the problem tissues six probes per event care must be taken to avoid overfitting and to ensure meaningful interpretations acknowledgments we would like to thank wen zhang naveed mohammad and timothy hughes for their contributions in generating the data set this work was funded in part by an operating and infrastructure grants from the cihr and cfi and a operating grants from nserc and a premiers research excellence award references j m johnson et al genome wide survey of human alternative pre mrna splicing with exon junction microarrays science l cartegni et al listening to silence and understanding nonsense exonic mutations that affect splicing nature gen rev q pan et al revealing global regulatory features of mammalian alternative splicing using a quantitative microarray platform molecular cell q pan et al alternative splicing of conserved exons is frequently species specific in human and mouse trends gen in press m i jordan z ghahramani t jaakkola and lawrence k saul an
device mismatch in vlsi degrades the accuracy of analog arithmetic circuits and lowers the learning performance of large scale neural networks implemented in this technology we show compact low power on chip calibration techniques that compensate for device mismatch our techniques enable large scale analog vlsi neural networks with learning performance on the order of bits we demonstrate our techniques on a synapse linear perceptron learning with the least mean squares lms algorithm and fabricated in a m cmos process
coreference analysis also known as record linkage or identity uncertainty is a difficult and important problem in natural language processing databases citation matching and many other tasks this paper introduces several discriminative conditional probability models for coreference analysis all examples of undirected graphical models unlike many historical approaches to coreference the models presented here are relational they do not assume that pairwise coreference decisions should be made independently from each other unlike other relational models of coreference that are generative the conditional model here can incorporate a great variety of features of the input without having to be concerned about their dependencies paralleling the advantages of conditional random fields over hidden markov models we present positive results on noun phrase coreference in two standard text data sets
we examine the marriage of recent probabilistic generative models for social networks with classical frameworks from mathematical economics we are particularly interested in how the statistical structure of such networks influences global economic quantities such as price variation our findings are a mixture of formal analysis simulation and experiments on an international trade data set from the united nations
decision trees are surprisingly adaptive in three important respects they automatically adapt to favorable conditions near the bayes decision boundary focus on data distributed on lower dimensional manifolds reject irrelevant features in this paper we examine a decision tree based on dyadic splits that adapts to each of these conditions to achieve minimax optimal rates of convergence the proposed classifier is the first known to achieve these optimal rates while being practical and implementable


given a directed graph in which some of the nodes are labeled we investigate the question of how to exploit the link structure of the graph to infer the labels of the remaining unlabeled nodes to that extent we propose a regularization framework for functions defined over nodes of a directed graph that forces the classification function to change slowly on densely linked subgraphs a powerful yet computationally simple classification algorithm is derived within the proposed framework the experimental evaluation on real world web classification problems demonstrates encouraging results that validate our approach
a generative probabilistic model for objects in images is presented an object consists of a constellation of features feature appearance and pose are modeled probabilistically scene images are generated by drawing a set of objects from a given database with random clutter sprinkled on the remaining image surface occlusion is allowed we study the case where features from the same object share a common reference frame moreover parameters for shape and appearance densities are shared across features this is to be contrasted with previous work on probabilistic constellation models where features depend on each other and each feature and model have different pose and appearance statistics these two differences allow us to build models containing hundreds of features as well as to train each model from a single example our model may also be thought of as a probabilistic revisitation of lowes model we propose an efficient entropy minimization inference algorithm that constructs the best interpretation of a scene as a collection of objects and clutter we test our ideas with experiments on two image databases we compare with lowes algorithm and demonstrate better performance in particular in presence of large amounts of background clutter
we address the problem of learning a symmetric positive definite matrix the central issue is to design parameter updates that preserve positive definiteness our updates are motivated with the von neumann divergence rather than treating the most general case we focus on two key applications that exemplify our methods on line learning with a simple square loss and finding a symmetric positive definite matrix subject to symmetric linear constraints the updates generalize the exponentiated gradient eg update and adaboost respectively the parameter is now a symmetric positive definite matrix of trace one instead of a probability vector which in this context is a diagonal positive definite matrix with trace one the generalized updates use matrix logarithms and exponentials to preserve positive definiteness most importantly we show how the analysis of each algorithm generalizes to the non diagonal case we apply both new algorithms called the matrix exponentiated gradient meg update and definiteboost to learn a kernel matrix from distance measurements


most existing tracking algorithms construct a representation of a target object prior to the tracking task starts and utilize invariant features to handle appearance variation of the target caused by lighting pose and view angle change in this paper we present an efficient and effective online algorithm that incrementally learns and adapts a low dimensional eigenspace representation to reflect appearance changes of the target thereby facilitating the tracking task furthermore our incremental method correctly updates the sample mean and the eigenbasis whereas existing incremental subspace update methods ignore the fact the sample mean varies over time the tracking problem is formulated as a state inference problem within a markov chain monte carlo framework and a particle filter is incorporated for propagating sample distributions over time numerous experiments demonstrate the effectiveness of the proposed tracking algorithm in indoor and outdoor environments where the target objects undergo large pose and lighting changes
we study a number of open issues in spectral clustering i selecting the appropriate scale of analysis ii handling multi scale data iii clustering with irregular background clutter and iv finding automatically the number of groups we first propose that a local scale should be used to compute the affinity between each pair of points this local scaling leads to better clustering especially when the data includes multiple scales and when the clusters are placed within a cluttered background we further suggest exploiting the structure of the eigenvectors to infer automatically the number of groups this leads to a new algorithm in which the final randomly initialized k means stage is eliminated
this paper presents an adaptive discriminative generative model that generalizes the conventional fisher linear discriminant algorithm and renders a proper probabilistic interpretation within the context of object tracking we aim to find a discriminative generative model that best separates the target from the background we present a computationally efficient algorithm to constantly update this discriminative model as time progresses while most tracking algorithms operate on the premise that the object appearance or ambient lighting condition does not significantly change as time progresses our method adapts a discriminative generative model to reflect appearance variation of the target and background thereby facilitating the tracking task in ever changing environments numerous experiments show that our method is able to learn a discriminative generative model for tracking target objects undergoing large pose and lighting changes
we present an unsupervised algorithm for registering d surface scans of an object undergoing significant deformations our algorithm does not need markers nor does it assume prior knowledge about object shape the dynamics of its deformation or scan alignment the algorithm registers two meshes by optimizing a joint probabilistic model over all point topoint correspondences between them this model enforces preservation of local mesh geometry as well as more global constraints that capture the preservation of geodesic distance between corresponding point pairs the algorithm applies even when one of the meshes is an incomplete range scan thus it can be used to automatically fill in the remaining surfaces for this partial scan even if those surfaces were previously only seen in a different configuration we evaluate the algorithm on several real world datasets where we demonstrate good results in the presence of significant movement of articulated parts and non rigid surface deformation finally we show that the output of the algorithm can be used for compelling computer graphics tasks such as interpolation between two scans of a non rigid object and automatic recovery of articulated object models
we introduce a computationally efficient method to estimate the validity of the bp method as a function of graph topology the connectivity strength frustration and network size we present numerical results that demonstrate the correctness of our estimates for the uniform random model and for a real world network c elegans although the method is restricted to pair wise interactions no local evidence zero biases and binary variables we believe that its predictions correctly capture the limitations of bp for inference and map estimation on arbitrary graphical models using this approach we find that bp always performs better than mf especially for large networks with broad degree distributions such as scale free networks bp turns out to significantly outperform mf
in this paper we argue that the choice of the svm cost parameter can be critical we then derive an algorithm that can fit the entire path of svm solutions for every value of the cost parameter with essentially the same computational cost as fitting one svm model
complex objects can often be conveniently represented by finite sets of simpler components such as images by sets of patches or texts by bags of words we study the class of positive definite pd kernels for two such objects that can be expressed as a function of the merger of their respective sets of components we prove a general integral representation of such kernels and present two particular examples one of them leads to a kernel for sets of points living in a space endowed itself with a positive definite kernel we provide experimental results on a benchmark experiment of handwritten digits image classification which illustrate the validity of the approach
we present a novel approach to collaborative prediction using low norm instead of low rank factorizations the approach is inspired by and has strong connections to large margin linear discrimination we show how to learn low norm factorizations by solving a semi definite program and discuss generalization error bounds for them
this paper explores the computational consequences of simultaneous intrinsic and synaptic plasticity in individual model neurons it proposes a new intrinsic plasticity mechanism for a continuous activation model neuron based on low order moments of the neurons firing rate distribution the goal of the intrinsic plasticity mechanism is to enforce a sparse distribution of the neurons activity level in conjunction with hebbian learning at the neurons synapses the neuron is shown to discover sparse directions in the input
motor control depends on sensory feedback in multiple modalities with different latencies in this paper we consider within the framework of reinforcement learning how different sensory modalities can be combined and selected for real time optimal movement control we propose an actor critic architecture with multiple modules whose output are combined using a softmax function we tested our architecture in a simulation of a sequential reaching task reaching was initially guided by visual feedback with a long latency our learning scheme allowed the agent to utilize the somatosensory feedback with shorter latency when the hand is near the experienced trajectory in simulations with different latencies for visual and somatosensory feedback we found that the agent depended more on feedback with shorter latency

we consider the situation in semi supervised learning where the label sampling mechanism stochastically depends on the true response as well as potentially on the features we suggest a method of moments for estimating this stochastic dependence using the unlabeled data this is potentially useful for two distinct purposes a as an input to a supervised learning procedure which can be used to de bias its results using labeled data only and b as a potentially interesting learning task in itself we present several examples to illustrate the practical usefulness of our method
the context in which a name appears in a caption provides powerful cues as to who is depicted in the associated image we obtain face images using a face detector from approximately half a million captioned news images and automatically link names obtained using a named entity recognizer with these faces a simple clustering method can produce fair results we improve these results significantly by combining the clustering process with a model of the probability that an individual is depicted given its context once the labeling procedure is over we have an accurately labeled set of faces an appearance model for each individual depicted and a natural language model that can produce accurate results on captions in isolation

we describe a novel method for real time simultaneous multi view face detection and facial pose estimation the method employs a convolutional network to map face images to points on a manifold parametrized by pose and non face images to points far from that manifold this network is trained by optimizing a loss function of three variables image pose and facenon face label we test the resulting system in a single configuration on three standard data sets one for frontal pose one for rotated faces and one for profiles and find that its performance on each set is comparable to previous multi view face detectors that can only handle one form of pose variation we also show experimentally that the systems accuracy on both face detection and pose estimation is improved by training for the two tasks together
recently there have been several advances in the machine learning and pattern recognition communities for developing manifold learning algorithms to construct nonlinear low dimensional manifolds from sample data points embedded in high dimensional spaces in this paper we develop algorithms that address two key issues in manifold learning the adaptive selection of the neighborhood sizes and better fitting the local geometric structure to account for the variations in the curvature of the manifold and its interplay with the sampling density of the data set we also illustrate the effectiveness of our methods on some synthetic data sets
we propose a probabilistic generative account of configural learning phenomena in classical conditioning configural learning experiments probe how animals discriminate and generalize between patterns of simultaneously presented stimuli such as tones and lights that are differentially predictive of reinforcement previous models of these issues have been successful more on a phenomenological than an explanatory level they reproduce experimental findings but lacking formal foundations provide scant basis for understanding why animals behave as they do we present a theory that clarifies seemingly arbitrary aspects of previous models while also capturing a broader set of data key patterns of data eg concerning animals readiness to distinguish patterns with varying degrees of overlap are shown to follow from statistical inference
there is growing evidence from psychophysical and neurophysiological studies that the brain utilizes bayesian principles for inference and decision making an important open question is how bayesian inference for arbitrary graphical models can be implemented in networks of spiking neurons in this paper we show that recurrent networks of noisy integrate and fire neurons can perform approximate bayesian inference for dynamic and hierarchical graphical models the membrane potential dynamics of neurons is used to implement belief propagation in the log domain the spiking probability of a neuron is shown to approximate the posterior probability of the preferred state encoded by the neuron given past inputs we illustrate the model using two examples a motion detection network in which the spiking probability of a direction selective neuron becomes proportional to the posterior probability of motion in a preferred direction and a two level hierarchical network that produces attentional effects similar to those observed in visual cortical areas v and v the hierarchical model offers a new bayesian interpretation of attentional modulation in v and v
we study the synthesis of neural coding selective attention and perceptual decision making a hierarchical neural architecture is proposed which implements bayesian integration of noisy sensory input and topdown attentional priors leading to sound perceptual discrimination the model offers an explicit explanation for the experimentally observed modulation that prior information in one stimulus feature location can have on an independent feature orientation the networks intermediate levels of representation instantiate known physiological properties of visual cortical neurons the model also illustrates a possible reconciliation of cortical and neuromodulatory representations of uncertainty
we consider the problem of structured classification where the task is to predict a label y from an input x and y has meaningful internal structure our framework includes supervised training of markov random fields and weighted context free grammars as special cases we describe an algorithm that solves the large margin optimization problem defined in using an exponential family gibbs distribution representation of structured objects the algorithm is efficient even in cases where the number of labels y is exponential in size provided that certain expectations under gibbs distributions can be calculated efficiently the method for structured labels relies on a more general result specifically the application of exponentiated gradient updates to quadratic programs
we investigate an approach for simultaneously committing to multiple activities each modeled as a temporally extended action in a semi markov decision process smdp for each activity we define a set of admissible solutions consisting of the redundant set of optimal policies and those policies that ascend the optimal statevalue function associated with them a plan is then generated by merging them in such a way that the solutions to the subordinate activities are realized in the set of admissible solutions satisfying the superior activities we present our theoretical results and empirically evaluate our approach in a simulated domain
we present an algorithm to perform blind one microphone speech separation our algorithm separates mixtures of speech without modeling individual speakers instead we formulate the problem of speech separation as a problem in segmenting the spectrogram of the signal into two or more disjoint sets we build feature sets for our segmenter using classical cues from speech psychophysics we then combine these features into parameterized affinity matrices we also take advantage of the fact that we can generate training examples for segmentation by artificially superposing separately recorded signals thus the parameters of the affinity matrices can be tuned using recent work on learning spectral clustering this yields an adaptive speech specific segmentation algorithm that can successfully separate one microphone speech mixtures
embedding algorithms search for low dimensional structure in complex data but most algorithms only handle objects of a single type for which pairwise distances are specified this paper describes a method for embedding objects of different types such as images and text into a single common euclidean space based on their co occurrence statistics the joint distributions are modeled as exponentials of euclidean distances in the low dimensional embedding space which links the problem to convex optimization over positive semidefinite matrices the local structure of our embedding corresponds to the statistical correlations via random walks in the euclidean space we quantify the performance of our method on two text datasets and show that it consistently and significantly outperforms standard methods of statistical correspondence modeling such as multidimensional scaling and correspondence analysis
schema learning is a way to discover probabilistic constructivist predictive action models schemas from experience it includes methods for finding and using hidden state to make predictions more accurate we extend the original schema mechanism to handle arbitrary discrete valued sensors improve the original learning criteria to handle pomdp domains and better maintain hidden state by using schema predictions these extensions show large improvement over the original schema mechanism in several rewardless pomdps and achieve very low prediction error in a difficult speech modeling task further we compare extended schema learning to the recently introduced predictive state representations and find their predictions of next step action effects to be approximately equal in accuracy this work lays the foundation for a schema based system of integrated learning and planning
we derive an optimal learning rule in the sense of mutual information maximization for a spiking neuron model under the assumption of small fluctuations of the input we find a spike timing dependent plasticity stdp function which depends on the time course of excitatory postsynaptic potentials epsps and the autocorrelation function of the postsynaptic neuron we show that the stdp function has both positive and negative phases the positive phase is related to the shape of the epsp while the negative phase is controlled by neuronal refractoriness
many works have shown that strong connections relate learning from examples to regularization techniques for ill posed inverse problems nevertheless by now there was no formal evidence neither that learning from examples could be seen as an inverse problem nor that theoretical results in learning theory could be independently derived using tools from regularization theory in this paper we provide a positive answer to both questions indeed considering the square loss we translate the learning problem in the language of regularization theory and show that consistency results and optimal regularization parameter choice can be derived by the discretization of the corresponding inverse problem
statistical approaches to language learning typically focus on either short range syntactic dependencies or long range semantic dependencies between words we present a generative model that uses both kinds of dependencies and can be used to simultaneously find syntactic classes and semantic topics despite having no representation of syntax or semantics beyond statistical dependency this model is competitive on tasks like part of speech tagging and document classification with models that exclusively use shortand long range dependencies respectively
we examine the problem of approximating in the frobenius norm sense a positive semidefinite symmetric matrix by a rank one matrix with an upper bound on the cardinality of its eigenvector the problem arises in the decomposition of a covariance matrix into sparse factors and has wide applications ranging from biology to finance we use a modification of the classical variational representation of the largest eigenvalue of a symmetric matrix where cardinality is constrained and derive a semidefinite programming based relaxation for our problem
we have constructed a system that uses an array of spiking silicon neurons a fast microcontroller and digital memory to implement a reconfigurable network of integrate and fire neurons the system is designed for rapid prototyping of spiking neural networks that require high throughput communication with external address event hardware arbitrary network topologies can be implemented by selectively routing address events to specific internal or external targets according to a memory based projective field mapping the utility and versatility of the system is demonstrated by configuring it as a three stage network that accepts input from an address event imager detects salient regions of the image and performs spatial acuity modulation around a high resolution fovea that is centered on the location of highest salience

successful application of reinforcement learning algorithms often involves considerable hand crafting of the necessary non linear features to reduce the complexity of the value functions and hence to promote convergence of the algorithm in contrast the human brain readily and autonomously finds the complex features when provided with sufficient training recent work in machine learning and neurophysiology has demonstrated the role of the basal ganglia and the frontal cortex in mammalian reinforcement learning this paper develops and explores new reinforcement learning algorithms inspired by neurological evidence that provides potential new approaches to the feature construction problem the algorithms are compared and evaluated on the acrobot task
we investigate the problem of reducing the complexity of a graphical model g pg by finding a subgraph h of g chosen from a class of subgraphs h such that h is optimal with respect to kl divergence we do this by first defining a decomposition tree representation for g which is closely related to the junction tree representation for g we then give an algorithm which uses this representation to compute the optimal h h gavril and tarjan have used graph separation properties to solve several combinatorial optimization problems when the size of the minimal separators in the graph is bounded we present an extension of this technique which applies to some important choices of h even when the size of the minimal separators of g are arbitrarily large in particular this applies to problems such as finding an optimal subgraphical model over a k tree of a graphical model over a k tree for arbitrary k and selecting an optimal subgraphical model with a constant d fewer edges with respect to kl divergence can be solved in time polynomial in v g using this formulation
bayesian regularization and nonnegative deconvolution brand is proposed for estimating time delays of acoustic signals in reverberant environments sparsity of the nonnegative filter coefficients is enforced using an l norm regularization a probabilistic generative model is used to simultaneously estimate the regularization parameters and filter coefficients from the signal data iterative update rules are derived under a bayesian framework using the expectation maximization procedure the resulting time delay estimation algorithm is demonstrated on noisy acoustic data
in this paper we use the rollout method for policy improvement to analyze a version of klondike solitaire this version sometimes called thoughtful solitaire has all cards revealed to the player but then follows the usual klondike rules a strategy that we establish using iterated rollouts wins about twice as many games on average as an expert human player does
we describe an approach to building brain computer interfaces bci based on graphical models for probabilistic inference and learning we show how a dynamic bayesian network dbn can be used to infer probability distributions over brainand body states during planning and execution of actions the dbn is learned directly from observed data and allows measured signals such as eeg and emg to be interpreted in terms of internal states such as intent to move preparatory activity and movement execution unlike traditional classification based approaches to bci the proposed approach allows continuous tracking and prediction of internal states over time and generates control signals based on an entire probability distribution over states rather than binary yesno decisions we present preliminary results of brainand body state estimation using simultaneous eeg and emg signals recorded during a self paced leftright hand movement task
this paper investigates the effect of kernel principal component analysis kpca within the classification framework essentially the regularization properties of this dimensionality reduction method kpca has been previously used as a pre processing step before applying an svm but we point out that this method is somewhat redundant from a regularization point of view and we propose a new algorithm called kernel projection machine to avoid this redundancy based on an analogy with the statistical framework of regression for a gaussian white noise model preliminary experimental results show that this algorithm reaches the same performances as an svm
areas of the brain involved in various forms of memory exhibit patterns of neural activity quite unlike those in canonical computational models we show how to use well founded bayesian probabilistic autoassociative recall to derive biologically reasonable neuronal dynamics in recurrently coupled models together with appropriate values for parameters such as the membrane time constant and inhibition we explicitly treat two cases one arises from a standard hebbian learning rule and involves activity patterns that are coded by graded firing rates the other arises from a spike timing dependent learning rule and involves patterns coded by the phase of spike times relative to a coherent local field potential oscillation our model offers a new and more complete understanding of how neural dynamics may support autoassociation
existing algorithms for discrete partially observable markov decision processes can at best solve problems of a few thousand states due to two important sources of intractability the curse of dimensionality and the policy space complexity this paper describes a new algorithm vdcbpi that mitigates both sources of intractability by combining the value directed compression vdc technique with bounded policy iteration bpi the scalability of vdcbpi is demonstrated on synthetic network management problems with up to million states
we introduce a new algorithm based on linear programming that approximates the differential value function of an average cost markov decision process via a linear combination of pre selected basis functions the algorithm carries out a form of cost shaping and minimizes a version of bellman error we establish an error bound that scales gracefully with the number of states without imposing the strong lyapunov condition required by its counterpart in we propose a path following method that automates selection of important algorithm parameters which represent counterparts to the state relevance weights studied in
online mechanism design omd addresses the problem of sequential decision making in a stochastic environment with multiple self interested agents the goal in omd is to make value maximizing decisions despite this self interest in previous work we presented a markov decision process mdp based approach to omd in large scale problem domains in practice the underlying mdp needed to solve omd is too large and hence the mechanism must consider approximations this raises the possibility that agents may be able to exploit the approximation for selfish gain we adopt sparse sampling based mdp algorithms to implement efficient policies and retain truth revelation as an approximate bayesiannash equilibrium our approach is empirically illustrated in the context of the dynamic allocation of wifi connectivity to users in a coffeehouse
the area under the roc curve auc has been advocated as an evaluation criterion for the bipartite ranking problem we study large deviation properties of the auc in particular we derive a distribution free large deviation bound for the auc which serves to bound the expected accuracy of a ranking function in terms of its empirical auc on an independent test sequence a comparison of our result with a corresponding large deviation result for the classification error rate suggests that the test sample size required to obtain an accurate estimate of the expected accuracy of a ranking function with confidence is larger than that required to obtain an accurate estimate of the expected error rate of a classification function with the same confidence a simple application of the union bound allows the large deviation bound to be extended to learned ranking functions chosen from finite function classes
we claim and present arguments to the effect that a large class of manifold learning algorithms that are essentially local and can be framed as kernel learning algorithms will suffer from the curse of dimensionality at the dimension of the true underlying manifold this observation suggests to explore non local manifold learning algorithms which attempt to discover shared structure in the tangent planes at different positions a criterion for such an algorithm is proposed and experiments estimating a tangent plane prediction function are presented showing its advantages with respect to local manifold learning algorithms it is able to generalize very far from training data on learning handwritten character image rotations where a local non parametric method fails
an analog system on chip for kernel based pattern classification and sequence estimation is presented state transition probabilities conditioned on input data are generated by an integrated support vector machine dot product based kernels and support vector coefficients are implemented in analog programmable floating gate translinear circuits and probabilities are propagated and normalized using sub threshold current mode circuits a input state and support vector forward decoding kernel machine is integrated on a mmmm chip in m cmos technology experiments with the processor trained for speaker verification and phoneme sequence estimation demonstrate real time recognition accuracy at par with floating point software at sub microwatt power
we seek to both detect and segment objects in images to exploit both local image data as well as contextual information we introduce boosted random fields brfs which uses boosting to learn the graph structure and local evidence of a conditional random field crf the graph structure is learned by assembling graph fragments in an additive model the connections between individual pixels are not very informative but by using dense graphs we can pool information from large regions of the image dense models also support efficient inference we show how contextual information from other objects can improve detection performance both in terms of accuracy and speed by using a computational cascade we apply our system to detect stuff and things in office and street scenes
we propose a novel a framework for deriving approximations for intractable probabilistic models this framework is based on a free energy negative log marginal likelihood and can be seen as a generalization of adaptive tap and expectation propagation ep the free energy is constructed from two approximating distributions which encode different aspects of the intractable model such a single node constraints and couplings and are by construction consistent on a chosen set of moments we test the framework on a difficult benchmark problem with binary variables on fully connected graphs and d grid graphs we find good performance using sets of moments which either specify factorized nodes or a spanning tree on the nodes structured approximation surprisingly the bethe approximation gives very inferior results even on grids

clustering and prediction of sets of curves is an important problem in many areas of science and engineering it is often the case that curves tend to be misaligned from each other in a continuous manner either in space across the measurements or in time we develop a probabilistic framework that allows for joint clustering and continuous alignment of sets of curves in curve space as opposed to a fixed dimensional featurevector space the proposed methodology integrates new probabilistic alignment models with model based curve clustering algorithms the probabilistic approach allows for the derivation of consistent em learning algorithms for the joint clustering alignment problem experimental results are shown for alignment of human growth data and joint clustering and alignment of gene expression time course data

we present a graphical model for beat tracking in recorded music using a probabilistic graphical model allows us to incorporate local information and global smoothness constraints in a principled manner we evaluate our model on a set of varied and difficult examples and achieve impressive results by using a fast dual tree algorithm for graphical model inference our system runs in less time than the duration of the music being processed
we study a method of optimal data driven aggregation of classifiers in a convex combination and establish tight upper bounds on its excess risk with respect to a convex loss function under the assumption that the solution of optimal aggregation problem is sparse we use a boosting type algorithm of optimal aggregation to develop aggregate classifiers of activation patterns in fmri based on locally trained svm classifiers the aggregation coefficients are then used to design a boosting map of the brain needed to identify the regions with most significant impact on classification
psychologists call behavior intrinsically motivated when it is engaged in for its own sake rather than as a step toward solving a specific problem of clear practical value but what we learn during intrinsically motivated behavior is essential for our development as competent autonomous entities able to efficiently solve a wide range of practical problems as they arise in this paper we present initial results from a computational study of intrinsically motivated reinforcement learning aimed at allowing artificial agents to construct and extend hierarchies of reusable skills that are needed for competent autonomy
directed graphical models with one layer of observed random variables and one or more layers of hidden random variables have been the dominant modelling paradigm in many research fields although this approach has met with considerable success the causal semantics of these models can make it difficult to infer the posterior distribution over the hidden variables in this paper we propose an alternative two layer model based on exponential family distributions and the semantics of undirected models inference in these exponential family harmoniums is fast while learning is performed by minimizing contrastive divergence a member of this family is then studied as an alternative probabilistic model for latent semantic indexing in experiments it is shown that they perform well on document retrieval tasks and provide an elegant solution to searching with keywords
many sequential prediction tasks involve locating instances of patterns in sequences generative probabilistic language models such as hidden markov models hmms have been successfully applied to many of these tasks a limitation of these models however is that they cannot naturally handle cases in which pattern instances overlap in arbitrary ways we present an alternative approach based on conditional markov networks that can naturally represent arbitrarily overlapping elements we show how to efficiently train and perform inference with these models experimental results from a genomics domain show that our models are more accurate at locating instances of overlapping patterns than are baseline models based on hmms
go is an ancient oriental game whose complexity has defeated attempts to automate it we suggest using probability in a bayesian sense to model the uncertainty arising from the vast complexity of the game tree we present a simple conditional markov random field model for predicting the pointwise territory outcome of a game the topology of the model reflects the spatial structure of the go board we describe a version of the swendsen wang process for sampling from the model during learning and apply loopy belief propagation for rapid inference and prediction the model is trained on several hundred records of professional games our experimental results indicate that the model successfully learns to predict territory despite its simplicity
it has been suggested that the primary goal of the sensory system is to represent input in such a way as to reduce the high degree of redundancy given a noisy neural representation however solely reducing redundancy is not desirable since redundancy is the only clue to reduce the effects of noise here we propose a model that best balances redundancy reduction and redundant representation like previous models our model accounts for the localized and oriented structure of simple cells but it also predicts a different organization for the population with noisy limited capacity units the optimal representation becomes an overcomplete multi scale representation which compared to previous models is in closer agreement with physiological data these results offer a new perspective on the expansion of the number of neurons from retina to v and provide a theoretical model of incorporating useful redundancy into efficient neural representations
the correction of bias in magnetic resonance images is an important problem in medical image processing most previous approaches have used a maximum likelihood method to increase the likelihood of the pixels in a single image by adaptively estimating a correction to the unknown image bias field the pixel likelihoods are defined either in terms of a pre existing tissue model or non parametrically in terms of the images own pixel values in both cases the specific location of a pixel in the image is not used to calculate the likelihoods we suggest a new approach in which we simultaneously eliminate the bias from a set of images of the same anatomy but from different patients we use the statistics from the same location across different images rather than within an image to eliminate bias fields from all of the images simultaneously the method builds a multi resolution non parametric tissue model conditioned on image location while eliminating the bias fields associated with the original image set we present experiments on both synthetic and real mr data sets and present comparisons with other methods
we propose the hierarchical dirichlet process hdp a nonparametric bayesian model for clustering problems involving multiple groups of data each group of data is modeled with a mixture with the number of components being open ended and inferred automatically by the model further components can be shared across groups allowing dependencies across groups to be modeled effectively as well as conferring generalization to new groups such grouped clustering problems occur often in practice eg in the problem of topic discovery in document corpora we report experimental results on three text corpora showing the effective and superior performance of the hdp over previous models
we present a novel method for learning with gaussian process regression in a hierarchical bayesian framework in a first step kernel matrices on a fixed set of input points are learned from data using a simple and efficient em algorithm this step is nonparametric in that it does not require a parametric form of covariance function in a second step kernel functions are fitted to approximate the learned covariance matrix using a generalized nystr m method which results in a complex data o driven kernel we evaluate our approach as a recommendation engine for art images where the proposed hierarchical bayesian method leads to excellent prediction performance
various problems in machine learning databases and statistics involve pairwise distances among a set of objects it is often desirable for these distances to satisfy the properties of a metric especially the triangle inequality applications where metric data is useful include clustering classification metric based indexing and approximation algorithms for various graph problems this paper presents the metric nearness problem given a dissimilarity matrix find the nearest matrix of distances that satisfy the triangle inequalities for p nearness measures this paper develops efficient triangle fixing algorithms that compute globally optimal solutions by exploiting the inherent structure of the problem empirically the algorithms have time and storage costs that are linear in the number of triangle constraints the methods can also be easily parallelized for additional speed
in the context of binary classification we define disagreement as a measure of how often two independently trained models differ in their classification of unlabeled data we explore the use of disagreement for error estimation and model selection we call the procedure co validation since the two models effectively invalidate one another by comparing results on unlabeled data which we assume is relatively cheap and plentiful compared to labeled data we show that per instance disagreement is an unbiased estimate of the variance of error for that instance we also show that disagreement provides a lower bound on the prediction generalization error and a tight upper bound on the variance of prediction error or the variance of the average error across instances where variance is measured across training sets we present experimental results on several data sets exploring co validation for error estimation and model selection the procedure is especially effective in active learning settings where training sets are not drawn at random and cross validation overestimates error

the problem of learning a sparse conic combination of kernel functions or kernel matrices for classification or regression can be achieved via the regularization by a block norm in this paper we present an algorithm that computes the entire regularization path for these problems the path is obtained by using numerical continuation techniques and involves a running time complexity that is a constant times the complexity of solving the problem for one value of the regularization parameter working in the setting of kernel linear regression and kernel logistic regression we show empirically that the effect of the block norm regularization differs notably from the non block norm regularization commonly used for variable selection and that the regularization path is of particular value in the block case
we consider an mdp setting in which the reward function is allowed to change during each time step of play possibly in an adversarial manner yet the dynamics remain fixed similar to the experts setting we address the question of how well can an agent do when compared to the reward achieved under the best stationary policy over time we provide efficient algorithms which have regret bounds with no dependence on the size of state space instead these bounds depend only on a certain horizon time of the process and logarithmically on the number of actions we also show that in the case that the dynamics change over time the problem becomes computationally hard
many machine learning algorithms for clustering or dimensionality reduction take as input a cloud of points in euclidean space and construct a graph with the input data points as vertices this graph is then partitioned clustering or used to redefine metric information dimensionality reduction there has been much recent work on new methods for graph based clustering and dimensionality reduction but not much on constructing the graph itself graphs typically used include the fullyconnected graph a local fixed grid graph for image segmentation or a nearest neighbor graph we suggest that the graph should adapt locally to the structure of the data this can be achieved by a graph ensemble that combines multiple minimum spanning trees each fit to a perturbed version of the data set we show that such a graph ensemble usually produces a better representation of the data manifold than standard methods and that it provides robustness to a subsequent clustering or dimensionality reduction algorithm based on the graph
we study gender discrimination of human faces using a combination of psychophysical classification and discrimination experiments together with methods from machine learning we reduce the dimensionality of a set of face images using principal component analysis and then train a set of linear classifiers on this reduced representation linear support vector machines svms relevance vector machines rvms fisher linear discriminant fld and prototype prot classifiers using human classification data because we combine a linear preprocessor with linear classifiers the entire system acts as a linear classifier allowing us to visualise the decision image corresponding to the normal vector of the separating hyperplanes sh of each classifier we predict that the female tomaleness transition along the normal vector for classifiers closely mimicking human classification svm and rvm should be faster than the transition along any other direction a psychophysical discrimination experiment using the decision images as stimuli is consistent with this prediction
repeated spike patterns have often been taken as evidence for the synfire chain a phenomenon that a stable spike synchrony propagates through a feedforward network inter spike intervals which represent a repeated spike pattern are influenced by the propagation speed of a spike packet however the relation between the propagation speed and network structure is not well understood while it is apparent that the propagation speed depends on the excitatory synapse strength it might also be related to spike patterns we analyze a feedforward network with mexican hattype connectivity fmh using the fokker planck equation we show that both a uniform and a localized spike packet are stable in the fmh in a certain parameter region we also demonstrate that the propagation speed depends on the distinct firing patterns in the same network
survey propagation is a powerful technique from statistical physics that has been applied to solve the sat problem both in principle and in practice we give using only probability arguments a common derivation of survey propagation belief propagation and several interesting hybrid methods we then present numerical experiments which use wsat a widely used random walk based sat solver to quantify the complexity of the sat formulae as a function of their parameters both as randomly generated and after simplication guided by survey propagation some properties of wsat which have not previously been reported make it an ideal tool for this purpose its mean cost is proportional to the number of variables in the formula at a xed ratio of clauses to variables in the easy sat regime and slightly beyond and its behavior in the hardsat regime appears to reect the underlying structure of the solution space that has been predicted by replica symmetry breaking arguments an analysis of the tradeoffs between the various methods of search for satisfying assignments shows wsat to be far more powerful than has been appreciated and suggests some interesting new directions for practical algorithm development
this paper provides a foundation for multi task learning using reproducing kernel hilbert spaces of vector valued functions in this setting the kernel is a matrix valued function some explicit examples will be described which go beyond our earlier results in in particular we characterize classes of matrixvalued kernels which are linear and are of the dot product or the translation invariant type we discuss how these kernels can be used to model relations between the tasks and present linear multi task learning algorithms finally we present a novel proof of the representer theorem for a minimizer of a regularization functional which is based on the notion of minimal norm interpolation
we present a discriminative part based approach for the recognition of object classes from unsegmented cluttered scenes objects are modeled as flexible constellations of parts conditioned on local observations found by an interest operator for each object class the probability of a given assignment of parts to local features is modeled by a conditional random field crf we propose an extension of the crf framework that incorporates hidden variables and combines class conditional crfs into a unified framework for part based object recognition the parameters of the crf are estimated in a maximum likelihood framework and recognition proceeds by finding the most likely class under our model the main advantage of the proposed crf framework is that it allows us to relax the assumption of conditional independence of the observed data ie local features often used in generative approaches an assumption that might be too restrictive for a considerable number of object classes
we consider multi agent systems whose agents compete for resources by striving to be in the minority group the agents adapt to the environment by reinforcement learning of the preferences of the policies they hold diversity of preferences of policies is introduced by adding random biases to the initial cumulative payoffs of their policies we explain and provide evidence that agent cooperation becomes increasingly important when diversity increases analyses of these mechanisms yield excellent agreement with simulations over nine decades of data
finding the sparsest or minimum norm representation of a signal given an overcomplete dictionary of basis vectors is an important problem in many application domains unfortunately the required optimization problem is often intractable because there is a combinatorial increase in the number of local minima as the number of candidate basis vectors increases this deficiency has prompted most researchers to instead minimize surrogate measures such as the norm that lead to more tractable computational methods the downside of this procedure is that we have now introduced a mismatch between our ultimate goal and our objective function in this paper we demonstrate a sparse bayesian learning based method of minimizing the norm while reducing the number of troublesome local minima moreover we derive necessary conditions for local minima to occur via this approach and empirically demonstrate that there are typically many fewer for general problems of interest
we present a competitive analysis of bayesian learning algorithms in the online learning setting and show that many simple bayesian algorithms such as gaussian linear regression and bayesian logistic regression perform favorably when compared in retrospect to the single best model in the model class the analysis does not assume that the bayesian algorithms modeling assumptions are correct and our bounds hold even if the data is adversarially chosen for gaussian linear regression using logloss our error bounds are comparable to the best bounds in the online learning literature and we also provide a lower bound showing that gaussian linear regression is optimal in a certain worst case sense we also give bounds for some widely used maximum a posteriori map estimation algorithms including regularized logistic regression
in the multi armed bandit problem an online algorithm must choose from a set of strategies in a sequence of n trials so as to minimize the total cost of the chosen strategies while nearly tight upper and lower bounds are known in the case when the strategy set is finite much less is known when there is an infinite strategy set here we consider the case when the set of strategies is a subset of rd and the cost functions are continuous in the d case we improve on the best known upper and lower bounds closing the gap to a sublogarithmic factor we also consider the case where d and the cost functions are convex adapting a recent online convex optimization algorithm of zinkevich to the sparser feedback model of the multi armed bandit problem
the representation of acoustic signals at the cochlear nerve must serve a wide range of auditory tasks that require exquisite sensitivity in both time and frequency lewicki demonstrated that many of the filtering properties of the cochlea could be explained in terms of efficient coding of natural sounds this model however did not account for properties such as phase locking or how sound could be encoded in terms of action potentials here we extend this theoretical approach with algorithm for learning efficient auditory codes using a spiking population code here we propose an algorithm for learning efficient auditory codes using a theoretical model for coding sound in terms of spikes in this model each spike encodes the precise time position and magnitude of a localized time varying kernel function by adapting the kernel functions to the statistics natural sounds we show that compared to conventional signal representations the spike code achieves far greater coding efficiency furthermore the inferred kernels show both striking similarities to measured cochlear filters and a similar bandwidth versus frequency dependence
we propose a new method for clustering based on finding maximum margin hyperplanes through data by reformulating the problem in terms of the implied equivalence relation matrix we can pose the problem as a convex integer program although this still yields a difficult computational problem the hard clustering constraints can be relaxed to a soft clustering formulation which can be feasibly solved with a semidefinite program since our clustering technique only depends on the data through the kernel matrix we can easily achieve nonlinear clusterings in the same manner as spectral clustering experimental results show that our maximum margin clustering technique often obtains more accurate results than conventional clustering methods the real benefit of our approach however is that it leads naturally to a semi supervised training method for support vector machines by maximizing the margin simultaneously on labeled and unlabeled training data we achieve state of the art performance by using a single integrated learning principle
we describe a way of using multiple different types of similarity relationship to learn a low dimensional embedding of a dataset our method chooses different possibly overlapping representations of similarity by individually reweighting the dimensions of a common underlying latent space when applied to a single similarity relation that is based on euclidean distances between the input data points the method reduces to simple dimensionality reduction if additional information is available about the dataset or about subsets of it we can use this information to clean up or otherwise improve the embedding we demonstrate the potential usefulness of this form of semi supervised dimensionality reduction on some simple examples
we propose a family of kernels based on the binet cauchy theorem and its extension to fredholm operators this includes as special cases all currently known kernels derived from the behavioral framework diffusion processes marginalized kernels kernels on graphs and the kernels on sets arising from the subspace angle approach many of these kernels can be seen as the extrema of a new continuum of kernel functions which leads to numerous new special cases as an application we apply the new class of kernels to the problem of clustering of video sequences with encouraging results



choice based conjoint analysis builds models of consumer preferences over products with answers gathered in questionnaires our main goal is to bring tools from the machine learning community to solve this problem more efficiently thus we propose two algorithms to quickly and accurately estimate consumer preferences
we provide a principle for semi supervised learning based on optimizing the rate of communicating labels for unlabeled points with side information the side information is expressed in terms of identities of sets of points or regions with the purpose of biasing the labels in each region to be the same the resulting regularization objective is convex has a unique solution and the solution can be found with a pair of local propagation operations on graphs induced by the regions we analyze the properties of the algorithm and demonstrate its performance on document classification tasks
protein interactions typically arise from a physical interaction of one or more small sites on the surface of the two proteins identifying these sites is very important for drug and protein design in this paper we propose a computational method based on probabilistic relational model that attempts to address this task using high throughput protein interaction data and a set of short sequence motifs we learn the model using the em algorithm with a branch and bound algorithm as an approximate inference for the e step our method searches for motifs whose presence in a pair of interacting proteins can explain their observed interaction it also tries to determine which motif pairs have high affinity and can therefore lead to an interaction we show that our method is more accurate than others at predicting new protein protein interactions more importantly by examining solved structures of protein complexes we find that of the predicted active motifs correspond to actual interaction sites

while clustering is usually an unsupervised operation there are circumstances in which we believe with varying degrees of certainty that items a and b should be assigned to the same cluster while items a and c should not we would like such pairwise relations to influence cluster assignments of out of sample data in a manner consistent with the prior knowledge expressed in the training set our starting point is probabilistic clustering based on gaussian mixture models gmm of the data distribution we express clustering preferences in the prior distribution over assignments of data points to clusters this prior penalizes cluster assignments according to the degree with which they violate the preferences we fit the model parameters with em experiments on a variety of data sets show that ppc can consistently improve clustering results


we have recently proposed an extension of adaboost to regression that uses the median of the base regressors as the final regressor in this paper we extend theoretical results obtained for adaboost to median boosting and to its localized variant first we extend recent results on efficient margin maximizing to show that the algorithm can converge to the maximum achievable margin within a preset precision in a finite number of steps then we provide confidence interval type bounds on the generalization error
we describe a three dimensional geometric hand model suitable for visual tracking applications the kinematic constraints implied by the models joints have a probabilistic structure which is well described by a graphical model inference in this model is complicated by the hands many degrees of freedom as well as multimodal likelihoods caused by ambiguous image measurements we use nonparametric belief propagation nbp to develop a tracking algorithm which exploits the graphs structure to control complexity while avoiding costly discretization while kinematic constraints naturally have a local structure selfocclusions created by the imaging process lead to complex interpendencies in color and edge based likelihood functions however we show that local structure may be recovered by introducing binary hidden variables describing the occlusion state of each pixel we augment the nbp algorithm to infer these occlusion variables in a distributed fashion and then analytically marginalize over them to produce hand position estimates which properly account for occlusion events we provide simulations showing that nbp may be used to refine inaccurate model initializations as well as track hand motion through extended image sequences
semantic taxonomies such as wordnet provide a rich source of knowledge for natural language processing applications but are expensive to build maintain and extend motivated by the problem of automatically constructing and extending such taxonomies in this paper we present a new algorithm for automatically learning hypernym is a relations from text our method generalizes earlier work that had relied on using small numbers of hand crafted regular expression patterns to identify hypernym pairs using dependency path features extracted from parse trees we introduce a general purpose formalization and generalization of these patterns given a training set of text containing known hypernym pairs our algorithm automatically extracts useful dependency paths and applies them to new corpora to identify novel pairs on our evaluation task determining whether two nouns in a news article participate in a hypernym relationship our automatically extracted database of hypernyms attains both higher precision and higher recall than wordnet
first order markov models have been successfully applied to many problems for example in modeling sequential data using markov chains and modeling control problems using the markov decision processes mdp formalism if a first order markov models parameters are estimated from data the standard maximum likelihood estimator considers only the first order single step transitions but for many problems the firstorder conditional independence assumptions are not satisfied and as a result the higher order transition probabilities may be poorly approximated motivated by the problem of learning an mdps parameters for control we propose an algorithm for learning a first order markov model that explicitly takes into account higher order interactions during training our algorithm uses an optimization criterion different from maximum likelihood and allows us to learn models that capture longer range effects but without giving up the benefits of using first order markov models our experimental results also show the new algorithm outperforming conventional maximum likelihood estimation in a number of control problems where the mdps parameters are estimated from data


we consider the problem of localizing a set of microphones together with a set of external acoustic events eg hand claps emitted at unknown times and unknown locations we propose a solution that approximates this problem under a far field approximation defined in the calculus of affine geometry and that relies on singular value decomposition svd to recover the affine structure of the problem we then define low dimensional optimization techniques for embedding the solution into euclidean geometry and further techniques for recovering the locations and emission times of the acoustic events the approach is useful for the calibration of ad hoc microphone arrays and sensor networks
integrate and fire type models are usually criticized because of their simplicity on the other hand the integrate and fire model is the basis of most of the theoretical studies on spiking neuron models here we develop a sequential procedure to quantitatively evaluate an equivalent integrate and fire type model based on intracellular recordings of cortical pyramidal neurons we find that the resulting effective model is sufficient to predict the spike train of the real pyramidal neuron with high accuracy in in vivo like regimes predicted and recorded traces are almost indistinguishable and a significant part of the spikes can be predicted at the correct timing slow processes like spike frequency adaptation are shown to be a key feature in this context since they are necessary for the model to connect between different driving regimes
training a learning algorithm is a costly task a major goal of active learning is to reduce this cost in this paper we introduce a new algorithm kqbc which is capable of actively learning large scale problems by using selective sampling the algorithm overcomes the costly sampling step of the well known query by committee qbc algorithm by projecting onto a low dimensional space kqbc also enables the use of kernels providing a simple way of extending qbc to the non linear scenario sampling the low dimension space is done using the hit and run random walk we demonstrate the success of this novel algorithm by applying it to both artificial and a real world problems
an analog focal plane processor having a photodiode array has been developed for directional edge filtering it can perform pixel kernel convolution for entire pixels only with steps of simple analog processing newly developed cyclic line access and row parallel processing scheme in conjunction with the only nearest neighbor interconnects architecture has enabled a very simple implementation a proof of concept chip was fabricated in a m poly metal cmos technology and the edge filtering at a rate of framessec has been experimentally demonstrated

this paper presents a diffusion based probabilistic interpretation of spectral clustering and dimensionality reduction algorithms that use the eigenvectors of the normalized graph laplacian given the pairwise adjacency matrix of all points we define a diffusion distance between any two data points and show that the low dimensional representation of the data by the first few eigenvectors of the corresponding markov matrix is optimal under a certain mean squared error criterion furthermore assuming that data points are random samples from a density px e u x we identify these eigenvectors as discrete approximations of eigenfunctions of a fokker planck operator in a potential u x with reflecting boundary conditions finally applying known results regarding the eigenvalues and eigenfunctions of the continuous fokker planck operator we provide a mathematical justification for the success of spectral clustering and dimensional reduction algorithms based on these first few eigenvectors this analysis elucidates in terms of the characteristics of diffusion processes many empirical findings regarding spectral clustering algorithms keywords algorithms and architectures learning theory

this paper presents representation and logic for labeling contrast edges and ridges in visual scenes in terms of both surface occlusion border ownership and thinline objects in natural scenes thinline objects include sticks and wires while in human graphical communication thinlines include connectors dividers and other abstract devices our analysis is directed at both natural and graphical domains the basic problem is to formulate the logic of the interactions among local image events specifically contrast edges ridges junctions and alignment relations such as to encode the natural constraints among these events in visual scenes in a sparse heterogeneous markov random field framework we define a set of interpretation nodes and energypotential functions among them the minimum energy configuration found by loopy belief propagation is shown to correspond to preferred human interpretation across a wide range of prototypical examples including important illusory contour figures such as the kanizsa triangle as well as more difficult examples in practical terms the approach delivers correct interpretations of inherently ambiguous hand drawn box and connector diagrams at low computational cost

based on a large scale spiking neuron model of the input layers c and of macaque we identify neural mechanisms for the observed contrast dependent receptive field size of v cells we observe a rich variety of mechanisms for the phenomenon and analyze them based on the relative gain of excitatory and inhibitory synaptic inputs we observe an average growth in the spatial extent of excitation and inhibition for low contrast as predicted from phenomenological models however contrary to phenomenological models our simulation results suggest this is neither sufficient nor necessary to explain the phenomenon
in this paper we aim at analyzing the characteristic of neuronal population responses to instantaneous or time dependent inputs and the role of synapses in neural information processing we have derived an evolution equation of the membrane potential density function with synaptic depression and obtain the formulas for analytic computing the response of instantaneous re rate through a technical analysis we arrive at several signi cant conclusions the background inputs play an important role in information processing and act as a switch betwee temporal integration and coincidence detection the role of synapses can be regarded as a spatio temporal lter it is important in neural information processing for the spatial distribution of synapses and the spatial and temporal relation of inputs the instantaneous input frequency can affect the response amplitude and phase delay
in this paper we present our design and experiments of a planar biped robot runbot under pure reflexive neuronal control the goal of this study is to combine neuronal mechanisms with biomechanics to obtain very fast speed and the on line learning of circuit parameters our controller is built with biologically inspired sensorand motor neuron models including local reflexes and not employing any kind of position or trajectory tracking control algorithm instead this reflexive controller allows runbot to exploit its own natural dynamics during critical stages of its walking gait cycle to our knowledge this is the first time that dynamic biped walking is achieved using only a pure reflexive controller in addition this structure allows using a policy gradient reinforcement learning algorithm to tune the parameters of the reflexive controller in real time during walking this way runbot can reach a relative speed of leg lengths per second after a few minutes of online learning which is faster than that of any other biped robot and is also comparable to the fastest relative speed of human walking in addition the stability domain of stable walking is quite large supporting this design strategy
we define a probability distribution over equivalence classes of binary matrices with a finite number of rows and an unbounded number of columns this distribution is suitable for use as a prior in probabilistic models that represent objects using a potentially infinite array of features we identify a simple generative process that results in the same distribution over equivalence classes which we call the indian buffet process we illustrate the use of this distribution as a prior in an infinite latent feature model deriving a markov chain monte carlo algorithm for inference in this model and applying the algorithm to an image dataset
we prove the strongest known bound for the risk of hypotheses selected from the ensemble generated by running a learning algorithm incrementally on the training data our result is based on proof techniques that are remarkably different from the standard risk analysis based on uniform convergence arguments
we consider regularized least squares rls with a gaussian kernel we prove that if we let the gaussian bandwidth while letting the regularization parameter the rls solution tends to a polynomial whose order is controlled by the rielative rates of decay of and if k then as the rls solution tends to the kth order polynomial with minimal empirical error we illustrate the result with an example
in supervised learning scenarios feature selection has been studied widely in the literature selecting features in unsupervised learning scenarios is a much harder problem due to the absence of class labels that would guide the search for relevant information and almost all of previous unsupervised feature selection methods are wrapper techniques that require a learning algorithm to evaluate the candidate feature subsets in this paper we propose a filter method for feature selection which is independent of any learning algorithm our method can be performed in either supervised or unsupervised fashion the proposed method is based on the observation that in many real world classification problems data from the same class are often close to each other the importance of a feature is evaluated by its power of locality preserving or laplacian score we compare our method with data variance unsupervised and fisher score supervised on two data sets experimental results demonstrate the effectiveness and efficiency of our algorithm
we propose a mean field approximation that dramatically reduces the computational complexity of solving stochastic dynamic games we provide conditions that guarantee our method approximates an equilibrium as the number of agents grow we then derive a performance bound to assess how well the approximation performs for any given number of agents we apply our method to an important class of problems in applied microeconomics we show with numerical experiments that we are able to greatly expand the set of economic problems that can be analyzed computationally
given a redundant dictionary of basis vectors or atoms our goal is to find maximally sparse representations of signals previously we have argued that a sparse bayesian learning sbl framework is particularly well suited for this task showing that it has far fewer local minima than other bayesian inspired strategies in this paper we provide further evidence for this claim by proving a restricted equivalence condition based on the distribution of the nonzero generating model weights whereby the sbl solution will equal the maximally sparse representation we also prove that if these nonzero weights are drawn from an approximate jeffreys prior then with probability approaching one our equivalence condition is satisfied finally we motivate the worst case scenario for sbl and demonstrate that it is still better than the most widely used sparse representation algorithms these include basis pursuit bp which is based on a convex relaxation of the quasi norm and orthogonal matching pursuit omp a simple greedy strategy that iteratively selects basis vectors most aligned with the current residual
the variational bayesian framework has been widely used to approximate the bayesian learning in various applications it has provided computational tractability and good generalization performance in this paper we discuss the variational bayesian learning of the mixture of exponential families and provide some additional theoretical support by deriving the asymptotic form of the stochastic complexity the stochastic complexity which corresponds to the minimum free energy and a lower bound of the marginal likelihood is a key quantity for model selection it also enables us to discuss the effect of hyperparameters and the accuracy of the variational bayesian approach as an approximation of the true bayesian learning

we considered a gamma distribution of interspike intervals as a statistical model for neuronal spike generation the model parameters consist of a time dependent firing rate and a shape parameter that characterizes spiking irregularities of individual neurons because the environment changes with time observed data are generated from the time dependent firing rate which is an unknown function a statistical model with an unknown function is called a semiparametric model which is one of the unsolved problem in statistics and is generally very difficult to solve we used a novel method of estimating functions in information geometry to estimate the shape parameter without estimating the unknown function we analytically obtained an optimal estimating function for the shape parameter independent of the functional form of the firing rate this estimation is efficient without fisher information loss and better than maximum likelihood estimation
multiple visual cues are used by the visual system to analyze a scene achromatic cues include luminance texture contrast and motion singlecell recordings have shown that the mammalian visual cortex contains neurons that respond similarly to scene structure eg orientation of a boundary regardless of the cue type conveying this information this paper shows that cue invariant response properties of simpleand complex type cells can be learned from natural image data in an unsupervised manner in order to do this we also extend a previous conceptual model of cue invariance so that it can be applied to model simpleand complex cell responses our results relate cue invariant response properties to natural image statistics thereby showing how the statistical modeling approach can be used to model processing beyond the elemental response properties visual neurons this work also demonstrates how to learn from natural image data more sophisticated feature detectors than those based on changes in mean luminance thereby paving the way for new data driven approaches to image processing and computer vision
fast and frugal heuristics are well studied models of bounded rationality psychological research has proposed the take the best heuristic as a successful strategy in decision making with limited resources take thebest searches for a sufficiently good ordering of cues features in a task where objects are to be compared lexicographically we investigate the complexity of the problem of approximating optimal cue permutations for lexicographic strategies we show that no efficient algorithm can approximate the optimum to within any constant factor if p np we further consider a greedy approach for building lexicographic strategies and derive tight bounds for the performance ratio of a new and simple algorithm this algorithm is proven to perform better than take the best
recent experimental results suggest that dendritic and back propagating spikes can influence synaptic plasticity in different ways in this study we investigate how these signals could temporally interact at dendrites leading to changing plasticity properties at local synapse clusters similar to a previous study we employ a differential hebbian plasticity rule to emulate spike timing dependent plasticity we use dendritic d and back propagating bp spikes as post synaptic signals in the learning rule and investigate how their interaction will influence plasticity we will analyze a situation where synapse plasticity characteristics change in the course of time depending on the type of post synaptic activity momentarily elicited starting with weak synapses which only elicit local d spikes a slow unspecific growth process is induced as soon as the soma begins to spike this process is replaced by fast synaptic changes as the consequence of the much stronger and sharper bp spike which now dominates the plasticity rule this way a winner take all mechanism emerges in a two stage process enhancing the best correlated inputs these results suggest that synaptic plasticity is a temporal changing process by which the computational properties of dendrites or complete neurons can be substantially augmented
we present a competitive analysis of some non parametric bayesian algorithms in a worst case online learning setting where no probabilistic assumptions about the generation of the data are made we consider models which use a gaussian process prior over the space of all functions and provide bounds on the regret under the log loss for commonly used non parametric bayesian algorithms including gaussian regression and logistic regression which show how these algorithms can perform favorably under rather general conditions these bounds explicitly handle the infinite dimensionality of these non parametric classes in a natural way we also make formal connections to the minimax and minimum description length mdl framework here we show precisely how bayesian gaussian regression is a minimax strategy
we consider the problem of constructing an aggregated estimator from a finite class of base functions which approximately minimizes a convex risk functional under the constraint for this purpose we propose a stochastic procedure the mirror descent which performs gradient descent in the dual space the generated estimates are additionally averaged in a recursive fashion with specific weights mirror descent algorithms have been developed in different contexts and they are known to be particularly efficient in high dimensional problems moreover their implementation is adapted to the online setting the main result of the paper is the upper bound on the convergence rate for the generalization error

separation of music signals is an interesting but difficult problem it is helpful for many other music researches such as audio content analysis in this paper a new music signal separation method is proposed which is based on harmonic structure modeling the main idea of harmonic structure modeling is that the harmonic structure of a music signal is stable so a music signal can be represented by a harmonic structure model accordingly a corresponding separation algorithm is proposed the main idea is to learn a harmonic structure model for each music signal in the mixture and then separate signals by using these models to distinguish harmonic structures of different signals experimental results show that the algorithm can separate signals and obtain not only a very high signalto noise ratio snr but also a rather good subjective audio quality
survival in the natural world demands the selection of relevant visual cues to rapidly and reliably guide attention towards prey and predators in cluttered environments we investigate whether our visual system selects cues that guide search in an optimal manner we formally obtain the optimal cue selection strategy by maximizing the signal to noise ratio sn r between a search target and surrounding distractors this optimal strategy successfully accounts for several phenomena in visual search behavior including the effect of target distractor discriminability uncertainty in targets features distractor heterogeneity and linear separability furthermore the theory generates a new prediction which we verify through psychophysical experiments with human subjects our results provide direct experimental evidence that humans select visual cues so as to maximize sn r between the targets and surrounding clutter
we present a non linear simple yet effective feature subset selection method for regression and use it in analyzing cortical neural activity our algorithm involves a feature weighted version of the k nearest neighbor algorithm it is able to capture complex dependency of the target function on its input and makes use of the leave one out error as a natural regularization we explain the characteristics of our algorithm on synthetic problems and use it in the context of predicting hand velocity from spikes recorded in motor cortex of a behaving monkey by applying feature selection we are able to improve prediction quality and suggest a novel way of exploring neural data
images represent an important and abundant source of data understanding their statistical structure has important applications such as image compression and restoration in this paper we propose a particular kind of probabilistic model dubbed the products of edge perts model to describe the structure of wavelet transformed images we develop a practical denoising algorithm based on a single edge pert and show state ofthe art denoising performance on benchmark images
the perceptron algorithm despite its simplicity often performs well on online classification tasks the perceptron becomes especially effective when it is used in conjunction with kernels however a common difficulty encountered when implementing kernel based online algorithms is the amount of memory required to store the online hypothesis which may grow unboundedly in this paper we present and analyze the forgetron algorithm for kernel based online learning on a fixed memory budget to our knowledge this is the first online learning algorithm which on one hand maintains a strict limit on the number of examples it stores while on the other hand entertains a relative mistake bound in addition to the formal results we also present experiments with real datasets which underscore the merits of our approach
we present a new connectionist model for constructive intuitionistic modal reasoning we use ensembles of neural networks to represent intuitionistic modal theories and show that for each intuitionistic modal program there exists a corresponding neural network ensemble that computes the program this provides a massively parallel model for intuitionistic modal reasoning and sets the scene for integrated reasoning knowledge representation and learning of intuitionistic theories in neural networks since the networks in the ensemble can be trained by examples using standard neural learning algorithms

nonnegative matrix approximation nnma is a recent technique for dimensionality reduction and data analysis that yields a parts based sparse nonnegative representation for nonnegative input data nnma has found a wide variety of applications including text analysis document clustering faceimage recognition language modeling speech processing and many others despite these numerous applications the algorithmic development for computing the nnma factors has been relatively deficient this paper makes algorithmic progress by modeling and solving using multiplicative updates new generalized nnma problems that minimize bregman divergences between the input matrix and its lowrank approximation the multiplicative update formulae in the pioneering work by lee and seung arise as a special case of our algorithms in addition the paper shows how to use penalty functions for incorporating constraints other than nonnegativity into the problem further some interesting extensions to the use of link functions for modeling nonlinear relationships are also discussed
spectral clustering enjoys its success in both data clustering and semisupervised learning but most spectral clustering algorithms cannot handle multi class clustering problems directly additional strategies are needed to extend spectral clustering algorithms to multi class clustering problems furthermore most spectral clustering algorithms employ hard cluster membership which is likely to be trapped by the local optimum in this paper we present a new spectral clustering algorithm named soft cut it improves the normalized cut algorithm by introducing soft membership and can be efficiently computed using a bound optimization algorithm our experiments with a variety of datasets have shown the promising performance of the proposed clustering algorithm
we propose a probabilistic model based on independent component analysis for learning multiple related tasks in our model the task parameters are assumed to be generated from independent sources which account for the relatedness of the tasks we use laplace distributions to model hidden sources which makes it possible to identify the hidden independent components instead of just modeling correlations furthermore our model enjoys a sparsity property which makes it both parsimonious and robust we also propose efficient algorithms for both empirical bayes method and point estimation our experimental results on two multi label text classification data sets show that the proposed approach is promising
this paper presents a new framework based on walks in a graph for analysis and inference in gaussian graphical models the key idea is to decompose correlations between variables as a sum over all walks between those variables in the graph the weight of each walk is given by a product of edgewise partial correlations we provide a walk sum interpretation of gaussian belief propagation in trees and of the approximate method of loopy belief propagation in graphs with cycles this perspective leads to a better understanding of gaussian belief propagation and of its convergence in loopy graphs
kernel methods make it relatively easy to define complex highdimensional feature spaces this raises the question of how we can identify the relevant subspaces for a particular learning task when two views of the same phenomenon are available kernel canonical correlation analysis kcca has been shown to be an effective preprocessing step that can improve the performance of classification algorithms such as the support vector machine svm this paper takes this observation to its logical conclusion and proposes a method that combines this two stage learning kcca followed by svm into a single optimisation termed svm k we present both experimental and theoretical analysis of the approach showing encouraging results and insights
we propose an algorithm that uses gaussian process regression to learn common hidden structure shared between corresponding sets of heterogenous observations the observation spaces are linked via a single reduced dimensionality latent variable space we present results from two datasets demonstrating the algorithmss ability to synthesize novel data from learned correspondences we first show that the method can learn the nonlinear mapping between corresponding views of objects filling in missing data as needed to synthesize novel views we then show that the method can learn a mapping between human degrees of freedom and robotic degrees of freedom for a humanoid robot allowing robotic imitation of human poses from motion capture data
we present a method for nonparametric regression that performs bandwidth selection and variable selection simultaneously the approach is based on the technique of incrementally decreasing the bandwidth in directions where the gradient of the estimator with respect to bandwidth is large when the unknown function satisfies a sparsity condition our approach avoids the curse of dimensionality achieving the optimal minimax rate of convergence up to logarithmic factors as if the relevant variables were known in advance the method called rodeo regularization of derivative expectation operator conducts a sequence of hypothesis tests and is easy to implement a modified version that replaces hard with soft thresholding effectively solves a sequence of lasso problems
we study the statistical convergence and consistency of regularized boosting methods where the samples are not independent and identically distributed iid but come from empirical processes of stationary mixing sequences utilizing a technique that constructs a sequence of independent blocks close in distribution to the original samples we prove the consistency of the composite classifiers resulting from a regularization achieved by restricting the norm of the base classifiers weights when compared to the iid case the nature of sampling manifests in the consistency result only through generalization of the original condition on the growth of the regularization parameter
given a directed graphical model with binary valued hidden nodes and real valued noisy observations consider deciding upon the maximum a posteriori map or the maximum posterior marginal mpm assignment under the restriction that each node broadcasts only to its children exactly one single bit message we present a variational formulation viewing the processing rules local to all nodes as degrees of freedom that minimizes the loss in expected map or mpm performance subject to such online communication constraints the approach leads to a novel message passing algorithm to be executed offline or before observations are realized which mitigates the performance loss by iteratively coupling all rules in a manner implicitly driven by global statistics we also provide i illustrative examples ii assumptions that guarantee convergence and efficiency and iii connections to active research areas
there is a long standing controversy on the site of the cerebellar motor learning different theories and experimental results suggest that either the cerebellar flocculus or the brainstem learns the task and stores the memory with a dynamical system approach we clarify the mechanism of transferring the memory generated in the flocculus to the brainstem and that of so called savings phenomena the brainstem learning must comply with a sort of hebbian rule depending on purkinje cell activities in contrast to earlier numerical models our model is simple but it accommodates explanations and predictions of experimental situations as qualitative features of trajectories in the phase space of synaptic weights without fine parameter tuning
we study the problem of maximum entropy density estimation in the presence of known sample selection bias we propose three bias correction approaches the first one takes advantage of unbiased sufficient statistics which can be obtained from biased samples the second one estimates the biased distribution and then factors the bias out the third one approximates the second by only using samples from the sampling distribution we provide guarantees for the first two approaches and evaluate the performance of all three approaches in synthetic experiments and on real data from species habitat modeling where maxent has been successfully applied and where sample selection bias is a significant problem
this paper presents a new filter for online data association problems in high dimensional spaces the key innovation is a representation of the data association posterior in information form in which the proximity of objects and tracks are expressed by numerical links updating these links requires linear time compared to exponential time required for computing the exact posterior probabilities the paper derives the algorithm formally and provides comparative results using data obtained by a real world camera array and by a large scale sensor network simulation
previous work has demonstrated that the image variations of many objects human faces in particular under variable lighting can be effectively modeled by low dimensional linear spaces the typical linear subspace learning algorithms include principal component analysis pca linear discriminant analysis lda and locality preserving projection lpp all of these methods consider an n n image as a high dimensional vector in rn n while an image represented in the plane is intrinsically a matrix in this paper we propose a new algorithm called tensor subspace analysis tsa tsa considers an image as the second order tensor in rn rn where rn and rn are two vector spaces the relationship between the column vectors of the image matrix and that between the row vectors can be naturally characterized by tsa tsa detects the intrinsic local geometrical structure of the tensor space by learning a lower dimensional tensor subspace we compare our proposed approach with pca lda and lpp methods on two standard databases experimental results demonstrate that tsa achieves better recognition rate while being much more efficient
we propose consensus propagation an asynchronous distributed protocol for averaging numbers across a network we establish convergence characterize the convergence rate for regular graphs and demonstrate that the protocol exhibits better scaling properties than pairwise averaging an alternative that has received much recent attention consensus propagation can be viewed as a special case of belief propagation and our results contribute to the belief propagation literature in particular beyond singly connected graphs there are very few classes of relevant problems for which belief propagation is known to converge
we investigate the learning of the appearance of an object from a single image of it instead of using a large number of pictures of the object to recognize we use a labeled reference database of pictures of other objects to learn invariance to noise and variations in pose and illumination this acquired knowledge is then used to predict if two pictures of new objects which do not appear on the training pictures actually display the same object we propose a generic scheme called chopping to address this task it relies on hundreds of random binary splits of the training set chosen to keep together the images of any given object those splits are extended to the complete image space with a simple learning algorithm given two images the responses of the split predictors are combined with a bayesian rule into a posterior probability of similarity experiments with the coil database and with a database of dea graded ltex symbols compare our method to a classical learning with several examples of the positive class and to a direct learning of the similarity
fisher linear discriminant analysis lda can be sensitive to the problem data robust fisher lda can systematically alleviate the sensitivity problem by explicitly incorporating a model of data uncertainty in a classification problem and optimizing for the worst case scenario under this model the main contribution of this paper is show that with general convex uncertainty models on the problem data robust fisher lda can be carried out using convex optimization for a certain type of product form uncertainty model robust fisher lda can be carried out at a cost comparable to standard fisher lda the method is demonstrated with some numerical examples finally we show how to extend these results to robust kernel fisher discriminant analysis ie robust fisher lda in a high dimensional feature space
we present a generalization of temporal difference networks to include temporally abstract options on the links of the question network temporal difference td networks have been proposed as a way of representing and learning a wide variety of predictions about the interaction between an agent and its environment these predictions are compositional in that their targets are defined in terms of other predictions and subjunctive in that that they are about what would happen if an action or sequence of actions were taken in conventional td networks the inter related predictions are at successive time steps and contingent on a single action here we generalize them to accommodate extended time intervals and contingency on whole ways of behaving our generalization is based on the options framework for temporal abstraction the primary contribution of this paper is to introduce a new algorithm for intra option learning in td networks with function approximation and eligibility traces we present empirical examples of our algorithms effectiveness and of the greater representational expressiveness of temporallyabstract td networks the primary distinguishing feature of temporal difference td networks sutton tanner is that they permit a general compositional specification of the goals of learning the goals of learning are thought of as predictive questions being asked by the agent in the learning problem such as what will i see if i step forward and look right or if i open the fridge will i see a bottle of beer seeing a bottle of beer is of course a complicated perceptual act it might be thought of as obtaining a set of predictions about what would happen if certain reaching and grasping actions were taken about what would happen if the bottle were opened and turned upside down and of what the bottle would look like if viewed from various angles to predict seeing a bottle of beer is thus to make a prediction about a set of other predictions the target for the overall prediction is a composition in the mathematical sense of the first prediction with each of the other predictions td networks are the first framework for representing the goals of predictive learning in a compositional machine accessible form each node of a td network represents an individual question something to be predicted and has associated with it a value representing an answer to the question a prediction of that something the questions are represented by a set of directed links between nodes if node is linked to node then node re firesents a question incorporating node s question its value is a prediction about node s prediction higher level predictions can be composed in several ways from lower ones producing a powerful structured representation language for the targets of learning the compositional structure is not just in a human designers head it is expressed in the links and thus is accessible to the agent and its learning algorithm the network of these links is referred to as the question network an entirely separate set of directed links between the nodes is used to compute the values predictions answers associated with each node these links collectively are referred to as the answer network the computation in the answer network is compositional in a conventional way node values are computed from other node values the essential insight of td networks is that the notion of compositionality should apply to questions as well as to answers a secondary distinguishing feature of td networks is that the predictions node values at each moment in time can be used as a representation of the state of the world at that time in this way they are an instance of the idea of predictive state representations psrs introduced by littman sutton and singh jaeger and rivest and schapire representing a state by its predictions is a potentially powerful strategy for state abstraction rafols et al we note that the questions used in all previous work with psrs are defined in terms of concrete actions and observations not other predictions they are not compositional in the sense that td network questions are the questions we have discussed so far are subjunctive meaning that they are conditional on a certain way of behaving we predict what we would see if we were to step forward and look right or if we were to open the fridge the questions in conventional td networks are subjunctive but they are conditional only on primitive actions or open loop sequences of primitive actions as are conventional psrs it is natural to generalize this as we have in the informal examples above to questions that are conditional on closed loop temporally extended ways of behaving for example opening the fridge is a complex high level action the arm must be lifted to the door the hand shaped for grasping the handle etc to ask questions like if i were to go to the coffee room would i see john would require substantial temporal abstraction in addition to state abstraction the options framework sutton precup singh is a straightforward way of talking about temporally extended ways of behaving and about predictions of their outcomes in this paper we extend the options framework so that it can be applied to td networks significant extensions of the original options framework are needed novel features of our option extended td networks are that they predict components of option outcomes rather than full outcome probability distributions learn according to the first intra option method to use eligibility traces see sutton barto and include the possibility of options whose policies are indifferent to which of several actions are selected the options framework in this section we present the essential elements of the options framework sutton precup singh that we will need for our extension of td networks in this framework an agent and an environment interact at discrete time steps t in each state st s the agent selects an action at a determining the next state st an action is a way of behaving for one time step the options framework lets us talk about temporally extended ways of behaving an individual option consists of three parts the first is the initiation set i s the subset of states in which the option can be started the second component of an option is its policy s a specifying how the agent behaves when although the options framework includes rewards we omit them here because we are concerned only with prediction not control fifollowing the option finally a termination function s a specifies how the option ends s denotes the probability of terminating when in state s the option is thus completely and formally defined by the tuple i conventional td networks in this section we briefly present the details of the structure and the learning algorithm comprising td networks as introduced by sutton and tanner td networks address a prediction problem in which the agent may not have direct access to the state of the environment instead at each time step the agent receives an observation ot o dependent on the state the experience stream thus consists of a sequence of alternating actions and observations o a o a o the td network consists of a set of nodes each representing a single scalar prediction interlinked by the question and answer networks as suggested previously for a network n of n nodes the vector of all predictions at time step t is denoted yt yt yt t the predictions are estimates of the expected value of some scalar quantity typically of a bit in which case they can be interpreted as estimates of probabilities the predictions are updated at each time step according to a vector valued function u with modifiable parameter w which is often taken to be of a linear form yt uyt at ot wt wt xt where xt m is an m vector of features created from yt at ot wt is an n m matrix whose elements are sometimes referred to as weights and is the n vector form of either the identity function or the s shaped logistic function s e s the feature vector is an arbitrary vector valued function of yt at and ot for example in the simplest case the feature vector is a unit basis vector with the location of the one communicating the current state in a partially observable environment the feature vector may be a combination of the agents action observations and predictions from the previous time step the overall update u defines the answer network the question network consists of a set of target functions z i o n and condition i y functions ci a n n we define zt z i ot t as the target for prediction i i i yt similarly we define ct c at yt as the condition at time t the learning algorithm ij for each component wt of wt can then be written ij wt where is a positive step size parameter note that the targets here are functions of the observation and predictions exactly one time step later and that the conditions are functions of a single primitive action this is what makes this algorithm suitable only for learning about one step td relationships by chaining together multiple nodes sutton and tanner used it to predict k steps ahead for various particular values of k and to predict the outcome of specific action sequences as in psrs eg littman et al singh et al now we consider the extension to temporally abstract actions ij ij i i wt wt zt yt ci t i yt option extended td networks in this section we present our intra option learning algorithm for td networks with options and eligibility traces as suggested earlier each nodes outgoing link in the question the quantity is almost the same as y and we encourage the reader to think of them as identical y here the difference is that is calculated by weights that are one step out of date as compared to y y ie t uyt at ot wt cf equation y finetwork will now correspond to an option applying over possibly many steps the policy of the ith nodes option corresponds to the condition function ci which we think of as a recognizer for the option it inspects each action taken to assess whether the option is being followed ci if the agent is acting consistently with the option policy and ci othert t wise intermediate values are also possible when an agent ceases to act consistently with the option policy we say that the option has diverged the possibility of recognizing more than one action as consistent with the option is a significant generalization of the original idea of options if no actions are recognized as acceptable in a state then the option cannot be followed and thus cannot be initiated here we take the set of states with at least one recognized action to be the initiation set of the option the option termination function generalizes naturally to td networks each node i is i given a corresponding termination function i o n where t i ot yt i is the probability of terminating at time t t indicates that the option has terminated i at time t t indicates that it has not and intermediate values of correspond to soft i or stochastic termination conditions if an option terminates then zt acts as the target but if the option is ongoing without termination then the nodes own next value yt should i be the target the termination function specifies which of the two targets or mixture of the two targets is used to produce a form of td error for each node i i ii ii i t t zt t t yt y our option extended algorithm incorporates eligibility traces see sutton barto as short term memory variables organized in an n m matrix e paralleling the weight matrix the traces are a record of the effect that each weight could have had on each nodes prediction during the time the agent has been acting consistently with the nodes option the components eij of the eligibility matrix are updated by i eij ci eij t t t t i yt ij wt where is the trace decay parameter familiar from the td learning algorithm because of the ci factor all of a nodes traces will be immediately reset to zero whenever t the agent deviates from the nodes options policy if the agent follows the policy and the option does not terminate then the trace decays by and increments by the gradient in the way typical of eligibility traces if the policy is followed and the option does terminate then the trace will be reset to zero on the immediately following time step and a new trace will start building finally our algorithm updates the weights on each time step by ij ij i wt wt t eij t fully observable experiment this experiment was designed to test the correctness of the algorithm in a simple gridworld where the environmental state is observable we applied an options extended td network to the problem of learning to predict observations from interaction with the gridworld environment shown on the left in figure empty squares indicate spaces where the agent can move freely and colored squares shown shaded in the figure indicate walls the agent is egocentric at each time step the agent receives from the environment six bits representing the color it is facing red green blue orange yellow or white in this first experiment we also provided other bits directly indicating the complete state of the environment square and orientation the fact that the option depends only on the current predictions action and observation means that we are considering only markov options fifigure the test world left and the question network right used in the experiments the triangle in the world indicates the location and orientation of the agent the walls are labeled r o y g and b representing the colors red orange yellow green and blue note that the left wall is mostly blue but partly green the right diagram shows in full the portion of the question network corresponding to the red bit this structure is repeated but not shown for the other four non white colors l r and f are primitive actions and forward and wander are options there are three possible actions a f r l actions were selected according to a fixed stochastic policy independent of the state the probability of the f l and r actions were and respectively l and r cause the agent to rotate degrees to the left or right f causes the agent to move ahead one square with probability p and to stay in the same square with probability p the probability p is called the slipping probability if the forward movement would cause the agent to move into a wall then the agent does not move in this experiment we used p p and p in addition to these primitive actions we provided two temporally abstract options forward and wander the forward option takes the action f in every state and terminates when the agent senses a wall color in front of it the policy of the wander option is the same as that actually followed by the agent wander terminates with probability when a wall is sensed and spontaneously with probability otherwise we used the question network shown on the right in figure the predictions of nodes and are estimates of the probability that the red bit would be observed if the corresponding primitive action were taken node is a prediction of whether the agent will see the red bit upon termination of the wander option if it were taken node predicts the probability of observing the red bit given that the forward option is followed until termination nodes and represent predictions of the outcome of a primitive action followed by the forward option nodes and take this one step further they represent predictions of the red bit if the forward option were followed to termination then a primitive action were taken and then the forward option were followed again to termination we applied our algorithm to learn the parameter w of the answer network for this question network the step size parameter was and the trace decay parameter was the initial w e and y were all each run began with the agent in the state indicated in figure left in this experiment was the identity function for each value of p we ran runs of time steps on each time step the root meansquared rms error in each nodes prediction was computed and then averaged over all the nodes the nodes corresponding to the wander option were not included in the average because of the difficulty of calculating their correct predictions this average was then fi fully observable rms error p p p partially observable rms error steps steps figure learning curves in the fully observable experiment for each slippage probability left and in the partially observable experiment right itself averaged over the runs and bins of time steps to produce the learning curves shown on the left in figure for all slippage probabilities the error in all predictions fell almost to zero after approximately trials the agent made almost perfect predictions in all cases not surprisingly learning was slower at the higher slippage probabilities these results show that our augmented td network is able to make a complete temporally abstract model of this world partially observable experiment in our second experiment only the six color observation bits were available to the agent this experiment provides a more challenging test of our algorithm to model the environment well the td network must construct a representation of state from very sparse information in fact completely accurate prediction is not possible in this problem with our question network in this experiment the input vector consisted of three groups of components each in total if the action was r the first components were set to the node values and the six observation bits and the other components were if the action was l the next group of components was filled in in the same way and the first and third groups were zero if the action was f the third group was filled this technique enables the answer network as function approximator to represent a wider class of functions in a linear form than would otherwise be possible in this experiment was the s shaped logistic function the slippage probability was p as our performance measure we used the rms error as in the first experiment except that the predictions for the primitive actions nodes were not included these predictions can never become completely accurate because the agent cant tell in detail where it is located in the open space as before we averaged rms error over runs and time step bins to produce the learning curve shown on the right in figure as before the rms error approached zero node in figure holds the prediction of red if the agent were to march forward to the wall ahead of it corresponding nodes in the other subnetworks hold the predictions of the other colors upon forward to make these predictions accurately the agent must keep track of which wall it is facing even if it is many steps away from it it has to learn a sort of compass that it can keep updated as it turns in the middle of the space figure is a demonstration of the compass learned after a representative run of time steps at the end of the run the agent was driven manually to the state shown in the first row relative fitime index t on steps the agent was spun clockwise in place the third column shows the prediction for node in each portion of the question network that is the predictions shown are for each color observation bit at termination of the forward option at t the agent is facing the orange wall and it predicts that the forward option would result in seeing the orange bit and none other over steps we see that the predictions are maintained accurately as the agent spins despite the fact that its observation bits remain the same even after spinning for steps the agent knows exactly which way it is facing while spinning the agent correctly never predicts seeing the green bit after forward but if it is driven up and turned as in the last row of the figure the green bit is accurately predicted the fourth column shows the prediction for node in each portion of the question network recall that these nodes correspond to the sequence forward l forward at time t the agent accurately predicts that forward will bring it to orange third column and also predicts that forward l forward will bring it to green the predictions made for node at each subsequent step of the sequence are also correct these results show that the agent is able to accurately maintain its long term predictions without directly encountering sensory verification how much larger would the td network have to be to handle a x gridworld the answer is not at all the same question network applies to any size problem if the layout of the colored walls remain the same then even the answer network transfers across worlds of widely varying sizes in other experiments training on successively larger problems we have shown that the same td network as used here can learn to make all the long term predictions correctly on a x version of the x gridworld used here t st y t y t o y r bg o y r bg o y r bg o y r bg o y r bg o y r bg o y r bg o y r bg o y r bg o y r bg o y r bg o y r bg o y r bg o y r bg figure an illustration of part of what the agent learns in the partially observable environment the second column is a sequence of states with relative time index as given by the first column the sequence was generated by controlling the agent manually on steps the agent was spun clockwise in place and the trajectory after that is shown by the line in the last state diagram the third and fourth columns show the values of the nodes corresponding to and in figure one for each color observation bit fi conclusion our experiments show that option extended td networks can learn effectively they can learn facts about their environments that are not representable in conventional td networks or in any other method for learning models of the world one concern is that our intra option learning algorithm is an off policy learning method incorporating function approximation and bootstrapping learning from predictions the combination of these three is known to produce convergence problems for some methods see sutton barto and they may arise here a sound solution may require modifications to incorporate importance sampling see precup sutton dasgupta in this paper we have considered only intra option eligibility traces traces extending over the time span within an option but not persisting across options tanner and sutton have proposed a method for inter option traces that could perhaps be combined with our intra option traces the primary contribution of this paper is the
the classical bayes rule computes the posterior model probability from the prior probability and the data likelihood we generalize this rule to the case when the prior is a density matrix symmetric positive definite and trace one and the data likelihood a covariance matrix the classical bayes rule is retained as the special case when the matrices are diagonal in the classical setting the calculation of the probability of the data is an expected likelihood where the expectation is over the prior distribution in the generalized setting this is replaced by an expected variance calculation where the variance is computed along the eigenvectors of the prior density matrix and the expectation is over the eigenvalues of the density matrix which form a probability vector the variances along any direction is determined by the covariance matrix curiously enough this expected variance calculation is a quantum measurement where the co variance matrix specifies the instrument and the prior density matrix the mixture state of the particle we motivate both the classical and the generalized bayes rule with a minimum relative entropy principle where the kullbach leibler version gives the classical bayes rule and umegakis quantum relative entropy the new bayes rule for density matrices
we show how to learn a mahanalobis distance metric for k nearest neighbor knn classification by semidefinite programming the metric is trained with the goal that the k nearest neighbors always belong to the same class while examples from different classes are separated by a large margin on seven data sets of varying size and difficulty we find that metrics trained in this way lead to significant improvements in knn classification for example achieving a test error rate of on the mnist handwritten digits as in support vector machines svms the learning problem reduces to a convex optimization based on the hinge loss unlike learning in svms however our framework requires no modification or extension for problems in multiway as opposed to binary classification
functional magnetic resonance imaging fmri has enabled scientists to look into the active brain however interactivity between functional brain regions is still little studied in this paper we contribute a novel framework for modeling the interactions between multiple active brain regions using dynamic bayesian networks dbns as generative models for brain activation patterns this framework is applied to modeling of neuronal circuits associated with reward the novelty of our framework from a machine learning perspective lies in the use of dbns to reveal the brain connectivity and interactivity such interactivity models which are derived from fmri data are then validated through a group classification task we employ and compare four different types of dbns parallel hidden markov models coupled hidden markov models fully linked hidden markov models and dynamically multilinked hmms dml hmm moreover we propose and compare two schemes of learning dml hmms experimental results show that by using dbns group classification can be performed even if the dbns are constructed from as few as brain regions we also demonstrate that by using the proposed learning algorithms different dbn structures characterize drug addicted subjects vs control subjects this finding provides an independent test for the effect of psychopathology on brain function in general we demonstrate that incorporation of computer science principles into functional neuroimaging clinical studies provides a novel approach for probing human brain function
a layer neuromorphic vision processor whose components communicate spike events asychronously using the address eventrepresentation aer is demonstrated the system includes a retina chip two convolution chips a d winner take all chip a delay line chip a learning classifier chip and a set of pcbs for computer interfacing and address space remappings the components use a mixture of analog and digital computation and will learn to classify trajectories of a moving object a complete experimental setup and measurements results are shown
in this paper we provide a general theorem that establishes a correspondence between surrogate loss functions in classification and the family of f divergences moreover we provide constructive procedures for determining the f divergence induced by a given surrogate loss and conversely for finding all surrogate loss functions that realize a given f divergence next we introduce the notion of universal equivalence among loss functions and corresponding f divergences and provide necessary and sufficient conditions for universal equivalence to hold these ideas have applications to classification problems that also involve a component of experiment design in particular we leverage our results to prove consistency of a procedure for learning a classifier under decentralization requirements
we introduce a technique for dimensionality estimation based on the notion of quantization dimension which connects the asymptotic optimal quantization error for a probability distribution on a manifold to its intrinsic dimension the definition of quantization dimension yields a family of estimation algorithms whose limiting case is equivalent to a recent method based on packing numbers using the formalism of high rate vector quantization we address issues of statistical consistency and analyze the behavior of our scheme in the presence of noise
active learning is the problem in supervised learning to design the locations of training input points so that the generalization error is minimized existing active learning methods often assume that the model used for learning is correctly specified ie the learning target function can be expressed by the model at hand in many practical situations however this assumption may not be fulfilled in this paper we first show that the existing active learning method can be theoretically justified under slightly weaker condition the model does not have to be correctly specified but slightly misspecified models are also allowed however it turns out that the weakened condition is still restrictive in practice to cope with this problem we propose an alternative active learning method which can be theoretically justified for a wider class of misspecified models thus the proposed method has a broader range of applications than the existing method numerical studies show that the proposed active learning method is robust against the misspecification of models and is thus reliable
we analyze classification error on unseen cases ie cases that are different from those in the training set unlike standard generalization error this off training set error may differ significantly from the empirical error with high probability even with large sample sizes we derive a datadependent bound on the difference between off training set and standard generalization error our result is based on a new bound on the missing mass which for small samples is stronger than existing bounds based on good turing estimators as we demonstrate on uci data sets our bound gives nontrivial generalization guarantees in many practical cases in light of these results we show that certain claims made in the no free lunch literature are overly pessimistic
we propose a new linear method for dimension reduction to identify nongaussian components in high dimensional data our method ngca non gaussian component analysis uses a very general semi parametric framework in contrast to existing projection methods we define what is uninteresting gaussian by projecting out uninterestingness we can estimate the relevant non gaussian subspace we show that the estimation error of finding the non gaussian components tends to zero at a parametric rate once ngca components are identified and extracted various tasks can be applied in the data analysis process like data visualization clustering denoising or classification a numerical study demonstrates the usefulness of our method
we present a novel approach to the characterization of complex sensory neurons one of the main goals of characterizing sensory neurons is to characterize dimensions in stimulus space to which the neurons are highly sensitive causing large gradients in the neural responses or alternatively dimensions in stimulus space to which the neuronal response are invariant defining iso response manifolds we formulate this problem as that of learning a geometry on stimulus space that is compatible with the neural responses the distance between stimuli should be large when the responses they evoke are very different and small when the responses they evoke are similar here we show how to successfully train such distance functions using rather limited amount of information the data consisted of the responses of neurons in primary auditory cortex a of anesthetized cats to stimuli derived from natural sounds for each neuron a subset of all pairs of stimuli was selected such that the responses of the two stimuli in a pair were either very similar or very dissimilar the distance function was trained to fit these constraints the resulting distance functions generalized to predict the distances between the responses of a test stimulus and the trained stimuli
this paper provides a system level analysis of a scalable distributed sensing model for networked sensors in our system model a data center acquires data from a bunch of l sensors which each independently encode their noisy observations of an original binary sequence and transmit their encoded data sequences to the data center at a combined rate r which is limited supposing that the sensors use independent ldgm rate distortion codes we show that the system performance can be evaluated for any given finite r when the number of sensors l goes to infinity the analysis shows how the optimal strategy for the distributed sensing problem changes at critical values of the data rate r or the noise level
in this paper we show that the hinge loss can be interpreted as the neg log likelihood of a semi parametric model of posterior probabilities from this point of view svms represent the parametric component of a semi parametric model fitted by a maximum a posteriori estimation procedure this connection enables to derive a mapping from svm scores to estimated posterior probabilities unlike previous proposals the suggested mapping is interval valued providing a set of posterior probabilities compatible with each svm score this framework offers a new way to adapt the svm optimization problem to unbalanced classification when decisions result in unequal asymmetric losses experiments show improvements over state of the art procedures
brain computer interface bci systems create a novel communication channel from the brain to an output device by bypassing conventional motor output pathways of nerves and muscles therefore they could provide a new communication and control option for paralyzed patients modern bci technology is essentially based on techniques for the classification of single trial brain signals here we present a novel technique that allows the simultaneous optimization of a spatial and a spectral filter enhancing discriminability of multi channel eeg single trials the evaluation of experiments involving different subjects demonstrates the superiority of the proposed algorithm apart from the enhanced classification the spatial andor the spectral filter that are determined by the algorithm can also be used for further analysis of the data eg for source localization of the respective brain rhythms
given a set of points and a set of prototypes representing them how to create a graph of the prototypes whose topology accounts for that of the points this problem had not yet been explored in the framework of statistical learning theory in this work we propose a generative model based on the delaunay graph of the prototypes and the expectationmaximization algorithm to learn the parameters this work is a first step towards the construction of a topological model of a set of points grounded on statistics
while kernel canonical correlation analysis kernel cca has been applied in many problems the asymptotic convergence of the functions estimated from a finite sample to the true functions has not yet been established this paper gives a rigorous proof of the statistical convergence of kernel cca and a related method nocco which provides a theoretical justification for these methods the result also gives a sufficient condition on the decay of the regularization coefficient in the methods to ensure convergence
female crickets can locate males by phonotaxis to the mating song they produce the behaviour and underlying physiology has been studied in some depth showing that the cricket auditory system solves this complex problem in a unique manner we present an analogue very large scale integrated avlsi circuit model of this process and show that results from testing the circuit agree with simulation and what is known from the behaviour and physiology of the cricket auditory system the avlsi circuitry is now being extended to use on a robot along with previously modelled neural circuitry to better understand the complete sensorimotor pathway in trod u ction understanding how insects carry out complex sensorimotor tasks can help in the design of simple sensory and robotic systems often insect sensors have evolved into intricate filters matched to extract highly specific data from the environment which solves a particular problem directly with little or no need for further processing examples include head stabilisation in the fly which uses vision amongst other senses to estimate self rotation and thus to stabilise its head in flight and phonotaxis in the cricket because of the narrowness of the cricket body only a few millimetres the interaural time difference itd for sounds arriving at the two sides of the head is very small s even with the tympanal membranes eardrums located as they are on the forelegs of the cricket the itd only reaches about s which is too low to detect directly from timings of neural spikes because the wavelength of the cricket calling song is significantly greater than the width of the cricket body the interaural intensity difference iid is also very low in the absence of itd or iid information the cricket uses phase to determine direction this is possible because the male cricket produces an almost pure tone for its calling song school of electrical and information engineering institute of perception action and behaviour fifigure the cricket auditory system four acoustic inputs channel sounds directly or through tracheal tubes onto two tympanal membranes sound from contralateral inputs has to pass a double central membrane the medial septum inducing a phase delay and reduction in gain the sound transmission from the contralateral tympanum is very weak making each eardrum effectively a input system the physics of the cricket auditory system is well understood the system see figure uses a pair of sound receivers with four acoustic inputs two on the forelegs which are the external surfaces of the tympana and two on the body the prothoracic or acoustic spiracles the connecting tracheal tubes are such that interference occurs as sounds travel inside the cricket producing a directional response at the tympana to frequencies near to that of the calling song the amplitude of vibration of the tympana and hence the firing rate of the auditory afferent neurons attached to them vary as a sound source is moved around the cricket and the sounds from the different inputs move in and out of phase the outputs of the two tympana match when the sound is straight ahead and the inputs are bilaterally symmetric with respect to the sound source however when sound at the calling song frequency is off centre the phase of signals on the closer side comes better into alignment and the signal increases on that side and conversely decreases on the other it is that crossover of tympanal vibration amplitudes which allows the cricket to track a sound source see figure for example a simplified version of the auditory system using only two acoustic inputs was implemented in hardware and a simple neuron network was all that was required to then direct a robot to carry out phonotaxis towards a species specific calling song a simple simulator was also created to model the behaviour of the auditory system of figure at different frequencies data from michelsen et al figures and were digitised and used together with average and typical values from the paper to choose gains and delays for the simulation figure shows the model of the internal auditory system of the cricket from sound arriving at the acoustic inputs through to transmission down auditory receptor fibres the simulator implements this model up to the summing of the delayed inputs as well as modelling the external sound transmission results from the simulator were used to check the directionality of the system at different frequencies and to gain a better understanding of its response it was impractical to check the effect of leg movements or of complex sounds in the simulator due to the necessity of simulating the sound production and transmission an avlsi chip was designed to implement the same model both allowing more complex experiments such as leg movements to be run and experiments to be run in the real world fifigure a model of the auditory system of the cricket used to build the simulator and the avlsi implementation shown in boxes these experiments with the simulator and the circuits are being published in and the reader is referred to those papers for more details in the present paper we present the details of the circuits used for the avlsi implementation circuits the chip implementing the avlsi box in figure comprises two all pass delay filters three gain circuits a second order narrow band band pass filter a first order wide band band pass filter a first order high pass filter as well as supporting circuitry including reference voltages currents etc a single avlsi chip mosis tiny chip thus includes half the necessary circuitry to model the complete auditory system of a cricket the complete model of the auditory system can be obtained by using two appropriately connected chips only two all pass delay filters need to be implemented instead of three as suggested by figure because it is only the relative delay between the three pathways arriving at the one summing node that counts the delay circuits were implemented with fully differential gm c filters in order to extend the frequency range of the delay a first order all pass delay circuit was cascaded with a second order all pass delay circuit the resulting addition of the first order delay and the second order delay allowed for an approximately flat delay response for a wider bandwidth as the decreased delay around the corner frequency of the first order filter cancelled with the increased delay of the second order filter around its resonant frequency figure shows the firstand second order sections of the all pass delay circuit two of these ficircuits were used and based on data presented in were designed with delays of s and s by way of bias current manipulation the operational transconductance amplifier ota in figure is a standard ota which includes the common mode feedback necessary for fully differential designs the buffers figure are simple cascoded differential pairs v ii v ii v ii v ii v ii v ii figure the first order all pass delay circuit left and the second order all pass delay right the differential output of the delay circuits is converted into a current which is multiplied by a variable gain implemented as shown in figure the gain cell includes a differential pair with source degeneration via transistors n and n the source degeneration improves the linearity of the current the three gain cells implemented on the avlsi have default gains of and which are set by holding the default input high and appropriately ratioing the bias currents through the value of vbiasp to correct any on chip mismatches andor explore other gain configurations a current splitter cell p splitter figure allows the gain to be programmed by digital means post fabrication the current splitter takes an input current ibias figure and divides it into branches which recursively halve the current ie the first branch gives ibias the second branch ibias the third branch ibias and so on these currents can be used together with digitally controlled switches as a digital to analogue converter by holding default low and setting cc appropriately any gain from to can be set to save on output pins the program bits cc for each of the three gain cells are set via a single bit shift register in bit serial fashion summing the output of the three gain circuits in the current domain simply involves connecting three wires together therefore a natural option for the filters that follow is to use current domain filters in our case we have chosen to implement log domain filters using mos transistors operating in weak inversion figure shows the basic building blocks for the filters the tau cell and the multiplier cell and block diagrams showing how these blocks were connected to create the necessary filtering blocks the tau cell is a log domain filter which has the firstorder response i out i in s where nc avt ia and n the slope factor vt thermal voltage ca capacitance and ia bias current in figure the input currents to the tau cell imult and aia are only used fiwhen building a second order filter the multiplier cell is simply a translinear loop where i out i mult i out ai a or imult aiaioutiout the configurations of the tau cell to get particular responses are covered in along with the corresponding equations the high frequency filter of figure is implemented by the high pass filter in figure with a corner frequency of khz the low frequency filter however is divided into two parts since the biological filters response see for example figure a in separates well into a narrow second order band pass filter with a khz resonant frequency and a wide band pass filter made from a first order high pass filter with a khz corner frequency followed by a first order low pass filter with a khz corner frequency these filters are then added together to reproduce the biological filter the filters responses can be adjusted post fabrication via their bias currents this allows for compensation due to processing and matching errors figure the gain cell above is used to convert the differential voltage input from the delay cells into a single ended current output the gain of each cell is controllable via a programmable current cell psplitter an on chip bias generator was used to create all the necessary current biases on the chip all the main blocks delays gain cells and filters however can have their on chip bias currents overridden through external pins on the chip the chip was fabricated using the mosis ami m technology and designed using the cadence custom ic design tools methods the chip was tested using sound generated on a computer and played through a soundcard to the chip responses from the chip were recorded by an oscilloscope and uploaded back to the computer on completion given that the output from the fichip and the gain circuits is a current an external current sense circuit built with discrete components was used to enable the output to be probed by the oscilloscope figure the circuit diagrams for the log domain filter building blocks the tau cell and the multiplier along with the block diagrams for the three filters used in the avlsi model initial experiments were performed to tune the delays and gains after that recordings were taken of the directional frequency responses sounds were generated by computer for each chip input to simulate moving the forelegs by delaying the sound by the appropriate amount of time this was a much simpler solution than using microphones and moving them using motors results the avlsi chip was tested to measure its gains and delays which were successfully tuned to the appropriate values the chip was then compared with the simulation to check that it was faithfully modelling the system a result of this test at khz approximately the cricket calling song frequency is shown in figure apart from a drop in amplitude of the signal the response of the circuit was very similar to that of the simulator the differences were expected because the avlsi circuit has to deal with real world noise whereas the simulated version has perfect signals examples of the gain versus frequency response of the two log domain band pass filters are shown in figure note that the narrow band filter peaks at khz which is significantly above the mating song frequency of the cricket which is around khz this is not a mistake but is observed in real crickets as well as stated in the
we investigate under what conditions a neuron can learn by experimentally supported rules for spike timing dependent plasticity stdp to predict the arrival times of strong teacher inputs to the same neuron it turns out that in contrast to the famous perceptron convergence theorem which predicts convergence of the perceptron learning rule for a simplified neuron model whenever a stable solution exists no equally strong convergence guarantee can be given for spiking neurons with stdp but we derive a criterion on the statistical dependency structure of input spike trains which characterizes exactly when learning with stdp will converge on average for a simple model of a spiking neuron this criterion is reminiscent of the linear separability criterion of the perceptron convergence theorem but it applies here to the rows of a correlation matrix related to the spike inputs in addition we show through computer simulations for more realistic neuron models that the resulting analytically predicted positive learning results not only hold for the common interpretation of stdp where stdp changes the weights of synapses but also for a more realistic interpretation suggested by experimental data where stdp modulates the initial release probability of dynamic synapses
standard statistical models of language fail to capture one of the most striking properties of natural languages the power law distribution in the frequencies of word tokens we present a framework for developing statistical models that generically produce power laws augmenting standard generative models with an adaptor that produces the appropriate pattern of token frequencies we show that taking a particular stochastic process the pitman yor process as an adaptor justifies the appearance of type frequencies in formal analyses of natural language and improves the performance of a model for unsupervised learning of morphology
we present a model that learns the influence of interacting markov chains within a team the proposed model is a dynamic bayesian network dbn with a two level structure individual level and group level individual level models actions of each player and the group level models actions of the team as a whole experiments on synthetic multi player games and a multi party meeting corpus show the effectiveness of the proposed model
in this paper we derive an algorithm that computes the entire solution path of the support vector regression with essentially the same computational cost as fitting one svr model we also propose an unbiased estimate for the degrees of freedom of the svr model which allows convenient selection of the regularization parameter
we discuss a method for obtaining a subjects a priori beliefs from hisher behavior in a psychophysics context under the assumption that the behavior is nearly optimal from a bayesian perspective the method is nonparametric in the sense that we do not assume that the prior belongs to any fixed class of distributions eg gaussian despite this increased generality the method is relatively simple to implement being based in the simplest case on a linear programming algorithm and more generally on a straightforward maximum likelihood or maximum a posteriori formulation which turns out to be a convex optimization problem with no non global local maxima in many important cases in addition we develop methods for analyzing the uncertainty of these estimates we demonstrate the accuracy of the method in a simple simulated coin flipping setting in particular the method is able to precisely track the evolution of the subjects posterior distribution as more and more data are observed we close by briefly discussing an interesting connection to recent models of neural population coding
we present an efficient algorithm to actively select queries for learning the boundaries separating a function domain into regions where the function is above and below a given threshold we develop experiment selection methods based on entropy misclassification rate variance and their combinations and show how they perform on a number of data sets we then show how these algorithms are used to determine simultaneously valid confidence intervals for seven cosmological parameters experimentation shows that the algorithm reduces the computation necessary for the parameter estimation problem by an order of magnitude
while classical experiments on spike timing dependent plasticity analyzed synaptic changes as a function of the timing of pairs of preand postsynaptic spikes more recent experiments also point to the effect of spike triplets here we develop a mathematical framework that allows us to characterize timing based learning rules moreover we identify a candidate learning rule with five variables and free parameters that captures a variety of experimental data including the dependence of potentiation and depression upon preand postsynaptic firing frequencies the relation to the bienenstock cooper munro rule as well as to some timing based rules is discussed
the problem of resource allocation in sparse graphs with real variables is studied using methods of statistical physics an efficient distributed algorithm is devised on the basis of insight gained from the analysis and is examined using numerical simulations showing excellent performance and full agreement with the theoretical results

our understanding of the input output function of single cells has been substantially advanced by biophysically accurate multi compartmental models the large number of parameters needing hand tuning in these models has however somewhat hampered their applicability and interpretability here we propose a simple and well founded method for automatic estimation of many of these key parameters the spatial distribution of channel densities on the cells membrane the spatiotemporal pattern of synaptic input the channels reversal potentials the intercompartmental conductances and the noise level in each compartment we assume experimental access to a the spatiotemporal voltage signal in the dendrite or some contiguous subpart thereof eg via voltage sensitive imaging techniques b an approximate kinetic description of the channels and synapses present in each compartment and c the morphology of the part of the neuron under investigation the key observation is that given data a c all of the parameters may be simultaneously inferred by a version of constrained linear regression this regression in turn is efficiently solved using standard algorithms without any local minima problems despite the large number of parameters and complex dynamics the noise level may also be estimated by standard techniques we demonstrate the methods accuracy on several model datasets and describe techniques for quantifying the uncertainty in our estimates
the network topology of neurons in the brain exhibits an abundance of feedback connections but the computational function of these feedback connections is largely unknown we present a computational theory that characterizes the gain in computational power achieved through feedback in dynamical systems with fading memory it implies that many such systems acquire through feedback universal computational capabilities for analog computing with a non fading memory in particular we show that feedback enables such systems to process time varying input streams in diverse ways according to rules that are implemented through internal states of the dynamical system in contrast to previous attractor based computational models for neural networks these flexible internal states are high dimensional attractors of the circuit dynamics that still allow the circuit state to absorb new information from online input streams in this way one arrives at novel models for working memory integration of evidence and reward expectation in cortical circuits we show that they are applicable to circuits of conductance based hodgkin huxley hh neurons with high levels of noise that reflect experimental data on invivo conditions
we argue that when objects are characterized by many attributes clustering them on the basis of a relatively small random subset of these attributes can capture information on the unobserved attributes as well moreover we show that under mild technical conditions clustering the objects on the basis of such a random subset performs almost as well as clustering with the full attribute set we prove a finite sample generalization theorems for this novel learning scheme that extends analogous results from the supervised learning setting the scheme is demonstrated for collaborative filtering of users with movies rating as attributes
long distance language modeling is important not only in speech recognition and machine translation but also in high dimensional discrete sequence modeling in general however the problem of context length has almost been neglected so far and a nave bag of words history has been i employed in natural language processing in contrast in this paper we view topic shifts within a text as a latent stochastic process to give an explicit probabilistic generative model that has partial exchangeability we propose an online inference algorithm using particle filters to recognize topic shifts to employ the most appropriate length of context automatically experiments on the bnc corpus showed consistent improvement over previous methods involving no chronological order
motor imagery attenuates eeg and rhythms over sensorimotor cortices these amplitude changes are most successfully captured by the method of common spatial patterns csp and widely used in braincomputer interfaces bci bci methods based on amplitude information however have not incoporated the rich phase dynamics in the eeg rhythm this study reports on a bci method based on phase synchrony rate sr sr computed from binarized phase locking value describes the number of discrete synchronization events within a window statistical nonparametric tests show that srs contain significant differences between types of motor imageries classifiers trained on srs consistently demonstrate satisfactory results for all subjects it is further observed that for subjects phase is more discriminative than amplitude in the first s which suggests that phase has the potential to boost the information transfer rate in bcis
theories of visual attention commonly posit that early parallel processes extract conspicuous features such as color contrast and motion from the visual field these features are then combined into a saliency map and attention is directed to the most salient regions first top down attentional control is achieved by modulating the contribution of different feature types to the saliency map a key source of data concerning attentional control comes from behavioral studies in which the effect of recent experience is examined as individuals repeatedly perform a perceptual discrimination task eg what shape is the odd colored object the robust finding is that repetition of features of recent trials eg target color facilitates performance we view this facilitation as an adaptation to the statistical structure of the environment we propose a probabilistic model of the environment that is updated after each trial under the assumption that attentional control operates so as to make performance more efficient for more likely environmental states we obtain parsimonious explanations for data from four different experiments further our model provides a rational explanation for why the influence of past experience on attentional control is short lived
we present a series of theoretical arguments supporting the claim that a large class of modern learning algorithms that rely solely on the smoothness prior with similarity between examples expressed with a local kernel are sensitive to the curse of dimensionality or more precisely to the variability of the target our discussion covers supervised semisupervised and unsupervised learning algorithms these algorithms are found to be local in the sense that crucial properties of the learned function at x depend mostly on the neighbors of x in the training set this makes them sensitive to the curse of dimensionality well studied for classical non parametric statistical learning we show in the case of the gaussian kernel that when the function to be learned has many variations these algorithms require a number of training examples proportional to the number of variations which could be large even though there may exist short descriptions of the target function ie their kolmogorov complexity may be low this suggests that there exist non local learning algorithms that at least have the potential to learn about such structured but apparently complex functions because locally they have many variations while not using very specific prior domain knowledge
we initiate the study of learning from multiple sources of limited data each of which may be corrupted at a different rate we develop a complete theory of which data sources should be used for two fundamental problems estimating the bias of a coin and learning a classifier in the presence of label noise in both cases efficient algorithms are provided for computing the optimal subset of data
we design a new learning algorithm for the set covering machine from a pac bayes perspective and propose a pac bayes risk bound which is minimized for classifiers achieving a non trivial margin sparsity trade off
in this paper we consider the problem of finding sets of points that conform to a given underlying model from within a dense noisy set of observations this problem is motivated by the task of efficiently linking faint asteroid detections but is applicable to a range of spatial queries we survey current tree based approaches showing a trade off exists between single tree and multiple tree algorithms to this end we present a new type of multiple tree algorithm that uses a variable number of trees to exploit the advantages of both approaches we empirically show that this algorithm performs well using both simulated and astronomical data
we present a method for performing transductive inference on very large datasets our algorithm is based on multiclass gaussian processes and is effective whenever the multiplication of the kernel matrix or its inverse with a vector can be computed sufficiently fast this holds for instance for certain graph and string kernels transduction is achieved by variational inference over the unlabeled data subject to a balancing constraint
under natural viewing conditions small movements of the eye and body prevent the maintenance of a steady direction of gaze it is known that stimuli tend to fade when they are stabilized on the retina for several seconds however it is unclear whether the physiological self motion of the retinal image serves a visual purpose during the brief periods of natural visual fixation this study examines the impact of fixational instability on the statistics of visual input to the retina and on the structure of neural activity in the early visual system fixational instability introduces fluctuations in the retinal input signals that in the presence of natural images lack spatial correlations these input fluctuations strongly influence neural activity in a model of the lgn they decorrelate cell responses even if the contrast sensitivity functions of simulated cells are not perfectly tuned to counter balance the power law spectrum of natural images a decorrelation of neural activity has been proposed to be beneficial for discarding statistical redundancies in the input signals fixational instability might therefore contribute to establishing efficient representations of natural stimuli
in this paper we propose a new basis selection criterion for building sparse gp regression models that provides promising gains in accuracy as well as efficiency over previous methods our algorithm is much faster than that of smola and bartlett while in generalization it greatly outperforms the information gain approach proposed by seeger et al especially on the quality of predictive distributions
we present a conditional temporal probabilistic framework for reconstructing d human motion in monocular video based on descriptors encoding image silhouette observations for computational efficiency we restrict visual inference to low dimensional kernel induced non linear state spaces our methodology kbme combines kernel pca based non linear dimensionality reduction kpca and conditional bayesian mixture of experts bme in order to learn complex multivalued predictors between observations and model hidden states this is necessary for accurate inverse visual perception inferences where several probable distant d solutions exist due to noise or the uncertainty of monocular perspective projection low dimensional models are appropriate because many visual processes exhibit strong non linear correlations in both the image observations and the target hidden state variables the learned predictors are temporally combined within a conditional graphical model in order to allow a principled propagation of uncertainty we study several predictors and empirically show that the proposed algorithm positively compares with techniques based on regression kernel dependency estimation kde or pca alone and gives results competitive to those of high dimensional mixture predictors at a fraction of their computational cost we show that the method successfully reconstructs the complex d motion of humans in real monocular video sequences
we present a new kernel method for extracting semantic relations between entities in natural language text based on a generalization of subsequence kernels this kernel uses three types of subsequence patterns that are typically employed in natural language to assert relationships between two entities experiments on extracting protein interactions from biomedical corpora and top level relations from newspaper corpora demonstrate the advantages of this approach
a dynamic texture is a video model that treats a video as a sample from a spatio temporal stochastic process specifically a linear dynamical system one problem associated with the dynamic texture is that it cannot model video where there are multiple regions of distinct motion in this work we introduce the layered dynamic texture model which addresses this problem we also introduce a variant of the model and present the em algorithm for learning each of the models finally we demonstrate the efficacy of the proposed model for the tasks of segmentation and synthesis of video
we propose a fast manifold learning algorithm based on the methodology of domain decomposition starting with the set of sample points partitioned into two subdomains we develop the solution of the interface problem that can glue the embeddings on the two subdomains into an embedding on the whole domain we provide a detailed analysis to assess the errors produced by the gluing process using matrix perturbation theory numerical examples are given to illustrate the efficiency and effectiveness of the proposed methods

experimental data indicate that norepinephrine is critically involved in aspects of vigilance and attention previously we considered the function of this neuromodulatory system on a time scale of minutes and longer and suggested that it signals global uncertainty arising from gross changes in environmental contingencies however norepinephrine is also known to be activated phasically by familiar stimuli in welllearned tasks here we extend our uncertainty based treatment of norepinephrine to this phasic mode proposing that it is involved in the detection and reaction to state uncertainty within a task this role of norepinephrine can be understood through the metaphor of neural interrupts
although non parametric tests have already been proposed for that purpose statistical significance tests for non standard measures different from the classification error are less often used in the literature this paper is an attempt at empirically verifying how these tests compare with more classical tests on various conditions more precisely using a very large dataset to estimate the whole population we analyzed the behavior of several statistical test varying the class unbalance the compared models the performance measure and the sample size the main result is that providing big enough evaluation sets non parametric tests are relatively reliable in all conditions
we characterize the sample complexity of active learning problems in terms of a parameter which takes into account the distribution over the input space the specific target hypothesis and the desired accuracy
this paper presents a non asymptotic statistical analysis of kernel pca with a focus different from the one proposed in previous work on this topic here instead of considering the reconstruction error of kpca we are interested in approximation error bounds for the eigenspaces themselves we prove an upper bound depending on the spacing between eigenvalues but not on the dimensionality of the eigenspace as a consequence this allows to infer stability results for these estimated spaces
we describe a hierarchy of motif based kernels for multiple alignments of biological sequences particularly suitable to process regulatory regions of genes the kernels incorporate progressively more information with the most complex kernel accounting for a multiple alignment of orthologous regions the phylogenetic tree relating the species and the prior knowledge that relevant sequence patterns occur in conserved motif blocks these kernels can be used in the presence of a library of known transcription factor binding sites or de novo by iterating over all k mers of a given length in the latter mode a discriminative classifier built from such a kernel not only recognizes a given class of promoter regions but as a side effect simultaneously identifies a collection of relevant discriminative sequence motifs we demonstrate the utility of the motif based multiple alignment kernels by using a collection of aligned promoter regions from five yeast species to recognize classes of cell cycle regulated genes supplementary data is available at httpnoblegswashingtoneduprojpkernel
neurons can have rapidly changing spike train statistics dictated by the underlying network excitability or behavioural state of an animal to estimate the time course of such state dynamics from singleor multiple neuron recordings we have developed an algorithm that maximizes the likelihood of observed spike trains by optimizing the state lifetimes and the state conditional interspike interval isi distributions our nonparametric algorithm is free of time binning and spike counting problems and has the computational complexity of a mixed state markov model operating on a state sequence of length equal to the total number of recorded spikes as an example we fit a two state model to paired recordings of premotor neurons in the sleeping songbird we find that the two state conditional isi functions are highly similar to the ones measured during waking and singing respectively
we investigate the problem of automatically constructing efficient representations or basis functions for approximating value functions based on analyzing the structure and topology of the state space in particular two novel approaches to value function approximation are explored based on automatically constructing basis functions on state spaces that can be represented as graphs or manifolds one approach uses the eigenfunctions of the laplacian in effect performing a global fourier analysis on the graph the second approach is based on diffusion wavelets which generalize classical wavelets to graphs using multiscale dilations induced by powers of a diffusion operator or random walk on the graph together these approaches form the foundation of a new generation of methods for solving large markov decision processes in which the underlying representation and policies are simultaneously learned

we consider a framework for semi supervised learning using spectral decomposition based un supervised kernel design this approach subsumes a class of previously proposed semi supervised learning methods on data graphs we examine various theoretical properties of such methods in particular we derive a generalization performance bound and obtain the optimal kernel design by minimizing the bound based on the theoretical analysis we are able to demonstrate why spectral kernel design based methods can often improve the predictive performance experiments are used to illustrate the main consequences of our analysis

we propose a new bayesian method for spatial cluster detection the bayesian spatial scan statistic and compare this method to the standard frequentist scan statistic approach we demonstrate that the bayesian statistic has several advantages over the frequentist approach including increased power to detect clusters and since randomization testing is unnecessary much faster runtime we evaluate the bayesian and frequentist methods on the task of prospective disease surveillance detecting spatial clusters of disease cases resulting from emerging disease outbreaks we demonstrate that our bayesian methods are successful in rapidly detecting outbreaks while keeping number of false positives low
given a probability measure p and a reference measure one is often interested in the minimum measure set with p measure at least minimum volume sets of this type summarize the regions of greatest probability mass of p and are useful for detecting anomalies and constructing confidence regions this paper addresses the problem of estimating minimum volume sets based on independent samples distributed according to p other than these samples no other information is available regarding p but the reference measure is assumed to be known we introduce rules for estimating minimum volume sets that parallel the empirical risk minimization and structural risk minimization principles in classification as in classification we show that the performances of our estimators are controlled by the rate of uniform convergence of empirical to true probabilities over the class from which the estimator is drawn thus we obtain finite sample size performance bounds in terms of vc dimension and related quantities we also demonstrate strong universal consistency and an oracle inequality estimators based on histograms and dyadic partitions illustrate the proposed rules
recurrent networks that perform a winner take all computation have been studied extensively although some of these studies include spiking networks they consider only analog input rates we present results of this winner take all computation on a network of integrate and fire neurons which receives spike trains as inputs we show how we can configure the connectivity in the network so that the winner is selected after a pre determined number of input spikes we discuss spiking inputs with both regular frequencies and poisson distributed rates the robustness of the computation was tested by implementing the winner take all network on an analog vlsi array of integrate and fire neurons which have an innate variance in their operating parameters
consider the problem of joint parameter estimation and prediction in a markov random field ie the model parameters are estimated on the basis of an initial set of data and then the fitted model is used to perform prediction eg smoothing denoising interpolation on a new noisy observation working in the computation limited setting we analyze a joint method in which the same convex variational relaxation is used to construct an m estimator for fitting parameters and to perform approximate marginalization for the prediction step the key result of this paper is that in the computation limited setting using an inconsistent parameter estimator ie an estimator that returns the wrong model even in the infinite data limit is provably beneficial since the resulting errors can partially compensate for errors made by using an approximate prediction technique en route to this result we analyze the asymptotic properties of m estimators based on convex variational relaxations and establish a lipschitz stability property that holds for a broad class of variational methods we show that joint estimationprediction based on the reweighted sum product algorithm substantially outperforms a commonly used heuristic based on ordinary sum product keywords markov random fields variational method message passing algorithms sum product belief propagation parameter estimation learning
reinforcement learning models have long promised to unify computational psychological and neural accounts of appetitively conditioned behavior however the bulk of data on animal conditioning comes from free operant experiments measuring how fast animals will work for reinforcement existing reinforcement learning rl models are silent about these tasks because they lack any notion of vigor they thus fail to address the simple observation that hungrier animals will work harder for food as well as stranger facts such as their sometimes greater productivity even when working for irrelevant outcomes such as water here we develop an rl framework for free operant behavior suggesting that subjects choose how vigorously to perform selected actions by optimally balancing the costs and benefits of quick responding motivational states such as hunger shift these factors skewing the tradeoff this accounts normatively for the effects of motivation on response rates as well as many other classic findings finally we suggest that tonic levels of dopamine may be involved in the computation linking motivational state to optimal responding thereby explaining the complex vigor related effects of pharmacological manipulation of dopamine

when trying to understand the brain it is of fundamental importance to analyse eg from eegmeg measurements what parts of the cortex interact with each other in order to infer more accurate models of brain activity common techniques like blind source separation bss can estimate brain sources and single out artifacts by using the underlying assumption of source signal independence however physiologically interesting brain sources typically interact so bss will by construction fail to characterize them properly noting that there are truly interacting sources and signals that only seemingly interact due to effects of volume conduction this work aims to contribute by distinguishing these effects for this a new bss technique is proposed that uses anti symmetrized cross correlation matrices and subsequent diagonalization the resulting decomposition consists of the truly interacting brain sources and suppresses any spurious interaction stemming from volume conduction our new concept of interacting source analysis isa is successfully demonstrated on meg data
the two thirds power law an empirical law stating an inverse non linear relationship between the tangential hand speed and the curvature of its trajectory during curved motion is widely acknowledged to be an invariant of upper limb movement it has also been shown to exist in eyemotion locomotion and was even demonstrated in motion perception and prediction this ubiquity has fostered various attempts to uncover the origins of this empirical relationship in these it was generally attributed either to smoothness in handor joint space or to the result of mechanisms that damp noise inherent in the motor system to produce the smooth trajectories evident in healthy human motion we show here that white gaussian noise also obeys this power law analysis of signal and noise combinations shows that trajectories that were synthetically created not to comply with the power law are transformed to power law compliant ones after combination with low levels of noise furthermore there exist colored noise types that drive non power law trajectories to power law compliance and are not affected by smoothing these results suggest caution when running experiments aimed at verifying the power law or assuming its underlying existence without proper analysis of the noise our results could also suggest that the power law might be derived not from smoothness or smoothness inducing mechanisms operating on the noise inherent in our motor system but rather from the correlated noise which is inherent in this motor system
to escape from the curse of dimensionality we claim that one can learn non local functions in the sense that the value and shape of the learned function at x must be inferred using examples that may be far from x with this objective we present a non local non parametric density estimator it builds upon previously proposed gaussian mixture models with regularized covariance matrices to take into account the local shape of the manifold it also builds upon recent work on non local estimators of the tangent plane of a manifold which are able to generalize in places with little training data unlike traditional local non parametric models
we present a new gaussian process gp regression model whose covariance is parameterized by the the locations of m pseudo input points which we learn by a gradient based optimization we take m n where n is the number of real data points and hence obtain a sparse regression method which has om n training cost and om prediction cost per test case we also find hyperparameters of the covariance function in the same joint optimization the method can be viewed as a bayesian regression model with particular input dependent noise the method turns out to be closely related to several other sparse gp approaches and we discuss the relation in detail we finally demonstrate its performance on some large data sets and make a direct comparison to other sparse gp methods we show that our method can match full gp performance with small m ie very sparse solutions and it significantly outperforms other approaches in this regime
we describe a hierarchical compositional system for detecting deformable objects in images objects are represented by graphical models the algorithm uses a hierarchical tree where the root of the tree corresponds to the full object and lower level elements of the tree correspond to simpler features the algorithm proceeds by passing simple messages up and down the tree the method works rapidly in under a second on images we demonstrate the approach on detecting cats horses and hands the method works in the presence of background clutter and occlusions our approach is contrasted with more traditional methods such as dynamic programming and belief propagation
a standard method to obtain stochastic models for symbolic time series is to train state emitting hidden markov models se hmms with the baum welch algorithm based on observable operator models ooms in the last few months a number of novel learning algorithms for similar purposes have been developed two versions of an efficiency sharpening es algorithm which iteratively improves the statistical efficiency of a sequence of oom estimators a constrained gradient descent ml estimator for transition emitting hmms te hmms we give an overview on these algorithms and compare them with se hmmem learning on synthetic and real life data
this paper introduces gaussian process dynamical models gpdm for nonlinear time series analysis a gpdm comprises a low dimensional latent space with associated dynamics and a map from the latent space to an observation space we marginalize out the model parameters in closed form using gaussian process gp priors for both the dynamics and the observation mappings this results in a nonparametric model for dynamical systems that accounts for uncertainty in the model we demonstrate the approach on human motion capture data in which each pose is dimensional despite the use of small data sets the gpdm learns an effective representation of the nonlinear dynamics in these spaces webpage httpwwwdgptorontoedu jmwanggpdm
there have been many graph based approaches for semi supervised classification one problem is that of hyperparameter learning performance depends greatly on the hyperparameters of the similarity graph transformation of the graph laplacian and the noise model we present a bayesian framework for learning hyperparameters for graph based semisupervised classification given some labeled data which can contain inaccurate labels we pose the semi supervised classification as an inference problem over the unknown labels expectation propagation is used for approximate inference and the mean of the posterior is used for classification the hyperparameters are learned using em for evidence maximization we also show that the posterior mean can be written in terms of the kernel matrix providing a bayesian classifier to classify new points tests on synthetic and real datasets show cases where there are significant improvements in performance over the existing approaches
online learning algorithms are typically fast memory efficient and simple to implement however many common learning problems fit more naturally in the batch learning setting the power of online learning algorithms can be exploited in batch settings by using online to batch conversions techniques which build a new batch algorithm from an existing online algorithm we first give a unified overview of three existing online to batch conversion techniques which do not use training data in the conversion process we then build upon these data independent conversions to derive and analyze data driven conversions our conversions find hypotheses with a small risk by explicitly minimizing datadependent generalization bounds we experimentally demonstrate the usefulness of our approach and in particular show that the data driven conversions consistently outperform the data independent conversions
we present a novel spectral clustering method that enables users to incorporate prior knowledge of the size of clusters into the clustering process the cost function which is named size regularized cut srcut is defined as the sum of the inter cluster similarity and a regularization term measuring the relative size of two clusters finding a partition of the data set to minimize srcut is proved to be np complete an approximation algorithm is proposed to solve a relaxed version of the optimization problem as an eigenvalue problem evaluations over different data sets demonstrate that the method is not sensitive to outliers and performs better than normalized cut
we derive a bayesian ideal observer bio for detecting motion and solving the correspondence problem we obtain barlow and tripathys classic model as an approximation our psychophysical experiments show that the trends of human performance are similar to the bayesian ideal but overall human performance is far worse we investigate ways to degrade the bayesian ideal but show that even extreme degradations do not approach human performance instead we propose that humans perform motion tasks using generic general purpose models of motion we perform more psychophysical experiments which are consistent with humans using a slow and smooth model and which rule out an alternative model using slowness
we propose a simple information theoretic approach to soft clustering based on maximizing the mutual information ix y between the unknown cluster labels y and the training patterns x with respect to parameters of specifically constrained encoding distributions the constraints are chosen such that patterns are likely to be clustered similarly if they lie close to specific unknown vectors in the feature space the method may be conveniently applied to learning the optimal affinity matrix which corresponds to learning parameters of the kernelized encoder the procedure does not require computations of eigenvalues of the gram matrices which makes it potentially attractive for clustering large data sets

we show that linear generalizations of rescorla wagner can perform maximum likelihood estimation of the parameters of all generative models for causal reasoning our approach involves augmenting variables to deal with conjunctions of causes similar to the agumented model of rescorla our results involve genericity assumptions on the distributions of causes if these assumptions are violated for example for the cheng causal power theory then we show that a linear rescorla wagner can estimate the parameters of the model up to a nonlinear transformtion moreover a nonlinear rescorla wagner is able to estimate the parameters directly to within arbitrary accuracy previous results can be used to determine convergence and to estimate convergence rates
it is well known that everything that is learnable in the difficult online setting where an arbitrary sequences of examples must be labeled one at a time is also learnable in the batch setting where examples are drawn independently from a distribution we show a result in the opposite direction we give an efficient conversion algorithm from batch to online that is transductive it uses future unlabeled data this demonstrates the equivalence between what is properly and efficiently learnable in a batch model and a transductive online model

convexity has recently received a lot of attention in the machine learning community and the lack of convexity has been seen as a major disadvantage of many learning algorithms such as multi layer artificial neural networks we show that training multi layer neural networks in which the number of hidden units is learned can be viewed as a convex optimization problem this problem involves an infinite number of variables but can be solved by incrementally inserting a hidden unit at a time each time finding a linear classifier that minimizes a weighted sum of errors
this paper explores the statistical relationship between natural images and their underlying range depth images we look at how this relationship changes over scale and how this information can be used to enhance low resolution range data using a full resolution intensity image based on our findings we propose an extension to an existing technique known as shape recipes and the success of the two methods are compared using images and laser scans of real scenes our extension is shown to provide a two fold improvement over the current method furthermore we demonstrate that ideal linear shape from shading filters when learned from natural scenes may derive even more strength from shadow cues than from the traditional linear lambertian shading cues
we present an improvement to the dp slam algorithm for simultaneous localization and mapping slam that maintains multiple hypotheses about densely populated maps one full map per particle in a particle filter in time that is linear in all significant algorithm parameters and takes constant amortized time per iteration this means that the asymptotic complexity of the algorithm is no greater than that of a pure localization algorithm using a single map and the same number of particles we also present a hierarchical extension of dp slam that uses a two level particle filter which models drift in the particle filtering process itself the hierarchical approach enables recovery from the inevitable drift that results from using a finite number of particles in a particle filter and permits the use of dp slam in more challenging domains while maintaining linear time asymptotic complexity
we determine the asymptotic limit of the function computed by support vector machines svm and related algorithms that minimize a regularized empirical convex loss function in the reproducing kernel hilbert space of the gaussian rbf kernel in the situation where the number of examples tends to infinity the bandwidth of the gaussian kernel tends to and the regularization parameter is held fixed non asymptotic convergence bounds to this limit in the l sense are provided together with upper bounds on the classification error that is shown to converge to the bayes risk therefore proving the bayes consistency of a variety of methods although the regularization term does not vanish these results are particularly relevant to the one class svm for which the regularization can not vanish by construction and which is shown for the first time to be a consistent density level set estimator
a good image object detection algorithm is accurate fast and does not require exact locations of objects in a training set we can create such an object detector by taking the architecture of the viola jones detector cascade and training it with a new variant of boosting that we call milboost milboost uses cost functions from the multiple instance learning literature combined with the anyboost framework we adapt the feature selection criterion of milboost to optimize the performance of the viola jones cascade experiments show that the detection rate is up to times better using milboost this increased detection rate shows the advantage of simultaneously learning the locations and scales of the objects in the training set along with the parameters of the classifier
probabilistic modeling of correlated neural population firing activity is central to understanding the neural code and building practical decoding algorithms no parametric models currently exist for modeling multivariate correlated neural data and the high dimensional nature of the data makes fully non parametric methods impractical to address these problems we propose an energy based model in which the joint probability of neural activity is represented using learned functions of the d marginal histograms of the data the parameters of the model are learned using contrastive divergence and an optimization procedure for finding appropriate marginal directions we evaluate the method using real data recorded from a population of motor cortical neurons in particular we model the joint probability of population spiking times and d hand position and show that the likelihood of test data under our model is significantly higher than under other models these results suggest that our model captures correlations in the firing activity our rich probabilistic model of neural population activity is a step towards both measurement of the importance of correlations in neural coding and improved decoding of population activity
the observed physiological dynamics of an infant receiving intensive care are affected by many possible factors including interventions to the baby the operation of the monitoring equipment and the state of health the factorial switching kalman filter can be used to infer the presence of such factors from a sequence of observations and to estimate the true values where these observations have been corrupted we apply this model to clinical time series data and show it to be effective in identifying a number of artifactual and physiological patterns

hybrid cmol integrated circuits combining cmos subsystem with nanowire crossbars and simple two terminal nanodevices promise to extend the exponential moore law development of microelectronics into the sub nm range we are developing neuromorphic network crossnet architectures for this future technology in which neural cell bodies are implemented in cmos nanowires are used as axons and dendrites while nanodevices bistable latching switches are used as elementary synapses we have shown how crossnets may be trained to perform pattern recovery and classification despite the limitations imposed by the cmol hardware preliminary estimates have shown that cmol crossnets may be extremely dense cells per cm and operate approximately a million times faster than biological neural networks at manageable power consumption in conclusion we discuss in brief possible short term and long term applications of the emerging technology
a general analysis of the limiting distribution of neural network functions is performed with emphasis on non gaussian limits we show that with iid symmetric stable output weights and more generally with weights distributed from the normal domain of attraction of a stable variable that the neural functions converge in distribution to stable processes conditions are also investigated under which gaussian limits do occur when the weights are independent but not identically distributed some particularly tractable classes of stable distributions are examined and the possibility of learning with such processes
compressed sensing is an emerging field based on the revelation that a small group of linear projections of a sparse signal contains enough information for reconstruction in this paper we introduce a new theory for distributed compressed sensing dcs that enables new distributed coding algorithms for multi signal ensembles that exploit both intraand inter signal correlation structures the dcs theory rests on a new concept that we term the joint sparsity of a signal ensemble we study three simple models for jointly sparse signals propose algorithms for joint recovery of multiple signals from incoherent projections and characterize theoretically and empirically the number of measurements per sensor required for accurate reconstruction in some sense dcs is a framework for distributed compression of sources with memory which has remained a challenging problem in information theory for some time dcs is immediately applicable to a range of problems in sensor networks and arrays
this paper presents a rigorous statistical analysis characterizing regimes in which active learning significantly outperforms classical passive learning active learning algorithms are able to make queries or select sample locations in an online fashion depending on the results of the previous queries in some regimes this extra flexibility leads to significantly faster rates of error decay than those possible in classical passive learning settings the nature of these regimes is explored by studying fundamental performance limits of active and passive learning in two illustrative nonparametric function classes in addition to examining the theoretical potential of active learning this paper describes a practical algorithm capable of exploiting the extra flexibility of the active setting and provably improving upon the classical passive techniques our active learning theory and methods show promise in a number of applications including field estimation using wireless sensor networks and fault line detection
predictive state representations psrs are a method of modeling dynamical systems using only observable data such as actions and observations to describe their model psrs use predictions about the outcome of future tests to summarize the system state the best existing techniques for discovery and learning of psrs use a monte carlo approach to explicitly estimate these outcome probabilities in this paper we present a new algorithm for discovery and learning of psrs that uses a gradient descent approach to compute the predictions for the current state the algorithm takes advantage of the large amount of structure inherent in a valid prediction matrix to constrain its predictions furthermore the algorithm can be used online by an agent to constantly improve its prediction quality something that current state of the art discovery and learning algorithms are unable to do we give empirical results to show that our constrained gradient algorithm is able to discover core tests using very small amounts of data and with larger amounts of data can compute accurate predictions of the system dynamics
we introduce a method to automatically improve character models for a handwritten script without the use of transcriptions and using a minimum of document specific training data we show that we can use searches for the words in a dictionary to identify portions of the document whose transcriptions are unambiguous using templates extracted from those regions we retrain our character prediction model to drastically improve our search retrieval performance for words in the document
we develop an approach for estimation with gaussian markov processes that imposes a smoothness prior while allowing for discontinuities instead of propagating information laterally between neighboring nodes in a graph we study the posterior distribution of the hidden nodes as a whole how it is perturbed by invoking discontinuities or weakening the edges in the graph we show that the resulting computation amounts to feed forward fan in operations reminiscent of v neurons moreover using suitable matrix preconditioners the incurred matrix inverse and determinant can be approximated without iteration in the same computational style simulation results illustrate the merits of this approach
linear implementations of the efficient coding hypothesis such as independent component analysis ica and sparse coding models have provided functional explanations for properties of simple cells in v these models however ignore the non linear behavior of neurons and fail to match individual and population properties of neural receptive fields in subtle but important ways hierarchical models including gaussian scale mixtures and other generative statistical models can capture higher order regularities in natural images and explain nonlinear aspects of neural processing such as normalization and context effects previously it had been assumed that the lower level representation is independent of the hierarchy and had been fixed when training these models here we examine the optimal lower level representations derived in the context of a hierarchical model and find that the resulting representations are strikingly different from those based on linear models unlike the the basis functions and filters learned by ica or sparse coding these functions individually more closely resemble simple cell receptive fields and collectively span a broad range of spatial scales our work unifies several related approaches and observations about natural image structure and suggests that hierarchical models might yield better representations of image structure throughout the hierarchy
we extend radial basis function rbf networks to the scenario in which multiple correlated tasks are learned simultaneously and present the corresponding learning algorithms we develop the algorithms for learning the network structure in either a supervised or unsupervised manner training data may also be actively selected to improve the networks generalization to test data experimental results based on real data demonstrate the advantage of the proposed algorithms and support our conclusions
we propose a simple clustering framework on graphs encoding pairwise data similarities unlike usual similarity based methods the approach softly assigns data to clusters in a probabilistic way more importantly a hierarchical clustering is naturally derived in this framework to gradually merge lower level clusters into higher level ones a random walk analysis indicates that the algorithm exposes clustering structures in various resolutions ie a higher level statistically models a longer term diffusion on graphs and thus discovers a more global clustering structure finally we provide very encouraging experimental results
in this paper we propose a general framework to study the generalization properties of binary classifiers trained with data which may be dependent but are deterministically generated upon a sample of independent examples it provides generalization bounds for binary classification and some cases of ranking problems and clarifies the relationship between these learning tasks
there has been a surge of interest in learning non linear manifold models to approximate high dimensional data both for computational complexity reasons and for generalization capability sparsity is a desired feature in such models this usually means dimensionality reduction which naturally implies estimating the intrinsic dimension but it can also mean selecting a subset of the data to use as landmarks which is especially important because many existing algorithms have quadratic complexity in the number of observations this paper presents an algorithm for selecting landmarks based on lasso regression which is well known to favor sparse approximations because it uses regularization with an l norm as an added benefit a continuous manifold parameterization based on the landmarks is also found experimental results with synthetic and real data illustrate the algorithm
we present a computational model of human eye movements in an object class detection task the model combines state of the art computer vision object class detection methods sift features trained using adaboost with a biologically plausible model of human eye movement to produce a sequence of simulated fixations culminating with the acquisition of a target we validated the model by comparing its behavior to the behavior of human observers performing the identical object class detection task looking for a teddy bear among visually complex nontarget objects we found considerable agreement between the model and human data in multiple eye movement measures including number of fixations cumulative probability of fixating the target and scanpath distance
sparse pca seeks approximate sparse eigenvectors whose projections capture the maximal variance of data as a cardinality constrained and non convex optimization problem it is np hard and is encountered in a wide range of applied fields from bio informatics to finance recent progress has focused mainly on continuous approximation and convex relaxation of the hard cardinality constraint in contrast we consider an alternative discrete spectral formulation based on variational eigenvalue bounds and provide an effective greedy strategy as well as provably optimal solutions using branch and bound search moreover the exact methodology used reveals a simple renormalization step that improves approximate solutions obtained by any continuous method the resulting performance gain of discrete algorithms is demonstrated on real world benchmark data and in extensive monte carlo evaluation trials
lasso regression tends to assign zero weights to most irrelevant or redundant features and hence is a promising technique for feature selection its limitation however is that it only offers solutions to linear models kernel machines with feature scaling techniques have been studied for feature selection with non linear models however such approaches require to solve hard non convex optimization problems this paper proposes a new approach named the feature vector machine fvm it reformulates the standard lasso regression into a form isomorphic to svm and this form can be easily extended for feature selection with non linear models by introducing kernels defined on feature vectors fvm generates sparse solutions in the nonlinear feature space and it is much more tractable compared to feature scaling kernel machines our experiments with fvm on simulated data show encouraging results in identifying the small number of dominating features that are non linearly correlated to the response a task the standard lasso fails to complete
there is experimental evidence that cortical neurons show avalanche activity with the intensity of firing events being distributed as a power law we present a biologically plausible extension of a neural network which exhibits a power law avalanche distribution for a wide range of connectivity parameters
we address the problem of robust computationally efficient design of biological experiments classical optimal experiment design methods have not been widely adopted in biological practice in part because the resulting designs can be very brittle if the nominal parameter estimates for the model are poor and in part because of computational constraints we present a method for robust experiment design based on a semidefinite programming relaxation we present an application of this method to the design of experiments for a complex calcium signal transduction pathway where we have found that the parameter estimates obtained from the robust design are better than those obtained from an optimal design
an increasing number of projects in neuroscience requires the statistical analysis of high dimensional data sets as for instance in predicting behavior from neural firing or in operating artificial devices from brain recordings in brain machine interfaces linear analysis techniques remain prevalent in such cases but classical linear regression approaches are often numerically too fragile in high dimensions in this paper we address the question of whether emg data collected from arm movements of monkeys can be faithfully reconstructed with linear approaches from neural activity in primary motor cortex m to achieve robust data analysis we develop a full bayesian approach to linear regression that automatically detects and excludes irrelevant features in the data regularizing against overfitting in comparison with ordinary least squares stepwise regression partial least squares lasso regression and a brute force combinatorial search for the most predictive input features in the data we demonstrate that the new bayesian method offers a superior mixture of characteristics in terms of regularization against overfitting computational efficiency and ease of use demonstrating its potential as a drop in replacement for other linear regression techniques as neuroscientific results our analyses demonstrate that emg data can be well predicted from m neurons further opening the path for possible real time interfaces between brains and machines
we show that queyrannes algorithm for minimizing symmetric submodular functions can be used for clustering with a variety of different objective functions two specific criteria that we consider in this paper are the single linkage and the minimum description length criteria the first criterion tries to maximize the minimum distance between elements of different clusters and is inherently discriminative it is known that optimal clusterings into k clusters for any given k in polynomial time for this criterion can be computed the second criterion seeks to minimize the description length of the clusters given a probabilistic generative model we show that the optimal partitioning into clusters and approximate partitioning guaranteed to be within a factor of of the the optimal for more clusters can be computed to the best of our knowledge this is the first time that a tractable algorithm for finding the optimal clustering with respect to the mdl criterion for clusters has been given besides the optimality result for the mdl criterion the chief contribution of this paper is to show that the same algorithm can be used to optimize a broad class of criteria and hence can be used for many application specific criterion for which efficient algorithm are not known
we consider the scaling of the number of examples necessary to achieve good performance in distributed cooperative multi agent reinforcement learning as a function of the the number of agents n we prove a worstcase lower bound showing that algorithms that rely solely on a global reward signal to learn policies confront a fundamental limit they require a number of real world examples that scales roughly linearly in the number of agents for settings of interest with a very large number of agents this is impractical we demonstrate however that there is a class of algorithms that by taking advantage of local reward signals in large distributed markov decision processes are able to ensure good performance with a number of samples that scales as olog n this makes them applicable even in settings with a very large number of agents n
humans are extremely adept at learning new skills by imitating the actions of others a progression of imitative abilities has been observed in children ranging from imitation of simple body movements to goalbased imitation based on inferring intent in this paper we show that the problem of goal based imitation can be formulated as one of inferring goals and selecting actions using a learned probabilistic graphical model of the environment we first describe algorithms for planning actions to achieve a goal state using probabilistic inference we then describe how planning can be used to bootstrap the learning of goal dependent policies by utilizing feedback from the environment the resulting graphical model is then shown to be powerful enough to allow goal based imitation using a simple maze navigation task we illustrate how an agent can infer the goals of an observed teacher and imitate the teacher even when the goals are uncertain and the demonstration is incomplete
in this paper we propose a new receiver for digital communications we focus on the application of gaussian processes gps to the multiuser detection mud in code division multiple access cdma systems to solve the near far problem hence we aim to reduce the interference from other users sharing the same frequency band while usual approaches minimize the mean square error mmse to linearly retrieve the user of interest we exploit the same criteria but in the design of a nonlinear mud since the optimal solution is known to be nonlinear the performance of this novel method clearly improves that of the mmse detectors furthermore the gp based mud achieves excellent interference suppression even for short training sequences we also include some experiments to illustrate that other nonlinear detectors such as those based on support vector machines svms exhibit a worse performance

humans make optimal perceptual decisions in noisy and ambiguous conditions computations underlying such optimal behavior have been shown to rely on probabilistic inference according to generative models whose structure is usually taken to be known a priori we argue that bayesian model selection is ideal for inferring similar and even more complex model structures from experience we find in experiments that humans learn subtle statistical properties of visual scenes in a completely unsupervised manner we show that these findings are well captured by bayesian model learning within a class of models that seek to explain observed variables by independent hidden causes
calculations that quantify the dependencies between variables are vital to many operations with graphical models eg active learning and sensitivity analysis previously pairwise information gain calculation has involved a cost quadratic in network size in this work we show how to perform a similar computation with cost linear in network size the loss function that allows this is of a form amenable to computation by dynamic programming the message passing algorithm that results is described and empirical results demonstrate large speedups without decrease in accuracy in the cost sensitive domains examined superior accuracy is achieved
while classical kernel based learning algorithms are based on a single kernel in practice it is often desirable to use multiple kernels lankriet et al considered conic combinations of kernel matrices for classification leading to a convex quadratically constraint quadratic program we show that it can be rewritten as a semi infinite linear program that can be efficiently solved by recycling the standard svm implementations moreover we generalize the formulation and our method to a larger class of problems including regression and one class classification experimental results show that the proposed algorithm helps for automatic model selection improving the interpretability of the learning result and works for hundred thousands of examples or hundreds of kernels to be combined
this paper presents a novel technique for analyzing electromagnetic imaging data obtained using the stimulus evoked experimental paradigm the technique is based on a probabilistic graphical model which describes the data in terms of underlying evoked and interference sources and explicitly models the stimulus evoked paradigm a variational bayesian em algorithm infers the model from data suppresses interference sources and reconstructs the activity of separated individual brain sources the new algorithm outperforms existing techniques on two real datasets as well as on simulated data
most nervous systems encode information about stimuli in the responding activity of large neuronal networks this activity often manifests itself as dynamically coordinated sequences of action potentials since multiple electrode recordings are now a standard tool in neuroscience research it is important to have a measure of such network wide behavioral coordination and information sharing applicable to multiple neural spike train data we propose a new statistic informational coherence which measures how much better one unit can be predicted by knowing the dynamical state of another we argue informational coherence is a measure of association and shared information which is superior to traditional pairwise measures of synchronization and correlation to find the dynamical states we use a recently introduced algorithm which reconstructs effective state spaces from stochastic time series we then extend the pairwise measure to a multivariate analysis of the network by estimating the network multi information we illustrate our method by testing it on a detailed model of the transition from gamma to beta rhythms much of the most important information in neural systems is shared over multiple neurons or cortical areas in such forms as population codes and distributed representations on behavioral time scales neural information is stored in temporal patterns of activity as opposed to static markers therefore as information is shared between neurons or brain regions it is physically instantiated as coordination between entire sequences of neural spikes furthermore neural systems and regions of the brain often require coordinated neural activity to perform important functions acting in concert requires multiple neurons or cortical areas to share information thus if we want to measure the dynamic network wide behavior of neurons and test hypotheses about them we need reliable practical methods to detect and quantify behavioral coordination and the associated information sharing across multiple neural units these would be especially useful in testing ideas about how particular forms of coordination relate to distributed coding eg that of current techniques to analyze relations among spike trains handle only pairs of neurons so we further need a method which is extendible to analyze the coordination in the network system or region as a whole here we propose a new measure of behavioral coordination and information sharing informational coherence based on the notion of dynamical state section argues that coordinated behavior in neural systems is often not captured by exis fiing measures of synchronization or correlation and that something sensitive to nonlinear stochastic predictive relationships is needed section defines informational coherence as the normalized mutual information between the dynamical states of two systems and explains how looking at the states rather than just observables fulfills the needs laid out in section since we rarely know the right states a prori section briefly describes how we reconstruct effective state spaces from data section gives some details about how we calculate the informational coherence and approximate the global information stored in the network section applies our method to a model system a biophysically detailed conductance based model comparing our results to those of more familiar second order statistics in the interest of space we omit proofs and a full discussion of the existing literature giving only minimal references here proofs and references will appear in a longer paper now in preparation synchrony or coherence most hypotheses which involve the idea that information sharing is reflected in coordinated activity across neural units invoke a very specific notion of coordinated activity namely strict synchrony the units should be doing exactly the same thing eg spiking at exactly the same time investigators then measure coordination by measuring how close the units come to being strictly synchronized eg variance in spike times from an informational point of view there is no reason to favor strict synchrony over other kinds of coordination one neuron consistently spiking ms after another is just as informative a relationship as two simultaneously spiking but such stable phase relations are missed by strict synchrony approaches indeed whatever the exact nature of the neural code it uses temporally extended patterns of activity and so information sharing should be reflected in coordination of those patterns rather than just the instantaneous activity there are three common ways of going beyond strict synchrony cross correlation and related second order statistics mutual information and topological generalized synchrony the cross correlation function the normalized covariance function this includes for present purposes the joint peristimulus time histogram is one of the most widespread measures of synchronization it can be efficiently calculated from observable series it handles statistical as well as deterministic relationships between processes by incorporating variable lags it reduces the problem of phase locking fourier transformation of the covariance function xy h yields the cross spectrum fxy which in turn gives the spectral coherence cxy fxy fx fy a normalized correlation between the fourier components of x and y integrated over frequencies the spectral coherence measures essentially the degree of linear cross predictability of the two series applies spectral coherence to coordinated neural activity however such second order statistics only handle linear relationships since neural processes are known to be strongly nonlinear there is little reason to think these statistics adequately measure coordination and synchrony in neural systems mutual information is attractive because it handles both nonlinear and stochastic relationships and has a very natural and appealing interpretation unfortunately it often seems to fail in practice being disappointingly small even between signals which are known to be tightly coupled the major reason is that the neural codes use distinct patterns of activity over time rather than many different instantaneous actions and the usual approach misses these extended patterns consider two neurons one of which drives the other to spike ms after it does the driving neuron spiking once every ms these are very tightly coordinated but whether the first neuron spiked at time t conveys little information about what the second neuron is doing at t its not spiking but its not spiking most of the time anyway mutual information calculated from the direct observations conflates the fino spike of the second neuron preparing to fire with its just sitting around no spike here mutual information could find the coordination if we used a ms lag but that wont work in general take two rate coding neurons with base line firing rates of hz and suppose that a stimulus excites one to hz and suppresses the other to hz the spiking rates thus share a lot of information but whether the one neuron spiked at t is uninformative about what the other neuron did then and lagging wont help generalized synchrony is based on the idea of establishing relationships between the states of the various units state here is taken in the sense of physics dynamics and control theory the state at time t is a variable which fixes the distribution of observables at all times t rendering the past of the system irrelevant knowing the state allows us to predict as well as possible how the system will evolve and how it will respond to external forces two coupled systems are said to exhibit generalized synchrony if the state of one system is given by a mapping from the state of the other applications to data employ statespace reconstruction if the state x x evolves according to smooth d dimensional deterministic dynamics and we observe a generic function y f x then the space y of time delay vectors yt yt yt k is diffeomorphic to x if k d for generic choices of lag the various versions of generalized synchrony differ on how precisely to quantify the mappings between reconstructed state spaces but they all appear to be empirically equivalent to one another and to notions of phase synchronization based on hilbert transforms thus all of these measures accommodate nonlinear relationships and are potentially very flexible unfortunately there is essentially no reason to believe that neural systems have deterministic dynamics at experimentally accessible levels of detail much less that there are deterministic relationships among such states for different units what we want then but none of these alternatives provides is a quantity which measures predictive relationships among states but allows those relationships to be nonlinear and stochastic the next section introduces just such a measure which we call informational coherence states and informational coherence there are alternatives to calculating the surface mutual information between the sequences of observations themselves which as described fails to capture coordination if we know that the units are phase oscillators or rate coders we can estimate their instantaneous phase or rate and by calculating the mutual information between those variables see how coordinated the units patterns of activity are however phases and rates do not exhaust the repertoire of neural patterns and a more general common scheme is desirable the most general notion of pattern of activity is simply that of the dynamical state of the system in the sense mentioned above we now formalize this assuming the usual notation for shannon information the information content of a state variable x is hx and the mutual information between x and y is ix y as is well known ix y min hx hy we use this to normalize the mutual state information to a scale and this is the informational coherence ic x y ix y with min hx hy can be interpreted as follows ix y is the kullback leibler divergence between the joint distribution of x and y and the product of their marginal distributions indicating the error involved in ignoring the dependence between x and y the mutual information between predictive dynamical states thus gauges the error involved in assuming the two systems are independent ie how much predictions could improve by taking into account the dependence hence it measures the amount of dynamically relevant information shared fibetween the two systems simply normalizes this value and indicates the degree to which two systems have coordinated patterns of behavior cf although this only uses directly observable quantities reconstruction and estimation of effective state spaces as mentioned the state space of a deterministic dynamical system can be reconstructed from a sequence of observations this is the main tool of experimental nonlinear dynamics but the assumption of determinism is crucial and false for almost any interesting neural system while classical state space reconstruction wont work on stochastic processes such processes do have state space representations and in the special case of discretevalued discrete time series there are ways to reconstruct the state space here we use the cssr algorithm introduced in code available at httpbactraorgcssr this produces causal state models which are stochastic automata capable of statistically optimal nonlinear prediction the state of the machine is a minimal sufficient statistic for the future of the observable process the basic idea is to form a set of states which should be markovian sufficient statistics for the next observable and have deterministic transitions in the automata theory sense the algorithm begins with a minimal one state iid model and checks whether these properties hold by means of hypothesis tests if they fail the model is modified generally but not always by adding more states and the new model is checked again each state of the model corresponds to a distinct distribution over future events ie to a statistical pattern of behavior under mild conditions which do not involve prior knowledge of the state space cssr converges in probability to the unique causal state model of the data generating process in practice cssr is quite fast linear in the data size and generalizes at least as well as training hidden markov models with the em algorithm and using cross validation for selection the standard heuristic one advantage of the causal state approach which it shares with classical state space reconstruction is that state estimation is greatly simplified in the general case of nonlinear state estimation it is necessary to know not just the form of the stochastic dynamics in the state space and the observation function but also their precise parametric values and the distribution of observation and driving noises estimating the state from the observable time series then becomes a computationally intensive application of bayess rule due to the way causal states are built as statistics of the data with probability there is a finite time t at which the causal state at time t is certain this is not just with some degree of belief or confidence because of the way the states are constructed it is impossible for the process to be in any other state at that time once the causal state has been established it can be updated recursively ie the causal state at time t is an explicit function of the causal state at time t and the observation at t the causal state model can be automatically converted therefore into a finite state transducer which reads in an observation time series and outputs the corresponding series of states our implementation of cssr filters its training data automatically the result is a new time series of states from which all non predictive components have been filtered out estimating the coherence our algorithm for estimating the matrix of informational coherences is as follows for each unit we reconstruct the causal state model and filter the observable time series to produce a series of causal states then for each pair of neurons we construct a joint histogram of causal state models have the same expressive power as observable operator models or predictive state representations and greater power than variable length markov models fia b figure rastergrams of neuronal spike times in the network excitatory pyramidal neurons numbers to are shown in green inhibitory interneurons numbers to in red during the first seconds a the current connections among the pyramidal cells are suppressed and a gamma rhythm emerges left at t s those connections become active leading to a beta rhythm b right the state distribution estimate the mutual information between the states and normalize by the single unit state informations this gives a symmetric matrix of values even if two systems are independent their estimated ic will on average be positive because while they should have zero mutual information the empirical estimate of mutual information is non negative thus the significance of ic values must be assessed against the null hypothesis of system independence the easiest way to do so is to take the reconstructed state models for the two systems and run them forward independently of one another to generate a large number of simulated state sequences from these calculate values of the ic this procedure will approximate the sampling distribution of the ic under a null model which preserves the dynamics of each system but not their interaction we can then find p values as usual we omit them here to save space approximating the network multi information there is broad agreement that analyses of networks should not just be an analysis of pairs of neurons averaged over pairs ideally an analysis of information sharing in a network would look at the over all structure of statistical dependence between the various units reflected in the complete joint probability distribution p of the states this would then allow us for instance to calculate the n fold multi information ix x xn dp q the kullback leibler divergence between the joint distribution p and the product of marginal distributions q analogous to the pairwise mutual information calculated over the predictive states the multi information would give the total amount of shared dynamical information in the system just as we normalized the mutual information ix x by its maximum possible value min hx hx we normalize the multiinformation by its maximum which is the smallest sum of n marginal entropies ix x xn min k ik hxn unfortunately p is a distribution over a very high dimensional space and so hard to estimate well without strong parametric constraints we thus consider approximations the lowest order approximation treats all the units as independent this is the distribution q one step up are tree distributions where the global distribution is a function of the joint distributions of pairs of units not every pair of units needs to enter into such a distribution fithough every unit must be part of some pair graphically a tree distribution corresponds to a spanning tree with edges linking units whose interactions enter into the global probability and conversely spanning trees determine tree distributions writing et for the set of pairs i j and abbreviating x x x x xn xn by x x one has t x x ijet t xi xi xj xj t xi xi t xi xi t xj xj i n where the marginal distributions t xi and the pair distributions t xi xj are estimated by the empirical marginal and pair distributions we must now pick edges et so that t best approximates the true global distribution p a natural approach is to minimize dp t the divergence between p and its tree approximation chow and liu showed that the maximum weight spanning tree gives the divergence minimizing distribution taking an edges weight to be the mutual information between the variables it links there are three advantages to using the chow liu approximation estimating t from empirical probabilities gives a consistent maximum likelihood estimator of the ideal chowliu tree with reasonable rates of convergence so t can be reliably known even if p cannot there are efficient algorithms for constructing maximum weight spanning trees such as prims algorithm sec which runs in time on n log n thus the approximation is computationally tractable the kl divergence of the chow liu distribution from q gives a lower bound on the network multi information that bound is just the sum of the mutual informations along the edges in the tree ix x xn dt q ijet ixi xj even if we knew p exactly eq would be useful as an alternative to calculating dp q directly evaluating log p xqx for all the exponentially many configurations x it is natural to seek higher order approximations to p eg using three way interactions not decomposable into pairwise interactions but it is hard to do so effectively because finding the optimal approximation to p when such interactions are allowed is np and analytical formulas like eq generally do not exist we therefore confine ourselves to the chow liu approximation here example a model of gamma and beta rhythms we use simulated data as a test case instead of empirical multiple electrode recordings which allows us to try the method on a system of over neurons and compare the measure against expected results the model taken from was originally designed to study episodes of gamma hz and beta hz oscillations in the mammalian nervous system which often occur successively with a spontaneous transition between them more concretely the rhythms studied were those displayed by in vitro hippocampal ca slice preparations and by in vivo neocortical eegs the model contains two neuron populations excitatory ampa pyramidal neurons and inhibitory gabaa interneurons defined by conductance based hodgkin huxley style equations simulations were carried out in a network of pyramidal cells and interneurons each cell was modeled as a one compartment neuron with all to all coupling endowed with the basic sodium and potassium spiking currents an external applied current and some gaussian input noise the first seconds of the simulation correspond to the gamma rhythm in which only a group of neurons is made to spike via a linearly increasing applied current the beta rhythm fia b c d figure heat maps of coordination for the network as measured by zero lag cross correlation top row and informational coherence bottom contrasting the gamma rhythm left column with the beta right colors run from red no coordination through yellow to pale cream maximum subsequent seconds is obtained by activating pyramidal pyramidal recurrent connections potentiated by hebbian preprocessing as a result of synchrony during the gamma rhythm and a slow outward after hyper polarization ahp current the m current suppressed during gamma due to the metabotropic activation used in the generation of the rhythm during the beta rhythm pyramidal cells silent during gamma rhythm fire on a subset of interneurons cycles fig fig compares zero lag cross correlation a second order method of quantifying coordination with the informational coherence calculated from the reconstructed states in this simulation we could have calculated the actual states of the model neurons directly rather than reconstructing them but for purposes of testing our method we did not crosscorrelation finds some of the relationships visible in fig but is confused by for instance the phase shifts between pyramidal cells surface mutual information not shown gives similar results informational coherence however has no trouble recognizing the two populations as effectively coordinated blocks the presence of dynamical noise problematic for ordinary state reconstruction is not an issue the average ic is or if the inactive low numbered neurons are excluded the tree estimate of the global informational multi information is bits with a global coherence of the right half of fifig repeats this analysis for the beta rhythm in this stage the average ic is and the tree estimate of the global multi information is bits though the estimated global coherence falls very slightly to this is because low numbered neurons which were quiescent before are now active contributing to the global information but the over all pattern is somewhat weaker and more noisy as can be seen from fig b so as expected the total information content is higher but the overall coordination across the network is lower conclusion informational coherence provides a measure of neural information sharing and coordinated activity which accommodates nonlinear stochastic relationships between extended patterns of spiking it is robust to dynamical noise and leads to a genuinely multivariate measure of global coordination across networks or regions applied to data from multi electrode recordings it should be a valuable tool in evaluating hypotheses about distributed neural representation and function acknowledgments thanks to r haslinger e ionides and s page and for support to the santa fe institute under grants from intel the nsf and the macarthur foundation and darpa agreement f the clare booth luce foundation klk and the james s mcdonnell foundation crs references l f abbott and t j sejnowski eds neural codes and distributed representations mit press e n brown r e kass and p p mitra nature neuroscience d h ballard z zhang and r p n rao in r p n rao b a olshausen and m s lewicki eds probabilistic models of the brain pp mit press d r brillinger and a e p villa in d r brillinger l t fernholz and s morgenthaler eds the practice of data analysis pp princeton up r quian quiroga et al physical review e r f streater statistical dynamics imperial college press london m l littman r s sutton and s singh in t g dietterich s becker and z ghahramani eds advances in neural information processing systems pp mit press h kantz and t schreiber nonlinear time series analysis cambridge up t m cover and j a thomas elements of information theory wiley m palus et al physical review e f b knight annals of probability c r shalizi and k l shalizi in m chickering and j halpern eds uncertainty in artificial intelligence proceedings of the twentieth conference pp auai press c r shalizi and j p crutchfield journal of statistical physics h jaeger neural computation d ron y singer and n tishby machine learning p b hlmann and a j wyner annals of statistics u n u ahmed linear and nonlinear filtering for scientists and engineers world scientific d r upper phd thesis university of california berkeley e schneidman s still m j berry and w bialek physical review letters c k chow and c n liu ieee transactions on information theory it t h cormen et al
we consider the task of depth estimation from a single monocular image we take a supervised learning approach to this problem in which we begin by collecting a training set of monocular images of unstructured outdoor environments which include forests trees buildings etc and their corresponding ground truth depthmaps then we apply supervised learning to predict the depthmap as a function of the image depth estimation is a challenging problem since local features alone are insufficient to estimate depth at a point and one needs to consider the global context of the image our model uses a discriminatively trained markov random field mrf that incorporates multiscale localand global image features and models both depths at individual points as well as the relation between depths at different points we show that even on unstructured scenes our algorithm is frequently able to recover fairly accurate depthmaps

probabilistic temporal planning attempts to find good policies for acting in domains with concurrent durative tasks multiple uncertain outcomes and limited resources these domains are typically modelled as markov decision problems and solved using dynamic programming methods this paper demonstrates the application of reinforcement learning in the form of a policy gradient method to these domains our emphasis is large domains that are infeasible for dynamic programming our approach is to construct simple policies or agents for each planning task the result is a general probabilistic temporal planner named the factored policy gradient planner fpg planner which can handle hundreds of tasks optimising for probability of success duration and resource use
we present a family of approximation techniques for probabilistic graphical models based on the use of graphical preconditioners developed in the scientific computing literature our framework yields rigorous upper and lower bounds on event probabilities and the log partition function of undirected graphical models using non iterative procedures that have low time complexity as in mean field approaches the approximations are built upon tractable subgraphs however we recast the problem of optimizing the tractable distribution parameters and approximate inference in terms of the well studied linear systems problem of obtaining a good matrix preconditioner experiments are presented that compare the new approximation schemes to variational methods
diffusion tensor magnetic resonance imaging dt mri is a non invasive method for brain neuronal fibers delineation here we show a modification for dt mri that allows delineation of neuronal fibers which are infiltrated by edema we use the muliple tensor variational mtv framework which replaces the diffusion model of dt mri with a multiple component model and fits it to the signal attenuation with a variational regularization mechanism in order to reduce free water contamination we estimate the free water compartment volume fraction in each voxel remove it and then calculate the anisotropy of the remaining compartment the variational framework was applied on data collected with conventional clinical parameters containing only six diffusion directions by using the variational framework we were able to overcome the highly ill posed fitting the results show that we were able to find fibers that were not found by dt mri
this paper describes a highly successful application of mrfs to the problem of generating high resolution range images a new generation of range sensors combines the capture of low resolution range images with the acquisition of registered high resolution camera images the mrf in this paper exploits the fact that discontinuities in range and coloring tend to co align this enables it to generate high resolution low noise range images by integrating regular camera images into the range data we show that by using such an mrf we can substantially improve over existing range imaging technology
we describe a novel method for learning templates for recognition and localization of objects drawn from categories a generative model represents the configuration of multiple object parts with respect to an object coordinate system these parts in turn generate image features the complexity of the model in the number of features is low meaning our model is much more efficient to train than comparative methods moreover a variational approximation is introduced that allows learning to be orders of magnitude faster than previous approaches while incorporating many more features this results in both accuracy and localization improvements our model has been carefully tested on standard datasets we compare with a number of recent template models in particular we demonstrate state of the art results for detection and localization
inspired by googletm sets we consider the problem of retrieving items from a concept or cluster given a query consisting of a few items from that cluster we formulate this as a bayesian inference problem and describe a very simple algorithm for solving it our algorithm uses a modelbased concept of a cluster and ranks items using a score which evaluates the marginal probability that each item belongs to a cluster containing the query items for exponential family models with conjugate priors this marginal probability is a simple function of sufficient statistics we focus on sparse binary data and show that our score can be evaluated exactly using a single sparse matrix multiplication making it possible to apply our algorithm to very large datasets we evaluate our algorithm on three datasets retrieving movies from eachmovie finding completions of author sets from the nips dataset and finding completions of sets of words appearing in the grolier encyclopedia we compare to googletm sets and show that bayesian sets gives very reasonable set completions
the category of visual stimuli has been reliably decoded from patterns of neural activity in extrastriate visual cortex it has yet to be seen whether object identity can be inferred from this activity we present fmri data measuring responses in human extrastriate cortex to a set of distinct object images we use a simple winner take all classifier using half the data from each recording session as a training set to evaluate encoding of object identity across fmri voxels since this approach is sensitive to the inclusion of noisy voxels we describe two methods for identifying subsets of voxels in the data which optimally distinguish object identity one method characterizes the reliability of each voxel within subsets of the data while another estimates the mutual information of each voxel with the stimulus set we find that both metrics can identify subsets of the data which reliably encode object identity even when noisy measurements are artificially added to the data the mutual information metric is less efficient at this task likely due to constraints in fmri data
we demonstrate the first fully hardware implementation of retinotopic self organization from photon transduction to neural map formation a silicon retina transduces patterned illumination into correlated spike trains that drive a population of silicon growth cones to automatically wire a topographic mapping by migrating toward sources of a diffusible guidance cue that is released by postsynaptic spikes we varied the pattern of illumination to steer growth cones projected by different retinal ganglion cell types to self organize segregated or coordinated retinotopic maps

nested sampling is a new monte carlo method by skilling intended for general bayesian computation nested sampling provides a robust alternative to annealing based methods for computing normalizing constants it can also generate estimates of other quantities such as posterior expectations the key technical requirement is an ability to draw samples uniformly from the prior subject to a constraint on the likelihood we provide a demonstration with the potts model an undirected graphical model
linear text classification algorithms work by computing an inner product between a test document vector and a parameter vector in many such algorithms including naive bayes and most tfidf variants the parameters are determined by some simple closed form function of training set statistics we call this mapping mapping from statistics to parameters the parameter function much research in text classification over the last few decades has consisted of manual efforts to identify better parameter functions in this paper we propose an algorithm for automatically learning this function from related classification problems the parameter function found by our algorithm then defines a new learning algorithm for text classification which we can apply to novel classification tasks we find that our learned classifier outperforms existing methods on a variety of multiclass text classification tasks
this paper explores two aspects of social network modeling first we generalize a successful static model of relationships into a dynamic model that accounts for friendships drifting over time second we show how to make it tractable to learn such models from data even as the number of entities n gets large the generalized model associates each entity with a point in p dimensional euclidian latent space the points can move as time progresses but large moves in latent space are improbable observed links between entities are more likely if the entities are close in latent space we show how to make such a model tractable subquadratic in the number of entities by the use of appropriate kernel functions for similarity in latent space the use of low dimensional kd trees a new efficient dynamic adaptation of multidimensional scaling for a first pass of approximate projection of entities into latent space and an efficient conjugate gradient update rule for non linear local optimization in which amortized time per entity during an update is olog n we use both synthetic and real world data on upto entities which indicate linear scaling in computation time and improved performance over four alternative approaches we also illustrate the system operating on twelve years of nips co publication data we present a detailed version of this work in
to investigate how top down td and bottom up bu information is weighted in the guidance of human search behavior we manipulated the proportions of bu and td components in a saliency based model the model is biologically plausible and implements an artificial retina and a neuronal population code the bu component is based on featurecontrast the td component is defined by a feature template match to a stored target representation we compared the models behavior at different mixtures of td and bu components to the eye movement behavior of human observers performing the identical search task we found that a purely td model provides a much closer match to human behavior than any mixture model using bu information only when biological constraints are removed eg eliminating the retina did a butd mixture model begin to approximate human behavior
many real world classification problems involve the prediction of multiple inter dependent variables forming some structural dependency recent progress in machine learning has mainly focused on supervised classification of such structured variables in this paper we investigate structured classification in a semi supervised setting we present a discriminative approach that utilizes the intrinsic geometry of input patterns revealed by unlabeled data points and we derive a maximum margin formulation of semi supervised learning for structured variables unlike transductive algorithms our formulation naturally extends to new test points
this paper proposes an algorithm to convert a t stage stochastic decision problem with a continuous state space to a sequence of supervised learning problems the optimization problem associated with the trajectory tree and random trajectory methods of kearns mansour and ng is solved using the gauss seidel method the algorithm breaks a multistage reinforcement learning problem into a sequence of single stage reinforcement learning subproblems each of which is solved via an exact reduction to a weighted classification problem that can be solved using off the self methods thus the algorithm converts a reinforcement learning problem into simpler supervised learning subproblems it is shown that the method converges in a finite number of steps to a solution that cannot be further improved by componentwise optimization the implication of the proposed algorithm is that a plethora of classification methods can be applied to find policies in the reinforcement learning problem
we present a model of edge and region grouping using a conditional random field built over a scale invariant representation of images to integrate multiple cues our model includes potentials that capture low level similarity mid level curvilinear continuity and high level object shape maximum likelihood parameters for the model are learned from human labeled groundtruth on a large collection of horse images using belief propagation using held out test data we quantify the information gained by incorporating generic mid level cues and high level shape
biological sensory systems are faced with the problem of encoding a high fidelity sensory signal with a population of noisy low fidelity neurons this problem can be expressed in information theoretic terms as coding and transmitting a multi dimensional analog signal over a set of noisy channels previously we have shown that robust overcomplete codes can be learned by minimizing the reconstruction error with a constraint on the channel capacity here we present a theoretical analysis that characterizes the optimal linear coder and decoder for oneand twodimensional data the analysis allows for an arbitrary number of coding units thus including both underand over complete representations and provides a number of important insights into optimal coding strategies in particular we show how the form of the code adapts to the number of coding units and to different data and noise conditions to achieve robustness we also report numerical solutions for robust coding of highdimensional image data and show that these codes are substantially more robust compared against other image codes such as ica and wavelets
we describe a vision based obstacle avoidance system for off road mobile robots the system is trained from end to end to map raw input images to steering angles it is trained in supervised mode to predict the steering angles provided by a human driver during training runs collected in a wide variety of terrains weather conditions lighting conditions and obstacle types the robot is a cm off road truck with two forwardpointing wireless color cameras a remote computer processes the video and controls the robot via radio the learning system is a large layer convolutional network whose input is a single leftright pair of unprocessed low resolution images the robot exhibits an excellent ability to detect obstacles and navigate around them in real time at speeds of ms
the octopus arm is a highly versatile and complex limb how the octopus controls such a hyper redundant arm not to mention eight of them is as yet unknown robotic arms based on the same mechanical principles may render present day robotic arms obsolete in this paper we tackle this control problem using an online reinforcement learning algorithm based on a bayesian approach to policy evaluation known as gaussian process temporal difference gptd learning our substitute for the real arm is a computer simulation of a dimensional model of an octopus arm even with the simplifications inherent to this model the state space we face is a high dimensional one we apply a gptdbased algorithm to this domain and demonstrate its operation on several learning tasks of varying degrees of difficulty
we propose efficient algorithms for learning ranking functions from order constraints between sets ie classes of training samples our algorithms may be used for maximizing the generalized wilcoxon mann whitney statistic that accounts for the partial ordering of the classes special cases include maximizing the area under the roc curve for binary classification and its generalization for ordinal regression experiments on public benchmarks indicate that a the proposed algorithm is at least as accurate as the current state of the art b computationally it is several orders of magnitude faster and unlike current methods it is easily able to handle even large datasets with over samples
we introduce a new model of genetic diversity which summarizes a large input dataset into an epitome a short sequence or a small set of short sequences of probability distributions capturing many overlapping subsequences from the dataset the epitome as a representation has already been used in modeling real valued signals such as images and audio the discrete sequence model we introduce in this paper targets applications in genetics from multiple alignment to recombination and mutation inference in our experiments we concentrate on modeling the diversity of hiv where the epitome emerges as a natural model for producing relatively small vaccines covering a large number of immune system targets known as epitopes our experiments show that the epitome includes more epitopes than other vaccine designs of similar length including cocktails of consensus strains phylogenetic tree centers and observed strains we also discuss epitome designs that take into account uncertainty about tcell cross reactivity and epitope presentation in our experiments we find that vaccine optimization is fairly robust to these uncertainties
this paper proposes a new approach to feature selection based on a statistical feature mining technique for sequence and tree kernels since natural language data take discrete structures convolution kernels such as sequence and tree kernels are advantageous for both the concept and accuracy of many natural language processing tasks however experiments have shown that the best results can only be achieved when limited small sub structures are dealt with by these kernels this paper discusses this issue of convolution kernels and then proposes a statistical feature selection that enable us to use larger sub structures effectively the proposed method in order to execute efficiently can be embedded into an original kernel calculation process by using sub structure mining algorithms experiments on real nlp tasks confirm the problem in the conventional method and compare the performance of a conventional method to that of the proposed method
this paper addresses the issue of numerical computation in machine learning domains based on similarity metrics such as kernel methods spectral techniques and gaussian processes it presents a general solution strategy based on krylov subspace iteration and fast n body learning methods the experiments show significant gains in computation and storage on datasets arising in image segmentation object detection and dimensionality reduction the paper also presents theoretical bounds on the stability of these methods
although variants of value iteration have been proposed for finding nash or correlated equilibria in general sum markov games these variants have not been shown to be effective in general in this paper we demonstrate by construction that existing variants of value iteration cannot find stationary equilibrium policies in arbitrary general sum markov games instead we propose an alternative interpretation of the output of value iteration based on a new non stationary equilibrium concept that we call cyclic equilibria we prove that value iteration identifies cyclic equilibria in a class of games in which it fails to find stationary equilibria we also demonstrate empirically that value iteration finds cyclic equilibria in nearly all examples drawn from a random distribution of markov games
learning patterns of human behavior from sensor data is extremely important for high level activity inference we show how to extract and label a persons activities and significant places from traces of gps data in contrast to existing techniques our approach simultaneously detects and classifies the significant locations of a person and takes the highlevel context into account our system uses relational markov networks to represent the hierarchical activity model that encodes the complex relations among gps readings activities and significant places we apply fft based message passing to perform efficient summation over large numbers of nodes in the networks we present experiments that show significant improvements over existing techniques
topic models such as latent dirichlet allocation lda can be useful tools for the statistical analysis of document collections and other discrete data the lda model assumes that the words of each document arise from a mixture of topics each of which is a distribution over the vocabulary a limitation of lda is the inability to model topic correlation even though for example a document about genetics is more likely to also be about disease than x ray astronomy this limitation stems from the use of the dirichlet distribution to model the variability among the topic proportions in this paper we develop the correlated topic model ctm where the topic proportions exhibit correlation via the logistic normal distribution we derive a mean field variational inference algorithm for approximate posterior inference in this model which is complicated by the fact that the logistic normal is not conjugate to the multinomial the ctm gives a better fit than lda on a collection of ocred articles from the journal science furthermore the ctm provides a natural way of visualizing and exploring this and other unstructured data sets

motivated by the problem of learning to detect and recognize objects with minimal supervision we develop a hierarchical probabilistic model for the spatial structure of visual scenes in contrast with most existing models our approach explicitly captures uncertainty in the number of object instances depicted in a given image our scene model is based on the transformed dirichlet process tdp a novel extension of the hierarchical dp in which a set of stochastically transformed mixture components are shared between multiple groups of data for visual scenes mixture components describe the spatial structure of visual features in an object centered coordinate frame while transformations model the object positions in a particular image learning and inference in the tdp which has many potential applications beyond computer vision is based on an empirically effective gibbs sampler applied to a dataset of partially labeled street scenes we show that the tdps inclusion of spatial structure improves detection performance flexibly exploiting partially labeled training images
we present an infinite mixture model in which each component comprises a multivariate gaussian distribution over an input space and a gaussian process model over an output space our model is neatly able to deal with non stationary covariance functions discontinuities multimodality and overlapping output signals the work is similar to that by rasmussen and ghahramani however we use a full generative model over input and output space rather than just a conditional model this allows us to deal with incomplete data to perform inference over inverse functional mappings as well as for regression and also leads to a more powerful and consistent bayesian specification of the effective gating network for the different experts
clustering is a fundamental problem in machine learning and has been approached in many ways two general and quite different approaches include iteratively fitting a mixture model eg using em and linking together pairs of training cases that have high affinity eg using spectral methods pair wise clustering algorithms need not compute sufficient statistics and avoid poor solutions by directly placing similar examples in the same cluster however many applications require that each cluster of data be accurately described by a prototype or model so affinity based clustering and its benefits cannot be directly realized we describe a technique called affinity propagation which combines the advantages of both approaches the method learns a mixture model of the data by recursively propagating affinity messages we demonstrate affinity propagation on the problems of clustering image patches for image segmentation and learning mixtures of gene expression models from microarray data we find that affinity propagation obtains better solutions than mixtures of gaussians the k medoids algorithm spectral clustering and hierarchical clustering and is both able to find a pre specified number of clusters and is able to automatically determine the number of clusters interestingly affinity propagation can be viewed as belief propagation in a graphical model that accounts for pairwise training case likelihood functions and the identification of cluster centers
this paper presents a new sampling algorithm for approximating functions of variables representable as undirected graphical models of arbitrary connectivity with pairwise potentials as well as for estimating the notoriously difficult partition function of the graph the algorithm fits into the framework of sequential monte carlo methods rather than the more widely used mcmc and relies on constructing a sequence of intermediate distributions which get closer to the desired one while the idea of using tempered proposals is known we construct a novel sequence of target distributions where rather than dropping a global temperature parameter we sequentially couple individual pairs of variables that are initially sampled exactly from a spanning tree of the variables we present experimental results on inference and estimation of the partition function for sparse and densely connected graphs
we consider criteria for variational representations of non gaussian latent variables and derive variational em algorithms in general form we establish a general equivalence among convex bounding methods evidence based methods and ensemble learningvariational bayes methods which has previously been demonstrated only for particular cases
a foundational problem in semi supervised learning is the construction of a graph underlying the data we propose to use a method which optimally combines a number of differently constructed graphs for each of these graphs we associate a basic graph kernel we then compute an optimal combined kernel this kernel solves an extended regularization problem which requires a joint minimization over both the data and the set of graph kernels we present encouraging results on different ocr tasks where the optimal combined kernel is computed from graphs constructed with a variety of distances functions and the k in nearest neighbors
we present a probabilistic generative model of entity relationships and their attributes that simultaneously discovers groups among the entities and topics among the corresponding textual attributes block models of relationship data have been studied in social network analysis for some time here we simultaneously cluster in several modalities at once incorporating the attributes here words associated with certain relationships significantly joint inference allows the discovery of topics to be guided by the emerging groups and vice versa we present experimental results on two large data sets sixteen years of bills put before the us senate comprising their corresponding text and voting records and thirteen years of similar data from the united nations we show that in comparison with traditional separate latent variable models for words or blockstructures for votes the group topic models joint inference discovers more cohesive groups and improved topics
spiking activity from neurophysiological experiments often exhibits dynamics beyond that driven by external stimulation presumably reflecting the extensive recurrence of neural circuitry characterizing these dynamics may reveal important features of neural computation particularly during internally driven cognitive operations for example the activity of premotor cortex pmd neurons during an instructed delay period separating movement target specification and a movementinitiation cue is believed to be involved in motor planning we show that the dynamics underlying this activity can be captured by a lowdimensional non linear dynamical systems model with underlying recurrent structure and stochastic point process output we present and validate latent variable methods that simultaneously estimate the system parameters and the trial by trial dynamical trajectories these methods are applied to characterize the dynamics in pmd data recorded from a chronically implanted electrode array while monkeys perform delayed reach tasks
we present a bayesian framework for explaining how people reason about and predict the actions of an intentional agent based on observing its behavior action understanding is cast as a problem of inverting a probabilistic generative model which assumes that agents tend to act rationally in order to achieve their goals given the constraints of their environment working in a simple sprite world domain we show how this model can be used to infer the goal of an agent and predict how the agent will act in novel situations or when environmental constraints change the model provides a qualitative account of several kinds of inferences that preverbal infants have been shown to perform and also fits quantitative predictions that adult observers make in a new experiment
we consider the problem of modeling a helicopters dynamics based on state action trajectories collected from it the contribution of this paper is two fold first we consider the linear models such as learned by cifer the industry standard in helicopter identification and show that the linear parameterization makes certain properties of dynamical systems such as inertia fundamentally difficult to capture we propose an alternative acceleration based parameterization that does not suffer from this deficiency and that can be learned as efficiently from data second a markov decision process model of a helicopters dynamics would explicitly model only the one step transitions but we are often interested in a models predictive performance over longer timescales in this paper we present an efficient algorithm for approximately minimizing the prediction error over long time scales we present empirical results on two different helicopters although this work was motivated by the problem of modeling helicopters the ideas presented here are general and can be applied to modeling large classes of vehicular dynamics
reinforcement learning by direct policy gradient estimation is attractive in theory but in practice leads to notoriously ill behaved optimization problems we improve its robustness and speed of convergence with stochastic meta descent a gain vector adaptation method that employs fast hessian vector products in our experiments the resulting algorithms outperform previously employed online stochastic offline conjugate and natural policy gradient methods
the problem of computing a resample estimate for the reconstruction error in pca is reformulated as an inference problem with the help of the replica method using the expectation consistent ec approximation the intractable inference problem can be solved efficiently using only two variational parameters a perturbative correction to the result is computed and an alternative simplified derivation is also presented
we present micropower mixed signal vlsi hardware for real time blind separation and localization of acoustic sources gradient flow representation of the traveling wave signals acquired over a miniature cm diameter array of four microphones yields linearly mixed instantaneous observations of the time differentiated sources separated and localized by independent component analysis ica the gradient flow and ica processors each measure mm mm in m cmos and consume w and w power respectively from a v supply at kss sampling rate experiments demonstrate perceptually clear db separation and precise localization of two speech sources presented through speakers positioned at m from the array on a conference room table analysis of the multipath residuals shows that they are spectrally diffuse and void of the direct path
we present a simple and scalable algorithm for large margin estimation of structured models including an important class of markov networks and combinatorial models we formulate the estimation problem as a convex concave saddle point problem and apply the extragradient method yielding an algorithm with linear convergence using simple gradient and projection calculations the projection step can be solved using combinatorial algorithms for min cost quadratic flow this makes the approach an efficient alternative to formulations based on reductions to a quadratic program qp we present experiments on two very different structured prediction tasks d image segmentation and word alignment illustrating the favorable scaling properties of our algorithm
fusing multiple information sources can yield significant benefits to successfully accomplish learning tasks many studies have focussed on fusing information in supervised learning contexts we present an approach to utilize multiple information sources in the form of similarity data for unsupervised learning based on similarity information the clustering task is phrased as a non negative matrix factorization problem of a mixture of similarity measurements the tradeoff between the informativeness of data sources and the sparseness of their mixture is controlled by an entropy based weighting mechanism for the purpose of model selection a stability based approach is employed to ensure the selection of the most self consistent hypothesis the experiments demonstrate the performance of the method on toy as well as real world data sets
we use the k core decomposition to develop algorithms for the analysis of large scale complex networks this decomposition based on a recursive pruning of the least connected vertices allows to disentangle the hierarchical structure of networks by progressively focusing on their central cores by using this strategy we develop a general visualization algorithm that can be used to compare the structural properties of various networks and highlight their hierarchical structure the low computational complexity of the algorithm on e where n is the size of the network and e is the number of edges makes it suitable for the visualization of very large sparse networks we show how the proposed visualization tool allows to find specific structural fingerprints of networks
